{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from typing import List, Tuple, Union, Optional, Callable, Any, Dict\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 25 key-value pairs and 995 tensors from /Users/tommasofurlanello/Documents/Dev/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = ehartford_dolphin-2.5-mixtral-8x7b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:  898 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 46.22 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = ehartford_dolphin-2.5-mixtral-8x7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.38 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size = 47325.41 MiB, (47325.48 / 98304.00)\n",
      "llm_load_tensors: system memory used  = 47325.04 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: ggml.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/tommasofurlanello/.pyenv/versions/3.12.1/envs/babydragon/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9 (1009)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 103079.22 MB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  4096.00 MiB, (51427.11 / 98304.00)\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, (51427.12 / 98304.00)\n",
      "llama_build_graph: non-view tensors processed: 1124/1124\n",
      "llama_new_context_with_model: compute buffer total size = 2167.22 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  2164.05 MiB, (53591.16 / 98304.00)\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=\"/Users/tommasofurlanello/Documents/Dev/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q8_0.gguf\",chat_format=\"chatml\",n_ctx=0,n_gpu_layers = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2357.07 ms\n",
      "llama_print_timings:      sample time =       5.72 ms /    80 runs   (    0.07 ms per token, 13995.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2356.95 ms /    37 tokens (   63.70 ms per token,    15.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3434.93 ms /    79 runs   (   43.48 ms per token,    23.00 tokens per second)\n",
      "llama_print_timings:       total time =    5886.74 ms\n"
     ]
    }
   ],
   "source": [
    "#the schema of the df is: \n",
    "messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes python packages. GANG GANG\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"What is polars library in python?\"\n",
    "          }\n",
    "      ]\n",
    "results = llm.create_chat_completion(\n",
    "      messages = messages,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(str(messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an assistant who perfectly describes python packages. GANG GANG'},\n",
       " {'role': 'user', 'content': 'What is polars library in python?'},\n",
       " {'role': 'assistant',\n",
       "  'content': ' Polars is a Python package that provides a fast, efficient, and easy-to-use DataFrame implementation. It is designed to handle large datasets efficiently by leveraging the power of Rust programming language under the hood. Polars offers similar functionality as pandas but with better performance and memory usage. The library supports various operations like filtering, groupby, joins, and more.'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class ChatFrame:\n",
    "      chat_schema = {\"role\":pl.Utf8,\n",
    "                   \"content\":pl.Utf8,\n",
    "                   \"time_stamps\":pl.Datetime,\n",
    "                   \"author\":pl.Utf8,\n",
    "                   \"message_id\":pl.Utf8,\n",
    "                   \"chat_id\":pl.Utf8, \n",
    "                   \"author\":pl.Utf8,\n",
    "                   \"prompt\": pl.List(pl.Struct({\"role\":pl.Utf8,\"content\":pl.Utf8})),\n",
    "                   'prompt_tokens': pl.Int64, \n",
    "                   'completion_tokens': pl.Int64, \n",
    "                   'total_tokens': pl.Int64\n",
    "}     \n",
    "      chat_ml_schema =  {\"chat_ml\":pl.Struct({\"role\":pl.Utf8,\"content\":pl.Utf8}),\n",
    "                   \"time_stamps\":pl.Datetime,\n",
    "                   \"author\":pl.Utf8,\n",
    "                   \"message_id\":pl.Utf8,\n",
    "                   \"chat_id\":pl.Utf8, \n",
    "                   'completion_tokens': pl.Int64, \n",
    "}\n",
    "      chat_nested_schema = {\"role\":pl.Utf8,\n",
    "                   \"content\":pl.Utf8,\n",
    "                   \"time_stamps\":pl.Datetime,\n",
    "                   \"author\":pl.Utf8,\n",
    "                   \"message_id\":pl.Utf8,\n",
    "                   \"chat_id\":pl.Utf8, \n",
    "                   \"prompt\": pl.List(pl.Struct({\"role\":pl.Utf8,\"content\":pl.Utf8})),\n",
    "                  \"usage\": pl.Struct({'prompt_tokens': pl.Int64, 'completion_tokens': pl.Int64, 'total_tokens': pl.Int64})\n",
    "}\n",
    "      def __init__(self,llm: Llama):\n",
    "            self.chat_id = str(uuid.uuid4())\n",
    "            # self.messages = chat_ml_messages\n",
    "            # self.time_stamps = self.validate_timestamps(time_stamps)\n",
    "            self.df = pl.DataFrame(schema=self.chat_schema)\n",
    "            self.llm = llm\n",
    "                        \n",
    "      \n",
    "      def add_from_chat_ml(self,chat_ml_messages: List[Dict[str, str]] , \n",
    "                           time_stamps: Optional[List[datetime]]=None,\n",
    "                           author: Optional[List[str]]=None,):\n",
    "            if time_stamps is None:\n",
    "                  time_stamps = [datetime.now() for _ in range(len(chat_ml_messages))]\n",
    "\n",
    "            message_id = [\"chatml-\" + str(uuid.uuid4()) for _ in range(len(chat_ml_messages))]\n",
    "            if author is None:\n",
    "                  author = [\"import\" for _ in range(len(chat_ml_messages))]\n",
    "            chat_id = [self.chat_id for _ in range(len(chat_ml_messages))]\n",
    "            \n",
    "            message_tokens_len = [len(self.llm.tokenize(bytes(message[\"role\"]+ message[\"content\"], \"utf-8\"))) for message in chat_ml_messages]\n",
    "            \n",
    "\n",
    "            nested_df = pl.DataFrame({\"chat_ml\":chat_ml_messages,\n",
    "                                      \"time_stamps\":time_stamps,\n",
    "                                      \"author\":author,\n",
    "                                      \"message_id\":message_id,\n",
    "                                      \"chat_id\":chat_id,\n",
    "                                      \"completion_tokens\":message_tokens_len,\n",
    "                                    }\n",
    "                                      ,schema=self.chat_ml_schema)\n",
    "            # createa  list of [\"chatml-\" + str(uuid.uuid4())] with the same length as the number of messages\n",
    "            \n",
    "            message_df = nested_df.unnest(\"chat_ml\")\n",
    "            self.df = pl.concat([chat.df,message_df],how=\"diagonal\",)\n",
    "            return message_df                 \n",
    "      \n",
    "      def add_message_from_results_dict(self, results_dict: dict, prompt: List[Dict[str, str]]):\n",
    "            \n",
    "            data = {\"role\":[results_dict[\"choices\"][0][\"message\"][\"role\"]],\n",
    "                        \"content\":[results_dict[\"choices\"][0][\"message\"][\"content\"]],\n",
    "                        \"author\":[results_dict[\"model\"]],\n",
    "                        \"message_id\":[results_dict[\"id\"]],\n",
    "                        \"chat_id\":self.chat_id,\n",
    "                        \"prompt\":[prompt],\n",
    "                        \"usage\":[results_dict[\"usage\"]],\n",
    "                        \"time_stamps\":[datetime.fromtimestamp(results_dict[\"created\"])],\n",
    "            \n",
    "                        }\n",
    "            results_df = pl.DataFrame(data=data,schema=self.chat_nested_schema).unnest(\"usage\")\n",
    "            self.df = pl.concat([self.df,results_df],how=\"vertical\")\n",
    "            return results_df\n",
    "      \n",
    "      def to_chat_ml(self):\n",
    "            return self.df.select(pl.struct([\"role\",\"content\"]).alias(\"chat_ml\")).to_dict(as_series=False)[\"chat_ml\"]\n",
    "      \n",
    "      def reply(self, message: str):\n",
    "            user_as_chat_ml = {\"role\":\"user\",\"content\":message}\n",
    "            self.add_from_chat_ml([user_as_chat_ml])\n",
    "            prompt = self.to_chat_ml()\n",
    "            results = self.llm.create_chat_completion(\n",
    "                  messages = prompt,\n",
    "            )\n",
    "            self.add_message_from_results_dict(results, prompt=prompt)\n",
    "            return results\n",
    "\n",
    "            \n",
    "chat = ChatFrame(llm)\n",
    "message_df = chat.add_from_chat_ml(messages)\n",
    "results_df = chat.add_message_from_results_dict(results,prompt=messages)\n",
    "chat.to_chat_ml()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>role</th><th>content</th><th>time_stamps</th><th>author</th><th>message_id</th><th>chat_id</th><th>prompt</th><th>prompt_tokens</th><th>completion_tokens</th><th>total_tokens</th></tr><tr><td>str</td><td>str</td><td>datetime[μs]</td><td>str</td><td>str</td><td>str</td><td>list[struct[2]]</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;system&quot;</td><td>&quot;You are an ass…</td><td>2024-01-09 01:26:54.497928</td><td>&quot;import&quot;</td><td>&quot;chatml-0ab38b2…</td><td>&quot;dab3e3a6-af3b-…</td><td>null</td><td>null</td><td>16</td><td>null</td></tr><tr><td>&quot;user&quot;</td><td>&quot;What is polars…</td><td>2024-01-09 01:26:54.497932</td><td>&quot;import&quot;</td><td>&quot;chatml-c2ff0f0…</td><td>&quot;dab3e3a6-af3b-…</td><td>null</td><td>null</td><td>10</td><td>null</td></tr><tr><td>&quot;assistant&quot;</td><td>&quot; Polars is a P…</td><td>2024-01-09 01:26:35</td><td>&quot;/Users/tommaso…</td><td>&quot;chatcmpl-b8ef6…</td><td>&quot;dab3e3a6-af3b-…</td><td>[{&quot;system&quot;,&quot;You are an assistant who perfectly describes python packages. GANG GANG&quot;}, {&quot;user&quot;,&quot;What is polars library in python?&quot;}]</td><td>37</td><td>79</td><td>116</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 10)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ role      ┆ content   ┆ time_stam ┆ author    ┆ … ┆ prompt    ┆ prompt_to ┆ completio ┆ total_to │\n",
       "│ ---       ┆ ---       ┆ ps        ┆ ---       ┆   ┆ ---       ┆ kens      ┆ n_tokens  ┆ kens     │\n",
       "│ str       ┆ str       ┆ ---       ┆ str       ┆   ┆ list[stru ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆ datetime[ ┆           ┆   ┆ ct[2]]    ┆ i64       ┆ i64       ┆ i64      │\n",
       "│           ┆           ┆ μs]       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ system    ┆ You are   ┆ 2024-01-0 ┆ import    ┆ … ┆ null      ┆ null      ┆ 16        ┆ null     │\n",
       "│           ┆ an        ┆ 9 01:26:5 ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ assistant ┆ 4.497928  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ who       ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ perfect…  ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ user      ┆ What is   ┆ 2024-01-0 ┆ import    ┆ … ┆ null      ┆ null      ┆ 10        ┆ null     │\n",
       "│           ┆ polars    ┆ 9 01:26:5 ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ library   ┆ 4.497932  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ in        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ python…   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ assistant ┆ Polars is ┆ 2024-01-0 ┆ /Users/to ┆ … ┆ [{\"system ┆ 37        ┆ 79        ┆ 116      │\n",
       "│           ┆ a Python  ┆ 9         ┆ mmasofurl ┆   ┆ \",\"You    ┆           ┆           ┆          │\n",
       "│           ┆ package   ┆ 01:26:35  ┆ anello/Do ┆   ┆ are an    ┆           ┆           ┆          │\n",
       "│           ┆ that…     ┆           ┆ cumen…    ┆   ┆ assistant ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ …         ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2357.07 ms\n",
      "llama_print_timings:      sample time =      19.39 ms /   240 runs   (    0.08 ms per token, 12378.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2337.53 ms /    26 tokens (   89.90 ms per token,    11.12 tokens per second)\n",
      "llama_print_timings:        eval time =   10504.38 ms /   239 runs   (   43.95 ms per token,    22.75 tokens per second)\n",
      "llama_print_timings:       total time =   13149.42 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-96e10731-9dda-4434-bf44-936934fc23d3',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1704760022,\n",
       " 'model': '/Users/tommasofurlanello/Documents/Dev/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"To concatenate multiple columns into a single column row in Polars, you can use the `concat_str()` function. Here's an example:\\n\\n```python\\nimport polars as pl\\n\\n# Create a sample DataFrame\\ndf = pl.DataFrame({\\n    'name': ['Alice', 'Bob', 'Charlie'],\\n    'age': [25, 30, 35],\\n    'country': ['USA', 'Canada', 'Mexico']\\n})\\n\\n# Concatenate multiple columns into a single column row\\ndf_concat = df.select(pl.concat_str(['name', 'age', 'country']))\\n\\nprint(df_concat)\\n```\\n\\nOutput:\\n\\n```\\nshape: (3, 1)\\n┌─────┐\\n│ name │\\n│ ---  │\\n├─────┤\\n│ Alice25USA │\\n│ Bob30Canada │\\n│ Charlie35Mexico │\\n└─────┘\\n```\"},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 142,\n",
       "  'completion_tokens': 239,\n",
       "  'total_tokens': 381}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.reply(\"And how do I concatenate multiple columns into a single column row?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3988.32 ms\n",
      "llama_print_timings:      sample time =      30.91 ms /   381 runs   (    0.08 ms per token, 12326.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1081.92 ms /    23 tokens (   47.04 ms per token,    21.26 tokens per second)\n",
      "llama_print_timings:        eval time =   44135.18 ms /   380 runs   (  116.15 ms per token,     8.61 tokens per second)\n",
      "llama_print_timings:       total time =   45751.34 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-f9c4160c-98d5-4a92-949f-3a097a674f75',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1704759076,\n",
       " 'model': '/Users/tommasofurlanello/Documents/Dev/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'To concatenate multiple columns into a single column row for multiple rows, you can use the same `concat_str` function as before. Here\\'s an example:\\n\\n```python\\nimport polars as pl\\n\\n# Create a sample DataFrame with more rows\\ndf = pl.DataFrame({\\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\\n    \"age\": [25, 30, 35, 40],\\n    \"city\": [\"New York\", \"San Francisco\", \"Los Angeles\", \"Chicago\"]\\n})\\n\\n# Concatenate multiple columns into a single column row for multiple rows\\ndf_concat = df.select(pl.concat_str([\"name\", \"age\", \"city\"], separator=\"_\").alias(\"full_info\"))\\n\\nprint(df_concat)\\n```\\n\\nOutput:\\n\\n```\\nshape: (4, 1)\\n┌─────────────┐\\n│ full_info     │\\n│ ---            │\\n├╌╌╌╌╌╌╌╌╌╌╌╌┤\\n│ Alice_25_New │\\n│ Bob_30_San   │\\n│ Charlie_35_Los │\\n│ David_40_Chic  │\\n└─────────────┘\\n```\\n\\nIn this example, we concatenate the \"name\", \"age\", and \"city\" columns into a single column called \"full_info\" for all rows in the DataFrame. The output shows that each row has been concatenated as expected.'},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 563,\n",
       "  'completion_tokens': 380,\n",
       "  'total_tokens': 943}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.reply(\"what about if I want to do it for multiple rows?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>role</th><th>content</th><th>time_stamps</th><th>author</th><th>message_id</th><th>chat_id</th><th>prompt:</th><th>prompt_tokens</th><th>completion_tokens</th><th>total_tokens</th></tr><tr><td>enum</td><td>str</td><td>datetime[μs]</td><td>str</td><td>str</td><td>str</td><td>list[struct[2]]</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;system&quot;</td><td>&quot;You are an ass…</td><td>2024-01-09 00:11:42.046525</td><td>&quot;import&quot;</td><td>&quot;chatml-a94266f…</td><td>&quot;23d35fdf-793c-…</td><td>null</td><td>null</td><td>10</td><td>null</td></tr><tr><td>&quot;user&quot;</td><td>&quot;What is polars…</td><td>2024-01-09 00:11:42.046527</td><td>&quot;import&quot;</td><td>&quot;chatml-af003c5…</td><td>&quot;23d35fdf-793c-…</td><td>null</td><td>null</td><td>8</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 10)\n",
       "┌────────┬────────────┬────────────┬────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
       "│ role   ┆ content    ┆ time_stamp ┆ author ┆ … ┆ prompt:    ┆ prompt_tok ┆ completion ┆ total_tok │\n",
       "│ ---    ┆ ---        ┆ s          ┆ ---    ┆   ┆ ---        ┆ ens        ┆ _tokens    ┆ ens       │\n",
       "│ enum   ┆ str        ┆ ---        ┆ str    ┆   ┆ list[struc ┆ ---        ┆ ---        ┆ ---       │\n",
       "│        ┆            ┆ datetime[μ ┆        ┆   ┆ t[2]]      ┆ i64        ┆ i64        ┆ i64       │\n",
       "│        ┆            ┆ s]         ┆        ┆   ┆            ┆            ┆            ┆           │\n",
       "╞════════╪════════════╪════════════╪════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
       "│ system ┆ You are an ┆ 2024-01-09 ┆ import ┆ … ┆ null       ┆ null       ┆ 10         ┆ null      │\n",
       "│        ┆ assistant  ┆ 00:11:42.0 ┆        ┆   ┆            ┆            ┆            ┆           │\n",
       "│        ┆ who        ┆ 46525      ┆        ┆   ┆            ┆            ┆            ┆           │\n",
       "│        ┆ perfect…   ┆            ┆        ┆   ┆            ┆            ┆            ┆           │\n",
       "│ user   ┆ What is    ┆ 2024-01-09 ┆ import ┆ … ┆ null       ┆ null       ┆ 8          ┆ null      │\n",
       "│        ┆ polars     ┆ 00:11:42.0 ┆        ┆   ┆            ┆            ┆            ┆           │\n",
       "│        ┆ library in ┆ 46527      ┆        ┆   ┆            ┆            ┆            ┆           │\n",
       "│        ┆ python…    ┆            ┆        ┆   ┆            ┆            ┆            ┆           │\n",
       "└────────┴────────────┴────────────┴────────┴───┴────────────┴────────────┴────────────┴───────────┘"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.concat([chat.df,message_df],how=\"diagonal\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_ml': [{'role': 'system',\n",
       "   'content': 'You are an assistant who perfectly describes python packages.'},\n",
       "  {'role': 'user', 'content': 'What is polars library in python?'}]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.df.select(pl.struct([pl.col(\"role\"),pl.col(\"content\")]).alias(\"chat_ml\")).to_dict(as_series=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "String(c7a38370-a6d0-4f64-8783-b9483b082589)"
      ],
      "text/plain": [
       "<Expr ['String(c7a38370-a6d0-4f64-8783…'] at 0x10BC09430>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = str(uuid.uuid4())\n",
    "print(type(id))\n",
    "pl.lit(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>role</th><th>content</th><th>time_stamps</th><th>chat_id</th></tr><tr><td>enum</td><td>str</td><td>datetime[μs]</td><td>i32</td></tr></thead><tbody><tr><td>&quot;system&quot;</td><td>&quot;You are an ass…</td><td>2024-01-08 20:46:11.375016</td><td>1</td></tr><tr><td>&quot;user&quot;</td><td>&quot;What is polars…</td><td>2024-01-08 20:46:11.375020</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 4)\n",
       "┌────────┬───────────────────────────────────┬────────────────────────────┬─────────┐\n",
       "│ role   ┆ content                           ┆ time_stamps                ┆ chat_id │\n",
       "│ ---    ┆ ---                               ┆ ---                        ┆ ---     │\n",
       "│ enum   ┆ str                               ┆ datetime[μs]               ┆ i32     │\n",
       "╞════════╪═══════════════════════════════════╪════════════════════════════╪═════════╡\n",
       "│ system ┆ You are an assistant who perfect… ┆ 2024-01-08 20:46:11.375016 ┆ 1       │\n",
       "│ user   ┆ What is polars library in python… ┆ 2024-01-08 20:46:11.375020 ┆ 1       │\n",
       "└────────┴───────────────────────────────────┴────────────────────────────┴─────────┘"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_single = pl.DataFrame(data={\"chat_id\": [1]})\n",
    "df_single = df.with_columns(pl.lit(1).alias(\"chat_id\"))\n",
    "df_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3724.28 ms\n",
      "llama_print_timings:      sample time =       8.34 ms /   103 runs   (    0.08 ms per token, 12347.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1077.72 ms /    23 tokens (   46.86 ms per token,    21.34 tokens per second)\n",
      "llama_print_timings:        eval time =   12324.44 ms /   102 runs   (  120.83 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =   13537.53 ms\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-2e9e5958-e306-43f1-b92c-d304c817da98',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1704751139,\n",
       " 'model': '/Users/tommasofurlanello/Documents/Dev/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \" Polars is a fast, efficient, and easy-to-use data processing library for Python. It provides high-performance DataFrame operations, similar to pandas, but with optimized memory usage and faster execution times. Polars is built on top of the Rust programming language, which allows it to leverage the power of Rust's performance and safety features. The library includes support for reading and writing various file formats, handling missing data, groupby operations, joins, and more.\"},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 37, 'completion_tokens': 102, 'total_tokens': 139}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'id': 'chatcmpl-eab5813e-3c8a-474b-bbee-636b9385d955',\n",
    " 'object': 'chat.completion',\n",
    " 'created': 1704751049,\n",
    " 'model': '/Users/tommasofurlanello/Documents/Dev/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q8_0.gguf',\n",
    " 'choices': [{'index': 0,\n",
    "   'message': {'role': 'assistant',\n",
    "    'content': ' The Polars library in Python is a high-performance DataFrame library that provides fast and efficient data processing capabilities. It is designed to handle large datasets with ease, offering features such as lazy evaluation, parallel execution, and memory optimization. Polars supports various data types, including integers, floats, strings, booleans, and timestamps, making it suitable for a wide range of applications in data science, machine learning, and analytics.'},\n",
    "   'finish_reason': 'stop'}],\n",
    " 'usage': {'prompt_tokens': 33, 'completion_tokens': 92, 'total_tokens': 125}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Polars is a fast, efficient, and easy-to-use data processing library for Python. It provides high-performance DataFrame operations, similar to pandas, but with optimized memory usage and faster execution times. Polars is built on top of the Rust programming language, which allows it to leverage the power of Rust's performance and safety features. The library includes support for reading and writing various file formats, handling missing data, groupby operations, joins, and more.\"]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[results_dict[\"choices\"][0][\"message\"][\"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an assistant who perfectly describes python packages.'},\n",
       " {'role': 'user', 'content': 'What is polars library in python?'}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('', Struct({'role': String, 'content': String}))])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_series = pl.Series(messages)\n",
    "messages_series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>prompt:</th></tr><tr><td>list[struct[2]]</td></tr></thead><tbody><tr><td>[{&quot;system&quot;,&quot;You are an assistant who perfectly describes python packages.&quot;}, {&quot;user&quot;,&quot;What is polars library in python?&quot;}]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 1)\n",
       "┌───────────────────────────────────┐\n",
       "│ prompt:                           │\n",
       "│ ---                               │\n",
       "│ list[struct[2]]                   │\n",
       "╞═══════════════════════════════════╡\n",
       "│ [{\"system\",\"You are an assistant… │\n",
       "└───────────────────────────────────┘"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = {\"prompt:\": pl.List(pl.Struct({\"role\":pl.Utf8,\"content\":pl.Utf8})),\n",
    "          \"role\":pl.Utf8,\n",
    "            \"content\":pl.Utf8,\n",
    "            \"author\":pl.Utf8,\n",
    "            \"message_id\":pl.Utf8,}\n",
    "data = {\"prompt:\": [messages_series],\n",
    "        \"role\"}\n",
    "df = pl.DataFrame(data=data,schema=schema)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>role</th><th>content</th><th>author</th><th>message_id</th><th>chat_id</th><th>prompt</th><th>usage</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>list[struct[2]]</td><td>struct[3]</td></tr></thead><tbody><tr><td>&quot;assistant&quot;</td><td>&quot; Polars is a f…</td><td>&quot;/Users/tommaso…</td><td>&quot;chatcmpl-2e9e5…</td><td>null</td><td>[{&quot;system&quot;,&quot;You are an assistant who perfectly describes python packages.&quot;}, {&quot;user&quot;,&quot;What is polars library in python?&quot;}]</td><td>{37,102,139}</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 7)\n",
       "┌───────────┬───────────────┬───────────────┬──────────────┬─────────┬──────────────┬──────────────┐\n",
       "│ role      ┆ content       ┆ author        ┆ message_id   ┆ chat_id ┆ prompt       ┆ usage        │\n",
       "│ ---       ┆ ---           ┆ ---           ┆ ---          ┆ ---     ┆ ---          ┆ ---          │\n",
       "│ str       ┆ str           ┆ str           ┆ str          ┆ str     ┆ list[struct[ ┆ struct[3]    │\n",
       "│           ┆               ┆               ┆              ┆         ┆ 2]]          ┆              │\n",
       "╞═══════════╪═══════════════╪═══════════════╪══════════════╪═════════╪══════════════╪══════════════╡\n",
       "│ assistant ┆ Polars is a   ┆ /Users/tommas ┆ chatcmpl-2e9 ┆ null    ┆ [{\"system\",\" ┆ {37,102,139} │\n",
       "│           ┆ fast,         ┆ ofurlanello/D ┆ e5958-e306-4 ┆         ┆ You are an   ┆              │\n",
       "│           ┆ efficient,    ┆ ocumen…       ┆ 3f1-b92c…    ┆         ┆ assistant…   ┆              │\n",
       "│           ┆ an…           ┆               ┆              ┆         ┆              ┆              │\n",
       "└───────────┴───────────────┴───────────────┴──────────────┴─────────┴──────────────┴──────────────┘"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_schema = {\"role\":pl.Utf8,\n",
    "                   \"content\":pl.Utf8,\n",
    "                   \"author\":pl.Utf8,\n",
    "                   \"message_id\":pl.Utf8,\n",
    "                   \"chat_id\":pl.Utf8, \n",
    "                   \"author\":pl.Utf8,\n",
    "                   \"prompt\": pl.List(pl.Struct({\"role\":role_dtype,\"content\":pl.Utf8})),\n",
    "                   \"usage\": pl.Struct({'prompt_tokens': pl.Int64, 'completion_tokens': pl.Int64, 'total_tokens': pl.Int64})\n",
    "}\n",
    "def add_message_from_results_dict(self, results_dict: dict, prompt: List[Dict[str, str]]):\n",
    "    data = {\"role\":[results_dict[\"choices\"][0][\"message\"][\"role\"]],\n",
    "            \"content\":[results_dict[\"choices\"][0][\"message\"][\"content\"]],\n",
    "            \"author\":[results_dict[\"model\"]],\n",
    "            \"message_id\":[results_dict[\"id\"]],\n",
    "            \"chat_id\":self.chat_id,\n",
    "            \"prompt\":[prompt],\n",
    "            \"usage\":[results_dict[\"usage\"]]\n",
    "    \n",
    "            }\n",
    "    df = pl.DataFrame(data=data,schema=chat_schema)\n",
    "    self.df = pl.concat([self.df,df],how=\"vertical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babydragon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
