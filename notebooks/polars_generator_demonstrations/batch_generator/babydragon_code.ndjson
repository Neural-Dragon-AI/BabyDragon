{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass EmbeddableType(Enum):\n    TEXT = \"text\"\n    NUMERIC = \"numeric\"\n    CATEGORICAL = \"categorical\"\n    # Add more data types as required\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\n    # Infer the data type of the column\n    # This will depend on the type of `column` (whether it's a string, Series, etc.)\n    # Here we'll assume `column` is a pandas Series for simplicity\n    column_type = str(column.dtype)\n    print(column_type)\n    if column_type == \"Utf8\":\n        # If it's an object, we'll assume it's text\n        return EmbeddableType.TEXT, OpenAiEmbedder()\n    elif np.issubdtype(column.dtype, np.number):\n        # If it's a number, we'll use a different embedding strategy\n        return EmbeddableType.NUMERIC, numeric_embedder\n    else:\n        # For other types, we could throw an error or have a default strategy\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef numeric_embedder(column):\n    # Implement the numeric embedding strategy\n    # This will depend on the type of `column` (whether it's a string, Series, etc.)\n    # Here we'll assume `column` is a pandas Series for simplicity\n    return column.values\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass EmbeddingTask(BaseTask):\n    def __init__(\n        self,\n        embedder: OpenAiEmbedder,\n        values: List[Any],\n        path: List[List[int]],\n        max_workers: int = 1,\n        task_id: str = \"task\",\n        calls_per_minute: int = 1500,\n        backup: bool = True,\n    ):\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup)\n        self.embedder = embedder\n        self.values = values\n\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n        # expected to work with a lig of a single element\n        if len(sub_path) != 1:\n            raise ValueError(\n                \"Embedding task expected to work with a list of a single element\"\n            )\n        sub_results = {}\n        for i in sub_path:\n            embedded_value = self.embedder.embed(self.values[i])\n            sub_results[i] = embedded_value\n        return sub_results\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    embedder: OpenAiEmbedder,\n    values: List[Any],\n    path: List[List[int]],\n    max_workers: int = 1,\n    task_id: str = \"task\",\n    calls_per_minute: int = 1500,\n    backup: bool = True,\n):\n    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup)\n    self.embedder = embedder\n    self.values = values\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n    # expected to work with a lig of a single element\n    if len(sub_path) != 1:\n        raise ValueError(\n            \"Embedding task expected to work with a list of a single element\"\n        )\n    sub_results = {}\n    for i in sub_path:\n        embedded_value = self.embedder.embed(self.values[i])\n        sub_results[i] = embedded_value\n    return sub_results\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\n        # Prepare the paths for the EmbeddingTask\n        print(\"Embedding {} values\".format(len(values)))\n        paths = [[i] for i in range(len(values))]\n\n        # Initialize the EmbeddingTask and execute it\n        embedding_task = EmbeddingTask(\n            embedder,\n            values,\n            path=paths,\n            max_workers=max_workers,\n            task_id=name + \"_embedding_task\",\n            backup=backup,\n        )\n        embeddings = embedding_task.work()\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\n        return embeddings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass TopicTreeTask(BaseTask):\n    def __init__(\n        self,\n        memory_kernel_dict: Dict,\n        supplement_indexes: Dict,\n        sim_threshold: float,\n        chatbot: BaseChat,\n        parent_kernel_label: str,\n        child_kernel_label: str,\n        system_prompt: str,\n        clustering_method: str,\n        task_id: str = \"TopicTreeTask\",\n        max_workers: int = 1,\n        calls_per_minute: int = 20,\n    ):\n        self.clustering_method = clustering_method\n        self.supplement_indexes = supplement_indexes\n        self.sim_threshold = sim_threshold\n        self.parent_kernel_label = parent_kernel_label\n        self.child_kernel_label = child_kernel_label\n        self.memory_kernel_dict = memory_kernel_dict\n        self._setup_memory_kernel_group()\n        self.generate_task_paths()\n        self.system_prompt = system_prompt\n        self.chatbot = chatbot\n        self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n        super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n\n\n    def _setup_memory_kernel_group(self):\n        if self.clustering_method == \"HDBSCAN\":\n            print(\"Using HDBSCAN\")\n            self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n        elif self.clustering_method == \"Spectral\":\n            print(\"Using Spectral\")\n            self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n        else:\n            raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n\n    def generate_task_paths(self):\n        print(\"Generating task paths\")\n\n        self.memory_kernel_group.generate_path_groups()\n\n    def llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n        return chatbot.reply(message)\n\n    def _execute_sub_task(self, sub_path) -> List[str]:\n        if self.parallel:\n            chatbot_instance = copy.deepcopy(self.chatbot)\n        else:\n            chatbot_instance = self.chatbot\n\n        sub_results = {}\n        for i in sub_path:\n            print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n            try:\n                current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n                supplement_values = []\n                for key, index in self.supplement_indexes.items():\n                    results, scores, indeces = index.faiss_query(current_val, k=5)\n                    for result, score in zip(results, scores):\n                        if score > self.sim_threshold:\n                            supplement_values.append(result)\n                topic_tree = self.create_topic_tree(supplement_values)\n                #response = self.llm_response(chatbot_instance, current_val, id=i)\n                sub_results[i] = topic_tree\n            except IndexError:\n                print(f\"Error: Invalid index {i} in sub_path\")\n                sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n            except Exception as e:\n                print(f\"Error in sub_task for index {i}: {e}\")\n                sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n\n        return sub_results\n\n    def execute_task(self) -> None:\n        BaseTask.execute_task(self)\n\n        # Load the results from the JSON file\n        # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n        #     task_results = json.load(f)\n        self._load_results_from_file()\n        task_results = self.results\n        new_values = []\n        #sort task_results by index and add to new_values 0- max values ascending\n        for task_result in task_results:\n            if isinstance(task_result, dict):\n                for key, value in task_result.items():\n                    new_values.append((int(key), value))\n            elif isinstance(task_result, str):\n                print(f\"Error in task_result: {task_result}\")\n\n        new_values.sort(key=lambda x: x[0])\n        values = [x[1] for x in new_values]\n\n        task_memory_index = MemoryIndex()\n        task_memory_index.init_index(values=values)\n        # Create a new MemoryKernel with the results\n        new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n\n        # Add the new MemoryKernel to the MultiKernel\n        self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n        self.generate_task_paths()\n\n        #delete the results file\n        # os.remove(f\"{self.task_id}_results.json\")\n\n\n    def create_topic_tree(self, docs):\n        return None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    memory_kernel_dict: Dict,\n    supplement_indexes: Dict,\n    sim_threshold: float,\n    chatbot: BaseChat,\n    parent_kernel_label: str,\n    child_kernel_label: str,\n    system_prompt: str,\n    clustering_method: str,\n    task_id: str = \"TopicTreeTask\",\n    max_workers: int = 1,\n    calls_per_minute: int = 20,\n):\n    self.clustering_method = clustering_method\n    self.supplement_indexes = supplement_indexes\n    self.sim_threshold = sim_threshold\n    self.parent_kernel_label = parent_kernel_label\n    self.child_kernel_label = child_kernel_label\n    self.memory_kernel_dict = memory_kernel_dict\n    self._setup_memory_kernel_group()\n    self.generate_task_paths()\n    self.system_prompt = system_prompt\n    self.chatbot = chatbot\n    self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n    super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef _setup_memory_kernel_group(self):\n    if self.clustering_method == \"HDBSCAN\":\n        print(\"Using HDBSCAN\")\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n    elif self.clustering_method == \"Spectral\":\n        print(\"Using Spectral\")\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n    else:\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef generate_task_paths(self):\n    print(\"Generating task paths\")\n\n    self.memory_kernel_group.generate_path_groups()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n    return chatbot.reply(message)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _execute_sub_task(self, sub_path) -> List[str]:\n    if self.parallel:\n        chatbot_instance = copy.deepcopy(self.chatbot)\n    else:\n        chatbot_instance = self.chatbot\n\n    sub_results = {}\n    for i in sub_path:\n        print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n        try:\n            current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n            supplement_values = []\n            for key, index in self.supplement_indexes.items():\n                results, scores, indeces = index.faiss_query(current_val, k=5)\n                for result, score in zip(results, scores):\n                    if score > self.sim_threshold:\n                        supplement_values.append(result)\n            topic_tree = self.create_topic_tree(supplement_values)\n            #response = self.llm_response(chatbot_instance, current_val, id=i)\n            sub_results[i] = topic_tree\n        except IndexError:\n            print(f\"Error: Invalid index {i} in sub_path\")\n            sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n        except Exception as e:\n            print(f\"Error in sub_task for index {i}: {e}\")\n            sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n\n    return sub_results\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef execute_task(self) -> None:\n    BaseTask.execute_task(self)\n\n    # Load the results from the JSON file\n    # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n    #     task_results = json.load(f)\n    self._load_results_from_file()\n    task_results = self.results\n    new_values = []\n    #sort task_results by index and add to new_values 0- max values ascending\n    for task_result in task_results:\n        if isinstance(task_result, dict):\n            for key, value in task_result.items():\n                new_values.append((int(key), value))\n        elif isinstance(task_result, str):\n            print(f\"Error in task_result: {task_result}\")\n\n    new_values.sort(key=lambda x: x[0])\n    values = [x[1] for x in new_values]\n\n    task_memory_index = MemoryIndex()\n    task_memory_index.init_index(values=values)\n    # Create a new MemoryKernel with the results\n    new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n\n    # Add the new MemoryKernel to the MultiKernel\n    self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n    self.generate_task_paths()\n\n    #delete the results file\n    # os.remove(f\"{self.task_id}_results.json\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef create_topic_tree(self, docs):\n    return None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass MultiKernelTask(BaseTask):\n    def __init__(\n        self,\n        memory_kernel_dict: Dict,\n        chatbot: BaseChat,\n        parent_kernel_label: str,\n        child_kernel_label: str,\n        system_prompt: str,\n        clustering_method: str,\n        path_group: Dict[str, List[List[int]]],\n        task_id: str = \"MultiKernelTask\",\n        max_workers: int = 1,\n        calls_per_minute: int = 20,\n    ):\n        self.clustering_method = clustering_method\n        self.parent_kernel_label = parent_kernel_label\n        self.child_kernel_label = child_kernel_label\n        self.memory_kernel_dict = memory_kernel_dict\n        \n        self._setup_memory_kernel_group()\n        if path_group:\n            self.memory_kernel_group.path_group = path_group\n        else:\n            self.generate_task_paths()\n        self.system_prompt = system_prompt\n        self.chatbot = chatbot\n        self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n        super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n\n\n    def _setup_memory_kernel_group(self):\n        if self.clustering_method == \"HDBSCAN\":\n            print(\"Using HDBSCAN\")\n            self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n        elif self.clustering_method == \"Spectral\":\n            print(\"Using Spectral\")\n            self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n        else:\n            raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n\n    def generate_task_paths(self):\n        print(\"Generating task paths\")\n\n        self.memory_kernel_group.generate_path_groups()\n\n    def llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n        return chatbot.reply(message)\n\n    def _execute_sub_task(self, sub_path) -> List[str]:\n        if self.parallel:\n            chatbot_instance = copy.deepcopy(self.chatbot)\n        else:\n            chatbot_instance = self.chatbot\n\n        sub_results = {}\n        for i in sub_path:\n            print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n            try:\n                current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n                response = self.llm_response(chatbot_instance, current_val, id=i)\n                sub_results[i] = response\n            except IndexError:\n                print(f\"Error: Invalid index {i} in sub_path\")\n                sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n            except Exception as e:\n                print(f\"Error in sub_task for index {i}: {e}\")\n                sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n\n        return sub_results\n\n    def execute_task(self) -> None:\n        BaseTask.execute_task(self)\n\n        # Load the results from the JSON file\n        # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n        #     task_results = json.load(f)\n        self._load_results_from_file()\n        task_results = self.results\n        new_values = []\n        #sort task_results by index and add to new_values 0- max values ascending\n        for task_result in task_results:\n            if isinstance(task_result, dict):\n                for key, value in task_result.items():\n                    new_values.append((int(key), value))\n            elif isinstance(task_result, str):\n                print(f\"Error in task_result: {task_result}\")\n\n        new_values.sort(key=lambda x: x[0])\n        values = [x[1] for x in new_values]\n\n        task_memory_index = MemoryIndex()\n        task_memory_index.init_index(values=values)\n        # Create a new MemoryKernel with the results\n        new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n\n        # Add the new MemoryKernel to the MultiKernel\n        self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n        self.generate_task_paths()\n\n        #delete the results file\n        # os.remove(f\"{self.task_id}_results.json\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    memory_kernel_dict: Dict,\n    chatbot: BaseChat,\n    parent_kernel_label: str,\n    child_kernel_label: str,\n    system_prompt: str,\n    clustering_method: str,\n    path_group: Dict[str, List[List[int]]],\n    task_id: str = \"MultiKernelTask\",\n    max_workers: int = 1,\n    calls_per_minute: int = 20,\n):\n    self.clustering_method = clustering_method\n    self.parent_kernel_label = parent_kernel_label\n    self.child_kernel_label = child_kernel_label\n    self.memory_kernel_dict = memory_kernel_dict\n    \n    self._setup_memory_kernel_group()\n    if path_group:\n        self.memory_kernel_group.path_group = path_group\n    else:\n        self.generate_task_paths()\n    self.system_prompt = system_prompt\n    self.chatbot = chatbot\n    self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n    super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef _setup_memory_kernel_group(self):\n    if self.clustering_method == \"HDBSCAN\":\n        print(\"Using HDBSCAN\")\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n    elif self.clustering_method == \"Spectral\":\n        print(\"Using Spectral\")\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n    else:\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef generate_task_paths(self):\n    print(\"Generating task paths\")\n\n    self.memory_kernel_group.generate_path_groups()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n    return chatbot.reply(message)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _execute_sub_task(self, sub_path) -> List[str]:\n    if self.parallel:\n        chatbot_instance = copy.deepcopy(self.chatbot)\n    else:\n        chatbot_instance = self.chatbot\n\n    sub_results = {}\n    for i in sub_path:\n        print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n        try:\n            current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n            response = self.llm_response(chatbot_instance, current_val, id=i)\n            sub_results[i] = response\n        except IndexError:\n            print(f\"Error: Invalid index {i} in sub_path\")\n            sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n        except Exception as e:\n            print(f\"Error in sub_task for index {i}: {e}\")\n            sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n\n    return sub_results\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef execute_task(self) -> None:\n    BaseTask.execute_task(self)\n\n    # Load the results from the JSON file\n    # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n    #     task_results = json.load(f)\n    self._load_results_from_file()\n    task_results = self.results\n    new_values = []\n    #sort task_results by index and add to new_values 0- max values ascending\n    for task_result in task_results:\n        if isinstance(task_result, dict):\n            for key, value in task_result.items():\n                new_values.append((int(key), value))\n        elif isinstance(task_result, str):\n            print(f\"Error in task_result: {task_result}\")\n\n    new_values.sort(key=lambda x: x[0])\n    values = [x[1] for x in new_values]\n\n    task_memory_index = MemoryIndex()\n    task_memory_index.init_index(values=values)\n    # Create a new MemoryKernel with the results\n    new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n\n    # Add the new MemoryKernel to the MultiKernel\n    self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n    self.generate_task_paths()\n\n    #delete the results file\n    # os.remove(f\"{self.task_id}_results.json\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass BaseTask:\n    def __init__(\n        self,\n        path: List[List[int]],\n        max_workers: int = 1,\n        task_id: str = \"task\",\n        calls_per_minute: int = 20,\n        backup: bool = True,\n        save_path: str = None,\n    ):\n        self.task_id = task_id\n        self.path = path\n        self.results = []\n        self.max_workers = max_workers\n        self.parallel = True if max_workers > 1 else False\n        self.rate_limiter = RateLimiter(calls_per_minute)\n        self.failed_sub_tasks = []\n        self.backup = backup\n        print(\"setting up savepath\")\n        self.save_path = save_path if save_path is not None else os.path.join(\"storage\", \"tasks\")\n\n    def _save_results_to_file(self) -> None:\n        os.makedirs(self.save_path, exist_ok=True)\n        with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"w\") as f:\n            json.dump(self.results, f)\n\n    def _load_results_from_file(self) -> None:\n        if os.path.exists(os.path.join(self.save_path, f\"{self.task_id}_results.json\")):\n            try:\n                with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"r\") as f:\n                    self.results = json.load(f)\n                    print(f\"Loaded {len(self.results)} results from file.\")\n            except Exception as e:\n                print(f\"Error loading results from file: {e}\")\n                print(\"Starting from scratch.\")\n        else:\n            print(\"No results file found, starting from scratch.\")\n\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n        sub_results = []\n        for i in sub_path:\n            response = \"Implement the response function in the subclass\"\n            sub_results.append(response)\n        return sub_results\n\n    def execute_task(self) -> None:\n        if self.backup:\n            self._load_results_from_file()\n\n        with RateLimitedThreadPoolExecutor(\n            max_workers=self.max_workers,\n            calls_per_minute=self.rate_limiter.calls_per_minute,\n        ) as executor:\n            futures = []\n            print(f\"Executing task {self.task_id} using {self.max_workers} workers.\")\n\n            for i, sub_path in enumerate(self.path):\n                if i < len(self.results):\n                    pass\n                else:\n                    future = executor.submit(self._execute_sub_task, sub_path)\n                    futures.append((i, future))\n\n            for i, future in futures:\n                try:\n                    execution_start_time = time.time()\n                    sub_task_result = future.result()\n                    execution_end_time = time.time()\n                    print(\n                        f\"Sub-task {i} executed in {execution_end_time - execution_start_time:.2f} seconds.\"\n                    )\n\n                    save_start_time = time.time()\n                    self.results.append(sub_task_result)\n                    # self.results.append(sub_task_result)\n                    if self.backup:\n                        self._save_results_to_file()\n                    save_end_time = time.time()\n                    print(\n                        f\"Sub-task {i} results saved in {save_end_time - save_start_time:.2f} seconds.\"\n                    )\n                except Exception as e:\n                    print(f\"Error in sub-task {i}: {e}\")\n                    # default_result = f\"Error in sub-task {i}: {e}\"\n                    default_result =  {i:f\"Error in sub-task {i}: {e}\"} \n                    self.results.append(default_result)\n                    if self.backup:\n                        self._save_results_to_file()\n                    self.failed_sub_tasks.append((self.path[i], str(e)))\n\n                except KeyboardInterrupt:\n                    print(\"Keyboard interrupt detected, stopping task execution.\")\n                    executor.shutdown(wait=False)\n                    break\n\n        print(\"Task execution completed.\")\n\n    def work(self) -> List[Any]:\n        self.execute_task()\n        if not self.backup:\n            self._save_results_to_file()\n        work = []\n        for sub_result in self.results:\n            for index_id, response in sub_result.items():\n                work.append((index_id, response))\n        # sort the content to write by index_id\n        work.sort(key=lambda x: int(x[0]))\n        return work\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    path: List[List[int]],\n    max_workers: int = 1,\n    task_id: str = \"task\",\n    calls_per_minute: int = 20,\n    backup: bool = True,\n    save_path: str = None,\n):\n    self.task_id = task_id\n    self.path = path\n    self.results = []\n    self.max_workers = max_workers\n    self.parallel = True if max_workers > 1 else False\n    self.rate_limiter = RateLimiter(calls_per_minute)\n    self.failed_sub_tasks = []\n    self.backup = backup\n    print(\"setting up savepath\")\n    self.save_path = save_path if save_path is not None else os.path.join(\"storage\", \"tasks\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _save_results_to_file(self) -> None:\n    os.makedirs(self.save_path, exist_ok=True)\n    with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"w\") as f:\n        json.dump(self.results, f)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _load_results_from_file(self) -> None:\n    if os.path.exists(os.path.join(self.save_path, f\"{self.task_id}_results.json\")):\n        try:\n            with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"r\") as f:\n                self.results = json.load(f)\n                print(f\"Loaded {len(self.results)} results from file.\")\n        except Exception as e:\n            print(f\"Error loading results from file: {e}\")\n            print(\"Starting from scratch.\")\n    else:\n        print(\"No results file found, starting from scratch.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n    sub_results = []\n    for i in sub_path:\n        response = \"Implement the response function in the subclass\"\n        sub_results.append(response)\n    return sub_results\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef execute_task(self) -> None:\n    if self.backup:\n        self._load_results_from_file()\n\n    with RateLimitedThreadPoolExecutor(\n        max_workers=self.max_workers,\n        calls_per_minute=self.rate_limiter.calls_per_minute,\n    ) as executor:\n        futures = []\n        print(f\"Executing task {self.task_id} using {self.max_workers} workers.\")\n\n        for i, sub_path in enumerate(self.path):\n            if i < len(self.results):\n                pass\n            else:\n                future = executor.submit(self._execute_sub_task, sub_path)\n                futures.append((i, future))\n\n        for i, future in futures:\n            try:\n                execution_start_time = time.time()\n                sub_task_result = future.result()\n                execution_end_time = time.time()\n                print(\n                    f\"Sub-task {i} executed in {execution_end_time - execution_start_time:.2f} seconds.\"\n                )\n\n                save_start_time = time.time()\n                self.results.append(sub_task_result)\n                # self.results.append(sub_task_result)\n                if self.backup:\n                    self._save_results_to_file()\n                save_end_time = time.time()\n                print(\n                    f\"Sub-task {i} results saved in {save_end_time - save_start_time:.2f} seconds.\"\n                )\n            except Exception as e:\n                print(f\"Error in sub-task {i}: {e}\")\n                # default_result = f\"Error in sub-task {i}: {e}\"\n                default_result =  {i:f\"Error in sub-task {i}: {e}\"} \n                self.results.append(default_result)\n                if self.backup:\n                    self._save_results_to_file()\n                self.failed_sub_tasks.append((self.path[i], str(e)))\n\n            except KeyboardInterrupt:\n                print(\"Keyboard interrupt detected, stopping task execution.\")\n                executor.shutdown(wait=False)\n                break\n\n    print(\"Task execution completed.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef work(self) -> List[Any]:\n    self.execute_task()\n    if not self.backup:\n        self._save_results_to_file()\n    work = []\n    for sub_result in self.results:\n        for index_id, response in sub_result.items():\n            work.append((index_id, response))\n    # sort the content to write by index_id\n    work.sort(key=lambda x: int(x[0]))\n    return work\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass LLMReader(BaseTask):\n    def __init__(\n        self,\n        index: MemoryIndex,\n        path: List[List[int]],\n        chatbot: Chat,\n        read_func=None,\n        max_workers: int = 1,\n        task_id: str = \"LLMReadTask\",\n        calls_per_minute: int = 20,\n    ):\n        \"\"\"\n        Initialize a LLMReadTask instance.\n\n        :param index: List of strings representing the queries.\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n        :param chatbot: Chatbot instance used for executing queries.\n        :param max_workers: Maximum number of worker threads (default is 4).\n        \"\"\"\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute)\n        self.index = index\n        self.chatbot = chatbot\n        self.read_func = read_func if read_func else self.llm_response\n\n    def llm_response(chatbot: Chat, message: str, string_out=False):\n        if string_out:\n            return chatbot.reply(message)\n        return chatbot.query(message)\n\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n        \"\"\"\n        Execute a sub-task using a separate copy of the chatbot instance. each sub-stasks uses a\n        a clean memory instance.\n\n        :param sub_path: List of indices representing the sub-task's sequence.\n        :return: List of strings representing the responses for each query in the sub-task.\n        \"\"\"\n        if self.parallel:\n            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n            chatbot_instance = copy.deepcopy(self.chatbot)\n        else:\n            chatbot_instance = self.chatbot\n        if isinstance(self.chatbot, BaseThread):\n            chatbot_instance.reset_memory()\n\n        sub_results = []\n        for i in sub_path:\n            response = self.read_func(chatbot_instance, self.index.values[i])\n            sub_results.append(response)\n        return sub_results\n\n    def read(self):\n        self.execute_task()\n        return self.results\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    index: MemoryIndex,\n    path: List[List[int]],\n    chatbot: Chat,\n    read_func=None,\n    max_workers: int = 1,\n    task_id: str = \"LLMReadTask\",\n    calls_per_minute: int = 20,\n):\n    \"\"\"\n        Initialize a LLMReadTask instance.\n\n        :param index: List of strings representing the queries.\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n        :param chatbot: Chatbot instance used for executing queries.\n        :param max_workers: Maximum number of worker threads (default is 4).\n        \"\"\"\n    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute)\n    self.index = index\n    self.chatbot = chatbot\n    self.read_func = read_func if read_func else self.llm_response\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef llm_response(chatbot: Chat, message: str, string_out=False):\n    if string_out:\n        return chatbot.reply(message)\n    return chatbot.query(message)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n    \"\"\"\n        Execute a sub-task using a separate copy of the chatbot instance. each sub-stasks uses a\n        a clean memory instance.\n\n        :param sub_path: List of indices representing the sub-task's sequence.\n        :return: List of strings representing the responses for each query in the sub-task.\n        \"\"\"\n    if self.parallel:\n        # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n        chatbot_instance = copy.deepcopy(self.chatbot)\n    else:\n        chatbot_instance = self.chatbot\n    if isinstance(self.chatbot, BaseThread):\n        chatbot_instance.reset_memory()\n\n    sub_results = []\n    for i in sub_path:\n        response = self.read_func(chatbot_instance, self.index.values[i])\n        sub_results.append(response)\n    return sub_results\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef read(self):\n    self.execute_task()\n    return self.results\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass LLMWriter(BaseTask):\n    def __init__(\n        self,\n        index: MemoryIndex,\n        path: List[List[int]],\n        chatbot: Chat,\n        write_func=None,\n        context=None,\n        task_name=\"summary\",\n        max_workers: int = 1,\n        task_id: str = \"LLMWriteTask\",\n        calls_per_minute: int = 20,\n        backup: bool = True,\n    ):\n        \"\"\"\n        Initialize a LLMWriteTask instance.\n\n        :param index: List of strings representing the queries.\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n        :param chatbot: Chatbot instance used for executing queries.\n        :param max_workers: Maximum number of worker threads (default is 4).\n        \"\"\"\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\n        self.index = index\n        self.chatbot = chatbot\n        self.write_func = write_func if write_func else self.llm_response\n        self.new_index_name = self.index.name + f\"_{task_name}\"\n        self.context = context\n\n    def llm_response(self, chatbot: Chat, message: str, context=None, id=None):\n        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n        # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\n        #     return \"the message is too long to be processed\"\n        # moved the error catching to multi-threading but custom method could report the error here\n        return chatbot.reply(message)\n\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n        \"\"\"\n        Execute a sub-task using a separate copy of the chatbot instance.\n\n        :param sub_path: List of indices representing the sub-task's sequence.\n        :return: List of strings representing the responses for each query in the sub-task.\n        \"\"\"\n        if self.parallel:\n            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n            chatbot_instance = copy.deepcopy(self.chatbot)\n        else:\n            chatbot_instance = self.chatbot\n        if isinstance(self.chatbot, BaseThread):\n            chatbot_instance.reset_memory()\n\n        sub_results = {}\n        for i in sub_path:\n            current_val = self.index.values[i]\n            response = self.write_func(\n                chatbot_instance, current_val, self.context, id=i\n            )\n            sub_results[i] = response\n        return sub_results\n\n    def write(self):\n        content_to_write = self.work()\n        self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\n        self.new_index.save()\n        return self.new_index\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    index: MemoryIndex,\n    path: List[List[int]],\n    chatbot: Chat,\n    write_func=None,\n    context=None,\n    task_name=\"summary\",\n    max_workers: int = 1,\n    task_id: str = \"LLMWriteTask\",\n    calls_per_minute: int = 20,\n    backup: bool = True,\n):\n    \"\"\"\n        Initialize a LLMWriteTask instance.\n\n        :param index: List of strings representing the queries.\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n        :param chatbot: Chatbot instance used for executing queries.\n        :param max_workers: Maximum number of worker threads (default is 4).\n        \"\"\"\n    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\n    self.index = index\n    self.chatbot = chatbot\n    self.write_func = write_func if write_func else self.llm_response\n    self.new_index_name = self.index.name + f\"_{task_name}\"\n    self.context = context\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef llm_response(self, chatbot: Chat, message: str, context=None, id=None):\n    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n    # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\n    #     return \"the message is too long to be processed\"\n    # moved the error catching to multi-threading but custom method could report the error here\n    return chatbot.reply(message)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n    \"\"\"\n        Execute a sub-task using a separate copy of the chatbot instance.\n\n        :param sub_path: List of indices representing the sub-task's sequence.\n        :return: List of strings representing the responses for each query in the sub-task.\n        \"\"\"\n    if self.parallel:\n        # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n        chatbot_instance = copy.deepcopy(self.chatbot)\n    else:\n        chatbot_instance = self.chatbot\n    if isinstance(self.chatbot, BaseThread):\n        chatbot_instance.reset_memory()\n\n    sub_results = {}\n    for i in sub_path:\n        current_val = self.index.values[i]\n        response = self.write_func(\n            chatbot_instance, current_val, self.context, id=i\n        )\n        sub_results[i] = response\n    return sub_results\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef write(self):\n    content_to_write = self.work()\n    self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\n    self.new_index.save()\n    return self.new_index\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass DiscreteDataInt(BDType):\n    alphabet: Optional[Set[int]] = Field(None, description=\"Set of allowed discrete variables. All elements should be integers.\")\n    value: int = Field(..., description=\"The discrete data value. It should be an integer.\")\n\n    @field_validator('alphabet')\n    def check_alphabet(cls, v):\n        if not all(isinstance(item, int) for item in v):\n            raise ValueError(\"All elements in 'alphabet' should be integers.\")\n        return v\n\n    @field_validator('value')\n    def check_value(cls, v, info: FieldValidationInfo):\n        alphabet = info.data.get('alphabet')\n        if alphabet is not None and v not in alphabet:\n            raise ValueError(\"Value must be in the alphabet.\")\n        return v\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator('alphabet')\ndef check_alphabet(cls, v):\n    if not all(isinstance(item, int) for item in v):\n        raise ValueError(\"All elements in 'alphabet' should be integers.\")\n    return v\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator('value')\ndef check_value(cls, v, info: FieldValidationInfo):\n    alphabet = info.data.get('alphabet')\n    if alphabet is not None and v not in alphabet:\n        raise ValueError(\"Value must be in the alphabet.\")\n    return v\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass DiscreteDataStr(BDType):\n    alphabet: Optional[Set[str]] = Field(None, description=\"Set of allowed discrete variables. All elements should be strings.\")\n    value: str = Field(..., description=\"The discrete data value. It should be a string.\")\n\n    @field_validator('alphabet')\n    def check_alphabet(cls, v):\n        if not all(isinstance(item, str) for item in v):\n            raise ValueError(\"All elements in 'alphabet' should be strings.\")\n        return v\n\n    @field_validator('value')\n    def check_value(cls, v, info: FieldValidationInfo):\n        alphabet = info.data.get('alphabet')\n        if alphabet is not None and v not in alphabet:\n            raise ValueError(\"Value must be in the alphabet.\")\n        return v\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator('alphabet')\ndef check_alphabet(cls, v):\n    if not all(isinstance(item, str) for item in v):\n        raise ValueError(\"All elements in 'alphabet' should be strings.\")\n    return v\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator('value')\ndef check_value(cls, v, info: FieldValidationInfo):\n    alphabet = info.data.get('alphabet')\n    if alphabet is not None and v not in alphabet:\n        raise ValueError(\"Value must be in the alphabet.\")\n    return v\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# The DiscreteDataInt and DiscreteDataStr models are defined as before\n\nclass DiscreteDataList(BaseModel):\n    alphabet: Optional[Set[Union[int, str]]] = Field(None, description=\"Set of allowed discrete variables. All elements should be of the same type (either integers or strings).\")\n    value: List[Union[DiscreteDataInt, DiscreteDataStr]] = Field(..., description=\"The list of discrete data values. All elements should be either DiscreteDataInt or DiscreteDataStr, not a mix.\")\n\n    @field_validator('value')\n    def check_alphabets(cls, value, info: FieldValidationInfo):\n        list_alphabet = info.data.get('alphabet')\n        if list_alphabet is not None:\n            for item in value:\n                item_alphabet = item.alphabet\n                if item_alphabet is not None and not set(item_alphabet).issubset(list_alphabet):\n                    raise ValueError(f\"Item alphabet {item_alphabet} is not a subset of the list alphabet {list_alphabet}.\")\n        return value\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator('value')\ndef check_alphabets(cls, value, info: FieldValidationInfo):\n    list_alphabet = info.data.get('alphabet')\n    if list_alphabet is not None:\n        for item in value:\n            item_alphabet = item.alphabet\n            if item_alphabet is not None and not set(item_alphabet).issubset(list_alphabet):\n                raise ValueError(f\"Item alphabet {item_alphabet} is not a subset of the list alphabet {list_alphabet}.\")\n    return value\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"class MultiDimensionalDiscrete(BDType):\n    value: List[Union[DiscreteDataInt, DiscreteDataStr]] = Field(..., description=\"The multidimensional discrete data value. It should be a list of either DiscreteDataInt or DiscreteDataStr.\")\n    type_dictionary: Dict[int, str] = Field(default_factory=dict, description=\"The helper dictionary containing the type of each dimension of the value list.\")\n    def __init__(self, **data):\n        super().__init__(**data)\n        self.type_dictionary = {i: item.__class__.__name__ for i, item in enumerate(self.value)}\n        \n    @field_validator('value')\n    def check_value(cls, value):\n        if len(value) < 2:\n            raise ValueError(\"For multidimensional discrete data, size of the list should be at least 2. For less than 2, use DiscreteDataInt or DiscreteDataStr.\")\n        return value\n    \n    from pydantic import BaseModel, Field, field_validator\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, **data):\n    super().__init__(**data)\n    self.type_dictionary = {i: item.__class__.__name__ for i, item in enumerate(self.value)}\n    \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"@field_validator('value')\ndef check_value(cls, value):\n    if len(value) < 2:\n        raise ValueError(\"For multidimensional discrete data, size of the list should be at least 2. For less than 2, use DiscreteDataInt or DiscreteDataStr.\")\n    return value\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MultiDimensionalDiscreteList(BDType):\n    values: List[MultiDimensionalDiscrete] = Field(..., description=\"The list of multidimensional discrete data values. All elements should be instances of MultiDimensionalDiscrete.\")\n    joint_alphabet: Optional[Set[Tuple[Any, ...]]] = Field(None, description=\"Set of tuples representing allowed discrete variable combinations. All elements should be tuples of the same length as the number of dimensions in each joint discrete variable.\")\n\n    @field_validator('values')\n    def check_type_dictionaries(cls, values):\n        first_type_dictionary = values[0].type_dictionary\n        for value in values[1:]:\n            if value.type_dictionary != first_type_dictionary:\n                raise ValueError(\"All elements in 'values' should have the same 'type_dictionary'.\")\n        return values\n\n    @field_validator('joint_alphabet')\n    def check_joint_alphabet(cls, v, info):\n        if v is not None and \"values\" in info.data:\n            expected_tuple_length = len(info.data[\"values\"][0].value)\n            for item in v:\n                if not isinstance(item, tuple) or len(item) != expected_tuple_length:\n                    raise ValueError(f\"Each element in 'joint_alphabet' should be a tuple of length {expected_tuple_length}.\")\n                for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\n                    if dim_alphabet is not None and dim_value not in dim_alphabet:\n                        raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\n        return v\n\n\n    \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator('values')\ndef check_type_dictionaries(cls, values):\n    first_type_dictionary = values[0].type_dictionary\n    for value in values[1:]:\n        if value.type_dictionary != first_type_dictionary:\n            raise ValueError(\"All elements in 'values' should have the same 'type_dictionary'.\")\n    return values\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator('joint_alphabet')\ndef check_joint_alphabet(cls, v, info):\n    if v is not None and \"values\" in info.data:\n        expected_tuple_length = len(info.data[\"values\"][0].value)\n        for item in v:\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\n                raise ValueError(f\"Each element in 'joint_alphabet' should be a tuple of length {expected_tuple_length}.\")\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\n    return v\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass RealData(BDType):\n    range: Optional[Tuple[Union[float, int], Union[float, int]]] = Field(None, description=\"An optional inclusive range (min, max) for the value.\")\n    value: Union[float, int] = Field(..., description=\"The real value data. It should be a float or an integer.\")\n\n    @field_validator(\"value\")\n    def validate_value(cls, v, values):\n        value_range = values.data[\"range\"]\n        if value_range is not None:\n            min_value, max_value = value_range\n            if not min_value <= v <= max_value:\n                raise ValueError(f\"Value {v} is not within the specified range {value_range}.\")\n        return v\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator(\"value\")\ndef validate_value(cls, v, values):\n    value_range = values.data[\"range\"]\n    if value_range is not None:\n        min_value, max_value = value_range\n        if not min_value <= v <= max_value:\n            raise ValueError(f\"Value {v} is not within the specified range {value_range}.\")\n    return v\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass RealDataList(BDType):\n    range: Optional[Tuple[Union[float, int], Union[float, int]]] = Field(None, description=\"An optional inclusive range (min, max) for the values.\")\n    values: List[RealData] = Field(..., description=\"The list of real value data. Each should be a RealeData object.\")\n\n    @field_validator(\"values\")\n    def validate_values(cls, values, values_dict):\n        list_range = values_dict.data.get(\"range\")\n        if list_range is not None:\n            min_value, max_value = list_range\n            for value in values:\n                if not min_value <= value.value <= max_value:\n                    raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {list_range}.\")\n        return values\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator(\"values\")\ndef validate_values(cls, values, values_dict):\n    list_range = values_dict.data.get(\"range\")\n    if list_range is not None:\n        min_value, max_value = list_range\n        for value in values:\n            if not min_value <= value.value <= max_value:\n                raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {list_range}.\")\n    return values\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MultiDimensionalReal(BDType):\n    range: Optional[Union[Tuple[Union[float, int], Union[float, int]], List[Tuple[Union[float, int], Union[float, int]]]]] = Field(\n        None, \n        description=\"An optional inclusive range (min, max) for the values. If a tuple, applies to all dimensions. If a list, it must match the dimension length.\"\n    )\n    values: List[RealData] = Field(\n        ..., \n        description=\"The list of real data for each dimension. Each should be a RealData object.\"\n    )\n\n    @field_validator(\"values\")\n    def validate_values(cls, values, values_dict):\n        range_values = values_dict.data.get(\"range\")\n        if range_values is not None:\n            # If range is a tuple, apply it to all dimensions\n            if isinstance(range_values, tuple):\n                min_value, max_value = range_values\n                for value in values:\n                    if not min_value <= value.value <= max_value:\n                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n            # If range is a list, it must have the same length as values\n            elif isinstance(range_values, list):\n                if len(values) != len(range_values):\n                    raise ValueError(\"If range is a list, it must have the same length as values.\")\n                for value, (min_value, max_value) in zip(values, range_values):\n                    if not min_value <= value.value <= max_value:\n                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n        return values\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator(\"values\")\ndef validate_values(cls, values, values_dict):\n    range_values = values_dict.data.get(\"range\")\n    if range_values is not None:\n        # If range is a tuple, apply it to all dimensions\n        if isinstance(range_values, tuple):\n            min_value, max_value = range_values\n            for value in values:\n                if not min_value <= value.value <= max_value:\n                    raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n        # If range is a list, it must have the same length as values\n        elif isinstance(range_values, list):\n            if len(values) != len(range_values):\n                raise ValueError(\"If range is a list, it must have the same length as values.\")\n            for value, (min_value, max_value) in zip(values, range_values):\n                if not min_value <= value.value <= max_value:\n                    raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n    return values\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MultiDimensionalRealList(BDType):\n    range: Optional[Union[Tuple[Union[float, int], Union[float, int]], List[Tuple[Union[float, int], Union[float, int]]]]] = Field(\n        None, \n        description=\"An optional inclusive range (min, max) for the values in all dimensions. If a tuple, applies to all dimensions. If a list, it must match the dimension length.\"\n    )\n    values: List[MultiDimensionalReal] = Field(\n        ..., \n        description=\"The list of multi-dimensional real data. Each should be a MultiDimensionalReal object.\"\n    )\n\n    @field_validator(\"values\")\n    def validate_values(cls, values, values_dict):\n        range_values = values_dict.data.get(\"range\")\n        dimension_length = len(values[0].values) if values else 0\n        if range_values is not None:\n            # If range is a tuple, apply it to all dimensions\n            if isinstance(range_values, tuple):\n                min_value, max_value = range_values\n                for multi_real in values:\n                    if len(multi_real.values) != dimension_length:\n                        raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n                    for value in multi_real.values:\n                        if not min_value <= value.value <= max_value:\n                            raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n            # If range is a list, it must have the same length as values in each dimension\n            elif isinstance(range_values, list):\n                if len(range_values) != dimension_length:\n                    raise ValueError(\"If range is a list, it must have the same length as values in each dimension.\")\n                for multi_real in values:\n                    if len(multi_real.values) != dimension_length:\n                        raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n                    for value, (min_value, max_value) in zip(multi_real.values, range_values):\n                        if not min_value <= value.value <= max_value:\n                            raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n        return values\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator(\"values\")\ndef validate_values(cls, values, values_dict):\n    range_values = values_dict.data.get(\"range\")\n    dimension_length = len(values[0].values) if values else 0\n    if range_values is not None:\n        # If range is a tuple, apply it to all dimensions\n        if isinstance(range_values, tuple):\n            min_value, max_value = range_values\n            for multi_real in values:\n                if len(multi_real.values) != dimension_length:\n                    raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n                for value in multi_real.values:\n                    if not min_value <= value.value <= max_value:\n                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n        # If range is a list, it must have the same length as values in each dimension\n        elif isinstance(range_values, list):\n            if len(range_values) != dimension_length:\n                raise ValueError(\"If range is a list, it must have the same length as values in each dimension.\")\n            for multi_real in values:\n                if len(multi_real.values) != dimension_length:\n                    raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n                for value, (min_value, max_value) in zip(multi_real.values, range_values):\n                    if not min_value <= value.value <= max_value:\n                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n    return values\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass NaturalLanguageSingle(BDType):\n    text: str = Field(..., description=\"The natural language text. It should be less than or equal to `max_tokens` in length when tokenized.\")\n    max_tokens: int = Field(8000, description=\"The maximum allowed length of the text in tokens. The default value is 8000.\")\n    \n    @field_validator(\"text\")\n    def validate_text(cls, v, info):\n        try:\n            # Tokenize the text and get the token count\n            token_count = len(tokenizer.encode(v))\n        except Exception as e:\n            raise ValueError(\"Failed to tokenize text.\") from e\n\n        # Get max_tokens from info.data, if not available, default to 8000\n        max_tokens = info.data.get(\"max_tokens\", 8000)\n\n        if token_count > max_tokens:\n            raise ValueError(f\"Text is longer than {max_tokens} tokens.\")\n\n        return v\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator(\"text\")\ndef validate_text(cls, v, info):\n    try:\n        # Tokenize the text and get the token count\n        token_count = len(tokenizer.encode(v))\n    except Exception as e:\n        raise ValueError(\"Failed to tokenize text.\") from e\n\n    # Get max_tokens from info.data, if not available, default to 8000\n    max_tokens = info.data.get(\"max_tokens\", 8000)\n\n    if token_count > max_tokens:\n        raise ValueError(f\"Text is longer than {max_tokens} tokens.\")\n\n    return v\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass NaturalLanguageList(BDType):\n    texts: List[NaturalLanguageSingle] = Field(..., description=\"A list of `NaturalLanguageSingle` objects. Each object should pass the validation requirements of the `NaturalLanguageSingle` class.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# Base class for all bd_types\nclass BDType(BaseModel):\n    source: str = Field(\"babydragon\", description=\"The source of the data.\")\n    timestamp: Optional[datetime.datetime] = Field(None, description=\"When the data was collected or created. If not provided, the current time is used.\")\n    id: uuid.UUID = Field(default_factory=uuid.uuid4, description=\"Unique identifier of the data.\")\n    data_name: Optional[str] = Field(None, description=\"Name of the data.\")\n    elements_name: Optional[List[str]] = Field(None, description=\"Names of the elements if the data is a list.\")\n\n    @field_validator(\"timestamp\")\n    def set_timestamp(cls, v):\n        return v or datetime.datetime.now()\n\n    @field_validator(\"id\")\n    def set_id(cls, values, **kwargs):\n        if \"id\" not in values:\n            values[\"id\"] = uuid.uuid4()\n        return values\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator(\"timestamp\")\ndef set_timestamp(cls, v):\n    return v or datetime.datetime.now()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@field_validator(\"id\")\ndef set_id(cls, values, **kwargs):\n    if \"id\" not in values:\n        values[\"id\"] = uuid.uuid4()\n    return values\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass MemoryKernel(MemoryIndex):\n    def __init__(\n        self,\n        mem_index: MemoryIndex,\n        name: str = \"memory_kernel\",\n        k: int = 2,\n        save_path: str = None,\n    ):\n        \"\"\"\n        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\n\n        Args:\n            mem_index (MemoryIndex): A MemoryIndex instance.\n            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\n            k (int, optional): The number of hops for message passing. Defaults to 2.\n            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\n        \"\"\"\n        super().__init__(\n            index=mem_index.index,\n            values=mem_index.values,\n            embeddings=mem_index.embeddings,\n            name=name,\n            save_path=save_path,\n        )\n        self.k = k\n        if len(self.values) > 0: \n            self.create_k_hop_index(k=k)\n        else:\n            raise ValueError(\"The input MemoryIndex is empty. Please check the input MemoryIndex.\")\n\n    def cos_sim(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n        \"\"\"\n        if not isinstance(a, np.ndarray):\n            a = np.array(a)\n\n        if not isinstance(b, np.ndarray):\n            b = np.array(b)\n\n        if len(a.shape) == 1:\n            a = a[np.newaxis, :]\n\n        if len(b.shape) == 1:\n            b = b[np.newaxis, :]\n\n        a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\n        b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\n        return np.dot(a_norm, b_norm.T)\n\n    def compute_kernel(\n        self,\n        embedding_set: np.ndarray,\n        threshold: float = 0.65,\n        use_softmax: bool = False,\n    ) -> np.ndarray:\n\n        \"\"\"\n        Compute the adjacency matrix of the graph.\n\n        Parameters:\n        embedding_set (numpy array): The embedding matrix of the nodes.\n        threshold (float): The threshold for the adjacency matrix.\n        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\n        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\n\n        Returns:\n        adj_matrix (numpy array): The adjacency matrix of the graph.\n        \"\"\"\n\n        A = self.cos_sim(embedding_set, embedding_set)\n        if use_softmax:\n            # softmax\n            A = np.exp(A)\n            A = A / np.sum(A, axis=1)[:, np.newaxis]\n        adj_matrix = np.zeros_like(A)\n        adj_matrix[A > threshold] = 1\n        adj_matrix[A <= threshold] = 0\n        adj_matrix = adj_matrix.astype(np.float32)\n        return adj_matrix\n\n    def k_hop_message_passing(\n        self, A: np.ndarray, node_features: np.ndarray, k: int\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Compute the k-hop adjacency matrix and aggregated features using message passing.\n\n        Parameters:\n        A (numpy array): The adjacency matrix of the graph.\n        node_features (numpy array): The feature matrix of the nodes.\n        k (int): The number of hops for message passing.\n\n        Returns:\n        A_k (numpy array): The k-hop adjacency matrix.\n        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n        \"\"\"\n\n        print(\"Compute the k-hop adjacency matrix\")\n        A_k = np.linalg.matrix_power(A, k)\n\n        print(\"Aggregate the messages from the k-hop neighborhood:\")\n        agg_features = node_features.copy()\n\n        for i in tqdm(range(k)):\n            agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n\n        return A_k, agg_features\n\n    def graph_sylvester_embedding(self, G: Tuple, m: int, ts: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\n\n        Args:\n            G (Tuple): A tuple containing the graph's vertices (V) and weights (W).\n            m (int): The number of singular values to consider.\n            ts (np.ndarray): The spectral scales.\n\n        Returns:\n            np.ndarray: The node_embeddings matrix.\n        \"\"\"\n        V, W = G\n        n = len(V)\n        D_BE = np.diag(W.sum(axis=1))\n        L_BE = np.identity(n) - np.dot(\n            np.diag(1 / np.sqrt(D_BE.diagonal())),\n            np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n        )\n\n        A = W\n        B = L_BE\n        C = np.identity(n)\n        X = solve_sylvester(A, B, C)\n\n        U, S, _ = svd(X, full_matrices=False)\n        U_m = U[:, :m]\n        S_m = S[:m]\n\n        node_embeddings = np.zeros((n, m))\n\n        for i in range(n):\n            for s in range(m):\n                # Spectral kernel descriptor\n                node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n\n        return node_embeddings\n\n    def gen_gse_embeddings(\n        self, A: np.ndarray, embeddings: np.ndarray, m: int = 7\n    ) -> np.ndarray:\n        \"\"\"\n        Generate Graph Sylvester Embeddings.\n\n        Args:\n            A (np.ndarray): The adjacency matrix of the graph.\n            embeddings (np.ndarray): The original node embeddings.\n            m (int, optional): The number of spectral scales. Defaults to 7.\n\n        Returns:\n            np.ndarray: The generated Graph Sylvester Embeddings.\n        \"\"\"\n        V = list(range(len(embeddings)))\n        W = A\n\n        G = (V, W)\n        ts = np.linspace(0, 1, m)  # equally spaced scales\n\n        gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\n        return gse_embeddings\n\n    def create_k_hop_index(self, k: int = 2):\n        \"\"\"\n        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\n        aggregated features, and updating the memory index.\n\n        Args:\n            k (int, optional): The number of hops for message passing. Defaults to 2.\n        \"\"\"\n        self.k = k\n        print(\"Computing the adjacency matrix\")\n        print(\"Embeddings shape: \", self.embeddings.shape)\n        self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n        print(\"Computing the k-hop adjacency matrix and aggregated features\")\n        self.A_k, self.node_embeddings = self.k_hop_message_passing(\n            self.A, self.embeddings, k\n        )\n        print(\"Updating the memory index\")\n        self.k_hop_index = MemoryIndex(name=self.name)\n        self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\n\n    @classmethod\n    def from_task_results(cls, task_memory_index):\n        new_memory_kernel = cls(mem_index=task_memory_index)\n\n        # Create a new index for the new MemoryKernel\n        new_memory_kernel.create_k_hop_index()\n\n        return new_memory_kernel\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    mem_index: MemoryIndex,\n    name: str = \"memory_kernel\",\n    k: int = 2,\n    save_path: str = None,\n):\n    \"\"\"\n        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\n\n        Args:\n            mem_index (MemoryIndex): A MemoryIndex instance.\n            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\n            k (int, optional): The number of hops for message passing. Defaults to 2.\n            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\n        \"\"\"\n    super().__init__(\n        index=mem_index.index,\n        values=mem_index.values,\n        embeddings=mem_index.embeddings,\n        name=name,\n        save_path=save_path,\n    )\n    self.k = k\n    if len(self.values) > 0: \n        self.create_k_hop_index(k=k)\n    else:\n        raise ValueError(\"The input MemoryIndex is empty. Please check the input MemoryIndex.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef cos_sim(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n        \"\"\"\n    if not isinstance(a, np.ndarray):\n        a = np.array(a)\n\n    if not isinstance(b, np.ndarray):\n        b = np.array(b)\n\n    if len(a.shape) == 1:\n        a = a[np.newaxis, :]\n\n    if len(b.shape) == 1:\n        b = b[np.newaxis, :]\n\n    a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\n    b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\n    return np.dot(a_norm, b_norm.T)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef compute_kernel(\n    self,\n    embedding_set: np.ndarray,\n    threshold: float = 0.65,\n    use_softmax: bool = False,\n) -> np.ndarray:\n\n    \"\"\"\n        Compute the adjacency matrix of the graph.\n\n        Parameters:\n        embedding_set (numpy array): The embedding matrix of the nodes.\n        threshold (float): The threshold for the adjacency matrix.\n        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\n        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\n\n        Returns:\n        adj_matrix (numpy array): The adjacency matrix of the graph.\n        \"\"\"\n\n    A = self.cos_sim(embedding_set, embedding_set)\n    if use_softmax:\n        # softmax\n        A = np.exp(A)\n        A = A / np.sum(A, axis=1)[:, np.newaxis]\n    adj_matrix = np.zeros_like(A)\n    adj_matrix[A > threshold] = 1\n    adj_matrix[A <= threshold] = 0\n    adj_matrix = adj_matrix.astype(np.float32)\n    return adj_matrix\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef k_hop_message_passing(\n    self, A: np.ndarray, node_features: np.ndarray, k: int\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n        Compute the k-hop adjacency matrix and aggregated features using message passing.\n\n        Parameters:\n        A (numpy array): The adjacency matrix of the graph.\n        node_features (numpy array): The feature matrix of the nodes.\n        k (int): The number of hops for message passing.\n\n        Returns:\n        A_k (numpy array): The k-hop adjacency matrix.\n        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n        \"\"\"\n\n    print(\"Compute the k-hop adjacency matrix\")\n    A_k = np.linalg.matrix_power(A, k)\n\n    print(\"Aggregate the messages from the k-hop neighborhood:\")\n    agg_features = node_features.copy()\n\n    for i in tqdm(range(k)):\n        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n\n    return A_k, agg_features\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef graph_sylvester_embedding(self, G: Tuple, m: int, ts: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\n\n        Args:\n            G (Tuple): A tuple containing the graph's vertices (V) and weights (W).\n            m (int): The number of singular values to consider.\n            ts (np.ndarray): The spectral scales.\n\n        Returns:\n            np.ndarray: The node_embeddings matrix.\n        \"\"\"\n    V, W = G\n    n = len(V)\n    D_BE = np.diag(W.sum(axis=1))\n    L_BE = np.identity(n) - np.dot(\n        np.diag(1 / np.sqrt(D_BE.diagonal())),\n        np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n    )\n\n    A = W\n    B = L_BE\n    C = np.identity(n)\n    X = solve_sylvester(A, B, C)\n\n    U, S, _ = svd(X, full_matrices=False)\n    U_m = U[:, :m]\n    S_m = S[:m]\n\n    node_embeddings = np.zeros((n, m))\n\n    for i in range(n):\n        for s in range(m):\n            # Spectral kernel descriptor\n            node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n\n    return node_embeddings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef gen_gse_embeddings(\n    self, A: np.ndarray, embeddings: np.ndarray, m: int = 7\n) -> np.ndarray:\n    \"\"\"\n        Generate Graph Sylvester Embeddings.\n\n        Args:\n            A (np.ndarray): The adjacency matrix of the graph.\n            embeddings (np.ndarray): The original node embeddings.\n            m (int, optional): The number of spectral scales. Defaults to 7.\n\n        Returns:\n            np.ndarray: The generated Graph Sylvester Embeddings.\n        \"\"\"\n    V = list(range(len(embeddings)))\n    W = A\n\n    G = (V, W)\n    ts = np.linspace(0, 1, m)  # equally spaced scales\n\n    gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\n    return gse_embeddings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef create_k_hop_index(self, k: int = 2):\n    \"\"\"\n        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\n        aggregated features, and updating the memory index.\n\n        Args:\n            k (int, optional): The number of hops for message passing. Defaults to 2.\n        \"\"\"\n    self.k = k\n    print(\"Computing the adjacency matrix\")\n    print(\"Embeddings shape: \", self.embeddings.shape)\n    self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n    print(\"Computing the k-hop adjacency matrix and aggregated features\")\n    self.A_k, self.node_embeddings = self.k_hop_message_passing(\n        self.A, self.embeddings, k\n    )\n    print(\"Updating the memory index\")\n    self.k_hop_index = MemoryIndex(name=self.name)\n    self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@classmethod\ndef from_task_results(cls, task_memory_index):\n    new_memory_kernel = cls(mem_index=task_memory_index)\n\n    # Create a new index for the new MemoryKernel\n    new_memory_kernel.create_k_hop_index()\n\n    return new_memory_kernel\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef calc_shgo_mode(scores: List[float]) -> float:\n    def objective(x):\n        return -estimate_pdf(scores)(x)\n\n    bounds = [(min(scores), max(scores))]\n    result = scipy.optimize.shgo(objective, bounds)\n    return result.x\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def objective(x):\n    return -estimate_pdf(scores)(x)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef estimate_pdf(scores: List[float]) -> callable:\n    pdf = scipy.stats.gaussian_kde(scores)\n    return pdf\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef sort_paths_by_mode_distance(\n    paths, memory_kernel, distance_metric: str = \"cosine\"\n) -> List[List[int]]:\n    sorted_paths = []\n    for i, path in enumerate(paths):\n        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n        cluster_embeddings = np.array(cluster_embeddings)\n        cluster_mean = np.mean(cluster_embeddings, axis=0)\n        if distance_metric == \"cosine\" or distance_metric == \"guassian\":\n            scores = [\n                (i, cosine(cluster_mean, emb))\n                for i, emb in zip(path, cluster_embeddings)\n            ]\n        elif distance_metric == \"euclidean\":\n            scores = [\n                (i, np.linalg.norm(cluster_mean - emb))\n                for i, emb in zip(path, cluster_embeddings)\n            ]\n        score_values = [score for _, score in scores]  # Extract score values\n        mu = calc_shgo_mode(score_values)\n        sigma = np.std(score_values)\n        if distance_metric == \"guassian\":\n            scores = [\n                (i, np.exp(-((x - mu) ** 2) / (2 * sigma**2))) for i, x in scores\n            ]\n        # Sort path by score\n        sorted_path_and_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n        sorted_path = [x[0] for x in sorted_path_and_scores]\n        sorted_paths.append(sorted_path)\n    return sorted_paths\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef sort_paths_by_kernel_density(\n    paths, memory_kernel, distance_metric: str = \"cosine\"\n) -> List[List[int]]:\n    sorted_paths = []\n    for i, path in enumerate(paths):\n        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n        cluster_embeddings = np.array(cluster_embeddings)\n        cluster_mean = np.mean(cluster_embeddings, axis=0)\n        if distance_metric == \"cosine\":\n            scores = [\n                (i, cosine(cluster_mean, emb))\n                for i, emb in zip(path, cluster_embeddings)\n            ]\n        elif distance_metric == \"euclidean\":\n            scores = [\n                (i, np.linalg.norm(cluster_mean - emb))\n                for i, emb in zip(path, cluster_embeddings)\n            ]\n        score_values = [score for _, score in scores]  # Extract score values\n\n        # Estimate PDF using Kernel Density Estimation\n        kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(\n            np.array(score_values).reshape(-1, 1)\n        )\n        kde_scores = [kde.score_samples([[x]])[0] for _, x in scores]\n\n        # Sort path by score\n        sorted_path_and_scores = sorted(\n            zip(path, kde_scores), key=lambda x: x[1], reverse=True\n        )\n        sorted_path = [x[0] for x in sorted_path_and_scores]\n        sorted_paths.append(sorted_path)\n    return sorted_paths\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass ClusterPaths:\n    def create_paths(\n        self, embeddings: np.ndarray, num_clusters: int\n    ) -> List[List[int]]:\n        raise NotImplementedError\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def create_paths(\n    self, embeddings: np.ndarray, num_clusters: int\n) -> List[List[int]]:\n    raise NotImplementedError\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass HDBSCANPaths(ClusterPaths):\n    def create_paths(\n        self, embeddings: np.ndarray, num_clusters: int\n    ) -> List[List[int]]:\n        clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n        cluster_assignments = clusterer.fit_predict(embeddings)\n        paths = [[] for _ in range(num_clusters)]\n        for i, cluster in enumerate(cluster_assignments):\n            paths[cluster].append(i)\n        paths = [path for path in paths if path]\n        return paths\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def create_paths(\n    self, embeddings: np.ndarray, num_clusters: int\n) -> List[List[int]]:\n    clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n    cluster_assignments = clusterer.fit_predict(embeddings)\n    paths = [[] for _ in range(num_clusters)]\n    for i, cluster in enumerate(cluster_assignments):\n        paths[cluster].append(i)\n    paths = [path for path in paths if path]\n    return paths\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass SpectralClusteringPaths(ClusterPaths):\n    def create_paths(\n        self, A: np.ndarray, num_clusters: int\n    ) -> List[List[int]]:\n        n_samples = A.shape[0]\n        n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\n        spectral_clustering = SpectralClustering(\n            n_clusters=num_clusters,\n            affinity=\"precomputed\",\n            n_neighbors=n_neighbors,\n            random_state=42,\n        )\n        cluster_assignments = spectral_clustering.fit_predict(A)\n        paths = [[] for _ in range(num_clusters)]\n        for i, cluster in enumerate(cluster_assignments):\n            paths[cluster].append(i)\n        paths = [path for path in paths if path]\n        return paths\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def create_paths(\n    self, A: np.ndarray, num_clusters: int\n) -> List[List[int]]:\n    n_samples = A.shape[0]\n    n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\n    spectral_clustering = SpectralClustering(\n        n_clusters=num_clusters,\n        affinity=\"precomputed\",\n        n_neighbors=n_neighbors,\n        random_state=42,\n    )\n    cluster_assignments = spectral_clustering.fit_predict(A)\n    paths = [[] for _ in range(num_clusters)]\n    for i, cluster in enumerate(cluster_assignments):\n        paths[cluster].append(i)\n    paths = [path for path in paths if path]\n    return paths\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MultiKernelVisualization:\n    def __init__(self, memory_kernel_group: MultiKernel):\n        self.memory_kernel_group = memory_kernel_group\n        self.memory_kernel_dict = memory_kernel_group.memory_kernel_dict\n        self.memory_kernel_group.generate_path_groups()\n\n    def plot_embeddings_with_path(self, embeddings, title, paths):\n        tsne = TSNE(n_components=2, random_state=42)\n        reduced_embeddings = tsne.fit_transform(embeddings)\n\n        plt.figure(figsize=(10, 8))\n        colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n        for i, path in enumerate(paths):\n            path_embeddings = reduced_embeddings[path]\n            plt.scatter(\n                path_embeddings[:, 0],\n                path_embeddings[:, 1],\n                color=colors[i],\n                label=f\"Cluster {i}\",\n            )\n            for j in range(len(path) - 1):\n                plt.plot(\n                    [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n                    [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n                    color=colors[i],\n                )\n        plt.title(title)\n        plt.legend()\n        plt.show()\n\n    def visualize_paths(self):\n        #loop through memory kernels and print path_group\n        for key, kernel in self.memory_kernel_dict.items():\n            print(f\"Kernel: {key}\")\n            paths = self.memory_kernel_group.path_group[key]\n            print(f\"Path Group: {paths}\")\n            node_embeddings = kernel.node_embeddings\n            self.plot_embeddings_with_path(\n                node_embeddings, f\"Node Embeddings for {key}\", paths\n            )\n    def plot_singular_values(self):\n        #loop through memory kernels and print path_group\n        for key, kernel in self.memory_kernel_dict.items():\n            print(f\"Kernel: {key}\")\n            A_k = kernel.A_k\n            U, S, V = np.linalg.svd(A_k)\n            plt.plot(np.log(S))\n            plt.show()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, memory_kernel_group: MultiKernel):\n    self.memory_kernel_group = memory_kernel_group\n    self.memory_kernel_dict = memory_kernel_group.memory_kernel_dict\n    self.memory_kernel_group.generate_path_groups()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef plot_embeddings_with_path(self, embeddings, title, paths):\n    tsne = TSNE(n_components=2, random_state=42)\n    reduced_embeddings = tsne.fit_transform(embeddings)\n\n    plt.figure(figsize=(10, 8))\n    colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n    for i, path in enumerate(paths):\n        path_embeddings = reduced_embeddings[path]\n        plt.scatter(\n            path_embeddings[:, 0],\n            path_embeddings[:, 1],\n            color=colors[i],\n            label=f\"Cluster {i}\",\n        )\n        for j in range(len(path) - 1):\n            plt.plot(\n                [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n                [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n                color=colors[i],\n            )\n    plt.title(title)\n    plt.legend()\n    plt.show()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visualize_paths(self):\n    #loop through memory kernels and print path_group\n    for key, kernel in self.memory_kernel_dict.items():\n        print(f\"Kernel: {key}\")\n        paths = self.memory_kernel_group.path_group[key]\n        print(f\"Path Group: {paths}\")\n        node_embeddings = kernel.node_embeddings\n        self.plot_embeddings_with_path(\n            node_embeddings, f\"Node Embeddings for {key}\", paths\n        )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def plot_singular_values(self):\n    #loop through memory kernels and print path_group\n    for key, kernel in self.memory_kernel_dict.items():\n        print(f\"Kernel: {key}\")\n        A_k = kernel.A_k\n        U, S, V = np.linalg.svd(A_k)\n        plt.plot(np.log(S))\n        plt.show()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MultiKernelStabilityAnalysis:\n    def __init__(self, memory_kernel_group: MultiKernel):\n        self.memory_kernel_group = memory_kernel_group\n\n    def get_cluster_labels(self, kernel_label: str) -> Tuple[np.ndarray, int]:\n        paths = self.memory_kernel_group.path_group[kernel_label]\n        num_clusters = len(paths)\n        cluster_labels = np.empty(len(self.memory_kernel_group.memory_kernel_dict[kernel_label].node_embeddings), dtype=int)\n\n        for cluster_index, path in enumerate(paths):\n            cluster_labels[path] = cluster_index\n\n        return cluster_labels, num_clusters\n\n    def compute_nmi(self, kernel_label1: str, kernel_label2: str) -> float:\n        cluster_labels1, _ = self.get_cluster_labels(kernel_label1)\n        cluster_labels2, _ = self.get_cluster_labels(kernel_label2)\n        nmi = normalized_mutual_info_score(cluster_labels1, cluster_labels2)\n        return nmi\n\n    def evaluate_stability(self) -> float:\n        kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\n        pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\n        nmi_sum = 0\n\n        for kernel_label1, kernel_label2 in pairwise_combinations:\n            nmi = self.compute_nmi(kernel_label1, kernel_label2)\n            nmi_sum += nmi\n\n        stability_score = nmi_sum / len(pairwise_combinations)\n        return stability_score\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, memory_kernel_group: MultiKernel):\n    self.memory_kernel_group = memory_kernel_group\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_cluster_labels(self, kernel_label: str) -> Tuple[np.ndarray, int]:\n    paths = self.memory_kernel_group.path_group[kernel_label]\n    num_clusters = len(paths)\n    cluster_labels = np.empty(len(self.memory_kernel_group.memory_kernel_dict[kernel_label].node_embeddings), dtype=int)\n\n    for cluster_index, path in enumerate(paths):\n        cluster_labels[path] = cluster_index\n\n    return cluster_labels, num_clusters\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef compute_nmi(self, kernel_label1: str, kernel_label2: str) -> float:\n    cluster_labels1, _ = self.get_cluster_labels(kernel_label1)\n    cluster_labels2, _ = self.get_cluster_labels(kernel_label2)\n    nmi = normalized_mutual_info_score(cluster_labels1, cluster_labels2)\n    return nmi\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef evaluate_stability(self) -> float:\n    kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\n    pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\n    nmi_sum = 0\n\n    for kernel_label1, kernel_label2 in pairwise_combinations:\n        nmi = self.compute_nmi(kernel_label1, kernel_label2)\n        nmi_sum += nmi\n\n    stability_score = nmi_sum / len(pairwise_combinations)\n    return stability_score\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass MultiKernel(MemoryKernel):\n    def __init__(\n        self,\n        memory_kernel_dict: Dict[str, MemoryKernel],\n        name: str = \"memory_kernel_group\",\n    ):\n        \"\"\"\n        Initialize the MultiKernel with a dictionary of MemoryKernel instances.\n\n        Args:\n            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\n            name (str, optional): The name of the MultiKernel. Defaults to \"memory_kernel_group\".\n        \"\"\"\n        self.memory_kernel_dict = memory_kernel_dict\n        self.path_group = {}\n        self.name = name\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    memory_kernel_dict: Dict[str, MemoryKernel],\n    name: str = \"memory_kernel_group\",\n):\n    \"\"\"\n        Initialize the MultiKernel with a dictionary of MemoryKernel instances.\n\n        Args:\n            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\n            name (str, optional): The name of the MultiKernel. Defaults to \"memory_kernel_group\".\n        \"\"\"\n    self.memory_kernel_dict = memory_kernel_dict\n    self.path_group = {}\n    self.name = name\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass HDBSCANMultiKernel(MultiKernel):\n    def __init__(\n        self,\n        memory_kernel_dict: Dict[str, MemoryKernel],\n        name: str = \"memory_kernel_group\",\n    ):\n        super().__init__(memory_kernel_dict, name)\n        self.cluster_paths = HDBSCANPaths()\n\n    def generate_path_groups(self, num_clusters: int = None) -> None:\n        path_group = {}\n        for k, v in self.memory_kernel_dict.items():\n            embeddings = v.node_embeddings\n            if num_clusters is None:\n                num_clusters = int(np.sqrt(len(embeddings)))\n            paths = self.cluster_paths.create_paths(embeddings, num_clusters)\n            path_group[k] = paths\n        self.path_group = path_group\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    memory_kernel_dict: Dict[str, MemoryKernel],\n    name: str = \"memory_kernel_group\",\n):\n    super().__init__(memory_kernel_dict, name)\n    self.cluster_paths = HDBSCANPaths()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef generate_path_groups(self, num_clusters: int = None) -> None:\n    path_group = {}\n    for k, v in self.memory_kernel_dict.items():\n        embeddings = v.node_embeddings\n        if num_clusters is None:\n            num_clusters = int(np.sqrt(len(embeddings)))\n        paths = self.cluster_paths.create_paths(embeddings, num_clusters)\n        path_group[k] = paths\n    self.path_group = path_group\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass SpectralClusteringMultiKernel(MultiKernel):\n    def __init__(\n        self,\n        memory_kernel_dict: Dict[str, MemoryKernel],\n        name: str = \"memory_kernel_group\",\n    ):\n        super().__init__(memory_kernel_dict, name)\n        self.cluster_paths = SpectralClusteringPaths()\n\n    def generate_path_groups(self, num_clusters: int = None) -> None:\n        path_group = {}\n        for k, v in self.memory_kernel_dict.items():\n            A_k = v.A_k\n            if num_clusters is None:\n                num_clusters = int(np.sqrt(len(A_k)))\n            paths = self.cluster_paths.create_paths(A_k, num_clusters)\n            path_group[k] = paths\n        self.path_group = path_group\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    memory_kernel_dict: Dict[str, MemoryKernel],\n    name: str = \"memory_kernel_group\",\n):\n    super().__init__(memory_kernel_dict, name)\n    self.cluster_paths = SpectralClusteringPaths()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef generate_path_groups(self, num_clusters: int = None) -> None:\n    path_group = {}\n    for k, v in self.memory_kernel_dict.items():\n        A_k = v.A_k\n        if num_clusters is None:\n            num_clusters = int(np.sqrt(len(A_k)))\n        paths = self.cluster_paths.create_paths(A_k, num_clusters)\n        path_group[k] = paths\n    self.path_group = path_group\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"class BaseFrame(ABC):\n    def __init__(self,\n                context_columns: List = [],\n                embeddable_columns: List = [],\n                embedding_columns: List = [],\n                name: str = \"base_frame\",\n                save_path: Optional[str] = \"/storage\",\n                text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n                markdown: str = \"text/markdown\",):\n        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n        self.meta_columns = ['ID', 'Name', 'Source', 'Author', 'Created At', 'Last Modified At']\n        self.context_columns = context_columns\n        self.embeddable_columns = embeddable_columns\n        self.embedding_columns = embedding_columns\n        self.name = name\n        self.save_path = save_path\n        self.save_dir = f'{self.save_path}/{self.name}'\n        self.text_embedder = text_embedder\n        self.markdown = markdown\n\n    @abstractmethod\n    def __getattr__(self, name: str):\n        pass\n\n    @abstractmethod\n    def get_overwritten_attr(self):\n        pass\n\n    @abstractmethod\n    def embed_columns(self, embeddable_columns: List):\n        pass\n\n    @abstractmethod\n    def _embed_column(self, column, embedder):\n        pass\n\n    @abstractmethod\n    def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n        pass\n\n    @abstractmethod\n    def search_column_polar(self, query, embeddable_column_name, top_k):\n        pass\n\n    @abstractmethod\n    def save(self):\n        pass\n\n    @classmethod\n    @abstractmethod\n    def load(cls, frame_path, name):\n        pass\n\n    @abstractmethod\n    def generate_column(self, row_generator, new_column_name):\n        pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self,\n            context_columns: List = [],\n            embeddable_columns: List = [],\n            embedding_columns: List = [],\n            name: str = \"base_frame\",\n            save_path: Optional[str] = \"/storage\",\n            text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n            markdown: str = \"text/markdown\",):\n    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n    self.meta_columns = ['ID', 'Name', 'Source', 'Author', 'Created At', 'Last Modified At']\n    self.context_columns = context_columns\n    self.embeddable_columns = embeddable_columns\n    self.embedding_columns = embedding_columns\n    self.name = name\n    self.save_path = save_path\n    self.save_dir = f'{self.save_path}/{self.name}'\n    self.text_embedder = text_embedder\n    self.markdown = markdown\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef __getattr__(self, name: str):\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef get_overwritten_attr(self):\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef embed_columns(self, embeddable_columns: List):\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef _embed_column(self, column, embedder):\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef search_column_polar(self, query, embeddable_column_name, top_k):\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef save(self):\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@classmethod\n@abstractmethod\ndef load(cls, frame_path, name):\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef generate_column(self, row_generator, new_column_name):\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n\nclass CodeFramePydantic(BaseModel):\n    df_path: str\n    context_columns: List\n    embeddable_columns: List\n    embedding_columns: List\n    name: str\n    save_path: Optional[str]\n    save_dir: str\n    text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder\n    markdown: str\n\n    class Config:\n        arbitrary_types_allowed = True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass Config:\n    arbitrary_types_allowed = True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass CodeFrame(BaseFrame):\n    def __init__(self, df: pl.DataFrame, **kwargs):\n        super().__init__(**kwargs)\n        self.df = df\n        self.frame_template = CodeFramePydantic(df_path=f'{self.save_dir}/{self.name}.parquet', context_columns=self.context_columns, embeddable_columns=self.embeddable_columns, embedding_columns=self.embedding_columns, name=self.name, save_path=self.save_path, save_dir=self.save_dir, load=True, text_embedder=self.text_embedder, markdown=self.markdown)\n\n    def __getattr__(self, name: str):\n        if \"df\" in self.__dict__:\n            return getattr(self.df.lazy(), name)\n        raise AttributeError(f\"{self.__class__.__name__} object has no attribute {name}\")\n\n    def get_overwritten_attr(self):\n        df_methods = [method for method in dir(self.df) if callable(getattr(self.df, method))]\n        memory_frame_methods = [method for method in dir(CodeFrame) if callable(getattr(CodeFrame, method))]\n        common_methods = list(set(df_methods) & set(memory_frame_methods))\n        return common_methods\n\n    def tokenize_column(self, column_name: str):\n        new_values = self.tokenizer.encode_batch(self.df[column_name].to_list())\n        new_series = pl.Series(f'tokens|{column_name}', new_values)\n        len_values = [len(x) for x in new_values]\n        new_series_len = pl.Series(f'tokens_len|{column_name}', len_values)\n        self.df = self.df.with_columns(new_series)\n        self.df = self.df.with_columns(new_series_len)\n        return self\n\n    def embed_columns(self, embeddable_columns: List):\n        for column_name in embeddable_columns:\n            column = self.df[column_name]\n            _, embedder = infer_embeddable_type(column)\n            self._embed_column(column, embedder)\n\n    def _embed_column(self, column, embedder):\n        # Add the embeddings as a new column\n        # Generate new values\n        new_values = embedder.embed(self.df[column.name].to_list())\n        # Add new column to DataFrame\n        new_column_name = f'embedding|{column.name}'\n        new_series = pl.Series(new_column_name, new_values)\n        self.df = self.df.with_columns(new_series)\n        self.embedding_columns.append(new_column_name)\n\n    def apply_validator_to_column(self, column_name: str, validator: type):\n        # Ensure the validator is a subclass of BaseModel from Pydantic\n        if not issubclass(validator, BaseModel):\n            raise TypeError('validator must be a subclass of BaseModel from Pydantic')\n        if column_name not in self.df.columns:\n            raise ValueError(f\"Column '{column_name}' does not exist.\")\n        if column_name not in self.embeddable_columns:\n            raise ValueError(f\"Column '{column_name}' is not set to embeddable.\")\n        # Iterate over the specified column\n        for text in self.df[column_name]:\n            # Create a validator instance and validate the text\n            try:\n                _ = validator(text=text).text\n            except Exception as e:\n                raise ValueError(f\"Failed to validate text in column '{column_name}'.\") from e\n\n        return self\n\n    def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n        df = self.df.filter(sql_query)\n        embedding_column_name = 'embedding|' + embeddable_column_name\n\n        query_as_series = pl.Series(query)\n        dot_product_frame = df.with_columns(df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n        # Sort by dot product and select top_k rows\n        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n        return result\n\n    def search_column_polar(self, query, embeddable_column_name, top_k):\n        embedding_column_name = 'embedding|' + embeddable_column_name\n\n        query_as_series = pl.Series(query)\n        dot_product_frame = self.df.with_columns(self.df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n        # Sort by dot product and select top_k rows\n        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n        return result\n\n    def save(self):\n        #create dir in storage if not exists\n        if not os.path.exists(self.save_dir):\n            os.makedirs(self.save_dir)\n        self.full_save_path = f'{self.save_path}/{self.name}/{self.name}.parquet'\n        self.df.write_parquet(self.full_save_path)\n        frame_template_json = self.frame_template.json()\n        with open(f'{self.save_dir}/{self.name}.json', 'w') as f:\n            f.write(frame_template_json)\n\n    @classmethod\n    def load(cls, frame_path, name):\n        df = pl.read_parquet(f'{frame_path}/{name}.parquet')\n        with open(f'{frame_path}/{name}.json', 'r') as f:\n            frame_template = CodeFramePydantic.parse_raw(f.read())\n        return cls(df=df, context_columns=frame_template.context_columns, embeddable_columns=frame_template.embeddable_columns, embedding_columns=frame_template.embedding_columns, name=frame_template.name, save_path=frame_template.save_path, text_embedder=frame_template.text_embedder, markdown=frame_template.markdown)\n\n    def generate_column(self, row_generator, new_column_name):\n        # Generate new values\n        new_values = row_generator.generate(self.df)\n        # Add new column to DataFrame\n        new_df = pl.DataFrame({ new_column_name: new_values })\n        # Concatenate horizontally\n        self.df = self.df.hstack([new_df])\n\n    def apply_visitor_to_column(self, column_name: str, visitor_class: type, new_column_prefix: Optional[str] = None):\n        # Ensure the visitor_class is a subclass of PythonCodeVisitor\n        if not issubclass(visitor_class, cst.CSTVisitor):\n            raise TypeError('visitor_class must be a subclass of PythonCodeVisitor')\n\n        # Iterate over the specified column\n        new_values = []\n        for code in self.df[column_name]:\n            # Create a visitor and apply it to the code\n            visitor = visitor_class(code)\n            new_value = visitor.collect()\n            new_values.append(new_value)\n        # Generate new column\n        new_column_name = f'{column_name}_{new_column_prefix}|{visitor_class.__name__}'\n        new_series = pl.Series(new_column_name, new_values)\n        self.df = self.df.with_columns(new_series)\n\n        return self\n\n    def count_node_types(self, column_name: str, new_column_prefix: str = 'node_count'):\n        for node_type_counter in NODETYPE_COUNTERS:\n            self.apply_visitor_to_column(column_name, globals()[node_type_counter], new_column_prefix)\n        return self\n\n    def count_operators(self, column_name: str, new_column_prefix: str = 'operator_count'):\n        for operator_counter in OPERATOR_COUNTERS:\n            self.apply_visitor_to_column(column_name, globals()[operator_counter], new_column_prefix)\n        return self\n\n\n    def replace_code_in_files(self, filename_column: str, original_code_column: str, replacing_code_column: str):\n        visitor = CodeReplacerVisitor(filename_column, original_code_column, replacing_code_column)\n        for row in self.df.rows():\n            filename = row[filename_column]\n            original_code = row[original_code_column]\n            replacing_code = row[replacing_code_column]\n\n            if filename and original_code and replacing_code and os.path.isfile(filename):\n                node = cst.parse_module(original_code)\n                node.metadata[original_code_column] = original_code\n                node.metadata[replacing_code_column] = replacing_code\n                node.metadata[filename_column] = filename\n\n                modified_node = node.visit(visitor)\n                modified_code = cst.Module(body=modified_node.body).code\n                row[original_code_column] = modified_code\n\n        return self\n\n    @classmethod\n    def from_python(\n        cls,\n        directory_path: str,\n        value_column: str,\n        minify_code: bool = False,\n        remove_docstrings: bool = False,\n        resolution: str = \"both\",\n        embeddings_column: List = [],\n        embeddable_columns: List = [],\n        context_columns: Optional[List[str]] = None,\n        name: str = \"code_frame\",\n        save_path: Optional[str] = \"./storage\",\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n        markdown: str = \"text/markdown\",\n    ) -> \"CodeFrame\":\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n        #convert retrieved data to polars dataframe\n        df = pl.DataFrame({value_column: values})\n        context_df = pl.DataFrame(context)\n        #merge context columns with dataframe\n        df = pl.concat([df, context_df], how='horizontal')\n        if value_column not in embeddable_columns:\n            embeddable_columns.append(value_column)\n\n        kwargs = {\n            \"context_columns\": context_columns,\n            \"embeddable_columns\": embeddable_columns,\n            \"embedding_columns\": embeddings_column,\n            \"name\": name,\n            \"save_path\": save_path,\n            \"text_embedder\": embedder,\n            \"markdown\": markdown\n        }\n        return cls(df, **kwargs)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, df: pl.DataFrame, **kwargs):\n    super().__init__(**kwargs)\n    self.df = df\n    self.frame_template = CodeFramePydantic(df_path=f'{self.save_dir}/{self.name}.parquet', context_columns=self.context_columns, embeddable_columns=self.embeddable_columns, embedding_columns=self.embedding_columns, name=self.name, save_path=self.save_path, save_dir=self.save_dir, load=True, text_embedder=self.text_embedder, markdown=self.markdown)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __getattr__(self, name: str):\n    if \"df\" in self.__dict__:\n        return getattr(self.df.lazy(), name)\n    raise AttributeError(f\"{self.__class__.__name__} object has no attribute {name}\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_overwritten_attr(self):\n    df_methods = [method for method in dir(self.df) if callable(getattr(self.df, method))]\n    memory_frame_methods = [method for method in dir(CodeFrame) if callable(getattr(CodeFrame, method))]\n    common_methods = list(set(df_methods) & set(memory_frame_methods))\n    return common_methods\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef tokenize_column(self, column_name: str):\n    new_values = self.tokenizer.encode_batch(self.df[column_name].to_list())\n    new_series = pl.Series(f'tokens|{column_name}', new_values)\n    len_values = [len(x) for x in new_values]\n    new_series_len = pl.Series(f'tokens_len|{column_name}', len_values)\n    self.df = self.df.with_columns(new_series)\n    self.df = self.df.with_columns(new_series_len)\n    return self\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef embed_columns(self, embeddable_columns: List):\n    for column_name in embeddable_columns:\n        column = self.df[column_name]\n        _, embedder = infer_embeddable_type(column)\n        self._embed_column(column, embedder)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _embed_column(self, column, embedder):\n    # Add the embeddings as a new column\n    # Generate new values\n    new_values = embedder.embed(self.df[column.name].to_list())\n    # Add new column to DataFrame\n    new_column_name = f'embedding|{column.name}'\n    new_series = pl.Series(new_column_name, new_values)\n    self.df = self.df.with_columns(new_series)\n    self.embedding_columns.append(new_column_name)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef apply_validator_to_column(self, column_name: str, validator: type):\n    # Ensure the validator is a subclass of BaseModel from Pydantic\n    if not issubclass(validator, BaseModel):\n        raise TypeError('validator must be a subclass of BaseModel from Pydantic')\n    if column_name not in self.df.columns:\n        raise ValueError(f\"Column '{column_name}' does not exist.\")\n    if column_name not in self.embeddable_columns:\n        raise ValueError(f\"Column '{column_name}' is not set to embeddable.\")\n    # Iterate over the specified column\n    for text in self.df[column_name]:\n        # Create a validator instance and validate the text\n        try:\n            _ = validator(text=text).text\n        except Exception as e:\n            raise ValueError(f\"Failed to validate text in column '{column_name}'.\") from e\n\n    return self\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n    df = self.df.filter(sql_query)\n    embedding_column_name = 'embedding|' + embeddable_column_name\n\n    query_as_series = pl.Series(query)\n    dot_product_frame = df.with_columns(df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n    # Sort by dot product and select top_k rows\n    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n    return result\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef search_column_polar(self, query, embeddable_column_name, top_k):\n    embedding_column_name = 'embedding|' + embeddable_column_name\n\n    query_as_series = pl.Series(query)\n    dot_product_frame = self.df.with_columns(self.df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n    # Sort by dot product and select top_k rows\n    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n    return result\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef save(self):\n    #create dir in storage if not exists\n    if not os.path.exists(self.save_dir):\n        os.makedirs(self.save_dir)\n    self.full_save_path = f'{self.save_path}/{self.name}/{self.name}.parquet'\n    self.df.write_parquet(self.full_save_path)\n    frame_template_json = self.frame_template.json()\n    with open(f'{self.save_dir}/{self.name}.json', 'w') as f:\n        f.write(frame_template_json)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@classmethod\ndef load(cls, frame_path, name):\n    df = pl.read_parquet(f'{frame_path}/{name}.parquet')\n    with open(f'{frame_path}/{name}.json', 'r') as f:\n        frame_template = CodeFramePydantic.parse_raw(f.read())\n    return cls(df=df, context_columns=frame_template.context_columns, embeddable_columns=frame_template.embeddable_columns, embedding_columns=frame_template.embedding_columns, name=frame_template.name, save_path=frame_template.save_path, text_embedder=frame_template.text_embedder, markdown=frame_template.markdown)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef generate_column(self, row_generator, new_column_name):\n    # Generate new values\n    new_values = row_generator.generate(self.df)\n    # Add new column to DataFrame\n    new_df = pl.DataFrame({ new_column_name: new_values })\n    # Concatenate horizontally\n    self.df = self.df.hstack([new_df])\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef apply_visitor_to_column(self, column_name: str, visitor_class: type, new_column_prefix: Optional[str] = None):\n    # Ensure the visitor_class is a subclass of PythonCodeVisitor\n    if not issubclass(visitor_class, cst.CSTVisitor):\n        raise TypeError('visitor_class must be a subclass of PythonCodeVisitor')\n\n    # Iterate over the specified column\n    new_values = []\n    for code in self.df[column_name]:\n        # Create a visitor and apply it to the code\n        visitor = visitor_class(code)\n        new_value = visitor.collect()\n        new_values.append(new_value)\n    # Generate new column\n    new_column_name = f'{column_name}_{new_column_prefix}|{visitor_class.__name__}'\n    new_series = pl.Series(new_column_name, new_values)\n    self.df = self.df.with_columns(new_series)\n\n    return self\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef count_node_types(self, column_name: str, new_column_prefix: str = 'node_count'):\n    for node_type_counter in NODETYPE_COUNTERS:\n        self.apply_visitor_to_column(column_name, globals()[node_type_counter], new_column_prefix)\n    return self\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef count_operators(self, column_name: str, new_column_prefix: str = 'operator_count'):\n    for operator_counter in OPERATOR_COUNTERS:\n        self.apply_visitor_to_column(column_name, globals()[operator_counter], new_column_prefix)\n    return self\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef replace_code_in_files(self, filename_column: str, original_code_column: str, replacing_code_column: str):\n    visitor = CodeReplacerVisitor(filename_column, original_code_column, replacing_code_column)\n    for row in self.df.rows():\n        filename = row[filename_column]\n        original_code = row[original_code_column]\n        replacing_code = row[replacing_code_column]\n\n        if filename and original_code and replacing_code and os.path.isfile(filename):\n            node = cst.parse_module(original_code)\n            node.metadata[original_code_column] = original_code\n            node.metadata[replacing_code_column] = replacing_code\n            node.metadata[filename_column] = filename\n\n            modified_node = node.visit(visitor)\n            modified_code = cst.Module(body=modified_node.body).code\n            row[original_code_column] = modified_code\n\n    return self\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@classmethod\ndef from_python(\n    cls,\n    directory_path: str,\n    value_column: str,\n    minify_code: bool = False,\n    remove_docstrings: bool = False,\n    resolution: str = \"both\",\n    embeddings_column: List = [],\n    embeddable_columns: List = [],\n    context_columns: Optional[List[str]] = None,\n    name: str = \"code_frame\",\n    save_path: Optional[str] = \"./storage\",\n    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n    markdown: str = \"text/markdown\",\n) -> \"CodeFrame\":\n    values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n    logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n    #convert retrieved data to polars dataframe\n    df = pl.DataFrame({value_column: values})\n    context_df = pl.DataFrame(context)\n    #merge context columns with dataframe\n    df = pl.concat([df, context_df], how='horizontal')\n    if value_column not in embeddable_columns:\n        embeddable_columns.append(value_column)\n\n    kwargs = {\n        \"context_columns\": context_columns,\n        \"embeddable_columns\": embeddable_columns,\n        \"embedding_columns\": embeddings_column,\n        \"name\": name,\n        \"save_path\": save_path,\n        \"text_embedder\": embedder,\n        \"markdown\": markdown\n    }\n    return cls(df, **kwargs)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"class MemoryFrame:\n    def __init__(self, df: pl.DataFrame,\n                context_columns: List = [],\n                embeddable_columns: List = [],\n                time_series_columns: List = [],\n                name: str = \"memory_frame\",\n                save_path: Optional[str] = None,\n                load: bool = False,\n                text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n                markdown: str = \"text/markdown\",):\n        self.df = df\n        self.context_columns = context_columns\n        self.time_series_columns = time_series_columns\n        self.embeddable_columns = embeddable_columns\n        self.meta_columns = ['ID', 'Name', 'Source', 'Author', 'Created At', 'Last Modified At']\n        self.embedding_columns = []\n        self.name = name\n        self.save_path = save_path\n        self.load = load\n        self.text_embedder = text_embedder\n        self.markdown = markdown\n\n\n    def __getattr__(self, name: str):\n        # delegate to the self.df object\n        return getattr(self.df, name)\n\n    def get_overwritten_attr(self):\n        df_methods = [method for method in dir(self.df) if callable(getattr(self.df, method))]\n        memory_frame_methods = [method for method in dir(MemoryFrame) if callable(getattr(MemoryFrame, method))]\n        common_methods = list(set(df_methods) & set(memory_frame_methods))\n        return common_methods\n\n    def embed_columns(self, embeddable_columns: List):\n        for column_name in embeddable_columns:\n            column = self.df[column_name]\n            _, embedder = infer_embeddable_type(column)\n            self._embed_column(column, embedder)\n\n    def _embed_column(self, column, embedder):\n        # Add the embeddings as a new column\n        # Generate new values\n        new_values = embedder.embed(self.df[column.name].to_list())\n        # Add new column to DataFrame\n        new_column_name = f'embedding|{column.name}'\n        new_series = pl.Series(new_column_name, new_values)\n        self.df = self.df.with_columns(new_series)\n        self.embedding_columns.append(new_column_name)\n\n\n    def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n        df = self.df.filter(sql_query)\n        embedding_column_name = 'embedding|' + embeddable_column_name\n\n        query_as_series = pl.Series(query)\n        dot_product_frame = df.with_columns(df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n        # Sort by dot product and select top_k rows\n        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n        return result\n\n\n    def search_column_polar(self, query, embeddable_column_name, top_k):\n        embedding_column_name = 'embedding|' + embeddable_column_name\n\n        query_as_series = pl.Series(query)\n        dot_product_frame = self.df.with_columns(self.df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n        # Sort by dot product and select top_k rows\n        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n        return result\n\n    def search_column_numpy(self, query, embeddable_column_name, top_k):\n        embedding_column_name = 'embedding|' + embeddable_column_name\n        #convert query and column to numpy arrays\n        column_np = self.df[embedding_column_name].to_numpy()\n        #calculate dot product\n        dot_product = np.dot(column_np, query)\n        #add dot products as column to dataframe\n        dot_product_frame = self.df.with_columns(dot_product)\n        # Sort by dot product and select top_k rows\n        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n        return result\n\n    def save_parquet(self):\n        #save to arrow\n        self.full_save_path = self.save_path + self.name + '.parquet'\n        self.df.write_parquet(self.full_save_path)\n\n    def load_parquet(self):\n        self.full_save_path = self.save_path + self.name + '.parquet'\n        self.df = pl.read_parquet(self.full_save_path)\n\n\n    def search_time_series_column(self, query, embeddable_column_name, top_k):\n        ## uses dtw to match any sub-sequence of the query to the time series in the column\n        ## time series column have a date o time or delta time column associated with them \n        ## each row-value is a list of across rows variable length for both the time series and the date or time column\n        pass\n\n    def generate_column(self, row_generator, new_column_name):\n        # Generate new values\n        new_values = row_generator.generate(self.df)\n        # Add new column to DataFrame\n        new_df = pl.DataFrame({ new_column_name: new_values })\n\n        # Concatenate horizontally\n        self.df = self.df.hstack([new_df])\n    \n    def create_stratas(self):\n        \"\"\"\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\n        \"\"\"\n        pass\n\n    def _create_strata(self, column_name: str):\n        \"\"\"\n        Determine the correct strata creation function to call based on the column's data type,\n        and then calls the corresponding function.\n\n        Args:\n            column_name (str): The name of the column.\n        \"\"\"\n        pass\n\n    def _create_strata_from_categorical(self, column_name: str):\n        \"\"\"\n        Create strata for a categorical column.\n\n        Args:\n            column_name (str): The name of the column.\n        \"\"\"\n        pass\n\n    def _create_strata_from_real(self, column_name: str):\n        \"\"\"\n        Create strata for a real valued column.\n\n        Args:\n            column_name (str): The name of the column.\n        \"\"\"\n        pass\n\n    def _create_strata_from_embeddings(self, column_name: str):\n        \"\"\"\n        Create strata for a column with embeddings.\n\n        Args:\n            column_name (str): The name of the column.\n        \"\"\"\n        pass\n\n    def _create_strata_from_episodic_time_series(self, column_name: str):\n        \"\"\"\n        Create strata for a column with episodic time series.\n\n        Args:\n            column_name (str): The name of the column.\n        \"\"\"\n        pass\n\n    def create_joint_strata(self, column_names: list):\n        \"\"\"\n        Create strata based on the unique combinations of values across given columns.\n        \n        Args:\n            column_names (list): The names of the columns.\n        \"\"\"\n        pass\n\n    def stratified_sampling(self, strata_columns: list, n_samples: int):\n        \"\"\"\n        Perform stratified sampling on given stratum columns.\n        \n        Args:\n            strata_columns (list): The names of the stratum columns.\n            n_samples (int): The number of samples to draw.\n        \"\"\"\n        pass\n\n    def stratified_cross_validation(self, strata_columns: list, n_folds: int):\n        \"\"\"\n        Perform stratified cross-validation on given stratum columns.\n        \n        Args:\n            strata_columns (list): The names of the stratum columns.\n            n_folds (int): The number of folds for the cross-validation.\n        \"\"\"\n        pass\n\n    @classmethod\n    def from_hf_dataset(\n        cls,\n        dataset_url: str,\n        value_column: str,\n        data_split: str = \"train\",\n        embeddings_column: Optional[str] = None,\n        embeddable_columns: List = [],\n        context_columns: Optional[List[str]] = None,\n        time_series_columns: List = [],\n        name: str = \"memory_frame\",\n        save_path: Optional[str] = None,\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n        markdown: str = \"text/markdown\",\n        token_overflow_strategy: str = \"ignore\",\n    ) -> \"MemoryFrame\":\n        dataset = load_dataset(dataset_url)[data_split]\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\n        if context_columns is not None:\n            context = get_context_from_hf(dataset, context_columns)\n        else:\n            context = None\n        #convert retrieved data to polars dataframe\n        if embeddings is not None:\n            df = pl.DataFrame({value_column: values, embeddings_column: embeddings})\n        else:\n            df = pl.DataFrame({value_column: values})\n        context_df = pl.DataFrame(context)\n        #merge context columns with dataframe\n        df = pl.concat([df, context_df], how='horizontal')\n        if value_column not in embeddable_columns:\n            embeddable_columns.append(value_column)\n        return cls(df, context_columns, embeddable_columns, time_series_columns, name, save_path, embedder, markdown, token_overflow_strategy)\n\n    @classmethod\n    def from_python(\n        cls,\n        directory_path: str,\n        value_column: str,\n        minify_code: bool = False,\n        remove_docstrings: bool = False,\n        resolution: str = \"both\",\n        embeddings_column: Optional[str] = None,\n        embeddable_columns: List = [],\n        context_columns: Optional[List[str]] = None,\n        time_series_columns: List = [],\n        name: str = \"memory_frame\",\n        save_path: Optional[str] = None,\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n        markdown: str = \"text/markdown\",\n        token_overflow_strategy: str = \"ignore\",\n    ) -> \"MemoryFrame\":\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n        #convert retrieved data to polars dataframe\n        df = pl.DataFrame({value_column: values})\n        context_df = pl.DataFrame(context)\n        #merge context columns with dataframe\n        df = pl.concat([df, context_df], how='horizontal')\n        if value_column not in embeddable_columns:\n            embeddable_columns.append(value_column)\n        return cls(df, context_columns, embeddable_columns, time_series_columns, name, save_path, embedder, markdown, token_overflow_strategy)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, df: pl.DataFrame,\n            context_columns: List = [],\n            embeddable_columns: List = [],\n            time_series_columns: List = [],\n            name: str = \"memory_frame\",\n            save_path: Optional[str] = None,\n            load: bool = False,\n            text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n            markdown: str = \"text/markdown\",):\n    self.df = df\n    self.context_columns = context_columns\n    self.time_series_columns = time_series_columns\n    self.embeddable_columns = embeddable_columns\n    self.meta_columns = ['ID', 'Name', 'Source', 'Author', 'Created At', 'Last Modified At']\n    self.embedding_columns = []\n    self.name = name\n    self.save_path = save_path\n    self.load = load\n    self.text_embedder = text_embedder\n    self.markdown = markdown\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef __getattr__(self, name: str):\n    # delegate to the self.df object\n    return getattr(self.df, name)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_overwritten_attr(self):\n    df_methods = [method for method in dir(self.df) if callable(getattr(self.df, method))]\n    memory_frame_methods = [method for method in dir(MemoryFrame) if callable(getattr(MemoryFrame, method))]\n    common_methods = list(set(df_methods) & set(memory_frame_methods))\n    return common_methods\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef embed_columns(self, embeddable_columns: List):\n    for column_name in embeddable_columns:\n        column = self.df[column_name]\n        _, embedder = infer_embeddable_type(column)\n        self._embed_column(column, embedder)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _embed_column(self, column, embedder):\n    # Add the embeddings as a new column\n    # Generate new values\n    new_values = embedder.embed(self.df[column.name].to_list())\n    # Add new column to DataFrame\n    new_column_name = f'embedding|{column.name}'\n    new_series = pl.Series(new_column_name, new_values)\n    self.df = self.df.with_columns(new_series)\n    self.embedding_columns.append(new_column_name)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n    df = self.df.filter(sql_query)\n    embedding_column_name = 'embedding|' + embeddable_column_name\n\n    query_as_series = pl.Series(query)\n    dot_product_frame = df.with_columns(df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n    # Sort by dot product and select top_k rows\n    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n    return result\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef search_column_polar(self, query, embeddable_column_name, top_k):\n    embedding_column_name = 'embedding|' + embeddable_column_name\n\n    query_as_series = pl.Series(query)\n    dot_product_frame = self.df.with_columns(self.df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n    # Sort by dot product and select top_k rows\n    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n    return result\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef search_column_numpy(self, query, embeddable_column_name, top_k):\n    embedding_column_name = 'embedding|' + embeddable_column_name\n    #convert query and column to numpy arrays\n    column_np = self.df[embedding_column_name].to_numpy()\n    #calculate dot product\n    dot_product = np.dot(column_np, query)\n    #add dot products as column to dataframe\n    dot_product_frame = self.df.with_columns(dot_product)\n    # Sort by dot product and select top_k rows\n    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n    return result\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef save_parquet(self):\n    #save to arrow\n    self.full_save_path = self.save_path + self.name + '.parquet'\n    self.df.write_parquet(self.full_save_path)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef load_parquet(self):\n    self.full_save_path = self.save_path + self.name + '.parquet'\n    self.df = pl.read_parquet(self.full_save_path)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef search_time_series_column(self, query, embeddable_column_name, top_k):\n    ## uses dtw to match any sub-sequence of the query to the time series in the column\n    ## time series column have a date o time or delta time column associated with them \n    ## each row-value is a list of across rows variable length for both the time series and the date or time column\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef generate_column(self, row_generator, new_column_name):\n    # Generate new values\n    new_values = row_generator.generate(self.df)\n    # Add new column to DataFrame\n    new_df = pl.DataFrame({ new_column_name: new_values })\n\n    # Concatenate horizontally\n    self.df = self.df.hstack([new_df])\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef create_stratas(self):\n    \"\"\"\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\n        \"\"\"\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _create_strata(self, column_name: str):\n    \"\"\"\n        Determine the correct strata creation function to call based on the column's data type,\n        and then calls the corresponding function.\n\n        Args:\n            column_name (str): The name of the column.\n        \"\"\"\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _create_strata_from_categorical(self, column_name: str):\n    \"\"\"\n        Create strata for a categorical column.\n\n        Args:\n            column_name (str): The name of the column.\n        \"\"\"\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _create_strata_from_real(self, column_name: str):\n    \"\"\"\n        Create strata for a real valued column.\n\n        Args:\n            column_name (str): The name of the column.\n        \"\"\"\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _create_strata_from_embeddings(self, column_name: str):\n    \"\"\"\n        Create strata for a column with embeddings.\n\n        Args:\n            column_name (str): The name of the column.\n        \"\"\"\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _create_strata_from_episodic_time_series(self, column_name: str):\n    \"\"\"\n        Create strata for a column with episodic time series.\n\n        Args:\n            column_name (str): The name of the column.\n        \"\"\"\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef create_joint_strata(self, column_names: list):\n    \"\"\"\n        Create strata based on the unique combinations of values across given columns.\n        \n        Args:\n            column_names (list): The names of the columns.\n        \"\"\"\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef stratified_sampling(self, strata_columns: list, n_samples: int):\n    \"\"\"\n        Perform stratified sampling on given stratum columns.\n        \n        Args:\n            strata_columns (list): The names of the stratum columns.\n            n_samples (int): The number of samples to draw.\n        \"\"\"\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef stratified_cross_validation(self, strata_columns: list, n_folds: int):\n    \"\"\"\n        Perform stratified cross-validation on given stratum columns.\n        \n        Args:\n            strata_columns (list): The names of the stratum columns.\n            n_folds (int): The number of folds for the cross-validation.\n        \"\"\"\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@classmethod\ndef from_hf_dataset(\n    cls,\n    dataset_url: str,\n    value_column: str,\n    data_split: str = \"train\",\n    embeddings_column: Optional[str] = None,\n    embeddable_columns: List = [],\n    context_columns: Optional[List[str]] = None,\n    time_series_columns: List = [],\n    name: str = \"memory_frame\",\n    save_path: Optional[str] = None,\n    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n    markdown: str = \"text/markdown\",\n    token_overflow_strategy: str = \"ignore\",\n) -> \"MemoryFrame\":\n    dataset = load_dataset(dataset_url)[data_split]\n    values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\n    if context_columns is not None:\n        context = get_context_from_hf(dataset, context_columns)\n    else:\n        context = None\n    #convert retrieved data to polars dataframe\n    if embeddings is not None:\n        df = pl.DataFrame({value_column: values, embeddings_column: embeddings})\n    else:\n        df = pl.DataFrame({value_column: values})\n    context_df = pl.DataFrame(context)\n    #merge context columns with dataframe\n    df = pl.concat([df, context_df], how='horizontal')\n    if value_column not in embeddable_columns:\n        embeddable_columns.append(value_column)\n    return cls(df, context_columns, embeddable_columns, time_series_columns, name, save_path, embedder, markdown, token_overflow_strategy)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@classmethod\ndef from_python(\n    cls,\n    directory_path: str,\n    value_column: str,\n    minify_code: bool = False,\n    remove_docstrings: bool = False,\n    resolution: str = \"both\",\n    embeddings_column: Optional[str] = None,\n    embeddable_columns: List = [],\n    context_columns: Optional[List[str]] = None,\n    time_series_columns: List = [],\n    name: str = \"memory_frame\",\n    save_path: Optional[str] = None,\n    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n    markdown: str = \"text/markdown\",\n    token_overflow_strategy: str = \"ignore\",\n) -> \"MemoryFrame\":\n    values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n    logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n    #convert retrieved data to polars dataframe\n    df = pl.DataFrame({value_column: values})\n    context_df = pl.DataFrame(context)\n    #merge context columns with dataframe\n    df = pl.concat([df, context_df], how='horizontal')\n    if value_column not in embeddable_columns:\n        embeddable_columns.append(value_column)\n    return cls(df, context_columns, embeddable_columns, time_series_columns, name, save_path, embedder, markdown, token_overflow_strategy)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass FunctionCallCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.function_call_count = 0\n\n    def visit_Call(self, node: cst.Call) -> bool:\n        self.function_call_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.function_call_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.function_call_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Call(self, node: cst.Call) -> bool:\n    self.function_call_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.function_call_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Argument Type Counter\nclass ArgumentTypeCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.argument_type_count = 0\n\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n        self.argument_type_count += len(node.params.params)\n        return True\n\n    def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n        for stmt in node.body.body:\n            if isinstance(stmt, cst.FunctionDef):\n                self.argument_type_count += len(stmt.params.params)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.argument_type_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.argument_type_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n    self.argument_type_count += len(node.params.params)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ClassDef(self, node: cst.ClassDef) -> bool:\n    for stmt in node.body.body:\n        if isinstance(stmt, cst.FunctionDef):\n            self.argument_type_count += len(stmt.params.params)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.argument_type_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Import Counter\nclass ImportCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.import_count = 0\n\n    def visit_Import(self, node: cst.Import) -> bool:\n        self.import_count += 1\n        return True\n\n    def visit_ImportFrom(self, node: cst.ImportFrom) -> bool:\n        self.import_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.import_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.import_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Import(self, node: cst.Import) -> bool:\n    self.import_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ImportFrom(self, node: cst.ImportFrom) -> bool:\n    self.import_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.import_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# If Statement Counter\nclass IfStatementCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.if_statement_count = 0\n\n    def visit_If(self, node: cst.If) -> bool:\n        self.if_statement_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.if_statement_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.if_statement_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_If(self, node: cst.If) -> bool:\n    self.if_statement_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.if_statement_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Base Compound Statement Counter\nclass BaseCompoundStatementCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.compound_statement_count = 0\n\n    def visit_BaseCompoundStatement(self, node: cst.BaseCompoundStatement) -> bool:\n        self.compound_statement_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.compound_statement_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.compound_statement_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BaseCompoundStatement(self, node: cst.BaseCompoundStatement) -> bool:\n    self.compound_statement_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.compound_statement_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# For Loop Counter\nclass ForLoopCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.for_loop_count = 0\n\n    def visit_For(self, node: cst.For) -> bool:\n        self.for_loop_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.for_loop_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.for_loop_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_For(self, node: cst.For) -> bool:\n    self.for_loop_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.for_loop_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# While Loop Counter\nclass WhileLoopCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.while_loop_count = 0\n\n    def visit_While(self, node: cst.While) -> bool:\n        self.while_loop_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.while_loop_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.while_loop_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_While(self, node: cst.While) -> bool:\n    self.while_loop_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.while_loop_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Try Except Counter\nclass TryExceptCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.try_except_count = 0\n\n    def visit_Try(self, node: cst.Try) -> bool:\n        self.try_except_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.try_except_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.try_except_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Try(self, node: cst.Try) -> bool:\n    self.try_except_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.try_except_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# With Statement Counter\nclass WithStatementCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.with_statement_count = 0\n\n    def visit_With(self, node: cst.With) -> bool:\n        self.with_statement_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.with_statement_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.with_statement_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_With(self, node: cst.With) -> bool:\n    self.with_statement_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.with_statement_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Lambda Function Counter\nclass LambdaFunctionCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.lambda_function_count = 0\n\n    def visit_Lambda(self, node: cst.Lambda) -> bool:\n        self.lambda_function_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.lambda_function_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.lambda_function_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Lambda(self, node: cst.Lambda) -> bool:\n    self.lambda_function_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.lambda_function_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Global Statement Counter\nclass GlobalStatementCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.global_statement_count = 0\n\n    def visit_Global(self, node: cst.Global) -> bool:\n        self.global_statement_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.global_statement_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.global_statement_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Global(self, node: cst.Global) -> bool:\n    self.global_statement_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.global_statement_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Nonlocal Statement Counter\nclass NonlocalStatementCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.nonlocal_statement_count = 0\n\n    def visit_Nonlocal(self, node: cst.Nonlocal) -> bool:\n        self.nonlocal_statement_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.nonlocal_statement_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.nonlocal_statement_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Nonlocal(self, node: cst.Nonlocal) -> bool:\n    self.nonlocal_statement_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.nonlocal_statement_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ListComprehensionCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.list_comprehension_count = 0\n\n    def visit_ListComp(self, node: cst.ListComp) -> bool:\n        self.list_comprehension_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.list_comprehension_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.list_comprehension_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ListComp(self, node: cst.ListComp) -> bool:\n    self.list_comprehension_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.list_comprehension_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass DictComprehensionCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.dict_comprehension_count = 0\n\n    def visit_DictComp(self, node: cst.DictComp) -> bool:\n        self.dict_comprehension_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.dict_comprehension_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.dict_comprehension_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_DictComp(self, node: cst.DictComp) -> bool:\n    self.dict_comprehension_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.dict_comprehension_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass SetComprehensionCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.set_comprehension_count = 0\n\n    def visit_SetComp(self, node: cst.SetComp) -> bool:\n        self.set_comprehension_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.set_comprehension_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.set_comprehension_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_SetComp(self, node: cst.SetComp) -> bool:\n    self.set_comprehension_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.set_comprehension_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass GeneratorExpressionCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.generator_expression_count = 0\n\n    def visit_GeneratorExp(self, node: cst.GeneratorExp) -> bool:\n        self.generator_expression_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.generator_expression_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.generator_expression_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_GeneratorExp(self, node: cst.GeneratorExp) -> bool:\n    self.generator_expression_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.generator_expression_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass YieldCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.yield_count = 0\n\n    def visit_Yield(self, node: cst.Yield) -> bool:\n        self.yield_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.yield_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.yield_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Yield(self, node: cst.Yield) -> bool:\n    self.yield_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.yield_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass AwaitCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.await_count = 0\n\n    def visit_Await(self, node: cst.Await) -> bool:\n        self.await_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.await_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.await_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Await(self, node: cst.Await) -> bool:\n    self.await_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.await_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ReturnCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.return_count = 0\n\n    def visit_Return(self, node: cst.Return) -> bool:\n        self.return_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.return_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.return_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Return(self, node: cst.Return) -> bool:\n    self.return_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.return_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BreakCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.break_count = 0\n\n    def visit_Break(self, node: cst.Break) -> bool:\n        self.break_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.break_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.break_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Break(self, node: cst.Break) -> bool:\n    self.break_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.break_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ContinueCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.continue_count = 0\n\n    def visit_Continue(self, node: cst.Continue) -> bool:\n        self.continue_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.continue_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.continue_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Continue(self, node: cst.Continue) -> bool:\n    self.continue_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.continue_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass RaiseCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.raise_count = 0\n\n    def visit_Raise(self, node: cst.Raise) -> bool:\n        self.raise_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.raise_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.raise_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Raise(self, node: cst.Raise) -> bool:\n    self.raise_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.raise_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass AssertCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.assert_count = 0\n\n    def visit_Assert(self, node: cst.Assert) -> bool:\n        self.assert_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.assert_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.assert_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Assert(self, node: cst.Assert) -> bool:\n    self.assert_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.assert_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass PassCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.pass_count = 0\n\n    def visit_Pass(self, node: cst.Pass) -> bool:\n        self.pass_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.pass_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.pass_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Pass(self, node: cst.Pass) -> bool:\n    self.pass_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.pass_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Unary Operators\nclass UnaryOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.unary_operators = []\n\n    def visit_UnaryOperation(self, node: cst.UnaryOperation) -> bool:\n        if isinstance(node.operator, cst.BitInvert) or isinstance(node.operator, cst.Minus) or \\\n                isinstance(node.operator, cst.Not) or isinstance(node.operator, cst.Plus):\n            self.unary_operators.append(node.operator.__class__.__name__)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.unary_operators\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.unary_operators = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_UnaryOperation(self, node: cst.UnaryOperation) -> bool:\n    if isinstance(node.operator, cst.BitInvert) or isinstance(node.operator, cst.Minus) or \\\n                isinstance(node.operator, cst.Not) or isinstance(node.operator, cst.Plus):\n        self.unary_operators.append(node.operator.__class__.__name__)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.unary_operators\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Boolean Operators\nclass BooleanOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.boolean_operators = []\n\n    def visit_BooleanOperation(self, node: cst.BooleanOperation) -> bool:\n        if isinstance(node.operator, cst.And) or isinstance(node.operator, cst.Or):\n            self.boolean_operators.append(node.operator.__class__.__name__)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.boolean_operators\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.boolean_operators = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BooleanOperation(self, node: cst.BooleanOperation) -> bool:\n    if isinstance(node.operator, cst.And) or isinstance(node.operator, cst.Or):\n        self.boolean_operators.append(node.operator.__class__.__name__)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.boolean_operators\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Binary Operators\nclass BinaryOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.binary_operators = []\n\n    def visit_BinaryOperation(self, node: cst.BinaryOperation) -> bool:\n        if isinstance(node.operator, cst.Add) or isinstance(node.operator, cst.BitAnd) or \\\n                isinstance(node.operator, cst.BitOr) or isinstance(node.operator, cst.BitXor) or \\\n                isinstance(node.operator, cst.Divide) or isinstance(node.operator, cst.FloorDivide) or \\\n                isinstance(node.operator, cst.LeftShift) or isinstance(node.operator, cst.MatrixMultiply) or \\\n                isinstance(node.operator, cst.Modulo) or isinstance(node.operator, cst.Multiply) or \\\n                isinstance(node.operator, cst.Power) or isinstance(node.operator, cst.RightShift) or \\\n                isinstance(node.operator, cst.Subtract):\n            self.binary_operators.append(node.operator.__class__.__name__)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.binary_operators\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.binary_operators = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BinaryOperation(self, node: cst.BinaryOperation) -> bool:\n    if isinstance(node.operator, cst.Add) or isinstance(node.operator, cst.BitAnd) or \\\n                isinstance(node.operator, cst.BitOr) or isinstance(node.operator, cst.BitXor) or \\\n                isinstance(node.operator, cst.Divide) or isinstance(node.operator, cst.FloorDivide) or \\\n                isinstance(node.operator, cst.LeftShift) or isinstance(node.operator, cst.MatrixMultiply) or \\\n                isinstance(node.operator, cst.Modulo) or isinstance(node.operator, cst.Multiply) or \\\n                isinstance(node.operator, cst.Power) or isinstance(node.operator, cst.RightShift) or \\\n                isinstance(node.operator, cst.Subtract):\n        self.binary_operators.append(node.operator.__class__.__name__)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.binary_operators\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Comparison Operators\nclass ComparisonOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.comparison_operators = []\n\n    def visit_Comparison(self, node: cst.Comparison) -> bool:\n        for operator in node.operators:\n            if isinstance(operator, cst.Equal) or isinstance(operator, cst.GreaterThan) or \\\n                    isinstance(operator, cst.GreaterThanEqual) or isinstance(operator, cst.In) or \\\n                    isinstance(operator, cst.Is) or isinstance(operator, cst.LessThan) or \\\n                    isinstance(operator, cst.LessThanEqual) or isinstance(operator, cst.NotEqual) or \\\n                    isinstance(operator, cst.IsNot) or isinstance(operator, cst.NotIn):\n                self.comparison_operators.append(operator.__class__.__name__)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.comparison_operators\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.comparison_operators = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Comparison(self, node: cst.Comparison) -> bool:\n    for operator in node.operators:\n        if isinstance(operator, cst.Equal) or isinstance(operator, cst.GreaterThan) or \\\n                    isinstance(operator, cst.GreaterThanEqual) or isinstance(operator, cst.In) or \\\n                    isinstance(operator, cst.Is) or isinstance(operator, cst.LessThan) or \\\n                    isinstance(operator, cst.LessThanEqual) or isinstance(operator, cst.NotEqual) or \\\n                    isinstance(operator, cst.IsNot) or isinstance(operator, cst.NotIn):\n            self.comparison_operators.append(operator.__class__.__name__)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.comparison_operators\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Augmented Assignment Operators\nclass AugmentedAssignmentOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.augmented_assignment_operators = []\n\n    def visit_AugAssign(self, node: cst.AugAssign) -> bool:\n        if isinstance(node.operator, cst.AddAssign) or isinstance(node.operator, cst.BitAndAssign) or \\\n                isinstance(node.operator, cst.BitOrAssign) or isinstance(node.operator, cst.BitXorAssign) or \\\n                isinstance(node.operator, cst.DivideAssign) or isinstance(node.operator, cst.FloorDivideAssign) or \\\n                isinstance(node.operator, cst.LeftShiftAssign) or isinstance(node.operator, cst.MatrixMultiplyAssign) or \\\n                isinstance(node.operator, cst.ModuloAssign) or isinstance(node.operator, cst.MultiplyAssign) or \\\n                isinstance(node.operator, cst.PowerAssign) or isinstance(node.operator, cst.RightShiftAssign) or \\\n                isinstance(node.operator, cst.SubtractAssign):\n            self.augmented_assignment_operators.append(node.operator.__class__.__name__)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.augmented_assignment_operators\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.augmented_assignment_operators = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_AugAssign(self, node: cst.AugAssign) -> bool:\n    if isinstance(node.operator, cst.AddAssign) or isinstance(node.operator, cst.BitAndAssign) or \\\n                isinstance(node.operator, cst.BitOrAssign) or isinstance(node.operator, cst.BitXorAssign) or \\\n                isinstance(node.operator, cst.DivideAssign) or isinstance(node.operator, cst.FloorDivideAssign) or \\\n                isinstance(node.operator, cst.LeftShiftAssign) or isinstance(node.operator, cst.MatrixMultiplyAssign) or \\\n                isinstance(node.operator, cst.ModuloAssign) or isinstance(node.operator, cst.MultiplyAssign) or \\\n                isinstance(node.operator, cst.PowerAssign) or isinstance(node.operator, cst.RightShiftAssign) or \\\n                isinstance(node.operator, cst.SubtractAssign):\n        self.augmented_assignment_operators.append(node.operator.__class__.__name__)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.augmented_assignment_operators\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Miscellaneous Operators\nclass MiscellaneousOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.miscellaneous_operators = []\n\n    def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n        self.miscellaneous_operators.append(cst.Module([node]).code)\n        return True\n\n    def visit_Colon(self, node: cst.Colon) -> bool:\n        self.miscellaneous_operators.append(cst.Module([node]).code)\n        return True\n\n    def visit_Comma(self, node: cst.Comma) -> bool:\n        self.miscellaneous_operators.append(cst.Module([node]).code)\n        return True\n\n    def visit_Dot(self, node: cst.Dot) -> bool:\n        self.miscellaneous_operators.append(cst.Module([node]).code)\n        return True\n\n    def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n        self.miscellaneous_operators.append(cst.Module([node]).code)\n        return True\n\n    def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n        self.miscellaneous_operators.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.miscellaneous_operators\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.miscellaneous_operators = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n    self.miscellaneous_operators.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Colon(self, node: cst.Colon) -> bool:\n    self.miscellaneous_operators.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Comma(self, node: cst.Comma) -> bool:\n    self.miscellaneous_operators.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Dot(self, node: cst.Dot) -> bool:\n    self.miscellaneous_operators.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ImportStar(self, node: cst.ImportStar) -> bool:\n    self.miscellaneous_operators.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Semicolon(self, node: cst.Semicolon) -> bool:\n    self.miscellaneous_operators.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.miscellaneous_operators\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# Unary Operators\nclass BitInvertOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_invert_count = []\n\n    def visit_BitInvert(self, node: cst.BitInvert) -> bool:\n        self.bit_invert_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_invert_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_invert_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitInvert(self, node: cst.BitInvert) -> bool:\n    self.bit_invert_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_invert_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MinusOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.minus_count = []\n\n    def visit_Minus(self, node: cst.Minus) -> bool:\n        self.minus_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.minus_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.minus_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Minus(self, node: cst.Minus) -> bool:\n    self.minus_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.minus_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass NotOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.not_count = []\n\n    def visit_Not(self, node: cst.Not) -> bool:\n        self.not_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.not_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.not_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Not(self, node: cst.Not) -> bool:\n    self.not_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.not_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass PlusOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.plus_count = []\n\n    def visit_Plus(self, node: cst.Plus) -> bool:\n        self.plus_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.plus_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.plus_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Plus(self, node: cst.Plus) -> bool:\n    self.plus_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.plus_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# Boolean Operators\nclass AndOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.and_count = []\n\n    def visit_And(self, node: cst.And) -> bool:\n        self.and_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.and_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.and_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_And(self, node: cst.And) -> bool:\n    self.and_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.and_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass OrOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.or_count = []\n\n    def visit_Or(self, node: cst.Or) -> bool:\n        self.or_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.or_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.or_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Or(self, node: cst.Or) -> bool:\n    self.or_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.or_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# Binary Operators\nclass AddOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.add_count = []\n\n    def visit_Add(self, node: cst.Add) -> bool:\n        self.add_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.add_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.add_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Add(self, node: cst.Add) -> bool:\n    self.add_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.add_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BitAndOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_and_count = []\n\n    def visit_BitAnd(self, node: cst.BitAnd) -> bool:\n        self.bit_and_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_and_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_and_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitAnd(self, node: cst.BitAnd) -> bool:\n    self.bit_and_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_and_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BitOrOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_or_count = []\n\n    def visit_BitOr(self, node: cst.BitOr) -> bool:\n        self.bit_or_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_or_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_or_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitOr(self, node: cst.BitOr) -> bool:\n    self.bit_or_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_or_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BitXorOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_xor_count = []\n\n    def visit_BitXor(self, node: cst.BitXor) -> bool:\n        self.bit_xor_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_xor_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_xor_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitXor(self, node: cst.BitXor) -> bool:\n    self.bit_xor_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_xor_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass DivideOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.divide_count = []\n\n    def visit_Divide(self, node: cst.Divide) -> bool:\n        self.divide_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.divide_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.divide_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Divide(self, node: cst.Divide) -> bool:\n    self.divide_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.divide_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass FloorDivideOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.floor_divide_count = []\n\n    def visit_FloorDivide(self, node: cst.FloorDivide) -> bool:\n        self.floor_divide_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.floor_divide_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.floor_divide_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_FloorDivide(self, node: cst.FloorDivide) -> bool:\n    self.floor_divide_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.floor_divide_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass LeftShiftOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.left_shift_count = []\n\n    def visit_LeftShift(self, node: cst.LeftShift) -> bool:\n        self.left_shift_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.left_shift_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.left_shift_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_LeftShift(self, node: cst.LeftShift) -> bool:\n    self.left_shift_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.left_shift_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MatrixMultiplyOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.matrix_multiply_count = []\n\n    def visit_MatrixMultiply(self, node: cst.MatrixMultiply) -> bool:\n        self.matrix_multiply_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.matrix_multiply_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.matrix_multiply_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_MatrixMultiply(self, node: cst.MatrixMultiply) -> bool:\n    self.matrix_multiply_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.matrix_multiply_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ModuloOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.modulo_count = []\n\n    def visit_Modulo(self, node: cst.Modulo) -> bool:\n        self.modulo_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.modulo_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.modulo_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Modulo(self, node: cst.Modulo) -> bool:\n    self.modulo_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.modulo_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MultiplyOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.multiply_count = []\n\n    def visit_Multiply(self, node: cst.Multiply) -> bool:\n        self.multiply_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.multiply_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.multiply_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Multiply(self, node: cst.Multiply) -> bool:\n    self.multiply_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.multiply_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass PowerOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.power_count = []\n\n    def visit_Power(self, node: cst.Power) -> bool:\n        self.power_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.power_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.power_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Power(self, node: cst.Power) -> bool:\n    self.power_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.power_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass RightShiftOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.right_shift_count = []\n\n    def visit_RightShift(self, node: cst.RightShift) -> bool:\n        self.right_shift_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.right_shift_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.right_shift_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_RightShift(self, node: cst.RightShift) -> bool:\n    self.right_shift_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.right_shift_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass SubtractOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.subtract_count = []\n\n    def visit_Subtract(self, node: cst.Subtract) -> bool:\n        self.subtract_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.subtract_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.subtract_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Subtract(self, node: cst.Subtract) -> bool:\n    self.subtract_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.subtract_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# Comparison Operators\nclass EqualOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.equal_count = []\n\n    def visit_Equal(self, node: cst.Equal) -> bool:\n        self.equal_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.equal_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.equal_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Equal(self, node: cst.Equal) -> bool:\n    self.equal_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.equal_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass GreaterThanOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.greater_than_count = []\n\n    def visit_GreaterThan(self, node: cst.GreaterThan) -> bool:\n        self.greater_than_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.greater_than_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.greater_than_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_GreaterThan(self, node: cst.GreaterThan) -> bool:\n    self.greater_than_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.greater_than_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# ... continue with other comparison operators\n\n\n# Augmented Assignment Operators\nclass AddAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.add_assign_count = []\n\n    def visit_AddAssign(self, node: cst.AddAssign) -> bool:\n        self.add_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.add_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.add_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_AddAssign(self, node: cst.AddAssign) -> bool:\n    self.add_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.add_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BitAndAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_and_assign_count = []\n\n    def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n        self.bit_and_assign_count.append(cst.Module([node]).code)\n        return True\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_and_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_and_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n    self.bit_and_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def collect(self):\n    self.module.visit(self)\n    return self.bit_and_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"class BitAndAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_and_assign_count = []\n\n    def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n        self.bit_and_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_and_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_and_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n    self.bit_and_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_and_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BitOrAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_or_assign_count = []\n\n    def visit_BitOrAssign(self, node: cst.BitOrAssign) -> bool:\n        self.bit_or_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_or_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_or_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitOrAssign(self, node: cst.BitOrAssign) -> bool:\n    self.bit_or_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_or_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BitXorAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_xor_assign_count = []\n\n    def visit_BitXorAssign(self, node: cst.BitXorAssign) -> bool:\n        self.bit_xor_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_xor_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_xor_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitXorAssign(self, node: cst.BitXorAssign) -> bool:\n    self.bit_xor_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_xor_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass DivideAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.divide_assign_count = []\n\n    def visit_DivideAssign(self, node: cst.DivideAssign) -> bool:\n        self.divide_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.divide_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.divide_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_DivideAssign(self, node: cst.DivideAssign) -> bool:\n    self.divide_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.divide_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass FloorDivideAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.floor_divide_assign_count = []\n\n    def visit_FloorDivideAssign(self, node: cst.FloorDivideAssign) -> bool:\n        self.floor_divide_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.floor_divide_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.floor_divide_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_FloorDivideAssign(self, node: cst.FloorDivideAssign) -> bool:\n    self.floor_divide_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.floor_divide_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass LeftShiftAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.left_shift_assign_count = []\n\n    def visit_LeftShiftAssign(self, node: cst.LeftShiftAssign) -> bool:\n        self.left_shift_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.left_shift_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.left_shift_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_LeftShiftAssign(self, node: cst.LeftShiftAssign) -> bool:\n    self.left_shift_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.left_shift_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MatrixMultiplyAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.matrix_multiply_assign_count = []\n\n    def visit_MatrixMultiplyAssign(self, node: cst.MatrixMultiplyAssign) -> bool:\n        self.matrix_multiply_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.matrix_multiply_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.matrix_multiply_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_MatrixMultiplyAssign(self, node: cst.MatrixMultiplyAssign) -> bool:\n    self.matrix_multiply_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.matrix_multiply_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ModuloAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.modulo_assign_count = []\n\n    def visit_ModuloAssign(self, node: cst.ModuloAssign) -> bool:\n        self.modulo_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.modulo_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.modulo_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ModuloAssign(self, node: cst.ModuloAssign) -> bool:\n    self.modulo_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.modulo_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MultiplyAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.multiply_assign_count = []\n\n    def visit_MultiplyAssign(self, node: cst.MultiplyAssign) -> bool:\n        self.multiply_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.multiply_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.multiply_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_MultiplyAssign(self, node: cst.MultiplyAssign) -> bool:\n    self.multiply_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.multiply_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass PowerAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.power_assign_count = []\n\n    def visit_PowerAssign(self, node: cst.PowerAssign) -> bool:\n        self.power_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.power_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.power_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_PowerAssign(self, node: cst.PowerAssign) -> bool:\n    self.power_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.power_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass RightShiftAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.right_shift_assign_count = []\n\n    def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n        self.right_shift_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.right_shift_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.right_shift_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n    self.right_shift_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.right_shift_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass RightShiftAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.right_shift_assign_count = []\n\n    def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n        self.right_shift_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.right_shift_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.right_shift_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n    self.right_shift_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.right_shift_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass SubtractAssignOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.subtract_assign_count = []\n\n    def visit_SubtractAssign(self, node: cst.SubtractAssign) -> bool:\n        self.subtract_assign_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.subtract_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.subtract_assign_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_SubtractAssign(self, node: cst.SubtractAssign) -> bool:\n    self.subtract_assign_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.subtract_assign_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Miscellaneous Operators\nclass AssignEqualOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.assign_equal_count = []\n\n    def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n        self.assign_equal_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.assign_equal_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.assign_equal_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n    self.assign_equal_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.assign_equal_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ColonOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.colon_count = []\n\n    def visit_Colon(self, node: cst.Colon) -> bool:\n        self.colon_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.colon_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.colon_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Colon(self, node: cst.Colon) -> bool:\n    self.colon_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.colon_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass CommaOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.comma_count = []\n\n    def visit_Comma(self, node: cst.Comma) -> bool:\n        self.comma_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.comma_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.comma_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Comma(self, node: cst.Comma) -> bool:\n    self.comma_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.comma_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass DotOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.dot_count = []\n\n    def visit_Dot(self, node: cst.Dot) -> bool:\n        self.dot_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.dot_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.dot_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Dot(self, node: cst.Dot) -> bool:\n    self.dot_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.dot_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ImportStarOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.import_star_count = []\n\n    def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n        self.import_star_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.import_star_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.import_star_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ImportStar(self, node: cst.ImportStar) -> bool:\n    self.import_star_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.import_star_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass SemicolonOperatorCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.semicolon_count = []\n\n    def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n        self.semicolon_count.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.semicolon_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.semicolon_count = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Semicolon(self, node: cst.Semicolon) -> bool:\n    self.semicolon_count.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.semicolon_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass DocstringCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.docstrings = []\n\n    def visit_Module(self, node: cst.Module) -> bool:\n        if node.body and isinstance(node.body[0].body, cst.SimpleStatementLine):\n            for stmt in node.body[0].body.body:\n                if isinstance(stmt, cst.Expr) and isinstance(stmt.value, cst.SimpleString):\n                    self.docstrings.append(stmt.value.value)\n        return True\n\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n        docstring = node.get_docstring()\n        if docstring is not None:\n            self.docstrings.append(docstring)\n        return True\n\n    def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n        docstring = node.get_docstring()\n        if docstring is not None:\n            self.docstrings.append(docstring)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.docstrings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.docstrings = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Module(self, node: cst.Module) -> bool:\n    if node.body and isinstance(node.body[0].body, cst.SimpleStatementLine):\n        for stmt in node.body[0].body.body:\n            if isinstance(stmt, cst.Expr) and isinstance(stmt.value, cst.SimpleString):\n                self.docstrings.append(stmt.value.value)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n    docstring = node.get_docstring()\n    if docstring is not None:\n        self.docstrings.append(docstring)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ClassDef(self, node: cst.ClassDef) -> bool:\n    docstring = node.get_docstring()\n    if docstring is not None:\n        self.docstrings.append(docstring)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.docstrings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass FunctionCallCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.function_calls = []\n\n    def visit_Call(self, node: cst.Call) -> bool:\n        if isinstance(node.func, cst.Name):\n            # Add the function name to the list\n            self.function_calls.append(node.func.value)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.function_calls\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.function_calls = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Call(self, node: cst.Call) -> bool:\n    if isinstance(node.func, cst.Name):\n        # Add the function name to the list\n        self.function_calls.append(node.func.value)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.function_calls\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ArgumentTypeCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.argument_types = []\n\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n        # Collect argument types for functions\n        arg_types = []\n        for param in node.params.params:\n            if isinstance(param.annotation, cst.Annotation):\n                arg_types.append(param.annotation.annotation.value)\n            else:\n                arg_types.append(None)\n\n        self.argument_types.append(arg_types)\n        return True\n\n    def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n        # Collect argument types for methods\n        for stmt in node.body.body:\n            if isinstance(stmt, cst.FunctionDef):\n                arg_types = []\n                for param in stmt.params.params:\n                    if isinstance(param.annotation, cst.Annotation):\n                        arg_types.append(param.annotation.annotation.value)\n                    else:\n                        arg_types.append(None)\n\n                self.argument_types.append(arg_types)\n\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.argument_types\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.argument_types = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n    # Collect argument types for functions\n    arg_types = []\n    for param in node.params.params:\n        if isinstance(param.annotation, cst.Annotation):\n            arg_types.append(param.annotation.annotation.value)\n        else:\n            arg_types.append(None)\n\n    self.argument_types.append(arg_types)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ClassDef(self, node: cst.ClassDef) -> bool:\n    # Collect argument types for methods\n    for stmt in node.body.body:\n        if isinstance(stmt, cst.FunctionDef):\n            arg_types = []\n            for param in stmt.params.params:\n                if isinstance(param.annotation, cst.Annotation):\n                    arg_types.append(param.annotation.annotation.value)\n                else:\n                    arg_types.append(None)\n\n            self.argument_types.append(arg_types)\n\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.argument_types\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ImportCollector(cst.CSTVisitor):\n    def __init__(self, filename: str):\n        with open(filename, \"r\") as file:\n            self.module = cst.parse_module(file.read())\n        self.imports = []\n\n    def visit_Import(self, node: cst.Import) -> bool:\n        for name in node.names:\n            self.imports.append(cst.Module([node]))\n        return True\n\n    def visit_ImportFrom(self, node: cst.ImportFrom) -> bool:\n        module = node.module.value if node.module else \"\"\n        for name in node.names:\n            self.imports.append(cst.Module([node]))\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        import_code = [import_statement.code for import_statement in self.imports]\n        return import_code\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, filename: str):\n    with open(filename, \"r\") as file:\n        self.module = cst.parse_module(file.read())\n    self.imports = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Import(self, node: cst.Import) -> bool:\n    for name in node.names:\n        self.imports.append(cst.Module([node]))\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ImportFrom(self, node: cst.ImportFrom) -> bool:\n    module = node.module.value if node.module else \"\"\n    for name in node.names:\n        self.imports.append(cst.Module([node]))\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    import_code = [import_statement.code for import_statement in self.imports]\n    return import_code\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass IfStatementCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.if_statements = []\n\n    def visit_If(self, node: cst.If) -> bool:\n        self.if_statements.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.if_statements\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.if_statements = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_If(self, node: cst.If) -> bool:\n    self.if_statements.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.if_statements\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BaseCompoundStatementCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.compound_statements = []\n\n    def visit_BaseCompoundStatement(self, node: cst.BaseCompoundStatement) -> bool:\n        self.compound_statements.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.compound_statements\n    \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.compound_statements = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BaseCompoundStatement(self, node: cst.BaseCompoundStatement) -> bool:\n    self.compound_statements.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.compound_statements\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"class ForLoopCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.for_loops = []\n\n    def visit_For(self, node: cst.For) -> bool:\n        self.for_loops.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.for_loops\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.for_loops = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_For(self, node: cst.For) -> bool:\n    self.for_loops.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.for_loops\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass WhileLoopCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.while_loops = []\n\n    def visit_While(self, node: cst.While) -> bool:\n        self.while_loops.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.while_loops\n    \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.while_loops = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_While(self, node: cst.While) -> bool:\n    self.while_loops.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.while_loops\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"class TryExceptCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.try_excepts = []\n\n    def visit_Try(self, node: cst.Try) -> bool:\n        self.try_excepts.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.try_excepts\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.try_excepts = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Try(self, node: cst.Try) -> bool:\n    self.try_excepts.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.try_excepts\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass WithCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.withs = []\n\n    def visit_With(self, node: cst.With) -> bool:\n        self.withs.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.withs\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.withs = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_With(self, node: cst.With) -> bool:\n    self.withs.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.withs\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass VariableDeclarationCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.variable_declarations = []\n\n    def visit_Assign(self, node: cst.Assign) -> bool:\n        self.variable_declarations.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.variable_declarations\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.variable_declarations = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Assign(self, node: cst.Assign) -> bool:\n    self.variable_declarations.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.variable_declarations\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ListComprehensionCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.list_comprehensions = []\n\n    def visit_ListComp(self, node: cst.ListComp) -> bool:\n        self.list_comprehensions.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.list_comprehensions\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.list_comprehensions = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ListComp(self, node: cst.ListComp) -> bool:\n    self.list_comprehensions.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.list_comprehensions\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass DictComprehensionCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.dict_comprehensions = []\n\n    def visit_DictComp(self, node: cst.DictComp) -> bool:\n        self.dict_comprehensions.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.dict_comprehensions\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.dict_comprehensions = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_DictComp(self, node: cst.DictComp) -> bool:\n    self.dict_comprehensions.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.dict_comprehensions\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# Set Comprehension Collector\nclass SetComprehensionCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.set_comprehensions = []\n\n    def visit_SetComp(self, node: cst.SetComp) -> bool:\n        self.set_comprehensions.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.set_comprehensions\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.set_comprehensions = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_SetComp(self, node: cst.SetComp) -> bool:\n    self.set_comprehensions.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.set_comprehensions\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Generator Expression Collector\nclass GeneratorExpressionCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.generator_expressions = []\n\n    def visit_GeneratorExp(self, node: cst.GeneratorExp) -> bool:\n        self.generator_expressions.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.generator_expressions\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.generator_expressions = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_GeneratorExp(self, node: cst.GeneratorExp) -> bool:\n    self.generator_expressions.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.generator_expressions\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Yield Statement Collector\nclass YieldCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.yields = []\n\n    def visit_Yield(self, node: cst.Yield) -> bool:\n        self.yields.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.yields\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.yields = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Yield(self, node: cst.Yield) -> bool:\n    self.yields.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.yields\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# Return Statement Collector\nclass ReturnCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.returns = []\n\n    def visit_Return(self, node: cst.Return) -> bool:\n        self.returns.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.returns\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.returns = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Return(self, node: cst.Return) -> bool:\n    self.returns.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.returns\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Raise Statement Collector\nclass RaiseCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.raises = []\n\n    def visit_Raise(self, node: cst.Raise) -> bool:\n        self.raises.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.raises\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.raises = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Raise(self, node: cst.Raise) -> bool:\n    self.raises.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.raises\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Assert Statement Collector\nclass AssertCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.asserts = []\n\n    def visit_Assert(self, node: cst.Assert) -> bool:\n        self.asserts.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.asserts\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.asserts = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Assert(self, node: cst.Assert) -> bool:\n    self.asserts.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.asserts\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Break Statement Collector\nclass BreakCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.breaks = []\n\n    def visit_Break(self, node: cst.Break) -> bool:\n        self.breaks.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.breaks\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.breaks = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Break(self, node: cst.Break) -> bool:\n    self.breaks.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.breaks\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Continue Statement Collector\nclass ContinueCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.continues = []\n\n    def visit_Continue(self, node: cst.Continue) -> bool:\n        self.continues.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.continues\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.continues = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Continue(self, node: cst.Continue) -> bool:\n    self.continues.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.continues\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Pass Statement Collector\nclass PassCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.passes = []\n\n    def visit_Pass(self, node: cst.Pass) -> bool:\n        self.passes.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.passes\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.passes = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Pass(self, node: cst.Pass) -> bool:\n    self.passes.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.passes\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n\n# With Statement Collector\nclass WithStatementCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.with_statements = []\n\n    def visit_With(self, node: cst.With) -> bool:\n        self.with_statements.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.with_statements\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.with_statements = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_With(self, node: cst.With) -> bool:\n    self.with_statements.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.with_statements\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Try Statement Collector\nclass TryStatementCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.try_statements = []\n\n    def visit_Try(self, node: cst.Try) -> bool:\n        self.try_statements.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.try_statements\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.try_statements = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Try(self, node: cst.Try) -> bool:\n    self.try_statements.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.try_statements\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Except Clause Collector\nclass ExceptClauseCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.except_clauses = []\n\n    def visit_ExceptHandler(self, node: cst.ExceptHandler) -> bool:\n        self.except_clauses.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.except_clauses\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.except_clauses = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ExceptHandler(self, node: cst.ExceptHandler) -> bool:\n    self.except_clauses.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.except_clauses\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Lambda Function Collector\nclass LambdaFunctionCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.lambda_functions = []\n\n    def visit_Lambda(self, node: cst.Lambda) -> bool:\n        self.lambda_functions.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.lambda_functions\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.lambda_functions = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Lambda(self, node: cst.Lambda) -> bool:\n    self.lambda_functions.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.lambda_functions\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Global Statement Collector\nclass GlobalStatementCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.global_statements = []\n\n    def visit_Global(self, node: cst.Global) -> bool:\n        self.global_statements.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.global_statements\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.global_statements = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Global(self, node: cst.Global) -> bool:\n    self.global_statements.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.global_statements\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Nonlocal Statement Collector\nclass NonlocalStatementCollector(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.nonlocal_statements = []\n\n    def visit_Nonlocal(self, node: cst.Nonlocal) -> bool:\n        self.nonlocal_statements.append(cst.Module([node]).code)\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.nonlocal_statements\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.nonlocal_statements = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Nonlocal(self, node: cst.Nonlocal) -> bool:\n    self.nonlocal_statements.append(cst.Module([node]).code)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.nonlocal_statements\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass PassInserter(cst.CSTTransformer):\n    def __init__(self):\n        super().__init__()\n        self.inside_class = False\n\n    def visit_ClassDef(self, node: cst.ClassDef):\n        self.inside_class = True\n        return super().visit_ClassDef(node)\n\n    def leave_ClassDef(self, original_node: cst.ClassDef, updated_node: cst.ClassDef) -> cst.ClassDef:\n        self.inside_class = False\n        return updated_node\n\n    def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\n        body = updated_node.body\n        body_elements = body.body\n\n        # Check if the first element is a docstring\n\n        #if body_elements and matchers.matches(body_elements[0], matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\n            #docstring = body_elements[0]\n        docstrings = []\n        for element in body_elements:\n            if matchers.matches(element, matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\n                docstrings.append(element)\n\n        # Prepare new body\n        new_body = [cst.SimpleStatementLine(body=(cst.Pass(),))]\n        if docstrings[0] is not None:\n            new_body.insert(0, docstrings[0])\n\n        return updated_node.with_changes(\n            body=cst.IndentedBlock(body=tuple(new_body))\n        )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self):\n    super().__init__()\n    self.inside_class = False\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ClassDef(self, node: cst.ClassDef):\n    self.inside_class = True\n    return super().visit_ClassDef(node)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef leave_ClassDef(self, original_node: cst.ClassDef, updated_node: cst.ClassDef) -> cst.ClassDef:\n    self.inside_class = False\n    return updated_node\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\n    body = updated_node.body\n    body_elements = body.body\n\n    # Check if the first element is a docstring\n\n    #if body_elements and matchers.matches(body_elements[0], matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\n        #docstring = body_elements[0]\n    docstrings = []\n    for element in body_elements:\n        if matchers.matches(element, matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\n            docstrings.append(element)\n\n    # Prepare new body\n    new_body = [cst.SimpleStatementLine(body=(cst.Pass(),))]\n    if docstrings[0] is not None:\n        new_body.insert(0, docstrings[0])\n\n    return updated_node.with_changes(\n        body=cst.IndentedBlock(body=tuple(new_body))\n    )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef generate_skeleton(code: str) -> str:\n    \"\"\"\n    Generate a skeleton for the given code, replacing function and class bodies with a pass statement.\n    \"\"\"\n    # Parse the code into a CST\n    module = cst.parse_module(code)\n\n    # Apply the transformer\n    transformer = PassInserter()\n    transformed_module = module.visit(transformer)\n\n    # Convert the transformed CST back to code\n    skeleton_code = transformed_module.code\n\n    return skeleton_code\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass CodeReplacerVisitor(cst.CSTTransformer):\n    def __init__(self, filename_column: str, original_code_column: str, replacing_code_column: str):\n        self.filename_column = filename_column\n        self.original_code_column = original_code_column\n        self.replacing_code_column = replacing_code_column\n\n    def visit_Module(self, node: cst.Module) -> cst.Module:\n        # Get the filename from the node metadata\n        filename = node.metadata.get(self.filename_column)\n        if filename is None:\n            return node\n\n        # Load the content of the file\n        with open(filename, \"r\") as f:\n            file_content = f.read()\n\n        # Get the original code and replacing code from the node metadata\n        original_code = node.metadata.get(self.original_code_column)\n        replacing_code = node.metadata.get(self.replacing_code_column)\n\n        if original_code is None or replacing_code is None:\n            return node\n\n        # Replace the original code with the replacing code in the file content\n        modified_content = file_content.replace(original_code, replacing_code)\n\n        # Save the modified content back to the file\n        with open(filename, \"w\") as f:\n            f.write(modified_content)\n\n        # Parse the modified content to update the node\n        return cst.parse_module(modified_content)\n\n    def leave_Module(self, original_node: cst.Module, updated_node: cst.Module) -> cst.Module:\n        # Copy the metadata from the original node to the updated node\n        updated_node.metadata = original_node.metadata\n        return updated_node\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, filename_column: str, original_code_column: str, replacing_code_column: str):\n    self.filename_column = filename_column\n    self.original_code_column = original_code_column\n    self.replacing_code_column = replacing_code_column\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Module(self, node: cst.Module) -> cst.Module:\n    # Get the filename from the node metadata\n    filename = node.metadata.get(self.filename_column)\n    if filename is None:\n        return node\n\n    # Load the content of the file\n    with open(filename, \"r\") as f:\n        file_content = f.read()\n\n    # Get the original code and replacing code from the node metadata\n    original_code = node.metadata.get(self.original_code_column)\n    replacing_code = node.metadata.get(self.replacing_code_column)\n\n    if original_code is None or replacing_code is None:\n        return node\n\n    # Replace the original code with the replacing code in the file content\n    modified_content = file_content.replace(original_code, replacing_code)\n\n    # Save the modified content back to the file\n    with open(filename, \"w\") as f:\n        f.write(modified_content)\n\n    # Parse the modified content to update the node\n    return cst.parse_module(modified_content)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef leave_Module(self, original_node: cst.Module, updated_node: cst.Module) -> cst.Module:\n    # Copy the metadata from the original node to the updated node\n    updated_node.metadata = original_node.metadata\n    return updated_node\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# Unary Operators\nclass BitInvertOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_invert_operator_count = 0\n\n    def visit_BitInvert(self, node: cst.BitInvert) -> bool:\n        self.bit_invert_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_invert_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_invert_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitInvert(self, node: cst.BitInvert) -> bool:\n    self.bit_invert_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_invert_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MinusOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.minus_operator_count = 0\n\n    def visit_Minus(self, node: cst.Minus) -> bool:\n        self.minus_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.minus_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.minus_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Minus(self, node: cst.Minus) -> bool:\n    self.minus_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.minus_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass NotOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.not_operator_count = 0\n\n    def visit_Not(self, node: cst.Not) -> bool:\n        self.not_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.not_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.not_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Not(self, node: cst.Not) -> bool:\n    self.not_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.not_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass PlusOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.plus_operator_count = 0\n\n    def visit_Plus(self, node: cst.Plus) -> bool:\n        self.plus_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.plus_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.plus_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Plus(self, node: cst.Plus) -> bool:\n    self.plus_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.plus_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Boolean Operators\nclass AndOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.and_operator_count = 0\n\n    def visit_And(self, node: cst.And) -> bool:\n        self.and_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.and_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.and_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_And(self, node: cst.And) -> bool:\n    self.and_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.and_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass OrOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.or_operator_count = 0\n\n    def visit_Or(self, node: cst.Or) -> bool:\n        self.or_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.or_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.or_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Or(self, node: cst.Or) -> bool:\n    self.or_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.or_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Binary Operators\nclass AddOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.add_operator_count = 0\n\n    def visit_Add(self, node: cst.Add) -> bool:\n        self.add_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.add_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.add_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Add(self, node: cst.Add) -> bool:\n    self.add_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.add_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Binary Operators\nclass BitAndOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_and_operator_count = 0\n\n    def visit_BitAnd(self, node: cst.BitAnd) -> bool:\n        self.bit_and_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_and_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_and_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitAnd(self, node: cst.BitAnd) -> bool:\n    self.bit_and_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_and_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BitOrOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_or_operator_count = 0\n\n    def visit_BitOr(self, node: cst.BitOr) -> bool:\n        self.bit_or_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_or_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_or_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitOr(self, node: cst.BitOr) -> bool:\n    self.bit_or_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_or_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BitXorOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_xor_operator_count = 0\n\n    def visit_BitXor(self, node: cst.BitXor) -> bool:\n        self.bit_xor_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_xor_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_xor_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitXor(self, node: cst.BitXor) -> bool:\n    self.bit_xor_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_xor_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass DivideOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.divide_operator_count = 0\n\n    def visit_Divide(self, node: cst.Divide) -> bool:\n        self.divide_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.divide_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.divide_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Divide(self, node: cst.Divide) -> bool:\n    self.divide_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.divide_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass FloorDivideOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.floor_divide_operator_count = 0\n\n    def visit_FloorDivide(self, node: cst.FloorDivide) -> bool:\n        self.floor_divide_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.floor_divide_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.floor_divide_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_FloorDivide(self, node: cst.FloorDivide) -> bool:\n    self.floor_divide_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.floor_divide_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass LeftShiftOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.left_shift_operator_count = 0\n\n    def visit_LeftShift(self, node: cst.LeftShift) -> bool:\n        self.left_shift_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.left_shift_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.left_shift_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_LeftShift(self, node: cst.LeftShift) -> bool:\n    self.left_shift_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.left_shift_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MatrixMultiplyOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.matrix_multiply_operator_count = 0\n\n    def visit_MatrixMultiply(self, node: cst.MatrixMultiply) -> bool:\n        self.matrix_multiply_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.matrix_multiply_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.matrix_multiply_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_MatrixMultiply(self, node: cst.MatrixMultiply) -> bool:\n    self.matrix_multiply_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.matrix_multiply_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ModuloOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.modulo_operator_count = 0\n\n    def visit_Modulo(self, node: cst.Modulo) -> bool:\n        self.modulo_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.modulo_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.modulo_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Modulo(self, node: cst.Modulo) -> bool:\n    self.modulo_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.modulo_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MultiplyOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.multiply_operator_count = 0\n\n    def visit_Multiply(self, node: cst.Multiply) -> bool:\n        self.multiply_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.multiply_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.multiply_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Multiply(self, node: cst.Multiply) -> bool:\n    self.multiply_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.multiply_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass PowerOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.power_operator_count = 0\n\n    def visit_Power(self, node: cst.Power) -> bool:\n        self.power_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.power_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.power_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Power(self, node: cst.Power) -> bool:\n    self.power_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.power_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass RightShiftOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.right_shift_operator_count = 0\n\n    def visit_RightShift(self, node: cst.RightShift) -> bool:\n        self.right_shift_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.right_shift_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.right_shift_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_RightShift(self, node: cst.RightShift) -> bool:\n    self.right_shift_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.right_shift_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass SubtractOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.subtract_operator_count = 0\n\n    def visit_Subtract(self, node: cst.Subtract) -> bool:\n        self.subtract_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.subtract_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.subtract_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Subtract(self, node: cst.Subtract) -> bool:\n    self.subtract_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.subtract_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# Comparison Operators\nclass EqualOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.equal_operator_count = 0\n\n    def visit_Equal(self, node: cst.Equal) -> bool:\n        self.equal_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.equal_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Equal(self, node: cst.Equal) -> bool:\n    self.equal_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Comparison Operators\nclass EqualOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.equal_operator_count = 0\n\n    def visit_Equal(self, node: cst.Equal) -> bool:\n        self.equal_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.equal_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Equal(self, node: cst.Equal) -> bool:\n    self.equal_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass GreaterThanOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.greater_than_operator_count = 0\n\n    def visit_GreaterThan(self, node: cst.GreaterThan) -> bool:\n        self.greater_than_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.greater_than_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.greater_than_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_GreaterThan(self, node: cst.GreaterThan) -> bool:\n    self.greater_than_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.greater_than_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass GreaterThanEqualOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.greater_than_equal_operator_count = 0\n\n    def visit_GreaterThanEqual(self, node: cst.GreaterThanEqual) -> bool:\n        self.greater_than_equal_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.greater_than_equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.greater_than_equal_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_GreaterThanEqual(self, node: cst.GreaterThanEqual) -> bool:\n    self.greater_than_equal_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.greater_than_equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass InOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.in_operator_count = 0\n\n    def visit_In(self, node: cst.In) -> bool:\n        self.in_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.in_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.in_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_In(self, node: cst.In) -> bool:\n    self.in_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.in_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass IsOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.is_operator_count = 0\n\n    def visit_Is(self, node: cst.Is) -> bool:\n        self.is_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.is_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.is_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Is(self, node: cst.Is) -> bool:\n    self.is_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.is_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass LessThanOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.less_than_operator_count = 0\n\n    def visit_LessThan(self, node: cst.LessThan) -> bool:\n        self.less_than_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.less_than_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.less_than_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_LessThan(self, node: cst.LessThan) -> bool:\n    self.less_than_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.less_than_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass LessThanEqualOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.less_than_equal_operator_count = 0\n\n    def visit_LessThanEqual(self, node: cst.LessThanEqual) -> bool:\n        self.less_than_equal_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.less_than_equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.less_than_equal_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_LessThanEqual(self, node: cst.LessThanEqual) -> bool:\n    self.less_than_equal_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.less_than_equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass NotEqualOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.not_equal_operator_count = 0\n\n    def visit_NotEqual(self, node: cst.NotEqual) -> bool:\n        self.not_equal_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.not_equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.not_equal_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_NotEqual(self, node: cst.NotEqual) -> bool:\n    self.not_equal_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.not_equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass IsNotOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.is_not_operator_count = 0\n\n    def visit_IsNot(self, node: cst.IsNot) -> bool:\n        self.is_not_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.is_not_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.is_not_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_IsNot(self, node: cst.IsNot) -> bool:\n    self.is_not_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.is_not_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass NotInOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.not_in_operator_count = 0\n\n    def visit_NotIn(self, node: cst.NotIn) -> bool:\n        self.not_in_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.not_in_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.not_in_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_NotIn(self, node: cst.NotIn) -> bool:\n    self.not_in_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.not_in_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# Augmented Assignment Operators\nclass AddAssignOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.add_assign_operator_count = 0\n\n    def visit_AddAssign(self, node: cst.AddAssign) -> bool:\n        self.add_assign_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.add_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.add_assign_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_AddAssign(self, node: cst.AddAssign) -> bool:\n    self.add_assign_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.add_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BitAndAssignOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_and_assign_operator_count = 0\n\n    def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n        self.bit_and_assign_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_and_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_and_assign_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n    self.bit_and_assign_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_and_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BitOrAssignOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_or_assign_operator_count = 0\n\n    def visit_BitOrAssign(self, node: cst.BitOrAssign) -> bool:\n        self.bit_or_assign_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_or_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_or_assign_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitOrAssign(self, node: cst.BitOrAssign) -> bool:\n    self.bit_or_assign_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_or_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BitXorAssignOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.bit_xor_assign_operator_count = 0\n\n    def visit_BitXorAssign(self, node: cst.BitXorAssign) -> bool:\n        self.bit_xor_assign_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.bit_xor_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.bit_xor_assign_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BitXorAssign(self, node: cst.BitXorAssign) -> bool:\n    self.bit_xor_assign_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.bit_xor_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass DivideAssignOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.divide_assign_operator_count = 0\n\n    def visit_DivideAssign(self, node: cst.DivideAssign) -> bool:\n        self.divide_assign_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.divide_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.divide_assign_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_DivideAssign(self, node: cst.DivideAssign) -> bool:\n    self.divide_assign_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.divide_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass FloorDivideAssignOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.floor_divide_assign_operator_count = 0\n\n    def visit_FloorDivideAssign(self, node: cst.FloorDivideAssign) -> bool:\n        self.floor_divide_assign_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.floor_divide_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.floor_divide_assign_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_FloorDivideAssign(self, node: cst.FloorDivideAssign) -> bool:\n    self.floor_divide_assign_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.floor_divide_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass LeftShiftAssignOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.left_shift_assign_operator_count = 0\n\n    def visit_LeftShiftAssign(self, node: cst.LeftShiftAssign) -> bool:\n        self.left_shift_assign_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.left_shift_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.left_shift_assign_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_LeftShiftAssign(self, node: cst.LeftShiftAssign) -> bool:\n    self.left_shift_assign_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.left_shift_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MatrixMultiplyAssignOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.matrix_multiply_assign_operator_count = 0\n\n    def visit_MatrixMultiplyAssign(self, node: cst.MatrixMultiplyAssign) -> bool:\n        self.matrix_multiply_assign_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.matrix_multiply_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.matrix_multiply_assign_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_MatrixMultiplyAssign(self, node: cst.MatrixMultiplyAssign) -> bool:\n    self.matrix_multiply_assign_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.matrix_multiply_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ModuloAssignOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.modulo_assign_operator_count = 0\n\n    def visit_ModuloAssign(self, node: cst.ModuloAssign) -> bool:\n        self.modulo_assign_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.modulo_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.modulo_assign_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ModuloAssign(self, node: cst.ModuloAssign) -> bool:\n    self.modulo_assign_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.modulo_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MultiplyAssignOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.multiply_assign_operator_count = 0\n\n    def visit_MultiplyAssign(self, node: cst.MultiplyAssign) -> bool:\n        self.multiply_assign_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.multiply_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.multiply_assign_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_MultiplyAssign(self, node: cst.MultiplyAssign) -> bool:\n    self.multiply_assign_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.multiply_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass PowerAssignOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.power_assign_operator_count = 0\n\n    def visit_PowerAssign(self, node: cst.PowerAssign) -> bool:\n        self.power_assign_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.power_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.power_assign_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_PowerAssign(self, node: cst.PowerAssign) -> bool:\n    self.power_assign_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.power_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass RightShiftAssignOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.right_shift_assign_operator_count = 0\n\n    def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n        self.right_shift_assign_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.right_shift_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.right_shift_assign_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n    self.right_shift_assign_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.right_shift_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass SubtractAssignOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.subtract_assign_operator_count = 0\n\n    def visit_SubtractAssign(self, node: cst.SubtractAssign) -> bool:\n        self.subtract_assign_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.subtract_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.subtract_assign_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_SubtractAssign(self, node: cst.SubtractAssign) -> bool:\n    self.subtract_assign_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.subtract_assign_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# Miscellaneous Operators\nclass AssignEqualOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.assign_equal_operator_count = 0\n\n    def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n        self.assign_equal_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.assign_equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.assign_equal_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n    self.assign_equal_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.assign_equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Miscellaneous Operators\nclass AssignEqualOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.assign_equal_operator_count = 0\n\n    def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n        self.assign_equal_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.assign_equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.assign_equal_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n    self.assign_equal_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.assign_equal_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ColonOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.colon_operator_count = 0\n\n    def visit_Colon(self, node: cst.Colon) -> bool:\n        self.colon_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.colon_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.colon_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Colon(self, node: cst.Colon) -> bool:\n    self.colon_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.colon_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass CommaOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.comma_operator_count = 0\n\n    def visit_Comma(self, node: cst.Comma) -> bool:\n        self.comma_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.comma_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.comma_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Comma(self, node: cst.Comma) -> bool:\n    self.comma_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.comma_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass DotOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.dot_operator_count = 0\n\n    def visit_Dot(self, node: cst.Dot) -> bool:\n        self.dot_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.dot_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.dot_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Dot(self, node: cst.Dot) -> bool:\n    self.dot_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.dot_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ImportStarOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.import_star_operator_count = 0\n\n    def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n        self.import_star_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.import_star_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.import_star_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ImportStar(self, node: cst.ImportStar) -> bool:\n    self.import_star_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.import_star_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass SemicolonOperatorCounter(cst.CSTVisitor):\n    def __init__(self, code: str):\n        self.module = cst.parse_module(code)\n        self.semicolon_operator_count = 0\n\n    def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n        self.semicolon_operator_count += 1\n        return True\n\n    def collect(self):\n        self.module.visit(self)\n        return self.semicolon_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str):\n    self.module = cst.parse_module(code)\n    self.semicolon_operator_count = 0\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Semicolon(self, node: cst.Semicolon) -> bool:\n    self.semicolon_operator_count += 1\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef collect(self):\n    self.module.visit(self)\n    return self.semicolon_operator_count\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass VectorThread(BaseThread, MemoryIndex):\n    \"\"\"vector BaseThread, creates a faiss index with the messages and allows to search for similar messages, memory BaseThread can return messages in either similarity or chronological order\n    add a parameter to choose the order of the messages\n    \"\"\"\n\n    def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\n        BaseThread.__init__(self, name=name, max_memory=None)\n        MemoryIndex.__init__(self, index=None, name=name)\n        self.max_context = max_context\n        self.use_mark = use_mark\n        self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n\n    def index_message(self, message: str, verbose: bool = False):\n        \"\"\"index a message in the faiss index, the message is embedded and added to the index\n        self.values and self.embeddings and self.index are updated\n        \"\"\"\n\n        self.add_to_index(value=message, verbose=verbose)\n\n    def add_message(self, message_dict: dict, verbose: bool = False):\n        \"\"\"add a message to the memory thread, the message is embedded and added to the index\n        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\n        \"\"\"\n        # print(\"checking the dict\")\n        message_dict = check_dict(message_dict)\n        # print(\"trying to add the message\")\n        BaseThread.add_message(self, message_dict)\n        # print(message_dict)\n        message = message_dict[\"content\"]\n        self.index_message(message, verbose=verbose)\n        return True\n\n    def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\n        \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\n        if self.use_mark:\n            query = mark_question(query)\n        return MemoryIndex.token_bound_query(self, query, k, max_tokens)\n\n    def sorted_query(\n        self,\n        query,\n        k: int = 10,\n        max_tokens: int = 4000,\n        reverse: bool = False,\n        return_from_thread=True,\n    ) -> Tuple[List[str], List[float], List[int]]:\n        \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\n        \"\"\"\n        unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\n\n        num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\n        # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\n        unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\n\n        # Sort the indices\n        sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\n        \n        print(sorted_indices)\n        print(type(sorted_indices))\n\n        if reverse:\n            sorted_indices.reverse()\n\n        # Fetch the sorted messages, scores, and indices based on sorted_indices\n        sorted_messages = [unsorted_messages[i] for i in sorted_indices]\n        sorted_scores = [unsorted_scores[i] for i in sorted_indices]\n        sorted_indices = [unsorted_indices[i] for i in sorted_indices]\n\n        if return_from_thread:\n            sorted_messages = [self.memory_thread[i] for i in sorted_indices]\n\n        return sorted_messages, sorted_scores, sorted_indices\n    def weighted_query(\n        self,\n        query,\n        k: int = 10,\n        max_tokens: int = 4000,\n        decay_factor: float = 0.1,\n        temporal_weight: float = 0.5,\n        order_by: str = \"chronological\",\n        reverse: bool = False,\n    ) -> list:\n        \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\n        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\n        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\n        The order_by parameter controls the order of the results. If it is set to 'similarity', the results are sorted in similarity order. If it is set to 'chronological', the results are sorted in chronological order with the most recent message first.\n        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\n        \"\"\"\n        # Validate order_by parameter\n        if order_by not in (\"similarity\", \"chronological\"):\n            raise ValueError(\n                \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n            )\n\n        # Get similarity-based results\n        sim_messages, sim_scores, sim_indices = self.sorted_query(\n            query, k, max_tokens=max_tokens\n        )\n\n        # Get token-bound history\n        hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\n\n        # Combine messages and indices\n        combined_messages = sim_messages + hist_messages\n        combined_indices = sim_indices + hist_indices\n\n        # Create the local_index and populate it\n        self.local_index = MemoryIndex(name=\"local_index\")\n        for message in combined_messages:\n            self.local_index.add_to_index(value=message, verbose=False)\n\n        # Perform a new query on the combined index\n        (\n            new_query_results,\n            new_query_scores,\n            new_query_indices,\n        ) = self.local_index.token_bound_query(\n            query, k=len(combined_messages), max_tokens=max_tokens\n        )\n\n        # Compute temporal weights\n        temporal_weights = [\n            np.exp(-decay_factor * i) for i in range(len(combined_messages))\n        ]\n        temporal_weights = [\n            w / sum(temporal_weights) for w in temporal_weights\n        ]  # Normalize the temporal weights\n\n        # Combine similarity scores and temporal weights\n        weighted_scores = []\n        for i in range(len(new_query_scores)):\n            sim_score = new_query_scores[i]\n            temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\n            weighted_score = (\n                1 - temporal_weight\n            ) * sim_score + temporal_weight * temp_weight\n            weighted_scores.append(weighted_score)\n\n        # Sort the results based on the order_by parameter\n        if order_by == \"similarity\":\n            sorting_key = lambda k: weighted_scores[k]\n        elif order_by == \"chronological\":  # order_by == 'chronological'\n            sorting_key = lambda k: new_query_indices[k]\n        else:\n            raise ValueError(\n                \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n            )\n\n        sorted_indices = [\n            new_query_indices[i]\n            for i in sorted(\n                range(len(new_query_indices)), key=sorting_key, reverse=not reverse\n            )\n        ]\n        sorted_results = [\n            new_query_results[i]\n            for i in sorted(\n                range(len(new_query_results)), key=sorting_key, reverse=not reverse\n            )\n        ]\n        sorted_scores = [\n            weighted_scores[i]\n            for i in sorted(\n                range(len(weighted_scores)), key=sorting_key, reverse=not reverse\n            )\n        ]\n\n        # Return only the top k results without exceeding max_tokens\n        final_results, final_scores, final_indices = [], [], []\n        current_tokens = 0\n        for i in range(min(k, len(sorted_results))):\n            message_tokens = self.get_message_tokens(sorted_results[i])\n            if current_tokens + message_tokens <= max_tokens:\n                final_results.append(sorted_results[i])\n                final_scores.append(sorted_scores[i])\n                final_indices.append(sorted_indices[i])\n                current_tokens += message_tokens\n            else:\n                break\n\n        return final_results, final_scores, final_indices\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\n    BaseThread.__init__(self, name=name, max_memory=None)\n    MemoryIndex.__init__(self, index=None, name=name)\n    self.max_context = max_context\n    self.use_mark = use_mark\n    self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef index_message(self, message: str, verbose: bool = False):\n    \"\"\"index a message in the faiss index, the message is embedded and added to the index\n        self.values and self.embeddings and self.index are updated\n        \"\"\"\n\n    self.add_to_index(value=message, verbose=verbose)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef add_message(self, message_dict: dict, verbose: bool = False):\n    \"\"\"add a message to the memory thread, the message is embedded and added to the index\n        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\n        \"\"\"\n    # print(\"checking the dict\")\n    message_dict = check_dict(message_dict)\n    # print(\"trying to add the message\")\n    BaseThread.add_message(self, message_dict)\n    # print(message_dict)\n    message = message_dict[\"content\"]\n    self.index_message(message, verbose=verbose)\n    return True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\n    \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\n    if self.use_mark:\n        query = mark_question(query)\n    return MemoryIndex.token_bound_query(self, query, k, max_tokens)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef sorted_query(\n    self,\n    query,\n    k: int = 10,\n    max_tokens: int = 4000,\n    reverse: bool = False,\n    return_from_thread=True,\n) -> Tuple[List[str], List[float], List[int]]:\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\n        \"\"\"\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\n\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\n\n    # Sort the indices\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\n    \n    print(sorted_indices)\n    print(type(sorted_indices))\n\n    if reverse:\n        sorted_indices.reverse()\n\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\n\n    if return_from_thread:\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\n\n    return sorted_messages, sorted_scores, sorted_indices\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def weighted_query(\n    self,\n    query,\n    k: int = 10,\n    max_tokens: int = 4000,\n    decay_factor: float = 0.1,\n    temporal_weight: float = 0.5,\n    order_by: str = \"chronological\",\n    reverse: bool = False,\n) -> list:\n    \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\n        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\n        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\n        The order_by parameter controls the order of the results. If it is set to 'similarity', the results are sorted in similarity order. If it is set to 'chronological', the results are sorted in chronological order with the most recent message first.\n        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\n        \"\"\"\n    # Validate order_by parameter\n    if order_by not in (\"similarity\", \"chronological\"):\n        raise ValueError(\n            \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n        )\n\n    # Get similarity-based results\n    sim_messages, sim_scores, sim_indices = self.sorted_query(\n        query, k, max_tokens=max_tokens\n    )\n\n    # Get token-bound history\n    hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\n\n    # Combine messages and indices\n    combined_messages = sim_messages + hist_messages\n    combined_indices = sim_indices + hist_indices\n\n    # Create the local_index and populate it\n    self.local_index = MemoryIndex(name=\"local_index\")\n    for message in combined_messages:\n        self.local_index.add_to_index(value=message, verbose=False)\n\n    # Perform a new query on the combined index\n    (\n        new_query_results,\n        new_query_scores,\n        new_query_indices,\n    ) = self.local_index.token_bound_query(\n        query, k=len(combined_messages), max_tokens=max_tokens\n    )\n\n    # Compute temporal weights\n    temporal_weights = [\n        np.exp(-decay_factor * i) for i in range(len(combined_messages))\n    ]\n    temporal_weights = [\n        w / sum(temporal_weights) for w in temporal_weights\n    ]  # Normalize the temporal weights\n\n    # Combine similarity scores and temporal weights\n    weighted_scores = []\n    for i in range(len(new_query_scores)):\n        sim_score = new_query_scores[i]\n        temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\n        weighted_score = (\n            1 - temporal_weight\n        ) * sim_score + temporal_weight * temp_weight\n        weighted_scores.append(weighted_score)\n\n    # Sort the results based on the order_by parameter\n    if order_by == \"similarity\":\n        sorting_key = lambda k: weighted_scores[k]\n    elif order_by == \"chronological\":  # order_by == 'chronological'\n        sorting_key = lambda k: new_query_indices[k]\n    else:\n        raise ValueError(\n            \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n        )\n\n    sorted_indices = [\n        new_query_indices[i]\n        for i in sorted(\n            range(len(new_query_indices)), key=sorting_key, reverse=not reverse\n        )\n    ]\n    sorted_results = [\n        new_query_results[i]\n        for i in sorted(\n            range(len(new_query_results)), key=sorting_key, reverse=not reverse\n        )\n    ]\n    sorted_scores = [\n        weighted_scores[i]\n        for i in sorted(\n            range(len(weighted_scores)), key=sorting_key, reverse=not reverse\n        )\n    ]\n\n    # Return only the top k results without exceeding max_tokens\n    final_results, final_scores, final_indices = [], [], []\n    current_tokens = 0\n    for i in range(min(k, len(sorted_results))):\n        message_tokens = self.get_message_tokens(sorted_results[i])\n        if current_tokens + message_tokens <= max_tokens:\n            final_results.append(sorted_results[i])\n            final_scores.append(sorted_scores[i])\n            final_indices.append(sorted_indices[i])\n            current_tokens += message_tokens\n        else:\n            break\n\n    return final_results, final_scores, final_indices\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass BaseThread:\n    \"\"\"\n    This class is used to keep track of the memory thread of a conversation and the total number of tokens.\n    All conversation memories should subclass this class. If max_memory is None, it has\n    no limit to the number of tokens that can be stored in the memory thread.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"memory\",\n        max_memory: Optional[int] = None,\n        tokenizer: Optional[Any] = None,\n        save_path: str = 'threads'\n\n    ) -> None:\n        \"\"\"\n        Initialize the BaseThread instance.\n\n        :param name: The name of the memory thread. Defaults to 'memory'.\n        :param max_memory: The maximum number of tokens allowed in the memory thread.\n                           Defaults to None, which means no limit.\n        :param tokenizer: The tokenizer to be used for tokenizing messages.\n                          Defaults to None, which means using the tiktoken encoding for the 'gpt-3.5-turbo' model.\n        \"\"\"\n        self.name = name\n        self.max_memory = max_memory\n        self.memory_schema = {\"role\": pl.Utf8, \"content\": pl.Utf8,\"timestamp\":pl.Float64,\"tokens_count\":pl.UInt16}\n        self.memory_thread: pl.DataFrame = pl.DataFrame(schema=self.memory_schema)\n        \"\"\" self.time_stamps = [] \"\"\"\n        \"\"\" self.message_tokens = [] \"\"\"\n        self.total_tokens = self.get_total_tokens_from_thread()\n        self.save_path = save_path\n        if tokenizer is None:\n            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n\n    def __getitem__(self, idx):\n        return self.memory_thread[idx]\n\n    def __len__(self):\n        return self.memory_thread.shape[0]\n\n    def save(self, path: Union[str,None]) -> None:\n        if path is None:\n            path = os.path.join(self.save_path,f'{self.name}.parquet')\n        self.memory_thread.write_parquet(\n            file=path,\n            compression='zstd',\n            compression_level=None,\n            statistics=False,\n            row_group_size=None,\n            use_pyarrow=True,\n            pyarrow_options=None\n        )\n\n    def load(self, path: Union[str,None]) -> None:\n        if path is None:\n            path = os.path.join(self.save_path,f'{self.name}.parquet')\n        self.memory_thread = pl.read_parquet(source = path,\n                        use_pyarrow= True,\n                        memory_map = True,\n                        )\n        \n    def get_total_tokens_from_thread(self):\n        return self.memory_thread[\"tokens_count\"].sum()\n\n    def reset_memory(self) -> None:\n        self.memory_thread = pl.DataFrame(schema=self.memory_schema) \n\n    def dict_to_row(self, message_dict:Dict[str,str]) -> pl.DataFrame:\n        timestamp = message_dict['timestamp'] if 'timestamp' in message_dict else [now()]\n        return pl.DataFrame(schema=self.memory_schema,\n                            data={ \"role\":[message_dict[\"role\"]],\n                                  \"content\":[message_dict[\"content\"]],\n                                  \"timestamp\": timestamp,\n                                  \"tokens_count\":[len(self.tokenizer.encode(message_dict[\"content\"]))+7]\n                                  })\n\n    def get_message_tokens_from_dict(self, message_dict: dict) -> int:\n        \"\"\"\n        Calculate the number of tokens in a message, including the role token.\n\n        :param message_dict: A dictionary containing the role and content of the message.\n        :return: The total number of tokens in the message.\n        \"\"\"\n        message_dict = check_dict(message_dict)\n        message = message_dict[\"content\"]\n        return len(self.tokenizer.encode(message)) + 7  # +7 for the role token\n\n    def get_message_role_from_dict(self, message_dict: dict) -> str:\n        \"\"\"\n        Get the role of the message from a message dictionary.\n\n        :param message_dict: A dictionary containing the role and content of the message.\n        :return: The role of the message.\n        \"\"\"\n        message_dict = check_dict(message_dict)\n        return message_dict[\"role\"]\n\n    def add_dict_to_thread(self, message_dict: dict) -> None:\n        \"\"\"\n        Add a message to the memory thread.\n\n        :param message_dict: A dictionary containing the role and content of the message.\n        \"\"\"\n        \n        new_message_row = self.dict_to_row(message_dict)\n\n        if (\n            self.max_memory is None\n            or self.total_tokens + new_message_row['tokens_count'] <= self.max_memory\n        ):\n            self.memory_thread = pl.concat([self.memory_thread, new_message_row], rechunk=True)\n            self.total_tokens = self.get_total_tokens_from_thread()\n        else:\n            print(\"The memory BaseThread is full, the last message was not added\")\n            \n\n    def remove_dict_from_thread(\n        self, message_dict: Union[Dict, None] = None, idx: Union[int, None] = None\n    ) -> None:\n        \n        if message_dict is None and idx is None:\n            raise Exception(\"You need to provide either a message_dict or an idx\")\n\n        elif idx is not None and idx < len(self.memory_thread):\n            \n            self.memory_thread = self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\"index\") != idx).drop(\"index\").collect()\n            self.total_tokens = self.get_total_tokens_from_thread() \n            return\n\n        elif message_dict is not None:\n            message_dict = check_dict(message_dict)\n            self.memory_thread = self.memory_thread.lazy().filter(self.memory_thread[\"content\"] != message_dict[\"content\"]).collect()\n            self.total_tokens = self.get_total_tokens_from_thread()\n            return\n\n        else:\n            raise Exception(\"Index was out bound and no corresponding content found.\")\n\n\n    def find_message(\n        self, message: Union[dict, str]\n        ) -> pl.DataFrame:\n        \"\"\"\n        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\n        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\n\n        message = message if isinstance(message, str) else check_dict(message)\n        \n        if isinstance(message,str):\n            return self.memory_thread.lazy().filter(pl.col(\"content\") == message).collect()\n\n        else:\n            return self.memory_thread.lazy().filter((pl.col(\"content\") == message[\"content\"]) & (pl.col(\"role\") == message['role'])).collect()\n\n\n        \n\n    def last_message(self, role: Union[str, None] = None) ->  pl.DataFrame:\n        \"\"\"\n        Get the last message in the memory thread with a specific role.\"\"\"\n        if role is None:\n            return self.memory_thread[-1]\n        else:\n            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[-1]\n\n    def first_message(self, role: Union[str, None] = None) -> pl.DataFrame: \n        \"\"\"\n        Get the first message in the memory thread with a specific role.\"\"\"\n        if role is None:\n            return self.memory_thread[0]\n        else:\n            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[0]\n\n\n    def messages_before( self, message: dict   ) -> pl.DataFrame:\n        \"\"\"\n        Get all messages before a specific message in the memory thread.\"\"\"\n        \n        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==message['content']) & (pl.col('role')==message['role'])).select('index').collect()[0][0]\n        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col('index')<index).collect()\n    \n    def messages_after( self, message: dict   ) -> pl.DataFrame:\n        \"\"\"\n        Get all messages after a specific message in the memory thread.\"\"\"\n        \n        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==message['content']) & (pl.col('role')==message['role'])).select('index').collect()[0][0]\n        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col('index')>index).collect()\n\n    def messages_between(\n        self, start_message: dict, end_message: dict ) -> pl.DataFrame:\n        \"\"\"\n        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\n        start_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==start_message['content']) & (pl.col('role')==start_message['role'])).select('index').collect()[0][0]\n        end_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==end_message['content']) & (pl.col('role')==end_message['role'])).select('index').collect()[0][0]\n        return self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('index')<end_index) & (pl.col('index'))>start_index).collect()\n\n    def messages_more_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\n        \"\"\"\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n        \n        return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('tokens_count'))>tokens).collect()\n\n    def messages_less_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\n        \"\"\"\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n        \n        return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('tokens_count'))<tokens).collect()\n\n\n    def messages_after_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\n        \"\"\"\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n        \n        return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('timestamp')>timestamp)).collect()\n\n    def messages_before_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\n        \"\"\"\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n        \n        return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('timestamp')<timestamp)).collect()\n\n    def select_col(self, feature: Union[List[str],str]):\n        return self.memory_thread[feature]\n\n    def filter_col(self, feature: str, filter: str):\n        try:\n            return self.memory_thread.lazy().filter(pl.col(feature) == filter).collect()\n        except Exception as e:\n            return str(e)\n\n\n\n    def token_bound_history(\n        self, max_tokens: int, role: Union[str, None] = None\n    ):\n\n        reversed_df = self.memory_thread.lazy().with_row_count(\"index\").reverse()   \n        reversed_df = reversed_df.with_columns(pl.col(\"tokens_count\").cumsum().alias(\"cum_tokens_count\"))  \n        filtered_df = reversed_df.filter((pl.col(\"cum_tokens_count\") <= max_tokens) & (pl.col(\"role\") != role)).collect()\n        messages = filtered_df['content']\n        indexes = filtered_df['index']\n\n        return messages,indexes\n\n    \n    def load_from_gpt_url(self, url: str):\n\n        response = requests.get(url)\n        if response.status_code == 200:\n        \n            soup = BeautifulSoup(response.text, 'html.parser')\n        else:\n            raise ValueError(f\"Non  stato possibile accedere alla pagina. Codice di stato: {response.status_code}\")\n            \n        \n        next_data = soup.find('script', id='__NEXT_DATA__')\n\n        if next_data is not None:\n\n            data_string = next_data.string  # pyright: ignore \n\n            json_obj = json.loads(data_string)  # pyright: ignore\n\n            conversation_data = json_obj['props']['pageProps']['serverResponse']['data']\n\n            messages = conversation_data['mapping']\n\n            for _,value in messages.items():\n                if ('parent' in value.keys()) and (value['message']['content']['parts'][0] != ''):\n                    message_dict = {'role':value['message']['author']['role'],\n                                    'content': value['message']['content']['parts'][0],\n                                    'timestamp': value['message']['create_time']}\n                    self.add_dict_to_thread(message_dict)\n            print(\"Conversation loaded succesfully\")\n        else:\n            raise ValueError(f\"Nessuna conversazione trovata a questo link!\")\n\n\n\n        \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(\n    self,\n    name: str = \"memory\",\n    max_memory: Optional[int] = None,\n    tokenizer: Optional[Any] = None,\n    save_path: str = 'threads'\n\n) -> None:\n    \"\"\"\n        Initialize the BaseThread instance.\n\n        :param name: The name of the memory thread. Defaults to 'memory'.\n        :param max_memory: The maximum number of tokens allowed in the memory thread.\n                           Defaults to None, which means no limit.\n        :param tokenizer: The tokenizer to be used for tokenizing messages.\n                          Defaults to None, which means using the tiktoken encoding for the 'gpt-3.5-turbo' model.\n        \"\"\"\n    self.name = name\n    self.max_memory = max_memory\n    self.memory_schema = {\"role\": pl.Utf8, \"content\": pl.Utf8,\"timestamp\":pl.Float64,\"tokens_count\":pl.UInt16}\n    self.memory_thread: pl.DataFrame = pl.DataFrame(schema=self.memory_schema)\n    \"\"\" self.time_stamps = [] \"\"\"\n    \"\"\" self.message_tokens = [] \"\"\"\n    self.total_tokens = self.get_total_tokens_from_thread()\n    self.save_path = save_path\n    if tokenizer is None:\n        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __getitem__(self, idx):\n    return self.memory_thread[idx]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __len__(self):\n    return self.memory_thread.shape[0]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef save(self, path: Union[str,None]) -> None:\n    if path is None:\n        path = os.path.join(self.save_path,f'{self.name}.parquet')\n    self.memory_thread.write_parquet(\n        file=path,\n        compression='zstd',\n        compression_level=None,\n        statistics=False,\n        row_group_size=None,\n        use_pyarrow=True,\n        pyarrow_options=None\n    )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef load(self, path: Union[str,None]) -> None:\n    if path is None:\n        path = os.path.join(self.save_path,f'{self.name}.parquet')\n    self.memory_thread = pl.read_parquet(source = path,\n                    use_pyarrow= True,\n                    memory_map = True,\n                    )\n    \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def get_total_tokens_from_thread(self):\n    return self.memory_thread[\"tokens_count\"].sum()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef reset_memory(self) -> None:\n    self.memory_thread = pl.DataFrame(schema=self.memory_schema) \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef dict_to_row(self, message_dict:Dict[str,str]) -> pl.DataFrame:\n    timestamp = message_dict['timestamp'] if 'timestamp' in message_dict else [now()]\n    return pl.DataFrame(schema=self.memory_schema,\n                        data={ \"role\":[message_dict[\"role\"]],\n                              \"content\":[message_dict[\"content\"]],\n                              \"timestamp\": timestamp,\n                              \"tokens_count\":[len(self.tokenizer.encode(message_dict[\"content\"]))+7]\n                              })\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_message_tokens_from_dict(self, message_dict: dict) -> int:\n    \"\"\"\n        Calculate the number of tokens in a message, including the role token.\n\n        :param message_dict: A dictionary containing the role and content of the message.\n        :return: The total number of tokens in the message.\n        \"\"\"\n    message_dict = check_dict(message_dict)\n    message = message_dict[\"content\"]\n    return len(self.tokenizer.encode(message)) + 7  # +7 for the role token\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_message_role_from_dict(self, message_dict: dict) -> str:\n    \"\"\"\n        Get the role of the message from a message dictionary.\n\n        :param message_dict: A dictionary containing the role and content of the message.\n        :return: The role of the message.\n        \"\"\"\n    message_dict = check_dict(message_dict)\n    return message_dict[\"role\"]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef add_dict_to_thread(self, message_dict: dict) -> None:\n    \"\"\"\n        Add a message to the memory thread.\n\n        :param message_dict: A dictionary containing the role and content of the message.\n        \"\"\"\n    \n    new_message_row = self.dict_to_row(message_dict)\n\n    if (\n        self.max_memory is None\n        or self.total_tokens + new_message_row['tokens_count'] <= self.max_memory\n    ):\n        self.memory_thread = pl.concat([self.memory_thread, new_message_row], rechunk=True)\n        self.total_tokens = self.get_total_tokens_from_thread()\n    else:\n        print(\"The memory BaseThread is full, the last message was not added\")\n        \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef remove_dict_from_thread(\n    self, message_dict: Union[Dict, None] = None, idx: Union[int, None] = None\n) -> None:\n    \n    if message_dict is None and idx is None:\n        raise Exception(\"You need to provide either a message_dict or an idx\")\n\n    elif idx is not None and idx < len(self.memory_thread):\n        \n        self.memory_thread = self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\"index\") != idx).drop(\"index\").collect()\n        self.total_tokens = self.get_total_tokens_from_thread() \n        return\n\n    elif message_dict is not None:\n        message_dict = check_dict(message_dict)\n        self.memory_thread = self.memory_thread.lazy().filter(self.memory_thread[\"content\"] != message_dict[\"content\"]).collect()\n        self.total_tokens = self.get_total_tokens_from_thread()\n        return\n\n    else:\n        raise Exception(\"Index was out bound and no corresponding content found.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef find_message(\n    self, message: Union[dict, str]\n    ) -> pl.DataFrame:\n    \"\"\"\n        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\n        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\n\n    message = message if isinstance(message, str) else check_dict(message)\n    \n    if isinstance(message,str):\n        return self.memory_thread.lazy().filter(pl.col(\"content\") == message).collect()\n\n    else:\n        return self.memory_thread.lazy().filter((pl.col(\"content\") == message[\"content\"]) & (pl.col(\"role\") == message['role'])).collect()\n\n\n    \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef last_message(self, role: Union[str, None] = None) ->  pl.DataFrame:\n    \"\"\"\n        Get the last message in the memory thread with a specific role.\"\"\"\n    if role is None:\n        return self.memory_thread[-1]\n    else:\n        return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[-1]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef first_message(self, role: Union[str, None] = None) -> pl.DataFrame: \n    \"\"\"\n        Get the first message in the memory thread with a specific role.\"\"\"\n    if role is None:\n        return self.memory_thread[0]\n    else:\n        return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[0]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef messages_before( self, message: dict   ) -> pl.DataFrame:\n    \"\"\"\n        Get all messages before a specific message in the memory thread.\"\"\"\n    \n    index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==message['content']) & (pl.col('role')==message['role'])).select('index').collect()[0][0]\n    return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col('index')<index).collect()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef messages_after( self, message: dict   ) -> pl.DataFrame:\n    \"\"\"\n        Get all messages after a specific message in the memory thread.\"\"\"\n    \n    index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==message['content']) & (pl.col('role')==message['role'])).select('index').collect()[0][0]\n    return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col('index')>index).collect()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef messages_between(\n    self, start_message: dict, end_message: dict ) -> pl.DataFrame:\n    \"\"\"\n        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\n    start_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==start_message['content']) & (pl.col('role')==start_message['role'])).select('index').collect()[0][0]\n    end_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==end_message['content']) & (pl.col('role')==end_message['role'])).select('index').collect()[0][0]\n    return self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('index')<end_index) & (pl.col('index'))>start_index).collect()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef messages_more_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\n    \"\"\"\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n    \n    return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('tokens_count'))>tokens).collect()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef messages_less_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\n    \"\"\"\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n    \n    return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('tokens_count'))<tokens).collect()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef messages_after_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\n    \"\"\"\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n    \n    return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('timestamp')>timestamp)).collect()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef messages_before_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\n    \"\"\"\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n    \n    return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('timestamp')<timestamp)).collect()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef select_col(self, feature: Union[List[str],str]):\n    return self.memory_thread[feature]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef filter_col(self, feature: str, filter: str):\n    try:\n        return self.memory_thread.lazy().filter(pl.col(feature) == filter).collect()\n    except Exception as e:\n        return str(e)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n\ndef token_bound_history(\n    self, max_tokens: int, role: Union[str, None] = None\n):\n\n    reversed_df = self.memory_thread.lazy().with_row_count(\"index\").reverse()   \n    reversed_df = reversed_df.with_columns(pl.col(\"tokens_count\").cumsum().alias(\"cum_tokens_count\"))  \n    filtered_df = reversed_df.filter((pl.col(\"cum_tokens_count\") <= max_tokens) & (pl.col(\"role\") != role)).collect()\n    messages = filtered_df['content']\n    indexes = filtered_df['index']\n\n    return messages,indexes\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef load_from_gpt_url(self, url: str):\n\n    response = requests.get(url)\n    if response.status_code == 200:\n        \n        soup = BeautifulSoup(response.text, 'html.parser')\n    else:\n        raise ValueError(f\"Non  stato possibile accedere alla pagina. Codice di stato: {response.status_code}\")\n        \n    \n    next_data = soup.find('script', id='__NEXT_DATA__')\n\n    if next_data is not None:\n\n        data_string = next_data.string  # pyright: ignore \n\n        json_obj = json.loads(data_string)  # pyright: ignore\n\n        conversation_data = json_obj['props']['pageProps']['serverResponse']['data']\n\n        messages = conversation_data['mapping']\n\n        for _,value in messages.items():\n            if ('parent' in value.keys()) and (value['message']['content']['parts'][0] != ''):\n                message_dict = {'role':value['message']['author']['role'],\n                                'content': value['message']['content']['parts'][0],\n                                'timestamp': value['message']['create_time']}\n                self.add_dict_to_thread(message_dict)\n        print(\"Conversation loaded succesfully\")\n    else:\n        raise ValueError(f\"Nessuna conversazione trovata a questo link!\")\n\n\n\n    \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass FifoThread(BaseThread):\n    \"\"\"FIFO Memory BaseThread, the oldest messages are removed first when reaching the max_memory limit, the memory is defined in terms of tokens,\n    outs are passe to the longterm_memory, lucid_memory is a redundant memory that stores all the messages\n    \"\"\"\n\n    def __init__(\n        self, name=\"fifo_memory\", max_memory=None, longterm_thread=None, redundant=True\n    ):\n\n        BaseThread.__init__(self, name=name, max_memory=None)\n        if redundant is True:\n            self.redundant_thread = BaseThread(name=\"lucid_memory\", max_memory=None)\n        else:\n            self.redundant_thread = None\n        if longterm_thread is None:\n            self.longterm_thread = BaseThread(name=\"longterm_memory\", max_memory=None)\n        else:\n            self.longterm_thread = longterm_thread\n        # create an alias for the memory_thread to make the code more readable\n        self.fifo_thread = self.memory_thread\n        self.max_memory = max_memory\n\n    def to_longterm(self, idx: int):\n        \"\"\"move the message at the index idx to the longterm_memory\"\"\"\n        # move the message at the index idx to the longterm_memory\n        display(\n            Markdown(\n                \"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(\n                    idx\n                )\n            )\n        )\n        # this should not remove everything\n        # there should be a check to make sure the thread isnt left empty\n        message = copy.deepcopy(self.memory_thread[idx])\n        # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\n        self.longterm_thread.add_message(message)\n        self.remove_message(idx=idx)\n\n    def add_message(self, message_dict: dict):\n        \"\"\"add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\n        # message_dict = {\"role\": role, \"content\": content}\n        # chek that the message_dict is a dictionary or a list of dictionaries\n        message_dict = check_dict(message_dict)\n        if self.redundant_thread is not None:\n            self.redundant_thread.add_message(message_dict)\n        message_tokens = self.get_message_tokens(message_dict)\n\n        if self.total_tokens + message_tokens > self.max_memory:\n            while self.total_tokens + message_tokens > self.max_memory:\n                if len(self.memory_thread) > 0:\n                    self.to_longterm(idx=0)\n                    message_tokens = self.get_message_tokens(message_dict)  # Update message_tokens\n                    self.total_tokens -= message_tokens  # Update self.total_tokens\n\n            super().add_message(message_dict)\n\n        else:\n            # add the message_dict to the memory_thread\n            # update the total number of tokens\n            super().add_message(message_dict)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(\n    self, name=\"fifo_memory\", max_memory=None, longterm_thread=None, redundant=True\n):\n\n    BaseThread.__init__(self, name=name, max_memory=None)\n    if redundant is True:\n        self.redundant_thread = BaseThread(name=\"lucid_memory\", max_memory=None)\n    else:\n        self.redundant_thread = None\n    if longterm_thread is None:\n        self.longterm_thread = BaseThread(name=\"longterm_memory\", max_memory=None)\n    else:\n        self.longterm_thread = longterm_thread\n    # create an alias for the memory_thread to make the code more readable\n    self.fifo_thread = self.memory_thread\n    self.max_memory = max_memory\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef to_longterm(self, idx: int):\n    \"\"\"move the message at the index idx to the longterm_memory\"\"\"\n    # move the message at the index idx to the longterm_memory\n    display(\n        Markdown(\n            \"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(\n                idx\n            )\n        )\n    )\n    # this should not remove everything\n    # there should be a check to make sure the thread isnt left empty\n    message = copy.deepcopy(self.memory_thread[idx])\n    # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\n    self.longterm_thread.add_message(message)\n    self.remove_message(idx=idx)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef add_message(self, message_dict: dict):\n    \"\"\"add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\n    # message_dict = {\"role\": role, \"content\": content}\n    # chek that the message_dict is a dictionary or a list of dictionaries\n    message_dict = check_dict(message_dict)\n    if self.redundant_thread is not None:\n        self.redundant_thread.add_message(message_dict)\n    message_tokens = self.get_message_tokens(message_dict)\n\n    if self.total_tokens + message_tokens > self.max_memory:\n        while self.total_tokens + message_tokens > self.max_memory:\n            if len(self.memory_thread) > 0:\n                self.to_longterm(idx=0)\n                message_tokens = self.get_message_tokens(message_dict)  # Update message_tokens\n                self.total_tokens -= message_tokens  # Update self.total_tokens\n\n        super().add_message(message_dict)\n\n    else:\n        # add the message_dict to the memory_thread\n        # update the total number of tokens\n        super().add_message(message_dict)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n\nclass MemoryIndex(NpIndex):\n    \"\"\"\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\n    \"\"\"\n    @staticmethod\n    def check_uniform_context_type(context: List[Any]) -> None:\n        \"\"\"Check if all context elements are of the same type.\"\"\"\n        if not all(isinstance(x, type(context[0])) for x in context):\n            raise ValueError(\"All context elements must be of the same type.\")\n\n    def __init__(\n        self,\n        values: Optional[List[str]] = None,\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n        context: Optional[List[Any]] = None,\n        name: str = \"memory_index\",\n        save_path: Optional[str] = None,\n        load: bool = False,\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n        markdown: str = \"text/markdown\",\n        token_overflow_strategy: str = \"ignore\",\n    ):\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\n        if context is not None and len(context) != len(values):\n            raise ValueError(\"The context must have the same length as the values\")\n\n\n        self.markdown = markdown\n        if context is not None and values is not None:\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\n\n        if context is not None:\n            self.check_uniform_context_type(context)\n            self.context_type = type(context[0])\n\n\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\n        if isinstance(identifier, list):\n            return [self.get_context(value) for value in identifier]\n        else:\n            id = self.identify_input(identifier)\n            value = self.values[id]\n            return self.context[value]\n\n    def clean_context(self):\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\n        self.context = {value: self.context[value] for value in self.values}\n\n    def add_to_context(self, value: str, context: Any):\n        \"\"\" add a context to a value \"\"\"\n        if not isinstance(context, self.context_type):\n            raise ValueError(\"The context must be of the same type as the other contexts\")\n        if value in self.values:\n            if value not in self.context:\n                self.context[value] = []\n            self.context[value].append(context)\n\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\n        if isinstance(values, str):\n            values = [values]\n        NpIndex.add(self, values, embedding)\n        if context is not None:\n            for value, cont in zip(values, context):\n                self.add_to_context(value, cont)\n\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\n        if not isinstance(identifier, list):\n            id = self.identify_input(identifier)\n            value = self.values[id]\n        NpIndex.remove(self, identifier)\n        if not isinstance(identifier, list):\n            self.context.pop(value)\n\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n        #recover value from old_identifier\n        if new_context is not None:\n            if not isinstance(new_context, self.context_type):\n                raise ValueError(\"The context must be of the same type as the other contexts\")\n        if not isinstance(old_identifier, list):\n            old_id = self.identify_input(old_identifier)\n            old_value = self.values[old_id]\n            # Only perform the update if the old_value is not the same as the new_value.\n            if old_value != new_value:\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\n\n            if new_context is not None:\n                self.context[new_value] = [new_context]\n            else:\n                self.context[new_value] = self.context.pop(old_value)\n        else:\n            self.context[new_value] = self.context.pop(old_value)\n\n    @classmethod\n    def from_pandas(\n        cls,\n        data_frame: Union[pd.DataFrame, str],\n        value_column: str,\n        embeddings_column: Optional[str] = None,\n        context_columns: Optional[List[str]] = None,\n        name: str = \"memory_index\",\n        save_path: Optional[str] = None,\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n        markdown: str = \"text/markdown\",\n        token_overflow_strategy: str = \"ignore\",\n    ) -> \"MemoryIndex\":\n        if (\n            isinstance(data_frame, str)\n            and data_frame.endswith(\".csv\")\n            and os.path.isfile(data_frame)\n        ):\n            logger.info(\"Loading the CSV file\")\n            data_frame = pd.read_csv(data_frame)\n            name = os.path.basename(data_frame).split(\".\")[0]\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\n            logger.info(\"Loading the pandas DataFrame\")\n        else:\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\n\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\n        if context_columns is not None:\n            context = get_context_from_pandas(data_frame, context_columns)\n\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n\n    @classmethod\n    def from_hf_dataset(\n        cls,\n        dataset_url: str,\n        value_column: str,\n        data_split: str = \"train\",\n        embeddings_column: Optional[str] = None,\n        context_columns: Optional[List[str]] = None,\n        name: str = \"memory_index\",\n        save_path: Optional[str] = None,\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n        markdown: str = \"text/markdown\",\n        token_overflow_strategy: str = \"ignore\",\n    ) -> \"MemoryIndex\":\n        dataset = load_dataset(dataset_url)[data_split]\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\n        if context_columns is not None:\n            context = get_context_from_hf(dataset, context_columns)\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\n\n    @classmethod\n    def from_polars(\n        cls,\n        data_frame: pl.DataFrame,\n        value_column: str,\n        embeddings_column: Optional[str] = None,\n        context_columns: Optional[List[str]] = None,\n        name: str = \"memory_index\",\n        save_path: Optional[str] = None,\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n        markdown: str = \"text/markdown\",\n        token_overflow_strategy: str = \"ignore\",\n    ) -> \"MemoryIndex\":\n        print(\"Loading the Polars DataFrame\")\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\n        if context_columns is not None:\n            context = get_context_from_polars(data_frame, context_columns)\n\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n\n\n    @classmethod\n    def from_python(\n        cls,\n        directory_path: str,\n        minify_code: bool = False,\n        remove_docstrings: bool = False,\n        name: str = \"memory_index\",\n        save_path: Optional[str] = None,\n        markdown: str = \"python/markdown\",\n        resolution: str = \"both\",\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n        token_overflow_strategy: str = \"ignore\",\n    ) -> \"MemoryIndex\":\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"@staticmethod\ndef check_uniform_context_type(context: List[Any]) -> None:\n    \"\"\"Check if all context elements are of the same type.\"\"\"\n    if not all(isinstance(x, type(context[0])) for x in context):\n        raise ValueError(\"All context elements must be of the same type.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(\n    self,\n    values: Optional[List[str]] = None,\n    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n    context: Optional[List[Any]] = None,\n    name: str = \"memory_index\",\n    save_path: Optional[str] = None,\n    load: bool = False,\n    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n    markdown: str = \"text/markdown\",\n    token_overflow_strategy: str = \"ignore\",\n):\n    NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\n    if context is not None and len(context) != len(values):\n        raise ValueError(\"The context must have the same length as the values\")\n\n\n    self.markdown = markdown\n    if context is not None and values is not None:\n        self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\n\n    if context is not None:\n        self.check_uniform_context_type(context)\n        self.context_type = type(context[0])\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\n    \"\"\" get the context of a value id or embedding or a list of \"\"\"\n    if isinstance(identifier, list):\n        return [self.get_context(value) for value in identifier]\n    else:\n        id = self.identify_input(identifier)\n        value = self.values[id]\n        return self.context[value]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef clean_context(self):\n    \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\n    self.context = {value: self.context[value] for value in self.values}\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef add_to_context(self, value: str, context: Any):\n    \"\"\" add a context to a value \"\"\"\n    if not isinstance(context, self.context_type):\n        raise ValueError(\"The context must be of the same type as the other contexts\")\n    if value in self.values:\n        if value not in self.context:\n            self.context[value] = []\n        self.context[value].append(context)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\n    \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\n    if isinstance(values, str):\n        values = [values]\n    NpIndex.add(self, values, embedding)\n    if context is not None:\n        for value, cont in zip(values, context):\n            self.add_to_context(value, cont)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\n    if not isinstance(identifier, list):\n        id = self.identify_input(identifier)\n        value = self.values[id]\n    NpIndex.remove(self, identifier)\n    if not isinstance(identifier, list):\n        self.context.pop(value)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n    #recover value from old_identifier\n    if new_context is not None:\n        if not isinstance(new_context, self.context_type):\n            raise ValueError(\"The context must be of the same type as the other contexts\")\n    if not isinstance(old_identifier, list):\n        old_id = self.identify_input(old_identifier)\n        old_value = self.values[old_id]\n        # Only perform the update if the old_value is not the same as the new_value.\n        if old_value != new_value:\n            NpIndex.update(self, old_identifier, new_value, new_embedding)\n\n        if new_context is not None:\n            self.context[new_value] = [new_context]\n        else:\n            self.context[new_value] = self.context.pop(old_value)\n    else:\n        self.context[new_value] = self.context.pop(old_value)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@classmethod\ndef from_pandas(\n    cls,\n    data_frame: Union[pd.DataFrame, str],\n    value_column: str,\n    embeddings_column: Optional[str] = None,\n    context_columns: Optional[List[str]] = None,\n    name: str = \"memory_index\",\n    save_path: Optional[str] = None,\n    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n    markdown: str = \"text/markdown\",\n    token_overflow_strategy: str = \"ignore\",\n) -> \"MemoryIndex\":\n    if (\n        isinstance(data_frame, str)\n        and data_frame.endswith(\".csv\")\n        and os.path.isfile(data_frame)\n    ):\n        logger.info(\"Loading the CSV file\")\n        data_frame = pd.read_csv(data_frame)\n        name = os.path.basename(data_frame).split(\".\")[0]\n    elif isinstance(data_frame, pd.core.frame.DataFrame):\n        logger.info(\"Loading the pandas DataFrame\")\n    else:\n        raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\n\n    values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\n    if context_columns is not None:\n        context = get_context_from_pandas(data_frame, context_columns)\n\n    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@classmethod\ndef from_hf_dataset(\n    cls,\n    dataset_url: str,\n    value_column: str,\n    data_split: str = \"train\",\n    embeddings_column: Optional[str] = None,\n    context_columns: Optional[List[str]] = None,\n    name: str = \"memory_index\",\n    save_path: Optional[str] = None,\n    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n    markdown: str = \"text/markdown\",\n    token_overflow_strategy: str = \"ignore\",\n) -> \"MemoryIndex\":\n    dataset = load_dataset(dataset_url)[data_split]\n    values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\n    if context_columns is not None:\n        context = get_context_from_hf(dataset, context_columns)\n    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@classmethod\ndef from_polars(\n    cls,\n    data_frame: pl.DataFrame,\n    value_column: str,\n    embeddings_column: Optional[str] = None,\n    context_columns: Optional[List[str]] = None,\n    name: str = \"memory_index\",\n    save_path: Optional[str] = None,\n    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n    markdown: str = \"text/markdown\",\n    token_overflow_strategy: str = \"ignore\",\n) -> \"MemoryIndex\":\n    print(\"Loading the Polars DataFrame\")\n    values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\n    if context_columns is not None:\n        context = get_context_from_polars(data_frame, context_columns)\n\n    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n@classmethod\ndef from_python(\n    cls,\n    directory_path: str,\n    minify_code: bool = False,\n    remove_docstrings: bool = False,\n    name: str = \"memory_index\",\n    save_path: Optional[str] = None,\n    markdown: str = \"python/markdown\",\n    resolution: str = \"both\",\n    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n    token_overflow_strategy: str = \"ignore\",\n) -> \"MemoryIndex\":\n    values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n    logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n    return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass PythonIndex(MemoryIndex, PythonParser):\n    def __init__(\n        self,\n        directory_path: str,\n        name: str = \"python_index\",\n        save_path: Optional[str] = None,\n        load: bool = False,\n        minify_code: bool = False,\n        remove_docstrings: bool = False,\n        tokenizer: Optional[tiktoken.Encoding] = None,\n        max_workers: int = 1,\n        backup: bool = False,\n        filter: str = \"class_function\"\n    ):\n\n        PythonParser.__init__(\n            self,\n            directory_path=directory_path,\n            minify_code=minify_code,\n            remove_docstrings=remove_docstrings,\n        )\n        #check if load folder exists\n        if save_path is None:\n            save_path = \"storage\"\n        load_directory = os.path.join(save_path, name)\n        loadcheck = not load or not os.path.exists(load_directory)\n        if load and not os.path.exists(load_directory):\n            print(\"No python-index found even if load=True, indexing from scratch\")\n        if loadcheck:\n            # Extract functions and classes source code\n            function_source_codes, class_source_codes, _, _ = self.process_directory()\n            print(\n                \"Indexing {} functions and {} classes\".format(\n                    len(function_source_codes), len(class_source_codes)\n                )\n            )\n            # Concatenate function and class source code and index them\n            if filter == \"function\":\n                codes = function_source_codes\n            elif filter == \"class\":\n                codes = class_source_codes\n            elif filter == \"class_function\":\n                codes = function_source_codes + class_source_codes\n            load = False\n            self.function_source_codes = function_source_codes\n            self.class_source_codes = class_source_codes\n\n         # Initialize the MemoryIndex\n        MemoryIndex.__init__(\n            self,\n            name=name,\n            values=codes if loadcheck else None,\n            save_path=save_path,\n            load=load,\n            tokenizer=tokenizer,\n            max_workers=max_workers,\n            backup=backup,\n        )\n        self.markdown = \"python\"\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    directory_path: str,\n    name: str = \"python_index\",\n    save_path: Optional[str] = None,\n    load: bool = False,\n    minify_code: bool = False,\n    remove_docstrings: bool = False,\n    tokenizer: Optional[tiktoken.Encoding] = None,\n    max_workers: int = 1,\n    backup: bool = False,\n    filter: str = \"class_function\"\n):\n\n    PythonParser.__init__(\n        self,\n        directory_path=directory_path,\n        minify_code=minify_code,\n        remove_docstrings=remove_docstrings,\n    )\n    #check if load folder exists\n    if save_path is None:\n        save_path = \"storage\"\n    load_directory = os.path.join(save_path, name)\n    loadcheck = not load or not os.path.exists(load_directory)\n    if load and not os.path.exists(load_directory):\n        print(\"No python-index found even if load=True, indexing from scratch\")\n    if loadcheck:\n        # Extract functions and classes source code\n        function_source_codes, class_source_codes, _, _ = self.process_directory()\n        print(\n            \"Indexing {} functions and {} classes\".format(\n                len(function_source_codes), len(class_source_codes)\n            )\n        )\n        # Concatenate function and class source code and index them\n        if filter == \"function\":\n            codes = function_source_codes\n        elif filter == \"class\":\n            codes = class_source_codes\n        elif filter == \"class_function\":\n            codes = function_source_codes + class_source_codes\n        load = False\n        self.function_source_codes = function_source_codes\n        self.class_source_codes = class_source_codes\n\n     # Initialize the MemoryIndex\n    MemoryIndex.__init__(\n        self,\n        name=name,\n        values=codes if loadcheck else None,\n        save_path=save_path,\n        load=load,\n        tokenizer=tokenizer,\n        max_workers=max_workers,\n        backup=backup,\n    )\n    self.markdown = \"python\"\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef prune_index(\n    cls: \"MemoryIndex\",\n    constraint: Optional[str] = None,\n    regex_pattern: Optional[str] = None,\n    length_constraint: Optional[Tuple[int,int]] = None,\n    tokenizer: Optional[tiktoken.Encoding] = None,\n) -> \"MemoryIndex\" :\n    if constraint is not None:\n        if constraint == \"regex\":\n            if regex_pattern is None:\n                raise ValueError(\"regex_pattern must be provided for regex constraint.\")\n            pruned_values, pruned_embeddings = _prune_by_regex(cls, regex_pattern)\n        elif constraint == \"length\":\n            if length_constraint is None:\n                raise ValueError(\"length_constraint must be provided for length constraint.\")\n            pruned_values, pruned_embeddings = _prune_by_length(cls, length_constraint, tokenizer)\n        else:\n            raise ValueError(\"Invalid constraint type provided.\")\n    else:\n        raise ValueError(\"constraint must be provided for pruning the index.\")\n\n    pruned_memory_index = cls.__class__(\n        values=pruned_values,\n        embeddings=pruned_embeddings,\n        name=cls.name + \"_pruned\",\n    )\n\n    return pruned_memory_index\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef _prune_by_regex(cls: \"MemoryIndex\", regex_pattern: str) -> Tuple[List[str], List[np.ndarray]]:\n    pruned_values = []\n    pruned_embeddings = []\n\n    for value in cls.values:\n        if re.search(regex_pattern, value):\n            pruned_values.append(value)\n            pruned_embeddings.append(cls.get_embedding_by_value(value))\n\n    return pruned_values, pruned_embeddings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef _prune_by_length(cls: \"MemoryIndex\", length_constraint: Tuple[int,int], tokenizer) -> Tuple[List[str], List[np.ndarray]]:\n    pruned_values = []\n    pruned_embeddings = []\n    if tokenizer is None:\n        len_func = len\n    else:\n        def len_func(value):\n            return len(tokenizer.encode(value))\n    print(\"Pruning by length\")\n    print(\"Length constraint: \", length_constraint)\n    print(\"Number of values: \", len(cls.values))\n    print(\"tokenizer: \", tokenizer)\n    for value in cls.values:\n        if len_func(value) <= length_constraint[1] and len_func(value) >= length_constraint[0]:\n            print(f\"value {value} is in range {length_constraint}\")\n            pruned_values.append(value)\n            pruned_embeddings.append(cls.get_embedding_by_value(value))\n\n    return pruned_values, pruned_embeddings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def len_func(value):\n    return len(tokenizer.encode(value))\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef save(cls):\n    save_directory = os.path.join(cls.save_path, cls.name)\n    os.makedirs(save_directory, exist_ok=True)\n\n    index_filename = os.path.join(save_directory, f\"{cls.name}_index.faiss\")\n    faiss.write_index(cls.index, index_filename)\n\n    values_filename = os.path.join(save_directory, f\"{cls.name}_values.json\")\n    with open(values_filename, \"w\") as f:\n        json.dump(cls.values, f)\n\n    # embeddings_filename = os.path.join(save_directory, f\"{cls.name}_embeddings.npz\")\n    # np.savez_compressed(embeddings_filename, cls.get_all_embeddings())\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef load(cls):\n    load_directory = os.path.join(cls.save_path, cls.name)\n    if not os.path.exists(load_directory):\n        cls.loaded = False\n        print(\"I did not find the directory to load the index from.\", load_directory)\n        return\n\n    print(f\"Loading index from {load_directory}\")\n\n    index_filename = os.path.join(load_directory, f\"{cls.name}_index.faiss\")\n    cls.index = faiss.read_index(index_filename)\n\n    values_filename = os.path.join(load_directory, f\"{cls.name}_values.json\")\n    with open(values_filename, \"r\") as f:\n        cls.values = json.load(f)\n    # embeddings_filename = os.path.join(load_directory, f\"{cls.name}_embeddings.npz\")\n    # print(embeddings_filename)\n    # embeddings_data = np.load(embeddings_filename)\n    # cls.embeddings = embeddings_data[\"arr_0\"]\n    cls.loaded = True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass MemoryIndex:\n    \"\"\"\n    this class is a wrapper for a faiss index, it contains information about the format of the index the faiss index itself\n    \"\"\"\n\n    def __init__(\n        self,\n        index: Optional[faiss.Index] = None,\n        values: Optional[List[str]] = None,\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n        name: str = \"memory_index\",\n        save_path: Optional[str] = None,\n        load: bool = False,\n        tokenizer: Optional[tiktoken.Encoding] = None,\n        max_workers: int = 1,\n        backup: bool = False,\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n        is_batched: bool = False,\n        markdown: str = \"text/markdown\",\n    ):\n\n        self.name = name\n        self.embedder = embedder()\n        self.save_path = save_path if save_path is not None else \"storage\"\n        os.makedirs(self.save_path, exist_ok=True)\n        self.values = []\n        self.embeddings = []\n        self.max_workers = max_workers\n        self.is_batched = is_batched\n        self.markdown = markdown\n\n        if load is True:\n            self.load()\n        else:\n            self.loaded = False\n        if not self.loaded:\n            if (\n                self.max_workers > 1\n                and values is not None\n                and embeddings is None\n                and index is None\n            ):\n                embeddings = parallel_embeddings(self.embedder,\n                    values, max_workers, backup=backup, name=name\n                )\n            self.init_index(index, values, embeddings, is_embed_batched=is_batched)\n        if tokenizer is None:\n            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n        else:\n            self.tokenizer = tokenizer\n        self.query_history = []\n        if not self.loaded:\n            self.save()\n\n    def init_index(\n        self,\n        index: Optional[faiss.Index] = None,\n        values: Optional[List[str]] = None,\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n        is_embed_batched: bool = False,\n    ) -> None:\n\n        \"\"\"\n        initializes the index, there are 4 cases:\n        1. we create a new index from scratch\n        2. we create a new index from a list of embeddings and values\n        3. we create a new index from a faiss index and values list\n        4. we load an index from a file\n        \"\"\"\n        if index is None and values is None and embeddings is None:\n            print(\"Creating a new index\")\n            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n            self.values = []\n        elif (\n            index is None\n            and values is not None\n            and embeddings is not None\n            and len(values) == len(embeddings)\n        ):\n            print(\"Creating a new index from a list of embeddings and values\")\n            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n            #add all the embeddings to the index\n            if is_embed_batched:\n                print(\"Adding batched embeddings to index\")\n                print(type(embeddings))\n                embeddings = np.array(embeddings)\n                self.add_batch_to_index(values=values, embeddings=embeddings)\n            else:\n                for embedding, value in zip(embeddings, values):\n                    self.add_to_index(value, embedding)\n\n        elif (\n            isinstance(index, faiss.Index)\n            and index.d == self.embedder.get_embedding_size()\n            and type(values) == list\n            and len(values) == index.ntotal\n        ):\n            print(\"Creating a new index from a faiss index and values list\")\n            self.index = index\n            self.values = values\n        elif index is None and values is not None and embeddings is None:\n            print(\"Creating a new index from a list of values\")\n            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n            if is_embed_batched:\n                batch = []\n                i = 0\n                for value in values:\n                    batch.append(value)\n                    if len(batch) == 1000:\n                        start = time.time()\n                        self.add_batch_to_index(values=batch)\n                        print(f\"Embedding batch {i} took \", time.time() - start, \" seconds\")\n                        print(f\"Batch {i} of {len(values)//1000}\")\n                        i +=1\n                        batch = []\n                if len(batch) > 0:\n                    self.add_batch_to_index(values=batch)\n            else:\n                i = 0\n                for value in values:\n                    print(\"Embedding value \", i, \" of \", len(values))\n                    start = time.time()\n                    self.add_to_index(value)\n                    print(\"Embedding value \", i, \" took \", time.time() - start, \" seconds\")\n                    i += 1\n        else:\n            print(type(values))\n            print(type(embeddings))\n            print(type(index))\n            raise ValueError(\n                \"The index is not a valid faiss index or the embedding dimension is not correct\"\n            )\n        print(len(self.values), \" values in the index\")\n        print(self.index.ntotal, \" embeddings in the index\")\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        del state[\"index\"]\n\n        index_buffer = io.BytesIO()\n        faiss.write_index(state[\"index\"], index_buffer)\n        state[\"index_bytes\"] = index_buffer.getvalue()\n\n        return state\n\n    def __setstate__(self, state):\n        index_buffer = io.BytesIO(state[\"index_bytes\"])\n        state[\"index\"] = faiss.read_index(index_buffer)\n\n        del state[\"index_bytes\"]\n\n        self.__dict__.update(state)\n\n    @classmethod\n    def from_pandas(\n        cls,\n        data_frame: Union[pd.DataFrame, str],\n        columns: Optional[Union[str, List[str]]] = None,\n        name: str = \"memory_index\",\n        save_path: Optional[str] = None,\n        in_place: bool = True,\n        embeddings_col: Optional[str] = None,\n        markdown: str = \"text/markdown\",\n    ) -> \"MemoryIndex\":\n        \"\"\"\n        Initialize a MemoryIndex object from a pandas DataFrame.\n\n        Args:\n            data_frame: The DataFrame or path to a CSV file.\n            columns: The columns of the DataFrame to use as values.\n            name: The name of the index.\n            save_path: The path to save the index.\n            in_place: Whether to work on the DataFrame in place or create a copy.\n            embeddings_col: The column name containing the embeddings.\n\n        Returns:\n            A MemoryIndex object initialized with values and embeddings from the DataFrame.\n        \"\"\"\n\n        if (\n            isinstance(data_frame, str)\n            and data_frame.endswith(\".csv\")\n            and os.path.isfile(data_frame)\n        ):\n            print(\"Loading the CSV file\")\n            try:\n                data_frame = pd.read_csv(data_frame)\n            except:\n                raise ValueError(\"The CSV file is not valid\")\n            name = data_frame.split(\"/\")[-1].split(\".\")[0]\n        elif isinstance(data_frame, pd.core.frame.DataFrame) and columns is not None:\n            print(\"Loading the DataFrame\")\n            if not in_place:\n                data_frame = copy.deepcopy(data_frame)\n        else:\n            raise ValueError(\n                \"The data_frame is not a valid pandas dataframe or the columns are not valid or the path is not valid\"\n            )\n\n        values, embeddings = extract_values_and_embeddings(\n            data_frame, columns, embeddings_col\n        )\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown)\n\n    @classmethod\n    def from_hf_dataset(\n        cls,\n        dataset_url: str,\n        value_column: str,\n        embeddings_column: Optional[str] = None,\n        name: str = \"memory_index\",\n        save_path: Optional[str] = None,\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= CohereEmbedder,\n        is_batched: bool = False,\n        markdown: str = \"text/markdown\"\n    ) -> \"MemoryIndex\":\n        \"\"\"\n        Initialize a MemoryIndex object from a Hugging Face dataset.\n\n        Args:\n            dataset_url: The URL of the Hugging Face dataset.\n            value_column: The column of the dataset to use as values.\n            embeddings_column: The column of the dataset containing the embeddings.\n            name: The name of the index.\n            save_path: The path to save the index.\n\n        Returns:\n            A MemoryIndex object initialized with values and embeddings from the Hugging Face dataset.\n        \"\"\"\n        dataset = load_dataset(dataset_url)['train']\n        if embeddings_column is not None:\n            values, embeddings = extract_values_and_embeddings_hf(\n                dataset, value_column, embeddings_column\n            )\n        elif embeddings_column is None:\n            values = extract_values_hf(dataset, value_column)\n            embeddings = None\n        else:\n            raise ValueError(\n                \"The dataset is not a valid Hugging Face dataset or the columns are not valid\"\n            )\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, is_batched=is_batched, markdown=markdown)\n\n    def add_to_index(\n        self,\n        value: str,\n        embedding: Optional[Union[List[float], np.ndarray, str]] = None,\n        verbose: bool = False,\n        default_save: bool = False,\n    ) -> None:\n        \"\"\"\n        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n        \"\"\"\n        if value not in self.values:\n            if embedding is None:\n                embedding = self.embedder.embed(value)\n                if verbose:\n                    display(\n                        Markdown(\"The value {value} was embedded\".format(value=value))\n                    )\n                if type(embedding) is list:\n                    embedding = np.array([embedding])\n                self.index.add(embedding)\n                self.values.append(value)\n            elif embedding is not None:\n                if type(embedding) is list:\n                    embedding = np.array([embedding])\n                elif type(embedding) is str:\n                    try:\n                        embedding = eval(embedding)\n                        embedding = np.array([embedding]).astype(np.float32)\n                    except (SyntaxError, ValueError):\n                        print(\"The string is not a valid list, probably an error:\", embedding)\n                        return\n                elif type(embedding) is not np.ndarray:\n                    raise ValueError(\"The embedding is not a valid type\")\n\n                if embedding.ndim == 1:\n                    embedding = embedding.reshape(1, -1)\n                self.index.add(embedding)\n                self.values.append(value)\n                if default_save:\n                    self.save()  # we should check here the save time is not too long\n        else:\n            if verbose:\n                display(\n                    Markdown(\n                        \"The value {value} was already in the index\".format(value=value)\n                    )\n                )\n    def add_batch_to_index(\n        self,\n        values: List[str],\n        embeddings: Optional[Union[List[float], np.ndarray, str]] = None,\n        verbose: bool = False,\n        default_save: bool = False,\n    ) -> None:\n        \"\"\"\n        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n        \"\"\"\n\n        if embeddings is None:\n            embeddings = self.embedder.batch_embed(values)\n            if verbose:\n                display(\n                    Markdown(\"The value batch was embedded\")\n                )\n            if type(embeddings) is list:\n                embeddings = np.array(embeddings)\n            self.index.add(embeddings)\n            self.values.extend(values)\n        elif embeddings is not None:\n            if type(embeddings) is list:\n                embeddings = np.array([embeddings])\n            elif type(embeddings) is str:\n                try:\n                    embeddings = eval(embeddings)\n                    embeddings = np.array([embeddings]).astype(np.float32)\n                except (SyntaxError, ValueError):\n                    print(\"The string is not a valid list, probably an error:\", embeddings)\n                    return\n            elif type(embeddings) is not np.ndarray:\n                raise ValueError(\"The embedding is not a valid type\")\n\n            self.index.add(embeddings)\n            self.values.extend(values)\n            if default_save:\n                self.save()  # we should check here the save time is not too long\n\n    def remove_from_id(self, id: int) -> None:\n        \"\"\"\n        Remove a value from the index and the values list.\n        Args:\n            value: The value to remove from the index.\n        \"\"\"\n        if id is not None:\n            self.values.pop(id)\n            self.embeddings = np.array(list(self.embeddings).pop(id))\n            id_selector = faiss.IDSelectorArray(n=1, ids=np.array([id], dtype=np.int64))\n            self.index.remove_ids(id_selector)            \n            self.save()\n        else:\n            print(f\"The value '{id}' was not found in the index.\")\n\n    def remove_from_index(self, value: str) -> None:\n        \"\"\"\n        Remove a value from the index and the values list.\n        Args:\n            value: The value to remove from the index.\n        \"\"\"\n        index = self.get_index_by_value(value)\n        if index is not None:\n            self.values.pop(index)\n\n            id_selector = faiss.IDSelectorArray(np.array([index], dtype=np.int64))\n            self.index.remove_ids(id_selector)\n\n            self.save()\n        else:\n            print(f\"The value '{value}' was not found in the index.\")\n\n    def get_embedding_by_index(self, index: int) -> np.ndarray:\n        \"\"\"\n        Get the embedding corresponding to a certain index value.\n        \"\"\"\n        if index < 0 or index >= len(self.values):\n            raise ValueError(\"The index is out of range\")\n\n        embedding = self.index.reconstruct(index)\n\n        return embedding\n\n    def get_index_by_value(self, value: str) -> Optional[int]:\n        \"\"\"\n        Get the index corresponding to a value in self.values.\n        \"\"\"\n        if value in self.values:\n            index = self.values.index(value)\n            return index\n        else:\n            return None\n\n    def get_embedding_by_value(self, value: str) -> Optional[np.ndarray]:\n        \"\"\"\n        Get the embedding corresponding to a certain value in self.values.\n        \"\"\"\n        index = self.get_index_by_value(value)\n        if index is not None:\n            embedding = self.get_embedding_by_index(index)\n            return embedding\n        else:\n            return None\n\n    def get_all_embeddings(self) -> np.ndarray:\n        \"\"\"\n        Get all the embeddings in the index.\n        \"\"\"\n        embeddings = []\n        for i in range(len(self.values)):\n            embeddings.append(self.get_embedding_by_index(i))\n        self.embeddings = np.array(embeddings)\n        return self.embeddings\n\n    def faiss_query(self, query: str, k: int = 10) -> Tuple[List[str], List[float]]:\n        \"\"\"Query the faiss index for the top-k most similar values to the query\"\"\"\n\n        embedding = self.embedder.embed(query)\n        if k > len(self.values):\n            k = len(self.values)\n        D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\n        values = [self.values[i] for i in I[0]]\n        scores = [d for d in D[0]]\n        return values, scores, I\n\n    def token_bound_query(self, query, k=10, max_tokens=4000):\n        \"\"\"Query the faiss index for the top-k most similar values to the query, but bound the number of tokens retrieved by the max_tokens parameter\"\"\"\n        returned_tokens = 0\n        top_k_hint = []\n        scores = []\n        tokens = []\n        indices = []\n\n        if len(self.values) > 0:\n            top_k, scores, indices = self.faiss_query(query, k=min(k, len(self.values)))\n\n            for hint in top_k:\n                message_tokens = len(self.tokenizer.encode(hint))\n                tokens.append(message_tokens)\n                if returned_tokens + message_tokens <= max_tokens:\n                    top_k_hint += [hint]\n                    returned_tokens += message_tokens\n\n            self.query_history.append(\n                {\n                    \"query\": query,\n                    \"hints\": top_k_hint,\n                    \"scores\": scores,\n                    \"indices\": indices,\n                    \"hints_tokens\": tokens,\n                    \"returned_tokens\": returned_tokens,\n                    \"max_tokens\": max_tokens,\n                    \"k\": k,\n                }\n            )\n\n        return top_k_hint, scores, indices\n\n    def save(self):\n        save(self)\n\n    def load(self):\n        load(self)\n\n    def prune(\n        self,\n        constraint: Optional[str] = None,\n        regex_pattern: Optional[str] = None,\n        length_constraint: Optional[int] = None,\n        tokenizer: Optional[tiktoken.Encoding] = None,\n    ) -> \"MemoryIndex\":\n        if tokenizer is None:\n            tokenizer = self.tokenizer\n        return prune_index(self, constraint, regex_pattern, length_constraint,tokenizer)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(\n    self,\n    index: Optional[faiss.Index] = None,\n    values: Optional[List[str]] = None,\n    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n    name: str = \"memory_index\",\n    save_path: Optional[str] = None,\n    load: bool = False,\n    tokenizer: Optional[tiktoken.Encoding] = None,\n    max_workers: int = 1,\n    backup: bool = False,\n    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n    is_batched: bool = False,\n    markdown: str = \"text/markdown\",\n):\n\n    self.name = name\n    self.embedder = embedder()\n    self.save_path = save_path if save_path is not None else \"storage\"\n    os.makedirs(self.save_path, exist_ok=True)\n    self.values = []\n    self.embeddings = []\n    self.max_workers = max_workers\n    self.is_batched = is_batched\n    self.markdown = markdown\n\n    if load is True:\n        self.load()\n    else:\n        self.loaded = False\n    if not self.loaded:\n        if (\n            self.max_workers > 1\n            and values is not None\n            and embeddings is None\n            and index is None\n        ):\n            embeddings = parallel_embeddings(self.embedder,\n                values, max_workers, backup=backup, name=name\n            )\n        self.init_index(index, values, embeddings, is_embed_batched=is_batched)\n    if tokenizer is None:\n        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n    else:\n        self.tokenizer = tokenizer\n    self.query_history = []\n    if not self.loaded:\n        self.save()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef init_index(\n    self,\n    index: Optional[faiss.Index] = None,\n    values: Optional[List[str]] = None,\n    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n    is_embed_batched: bool = False,\n) -> None:\n\n    \"\"\"\n        initializes the index, there are 4 cases:\n        1. we create a new index from scratch\n        2. we create a new index from a list of embeddings and values\n        3. we create a new index from a faiss index and values list\n        4. we load an index from a file\n        \"\"\"\n    if index is None and values is None and embeddings is None:\n        print(\"Creating a new index\")\n        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n        self.values = []\n    elif (\n        index is None\n        and values is not None\n        and embeddings is not None\n        and len(values) == len(embeddings)\n    ):\n        print(\"Creating a new index from a list of embeddings and values\")\n        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n        #add all the embeddings to the index\n        if is_embed_batched:\n            print(\"Adding batched embeddings to index\")\n            print(type(embeddings))\n            embeddings = np.array(embeddings)\n            self.add_batch_to_index(values=values, embeddings=embeddings)\n        else:\n            for embedding, value in zip(embeddings, values):\n                self.add_to_index(value, embedding)\n\n    elif (\n        isinstance(index, faiss.Index)\n        and index.d == self.embedder.get_embedding_size()\n        and type(values) == list\n        and len(values) == index.ntotal\n    ):\n        print(\"Creating a new index from a faiss index and values list\")\n        self.index = index\n        self.values = values\n    elif index is None and values is not None and embeddings is None:\n        print(\"Creating a new index from a list of values\")\n        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n        if is_embed_batched:\n            batch = []\n            i = 0\n            for value in values:\n                batch.append(value)\n                if len(batch) == 1000:\n                    start = time.time()\n                    self.add_batch_to_index(values=batch)\n                    print(f\"Embedding batch {i} took \", time.time() - start, \" seconds\")\n                    print(f\"Batch {i} of {len(values)//1000}\")\n                    i +=1\n                    batch = []\n            if len(batch) > 0:\n                self.add_batch_to_index(values=batch)\n        else:\n            i = 0\n            for value in values:\n                print(\"Embedding value \", i, \" of \", len(values))\n                start = time.time()\n                self.add_to_index(value)\n                print(\"Embedding value \", i, \" took \", time.time() - start, \" seconds\")\n                i += 1\n    else:\n        print(type(values))\n        print(type(embeddings))\n        print(type(index))\n        raise ValueError(\n            \"The index is not a valid faiss index or the embedding dimension is not correct\"\n        )\n    print(len(self.values), \" values in the index\")\n    print(self.index.ntotal, \" embeddings in the index\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __getstate__(self):\n    state = self.__dict__.copy()\n    del state[\"index\"]\n\n    index_buffer = io.BytesIO()\n    faiss.write_index(state[\"index\"], index_buffer)\n    state[\"index_bytes\"] = index_buffer.getvalue()\n\n    return state\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __setstate__(self, state):\n    index_buffer = io.BytesIO(state[\"index_bytes\"])\n    state[\"index\"] = faiss.read_index(index_buffer)\n\n    del state[\"index_bytes\"]\n\n    self.__dict__.update(state)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@classmethod\ndef from_pandas(\n    cls,\n    data_frame: Union[pd.DataFrame, str],\n    columns: Optional[Union[str, List[str]]] = None,\n    name: str = \"memory_index\",\n    save_path: Optional[str] = None,\n    in_place: bool = True,\n    embeddings_col: Optional[str] = None,\n    markdown: str = \"text/markdown\",\n) -> \"MemoryIndex\":\n    \"\"\"\n        Initialize a MemoryIndex object from a pandas DataFrame.\n\n        Args:\n            data_frame: The DataFrame or path to a CSV file.\n            columns: The columns of the DataFrame to use as values.\n            name: The name of the index.\n            save_path: The path to save the index.\n            in_place: Whether to work on the DataFrame in place or create a copy.\n            embeddings_col: The column name containing the embeddings.\n\n        Returns:\n            A MemoryIndex object initialized with values and embeddings from the DataFrame.\n        \"\"\"\n\n    if (\n        isinstance(data_frame, str)\n        and data_frame.endswith(\".csv\")\n        and os.path.isfile(data_frame)\n    ):\n        print(\"Loading the CSV file\")\n        try:\n            data_frame = pd.read_csv(data_frame)\n        except:\n            raise ValueError(\"The CSV file is not valid\")\n        name = data_frame.split(\"/\")[-1].split(\".\")[0]\n    elif isinstance(data_frame, pd.core.frame.DataFrame) and columns is not None:\n        print(\"Loading the DataFrame\")\n        if not in_place:\n            data_frame = copy.deepcopy(data_frame)\n    else:\n        raise ValueError(\n            \"The data_frame is not a valid pandas dataframe or the columns are not valid or the path is not valid\"\n        )\n\n    values, embeddings = extract_values_and_embeddings(\n        data_frame, columns, embeddings_col\n    )\n    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@classmethod\ndef from_hf_dataset(\n    cls,\n    dataset_url: str,\n    value_column: str,\n    embeddings_column: Optional[str] = None,\n    name: str = \"memory_index\",\n    save_path: Optional[str] = None,\n    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= CohereEmbedder,\n    is_batched: bool = False,\n    markdown: str = \"text/markdown\"\n) -> \"MemoryIndex\":\n    \"\"\"\n        Initialize a MemoryIndex object from a Hugging Face dataset.\n\n        Args:\n            dataset_url: The URL of the Hugging Face dataset.\n            value_column: The column of the dataset to use as values.\n            embeddings_column: The column of the dataset containing the embeddings.\n            name: The name of the index.\n            save_path: The path to save the index.\n\n        Returns:\n            A MemoryIndex object initialized with values and embeddings from the Hugging Face dataset.\n        \"\"\"\n    dataset = load_dataset(dataset_url)['train']\n    if embeddings_column is not None:\n        values, embeddings = extract_values_and_embeddings_hf(\n            dataset, value_column, embeddings_column\n        )\n    elif embeddings_column is None:\n        values = extract_values_hf(dataset, value_column)\n        embeddings = None\n    else:\n        raise ValueError(\n            \"The dataset is not a valid Hugging Face dataset or the columns are not valid\"\n        )\n    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, is_batched=is_batched, markdown=markdown)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef add_to_index(\n    self,\n    value: str,\n    embedding: Optional[Union[List[float], np.ndarray, str]] = None,\n    verbose: bool = False,\n    default_save: bool = False,\n) -> None:\n    \"\"\"\n        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n        \"\"\"\n    if value not in self.values:\n        if embedding is None:\n            embedding = self.embedder.embed(value)\n            if verbose:\n                display(\n                    Markdown(\"The value {value} was embedded\".format(value=value))\n                )\n            if type(embedding) is list:\n                embedding = np.array([embedding])\n            self.index.add(embedding)\n            self.values.append(value)\n        elif embedding is not None:\n            if type(embedding) is list:\n                embedding = np.array([embedding])\n            elif type(embedding) is str:\n                try:\n                    embedding = eval(embedding)\n                    embedding = np.array([embedding]).astype(np.float32)\n                except (SyntaxError, ValueError):\n                    print(\"The string is not a valid list, probably an error:\", embedding)\n                    return\n            elif type(embedding) is not np.ndarray:\n                raise ValueError(\"The embedding is not a valid type\")\n\n            if embedding.ndim == 1:\n                embedding = embedding.reshape(1, -1)\n            self.index.add(embedding)\n            self.values.append(value)\n            if default_save:\n                self.save()  # we should check here the save time is not too long\n    else:\n        if verbose:\n            display(\n                Markdown(\n                    \"The value {value} was already in the index\".format(value=value)\n                )\n            )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def add_batch_to_index(\n    self,\n    values: List[str],\n    embeddings: Optional[Union[List[float], np.ndarray, str]] = None,\n    verbose: bool = False,\n    default_save: bool = False,\n) -> None:\n    \"\"\"\n        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n        \"\"\"\n\n    if embeddings is None:\n        embeddings = self.embedder.batch_embed(values)\n        if verbose:\n            display(\n                Markdown(\"The value batch was embedded\")\n            )\n        if type(embeddings) is list:\n            embeddings = np.array(embeddings)\n        self.index.add(embeddings)\n        self.values.extend(values)\n    elif embeddings is not None:\n        if type(embeddings) is list:\n            embeddings = np.array([embeddings])\n        elif type(embeddings) is str:\n            try:\n                embeddings = eval(embeddings)\n                embeddings = np.array([embeddings]).astype(np.float32)\n            except (SyntaxError, ValueError):\n                print(\"The string is not a valid list, probably an error:\", embeddings)\n                return\n        elif type(embeddings) is not np.ndarray:\n            raise ValueError(\"The embedding is not a valid type\")\n\n        self.index.add(embeddings)\n        self.values.extend(values)\n        if default_save:\n            self.save()  # we should check here the save time is not too long\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef remove_from_id(self, id: int) -> None:\n    \"\"\"\n        Remove a value from the index and the values list.\n        Args:\n            value: The value to remove from the index.\n        \"\"\"\n    if id is not None:\n        self.values.pop(id)\n        self.embeddings = np.array(list(self.embeddings).pop(id))\n        id_selector = faiss.IDSelectorArray(n=1, ids=np.array([id], dtype=np.int64))\n        self.index.remove_ids(id_selector)            \n        self.save()\n    else:\n        print(f\"The value '{id}' was not found in the index.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef remove_from_index(self, value: str) -> None:\n    \"\"\"\n        Remove a value from the index and the values list.\n        Args:\n            value: The value to remove from the index.\n        \"\"\"\n    index = self.get_index_by_value(value)\n    if index is not None:\n        self.values.pop(index)\n\n        id_selector = faiss.IDSelectorArray(np.array([index], dtype=np.int64))\n        self.index.remove_ids(id_selector)\n\n        self.save()\n    else:\n        print(f\"The value '{value}' was not found in the index.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_embedding_by_index(self, index: int) -> np.ndarray:\n    \"\"\"\n        Get the embedding corresponding to a certain index value.\n        \"\"\"\n    if index < 0 or index >= len(self.values):\n        raise ValueError(\"The index is out of range\")\n\n    embedding = self.index.reconstruct(index)\n\n    return embedding\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_index_by_value(self, value: str) -> Optional[int]:\n    \"\"\"\n        Get the index corresponding to a value in self.values.\n        \"\"\"\n    if value in self.values:\n        index = self.values.index(value)\n        return index\n    else:\n        return None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_embedding_by_value(self, value: str) -> Optional[np.ndarray]:\n    \"\"\"\n        Get the embedding corresponding to a certain value in self.values.\n        \"\"\"\n    index = self.get_index_by_value(value)\n    if index is not None:\n        embedding = self.get_embedding_by_index(index)\n        return embedding\n    else:\n        return None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_all_embeddings(self) -> np.ndarray:\n    \"\"\"\n        Get all the embeddings in the index.\n        \"\"\"\n    embeddings = []\n    for i in range(len(self.values)):\n        embeddings.append(self.get_embedding_by_index(i))\n    self.embeddings = np.array(embeddings)\n    return self.embeddings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef faiss_query(self, query: str, k: int = 10) -> Tuple[List[str], List[float]]:\n    \"\"\"Query the faiss index for the top-k most similar values to the query\"\"\"\n\n    embedding = self.embedder.embed(query)\n    if k > len(self.values):\n        k = len(self.values)\n    D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\n    values = [self.values[i] for i in I[0]]\n    scores = [d for d in D[0]]\n    return values, scores, I\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef token_bound_query(self, query, k=10, max_tokens=4000):\n    \"\"\"Query the faiss index for the top-k most similar values to the query, but bound the number of tokens retrieved by the max_tokens parameter\"\"\"\n    returned_tokens = 0\n    top_k_hint = []\n    scores = []\n    tokens = []\n    indices = []\n\n    if len(self.values) > 0:\n        top_k, scores, indices = self.faiss_query(query, k=min(k, len(self.values)))\n\n        for hint in top_k:\n            message_tokens = len(self.tokenizer.encode(hint))\n            tokens.append(message_tokens)\n            if returned_tokens + message_tokens <= max_tokens:\n                top_k_hint += [hint]\n                returned_tokens += message_tokens\n\n        self.query_history.append(\n            {\n                \"query\": query,\n                \"hints\": top_k_hint,\n                \"scores\": scores,\n                \"indices\": indices,\n                \"hints_tokens\": tokens,\n                \"returned_tokens\": returned_tokens,\n                \"max_tokens\": max_tokens,\n                \"k\": k,\n            }\n        )\n\n    return top_k_hint, scores, indices\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef save(self):\n    save(self)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef load(self):\n    load(self)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef prune(\n    self,\n    constraint: Optional[str] = None,\n    regex_pattern: Optional[str] = None,\n    length_constraint: Optional[int] = None,\n    tokenizer: Optional[tiktoken.Encoding] = None,\n) -> \"MemoryIndex\":\n    if tokenizer is None:\n        tokenizer = self.tokenizer\n    return prune_index(self, constraint, regex_pattern, length_constraint,tokenizer)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass PandasIndex(MemoryIndex):\n    \"\"\"\n    A class to create an index of a pandas DataFrame, allowing querying on specified columns.\n    Inherits from MemoryIndex class.\n    \"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        row_func: Optional[Callable[[pd.Series], str]] = None,\n        name=\"pandas_index\",\n        columns: Optional[List[str]] = None,\n        load=False,\n    ):\n        \"\"\"\n        Initialize a PandasIndex object.\n\n        Args:\n            df: A pandas DataFrame to index.\n            row_func: An optional function to process rows before adding them to the index.\n            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n        \"\"\"\n        if row_func is None:\n            row_func = lambda row: str(row)\n        self.row_func = row_func\n\n        self.df = df\n        MemoryIndex.__init__(\n            self, name=name, load=load\n        )  # Initialize the parent MemoryIndex class\n\n        for _, row in df.iterrows():\n            self.add_to_index(row_func(row))\n\n        self.columns: Dict[str, MemoryIndex] = {}\n\n        # Set up columns during initialization\n        if columns is None:\n            self.setup_columns()\n        else:\n            self.setup_columns(columns)\n        self.save()\n        for col in self.columns:\n            self.columns[col].save()\n        self.executed_tasks = []\n\n    def setup_columns(self, columns: Optional[List[str]] = None):\n        \"\"\"\n        Set up columns for indexing.\n\n        Args:\n            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n        \"\"\"\n        if columns is None:\n            # Use string columns or columns with lists containing a single string by default\n            columns = []\n\n        for col in columns:\n            self.columns[col] = MemoryIndex.from_pandas(\n                self.df, columns=col, name=f\"{self.name}_{col}\"\n            )\n\n    def query_columns(self, query: str, columns: List[str]) -> List[Tuple[str, float]]:\n        \"\"\"\n        Query the indexed columns of the DataFrame.\n\n        Args:\n            query: The search query as a string.\n            columns: A list of column names to query.\n\n        Returns:\n            A list of tuples containing the matched value and its similarity score.\n        \"\"\"\n        results = []\n        for col in columns:\n            if col in self.columns:\n                results.extend(self.columns[col].faiss_query(query))\n            else:\n                raise KeyError(\n                    f\"Column '{col}' not found in PandaDb columns dictionary.\"\n                )\n        return results\n\n    def add_row(self, row: pd.Series) -> None:\n        \"\"\"\n        Add a row to the DataFrame and update the row and column indexes.\n\n        Args:\n            row: A pandas Series representing the row to add.\n        \"\"\"\n        self.df = pd.concat([self.df, row.to_frame().T], ignore_index=True)\n        self.add_to_index(self.row_func(row))\n\n        for col in self.columns:\n            if col in row:\n                self.columns[col].add_to_index(row[col])\n\n    def remove_row(self, index: int) -> None:\n        \"\"\"\n        Remove a row from the DataFrame and update the row and column indexes.\n\n        Args:\n            index: The index of the row to remove.\n        \"\"\"\n        if 0 <= index < len(self.df):\n            self.remove_from_index(self.values[index])\n\n            for col in self.columns:\n                self.columns[col].remove_from_index(self.columns[col].values[index])\n\n            self.df.drop(index, inplace=True)\n            self.df.reset_index(drop=True, inplace=True)\n        else:\n            raise IndexError(\n                f\"Index {index} is out of bounds for DataFrame with length {len(self.df)}\"\n            )\n\n    def rows_from_value(\n        self, value: Union[str, int, float], column: Optional[str] = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Return all rows of the DataFrame that have a particular value in the row index or a column index.\n\n        Args:\n            value: The value to search for in the DataFrame.\n            column: The name of the column to search in. If None, search in the row index.\n\n        Returns:\n            A pandas DataFrame containing the rows with the specified value.\n        \"\"\"\n        if column is None:\n            return self.df.loc[self.df.index == value]\n        else:\n            if column in self.df.columns:\n                return self.df.loc[self.df[column] == value]\n            else:\n                raise KeyError(f\"Column '{column}' not found in the DataFrame.\")\n\n    def apply_llmtask(\n        self,\n        path: List[List[int]],\n        chatbot: Chat,\n        task_name=None,\n        write_func=None,\n        columns: Optional[List[str]] = None,\n        task_id=None,\n        max_workers=1,\n        calls_per_minute: int = 20,\n    ) -> pd.DataFrame:\n        \"\"\"\n        Apply a writing task to the specified columns or the main index, and create new modified indexes and a corresponding DataFrame with new values.\n\n        Args:\n            write_task: An instance of a writing task (subclass of BaseTask).\n            columns: A list of column names to apply the writing task to, or None (default) to apply the task to the main index.\n\n        Returns:\n            A pandas DataFrame containing the modified values in the specified columns or a new column with the modified values of the main index.\n        \"\"\"\n        modified_df = self.df.copy()\n        if task_name is None and task_id is None:\n            task_name = \"llm_task\"\n        elif task_name is None:\n            task_name = task_id\n\n        if columns is None:\n            # Apply the writing task to the main index\n            write_index = self\n            write_task = LLMWriter(\n                write_index,\n                path,\n                chatbot,\n                task_name=task_name,\n                write_func=write_func,\n                context=self.df,\n                task_id=task_id,\n                max_workers=max_workers,\n                calls_per_minute=calls_per_minute,\n            )\n\n            new_index = write_task.write()\n\n            # Create a mapping of old values to new values\n            old_to_new_values = dict(zip(self.values, new_index.values))\n\n            # Update the row values in the modified DataFrame\n            modified_df[\"new_column\"] = modified_df.apply(\n                lambda row: old_to_new_values.get(\n                    self.row_func(row), self.row_func(row)\n                ),\n                axis=1,\n            )\n        else:\n            # Iterate over the specified columns\n            for col in columns:\n                if col in self.columns:\n                    # Apply the writing task to the column\n                    write_index = self.columns[col]\n                    write_task = LLMWriter(\n                        write_index,\n                        path,\n                        chatbot,\n                        write_func=write_func,\n                        context=self.df,\n                        task_id=task_id,\n                        max_workers=max_workers,\n                        calls_per_minute=calls_per_minute,\n                    )\n                    new_index = write_task.write()\n\n                    # Create a mapping of old values to new values\n                    old_to_new_values = dict(\n                        zip(self.columns[col].values, new_index.values)\n                    )\n\n                    # Update the column values in the modified DataFrame\n                    modified_df[col] = modified_df[col].apply(\n                        lambda x: old_to_new_values.get(x, x)\n                    )\n\n                    # Update the column's MemoryIndex\n                    self.columns[col] = new_index\n                    self.columns[col].save()\n                else:\n                    raise KeyError(\n                        f\"Column '{col}' not found in PandasIndex columns dictionary.\"\n                    )\n        # remove context from the write_task to avoid memory leak\n        write_task.context = None\n        self.executed_tasks.append({\"task\": write_task, \"output\": modified_df})\n\n        return modified_df\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(\n    self,\n    df: pd.DataFrame,\n    row_func: Optional[Callable[[pd.Series], str]] = None,\n    name=\"pandas_index\",\n    columns: Optional[List[str]] = None,\n    load=False,\n):\n    \"\"\"\n        Initialize a PandasIndex object.\n\n        Args:\n            df: A pandas DataFrame to index.\n            row_func: An optional function to process rows before adding them to the index.\n            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n        \"\"\"\n    if row_func is None:\n        row_func = lambda row: str(row)\n    self.row_func = row_func\n\n    self.df = df\n    MemoryIndex.__init__(\n        self, name=name, load=load\n    )  # Initialize the parent MemoryIndex class\n\n    for _, row in df.iterrows():\n        self.add_to_index(row_func(row))\n\n    self.columns: Dict[str, MemoryIndex] = {}\n\n    # Set up columns during initialization\n    if columns is None:\n        self.setup_columns()\n    else:\n        self.setup_columns(columns)\n    self.save()\n    for col in self.columns:\n        self.columns[col].save()\n    self.executed_tasks = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef setup_columns(self, columns: Optional[List[str]] = None):\n    \"\"\"\n        Set up columns for indexing.\n\n        Args:\n            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n        \"\"\"\n    if columns is None:\n        # Use string columns or columns with lists containing a single string by default\n        columns = []\n\n    for col in columns:\n        self.columns[col] = MemoryIndex.from_pandas(\n            self.df, columns=col, name=f\"{self.name}_{col}\"\n        )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef query_columns(self, query: str, columns: List[str]) -> List[Tuple[str, float]]:\n    \"\"\"\n        Query the indexed columns of the DataFrame.\n\n        Args:\n            query: The search query as a string.\n            columns: A list of column names to query.\n\n        Returns:\n            A list of tuples containing the matched value and its similarity score.\n        \"\"\"\n    results = []\n    for col in columns:\n        if col in self.columns:\n            results.extend(self.columns[col].faiss_query(query))\n        else:\n            raise KeyError(\n                f\"Column '{col}' not found in PandaDb columns dictionary.\"\n            )\n    return results\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef add_row(self, row: pd.Series) -> None:\n    \"\"\"\n        Add a row to the DataFrame and update the row and column indexes.\n\n        Args:\n            row: A pandas Series representing the row to add.\n        \"\"\"\n    self.df = pd.concat([self.df, row.to_frame().T], ignore_index=True)\n    self.add_to_index(self.row_func(row))\n\n    for col in self.columns:\n        if col in row:\n            self.columns[col].add_to_index(row[col])\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef remove_row(self, index: int) -> None:\n    \"\"\"\n        Remove a row from the DataFrame and update the row and column indexes.\n\n        Args:\n            index: The index of the row to remove.\n        \"\"\"\n    if 0 <= index < len(self.df):\n        self.remove_from_index(self.values[index])\n\n        for col in self.columns:\n            self.columns[col].remove_from_index(self.columns[col].values[index])\n\n        self.df.drop(index, inplace=True)\n        self.df.reset_index(drop=True, inplace=True)\n    else:\n        raise IndexError(\n            f\"Index {index} is out of bounds for DataFrame with length {len(self.df)}\"\n        )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef rows_from_value(\n    self, value: Union[str, int, float], column: Optional[str] = None\n) -> pd.DataFrame:\n    \"\"\"\n        Return all rows of the DataFrame that have a particular value in the row index or a column index.\n\n        Args:\n            value: The value to search for in the DataFrame.\n            column: The name of the column to search in. If None, search in the row index.\n\n        Returns:\n            A pandas DataFrame containing the rows with the specified value.\n        \"\"\"\n    if column is None:\n        return self.df.loc[self.df.index == value]\n    else:\n        if column in self.df.columns:\n            return self.df.loc[self.df[column] == value]\n        else:\n            raise KeyError(f\"Column '{column}' not found in the DataFrame.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef apply_llmtask(\n    self,\n    path: List[List[int]],\n    chatbot: Chat,\n    task_name=None,\n    write_func=None,\n    columns: Optional[List[str]] = None,\n    task_id=None,\n    max_workers=1,\n    calls_per_minute: int = 20,\n) -> pd.DataFrame:\n    \"\"\"\n        Apply a writing task to the specified columns or the main index, and create new modified indexes and a corresponding DataFrame with new values.\n\n        Args:\n            write_task: An instance of a writing task (subclass of BaseTask).\n            columns: A list of column names to apply the writing task to, or None (default) to apply the task to the main index.\n\n        Returns:\n            A pandas DataFrame containing the modified values in the specified columns or a new column with the modified values of the main index.\n        \"\"\"\n    modified_df = self.df.copy()\n    if task_name is None and task_id is None:\n        task_name = \"llm_task\"\n    elif task_name is None:\n        task_name = task_id\n\n    if columns is None:\n        # Apply the writing task to the main index\n        write_index = self\n        write_task = LLMWriter(\n            write_index,\n            path,\n            chatbot,\n            task_name=task_name,\n            write_func=write_func,\n            context=self.df,\n            task_id=task_id,\n            max_workers=max_workers,\n            calls_per_minute=calls_per_minute,\n        )\n\n        new_index = write_task.write()\n\n        # Create a mapping of old values to new values\n        old_to_new_values = dict(zip(self.values, new_index.values))\n\n        # Update the row values in the modified DataFrame\n        modified_df[\"new_column\"] = modified_df.apply(\n            lambda row: old_to_new_values.get(\n                self.row_func(row), self.row_func(row)\n            ),\n            axis=1,\n        )\n    else:\n        # Iterate over the specified columns\n        for col in columns:\n            if col in self.columns:\n                # Apply the writing task to the column\n                write_index = self.columns[col]\n                write_task = LLMWriter(\n                    write_index,\n                    path,\n                    chatbot,\n                    write_func=write_func,\n                    context=self.df,\n                    task_id=task_id,\n                    max_workers=max_workers,\n                    calls_per_minute=calls_per_minute,\n                )\n                new_index = write_task.write()\n\n                # Create a mapping of old values to new values\n                old_to_new_values = dict(\n                    zip(self.columns[col].values, new_index.values)\n                )\n\n                # Update the column values in the modified DataFrame\n                modified_df[col] = modified_df[col].apply(\n                    lambda x: old_to_new_values.get(x, x)\n                )\n\n                # Update the column's MemoryIndex\n                self.columns[col] = new_index\n                self.columns[col].save()\n            else:\n                raise KeyError(\n                    f\"Column '{col}' not found in PandasIndex columns dictionary.\"\n                )\n    # remove context from the write_task to avoid memory leak\n    write_task.context = None\n    self.executed_tasks.append({\"task\": write_task, \"output\": modified_df})\n\n    return modified_df\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BaseIndex(ABC):\n    def __init__(\n            self,\n            values: Optional[List[str]] = None,\n            embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n            name: str = \"np_index\",\n            save_path: Optional[str] = None,\n            load: bool = False,\n            embedder: Optional[Union[OpenAiEmbedder, CohereEmbedder]] = OpenAiEmbedder,\n            token_overflow_strategy: str = \"ignore\",\n    ):\n        self.name = name\n        self.embedder = embedder()\n        self.save_path = save_path or \"storage\"\n        os.makedirs(self.save_path, exist_ok=True)\n        self.values = []\n        self.embeddings = None  # initialize embeddings as None\n        self.queries_embeddings = None  # initialize query embeddings as None\n        self.token_overflow_strategy = token_overflow_strategy\n        self.queries = []\n        self.queries_set = set()  # add this to quickly check for duplicates\n        self.index_set = set()  # add this to quickly check for duplicates\n        self.loaded = False\n        self.setup_index(values, embeddings, load)\n    \n    @staticmethod\n    @abstractmethod\n    def compare_embeddings(query: Any, targets: Any) -> Any:\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def batched_l2_distance(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def batched_cosine_similarity(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\n        pass\n\n    @abstractmethod\n    def get(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Union[str, List[str]]:\n        pass\n\n    @abstractmethod\n    def add(self, values: List[str], embeddings: Optional[List[Union[List[float], np.ndarray]]] = None):\n        pass\n\n    @abstractmethod\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\n        pass\n\n    @abstractmethod\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n        pass\n\n    @abstractmethod\n    def search(self, query: Optional[str] = None, query_embedding: Optional[np.ndarray] = None, top_k: int = 10, metric: str = \"cosine\", filter_mask: Optional[np.ndarray] = None) -> Tuple[List[str], Optional[List[float]], List[int]]:\n        pass\n    \n    @abstractmethod\n    def setup_index(self, values: Optional[List[str]] = None, embeddings: Optional[List[Union[List[float], np.ndarray]]] = None, load: bool = False):\n        pass\n    \n    # Non-abstract method\n    def save_index(self):\n        save_directory = os.path.join(self.save_path, self.name)\n        os.makedirs(save_directory, exist_ok=True)\n\n        with open(os.path.join(save_directory, f\"{self.name}_values.json\"), \"w\") as f:\n            json.dump(self.values, f)\n        #check if queries exist\n        if len(self.queries) > 0:\n            with open(os.path.join(save_directory, f\"{self.name}_queries.json\"), \"w\") as f:\n                json.dump(self.queries, f)\n\n        # Save embeddings in a subclass-specific way\n        self._save_embeddings(save_directory)\n\n    def load_index(self):\n        load_directory = os.path.join(self.save_path, self.name)\n        if not os.path.exists(load_directory):\n            print(f\"I did not find the directory to load the index from: {load_directory}\")\n            return\n\n        print(f\"Loading index from {load_directory}\")\n\n        with open(os.path.join(load_directory, f\"{self.name}_values.json\"), \"r\") as f:\n            self.values = json.load(f)\n        self.values_set = set(self.values)\n        #check that queries exist\n        if os.path.exists(os.path.join(load_directory, f\"{self.name}_queries.json\")):\n            with open(os.path.join(load_directory, f\"{self.name}_queries.json\"), \"r\") as f:\n                self.queries = json.load(f)\n            self.queries_set = set(self.queries)\n\n        # Load embeddings in a subclass-specific way\n        self._load_embeddings(load_directory)\n        self.loaded = True\n\n    @abstractmethod\n    def _save_embeddings(self, directory: str):\n        pass\n\n    @abstractmethod\n    def _load_embeddings(self, directory: str):\n        pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n        self,\n        values: Optional[List[str]] = None,\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n        name: str = \"np_index\",\n        save_path: Optional[str] = None,\n        load: bool = False,\n        embedder: Optional[Union[OpenAiEmbedder, CohereEmbedder]] = OpenAiEmbedder,\n        token_overflow_strategy: str = \"ignore\",\n):\n    self.name = name\n    self.embedder = embedder()\n    self.save_path = save_path or \"storage\"\n    os.makedirs(self.save_path, exist_ok=True)\n    self.values = []\n    self.embeddings = None  # initialize embeddings as None\n    self.queries_embeddings = None  # initialize query embeddings as None\n    self.token_overflow_strategy = token_overflow_strategy\n    self.queries = []\n    self.queries_set = set()  # add this to quickly check for duplicates\n    self.index_set = set()  # add this to quickly check for duplicates\n    self.loaded = False\n    self.setup_index(values, embeddings, load)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@staticmethod\n@abstractmethod\ndef compare_embeddings(query: Any, targets: Any) -> Any:\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@staticmethod\n@abstractmethod\ndef batched_l2_distance(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@staticmethod\n@abstractmethod\ndef batched_cosine_similarity(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef get(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Union[str, List[str]]:\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef add(self, values: List[str], embeddings: Optional[List[Union[List[float], np.ndarray]]] = None):\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef search(self, query: Optional[str] = None, query_embedding: Optional[np.ndarray] = None, top_k: int = 10, metric: str = \"cosine\", filter_mask: Optional[np.ndarray] = None) -> Tuple[List[str], Optional[List[float]], List[int]]:\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef setup_index(self, values: Optional[List[str]] = None, embeddings: Optional[List[Union[List[float], np.ndarray]]] = None, load: bool = False):\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Non-abstract method\ndef save_index(self):\n    save_directory = os.path.join(self.save_path, self.name)\n    os.makedirs(save_directory, exist_ok=True)\n\n    with open(os.path.join(save_directory, f\"{self.name}_values.json\"), \"w\") as f:\n        json.dump(self.values, f)\n    #check if queries exist\n    if len(self.queries) > 0:\n        with open(os.path.join(save_directory, f\"{self.name}_queries.json\"), \"w\") as f:\n            json.dump(self.queries, f)\n\n    # Save embeddings in a subclass-specific way\n    self._save_embeddings(save_directory)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef load_index(self):\n    load_directory = os.path.join(self.save_path, self.name)\n    if not os.path.exists(load_directory):\n        print(f\"I did not find the directory to load the index from: {load_directory}\")\n        return\n\n    print(f\"Loading index from {load_directory}\")\n\n    with open(os.path.join(load_directory, f\"{self.name}_values.json\"), \"r\") as f:\n        self.values = json.load(f)\n    self.values_set = set(self.values)\n    #check that queries exist\n    if os.path.exists(os.path.join(load_directory, f\"{self.name}_queries.json\")):\n        with open(os.path.join(load_directory, f\"{self.name}_queries.json\"), \"r\") as f:\n            self.queries = json.load(f)\n        self.queries_set = set(self.queries)\n\n    # Load embeddings in a subclass-specific way\n    self._load_embeddings(load_directory)\n    self.loaded = True\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef _save_embeddings(self, directory: str):\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@abstractmethod\ndef _load_embeddings(self, directory: str):\n    pass\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass NpIndex(BaseIndex):\n    def __init__(\n            self,\n            values: Optional[List[str]] = None,\n            embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n            name: str = \"np_index\",\n            save_path: Optional[str] = None,\n            load: bool = False,\n            embedder: Optional[Union[OpenAiEmbedder, CohereEmbedder]] = OpenAiEmbedder,\n            token_overflow_strategy: str = \"ignore\",\n    ):\n        self.old_ids = collections.OrderedDict()\n        BaseIndex.__init__(self,values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\n\n\n    @staticmethod\n    def compare_embeddings(query: np.ndarray, targets: np.ndarray) -> np.ndarray:\n        return np.array([np.allclose(query, target, rtol=1e-05, atol=1e-08) for target in targets])\n\n    @staticmethod\n    def batched_l2_distance(query_embedding: np.ndarray, embeddings: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n        scores = np.linalg.norm(embeddings - query_embedding, axis=1)\n        if mask is not None:\n            scores[~mask.astype(bool)] = np.inf  # set scores of excluded embeddings to infinity\n        return scores\n\n    @staticmethod\n    def batched_cosine_similarity(query_embedding: np.ndarray, embeddings: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n        scores = np.dot(embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True),\n                        query_embedding / np.linalg.norm(query_embedding))\n        if mask is not None:\n            scores[~mask.astype(bool)] = -np.inf  # set scores of excluded embeddings to negative infinity\n        return scores\n\n    def _save_embeddings(self, directory: str):\n        np.save(os.path.join(directory, f\"{self.name}_embeddings.npy\"), self.embeddings)\n        if self.queries_embeddings is not None:\n            np.save(os.path.join(directory, f\"{self.name}_queries_embeddings.npy\"), self.queries_embeddings)\n\n\n    def _load_embeddings(self, directory: str):\n        load_directory = os.path.join(self.save_path, self.name)\n        if not os.path.exists(load_directory):\n            print(f\"I did not find the directory to load the embe from: {load_directory}\")\n            return\n\n        self.embeddings = np.load(os.path.join(load_directory, f\"{self.name}_embeddings.npy\"))\n        if len(self.values) != len(self.embeddings):\n            raise ValueError(\"Loaded values and embeddings are not the same length.\")\n        #check that queries embeddings exist\n        if os.path.exists(os.path.join(load_directory, f\"{self.name}_queries_embeddings.npy\")):\n            self.queries_embeddings = np.load(os.path.join(load_directory, f\"{self.name}_queries_embeddings.npy\"),allow_pickle=True)\n            print(self.embeddings.shape, len(self.values))\n            print(self.queries_embeddings.shape, len(self.queries))\n            print(self.queries, self.queries_embeddings, self.queries_embeddings is not None, type(self.queries_embeddings), self.queries_embeddings.shape)\n\n            if self.queries_embeddings is not None and len(self.queries) != len(self.queries_embeddings):\n                raise ValueError(\"Loaded queries and queries embeddings are not the same length.\")\n\n\n    def setup_index(self, input_values: Optional[List[str]], embeddings: Optional[List[Union[List[float], np.ndarray]]], load: bool):\n        if load and os.path.exists(os.path.join(self.save_path, self.name)):\n            self.load_index()\n\n        elif input_values and embeddings and len(input_values) == len(embeddings):\n            # Check that input_values and embeddings are the same length\n            unique_dict = collections.defaultdict(list)\n            for val, emb in zip(input_values, embeddings):\n                unique_dict[val].append(emb)\n\n            # Ensure that all embeddings for each value are identical\n            for val in unique_dict.keys():\n                if len(unique_dict[val]) > 1 and not all(np.array_equal(unique_dict[val][0], emb) for emb in unique_dict[val]):\n                    raise ValueError(f'Different embeddings for the same value \"{val}\" found.')\n\n            self.add(list(unique_dict.keys()), [unique_dict[val][0] for val in unique_dict.keys()])\n            self.save_index()\n\n        elif input_values:\n            # Embed the input_values\n            self.add(list(set(input_values)))\n            self.save_index()\n\n    def get(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], output_type: str = \"value\") -> Union[int, str, np.ndarray, Dict[str, Union[int, str, np.ndarray]]]:\n\n        index = self.identify_input(identifier)\n        # Define output types\n        output_types = {\n            'index': index,\n            'value': self.values[index],\n            'embedding': self.embeddings[index],\n            'all': {\n                'index': index,\n                'value': self.values[index],\n                'embedding': self.embeddings[index]}\n        }\n\n        # Check output type is valid\n        if output_type not in output_types:\n            raise ValueError(\"Invalid output_type. Expected 'index', 'value', or 'embedding'.\")\n\n        return output_types[output_type]\n\n    def validate_value_length(self, value: str, tokens: List[int]) -> Union[bool, str]:\n        if not isinstance(value, str):\n            raise TypeError(\"Value must be a string.\")\n        token_len = len(tokens)\n        overflow_check = token_len > MAX_CONTEXT_LENGTH\n        match self.token_overflow_strategy:\n            case \"ignore\":\n                if overflow_check:\n                    return False, value\n                else:\n                    return True, value\n            case \"truncate\":\n                if overflow_check:\n                    return True, TOKENIZER.decode(tokens[:MAX_CONTEXT_LENGTH])\n                else:\n                    return True, value\n            case \"error\":\n                if overflow_check:\n                    raise ValueError(f\" The input is too long for OpenAI, num tokens is {token_len}, instead of {MAX_CONTEXT_LENGTH}\")\n                else:\n                    return True, value\n            case _:\n                raise ValueError(\"Invalid token_overflow_strategy. Expected 'ignore', 'truncate', or 'error'.\")\n\n    def add(self, values: List[str], embeddings: Optional[List[Union[List[float], np.ndarray]]] = None):\n\n        if embeddings is not None and len(values) != len(embeddings):\n            raise ValueError(\"values and embeddings must be the same length\")\n\n        # Check for duplicates and only add unique values\n        unique_values = []\n        unique_embeddings = []\n        token_batch = TOKENIZER.encode_batch(values, allowed_special=\"all\")\n        #extract the max value in old_ids consider that each old_ids is a list take the max of the list itself\n        for i, (val, tokens) in enumerate(zip(values, token_batch)):\n            is_valid, value = self.validate_value_length(val, tokens)\n            if not is_valid:\n                print(f\"Value '{value[:50]}...' is too long and will be ignored.\")\n                continue\n            if value not in self.index_set:\n                unique_values.append(value)\n                self.index_set.add(value)\n\n                self.old_ids[value] = [i]\n                if embeddings is not None:\n                    unique_embeddings.append(embeddings[i])\n            else:\n                self.old_ids[value].append(i)\n\n        if not unique_values:\n            print(\"All values already exist in the index. No values were added.\")\n            return\n        if embeddings is None:\n            unique_embeddings = self.embedder.embed(unique_values)\n\n        # Add unique values to the set\n        self.index_set.update(unique_values)\n\n        # If embeddings array is not yet created, initialize it, else append to it\n        if self.embeddings is None:\n            logger.info(\"Initializing embeddings array\")\n            self.embeddings = np.vstack(unique_embeddings)\n        else:\n            self.embeddings = np.vstack((self.embeddings, unique_embeddings))\n\n        self.values.extend(unique_values)\n\n    def identify_input(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Union[int, str, np.ndarray]:\n        if isinstance(identifier, int):  # if given an index\n            if identifier < len(self.values):  # if valid index\n                index = identifier\n            else:\n                raise ValueError(\"Invalid index given.\")\n        elif isinstance(identifier, str):  # if given a value\n            if identifier in self.values:\n                index = self.values.index(identifier)\n            else:\n                raise ValueError(\"Value not found.\")\n        elif isinstance(identifier, np.ndarray):  # if given an embedding\n            indices = np.where(self.compare_embeddings(identifier, self.embeddings))[0]\n            if len(indices) == 0:\n                raise ValueError(\"Embedding not found.\")\n            index = indices[0]\n        else:\n            raise TypeError(\"Invalid identifier type. Expected int, str, np.ndarray, or list of these types\")\n\n        return index\n\n\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[str, np.ndarray]]]) -> None:\n\n        if isinstance(identifier, list):\n            if all(isinstance(i, type(identifier[0])) for i in identifier) and not isinstance(identifier[0], list) and not isinstance(identifier[0], int):\n                for i in identifier:\n                    self.remove(i)\n            else:\n                raise TypeError(\"All elements in the list must be of the same type.\")\n            return\n        id = self.identify_input(identifier)\n        value = self.values[id]\n        self.index_set.remove(value)\n        self.old_ids.pop(value)\n        self.values.pop(id)\n        self.embeddings = np.delete(self.embeddings, [id], axis=0)\n\n\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[str, np.ndarray]]], new_value: Union[str, List[str]], new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n        if isinstance(new_value,str) and new_value in self.index_set:\n            raise ValueError(\"new_value already exists in the index. Please remove it first.\")\n        elif isinstance(new_value, list) and any(v in self.index_set for v in new_value):\n            raise ValueError(\"One or more new_value already exists in the index. Please remove them first.\")\n        if isinstance(old_identifier, list) and not isinstance(old_identifier[0], list) and not isinstance(old_identifier[0], int):\n            if not isinstance(new_value, list) or len(old_identifier) != len(new_value):\n                raise ValueError(\"For list inputs, old_identifier and new_value must all be lists of the same length.\")\n            if new_embedding is not None:\n                if not isinstance(new_embedding, list) or len(old_identifier) != len(new_embedding):\n                    raise ValueError(\"new_embedding must be a list of same length as old_identifier and new_value.\")\n                # If new_embedding is a 2D array or list of lists\n                if isinstance(new_embedding[0], list) or isinstance(new_embedding[0], np.ndarray):\n                    if len(new_embedding[0]) != len(self.embeddings[0]):\n                        raise ValueError(\"Each row in new_embedding must have the same dimension as the original embeddings.\")\n            else:\n                new_embedding = self.embedder.embed(new_value)\n            for old_id, new_val, new_emb in zip(old_identifier, new_value, new_embedding):\n                self.update(old_id, new_val, new_emb)\n            return\n        old_id = self.identify_input(old_identifier)\n        self.index_set.remove(self.values[old_id])\n        self.old_ids[new_value] = self.old_ids.pop(self.values[old_id])\n        self.values[old_id] = new_value\n        self.index_set.add(new_value)\n        self.embeddings[old_id] = self.embedder.embed([new_value]) if new_embedding is None else new_embedding\n\n    def search(self, query: Optional[str] = None, query_embedding: Optional[np.ndarray] = None, top_k: int = 10, metric: str = \"cosine\", filter_mask: Optional[np.ndarray] = None) -> Tuple[List[str], Optional[List[float]], List[int]]:\n\n        # create a 2D numpy array from the embeddings list\n        embeddings_array = self.embeddings\n\n        # if no query or query embedding is provided, return random samples\n        if query is None and query_embedding is None:\n            indices = np.random.choice(len(self.values), size=top_k, replace=False)\n            return [self.values[i] for i in indices],None, indices  # return indices with dummy scores\n\n        # if the query is in the queries set, use the stored embedding\n        if query in self.queries_set:\n            print(\"Query is in queries set.\")\n            query_embedding = self.queries_embeddings[self.queries.index(query)]\n        # else if a query string is provided but not in queries set, compute its embedding\n        elif query is not None:\n            print(\"Query is not in queries set. Computing embedding...\")\n            query_embedding = self.embedder.embed([query])\n            # print(query_embedding)\n            self.queries_set.add(query)\n            self.queries.append(query)\n            # If queries_embeddings array is not yet created, initialize it, else append to it\n            if self.queries_embeddings is None:\n                self.queries_embeddings = np.array([query_embedding])\n            else:\n                self.queries_embeddings = np.vstack((self.queries_embeddings, query_embedding))\n\n        # compute distances or similarities\n        if metric == \"l2\":\n            scores = self.batched_l2_distance(query_embedding, embeddings_array, filter_mask)\n        elif metric == \"cosine\":\n            scores = self.batched_cosine_similarity(query_embedding, embeddings_array, filter_mask)\n        else:\n            raise ValueError(\"Invalid metric. Expected 'l2' or 'cosine'.\")\n\n        # sort by scores\n        sorted_indices = np.argsort(scores)\n\n        # for L2 distance, closer vectors are better (smaller distance)\n        # for cosine similarity, further vectors are better (larger similarity)\n        top_k = min(top_k, len(self.values))\n        top_indices = sorted_indices[:top_k] if metric == \"l2\" else sorted_indices[-top_k:][::-1]\n\n        # return indices and scores\n        return  [self.values[i] for i in top_indices],[scores[i] for i in top_indices], [i for i in top_indices]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n        self,\n        values: Optional[List[str]] = None,\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n        name: str = \"np_index\",\n        save_path: Optional[str] = None,\n        load: bool = False,\n        embedder: Optional[Union[OpenAiEmbedder, CohereEmbedder]] = OpenAiEmbedder,\n        token_overflow_strategy: str = \"ignore\",\n):\n    self.old_ids = collections.OrderedDict()\n    BaseIndex.__init__(self,values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n@staticmethod\ndef compare_embeddings(query: np.ndarray, targets: np.ndarray) -> np.ndarray:\n    return np.array([np.allclose(query, target, rtol=1e-05, atol=1e-08) for target in targets])\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@staticmethod\ndef batched_l2_distance(query_embedding: np.ndarray, embeddings: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n    scores = np.linalg.norm(embeddings - query_embedding, axis=1)\n    if mask is not None:\n        scores[~mask.astype(bool)] = np.inf  # set scores of excluded embeddings to infinity\n    return scores\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@staticmethod\ndef batched_cosine_similarity(query_embedding: np.ndarray, embeddings: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n    scores = np.dot(embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True),\n                    query_embedding / np.linalg.norm(query_embedding))\n    if mask is not None:\n        scores[~mask.astype(bool)] = -np.inf  # set scores of excluded embeddings to negative infinity\n    return scores\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _save_embeddings(self, directory: str):\n    np.save(os.path.join(directory, f\"{self.name}_embeddings.npy\"), self.embeddings)\n    if self.queries_embeddings is not None:\n        np.save(os.path.join(directory, f\"{self.name}_queries_embeddings.npy\"), self.queries_embeddings)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef _load_embeddings(self, directory: str):\n    load_directory = os.path.join(self.save_path, self.name)\n    if not os.path.exists(load_directory):\n        print(f\"I did not find the directory to load the embe from: {load_directory}\")\n        return\n\n    self.embeddings = np.load(os.path.join(load_directory, f\"{self.name}_embeddings.npy\"))\n    if len(self.values) != len(self.embeddings):\n        raise ValueError(\"Loaded values and embeddings are not the same length.\")\n    #check that queries embeddings exist\n    if os.path.exists(os.path.join(load_directory, f\"{self.name}_queries_embeddings.npy\")):\n        self.queries_embeddings = np.load(os.path.join(load_directory, f\"{self.name}_queries_embeddings.npy\"),allow_pickle=True)\n        print(self.embeddings.shape, len(self.values))\n        print(self.queries_embeddings.shape, len(self.queries))\n        print(self.queries, self.queries_embeddings, self.queries_embeddings is not None, type(self.queries_embeddings), self.queries_embeddings.shape)\n\n        if self.queries_embeddings is not None and len(self.queries) != len(self.queries_embeddings):\n            raise ValueError(\"Loaded queries and queries embeddings are not the same length.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef setup_index(self, input_values: Optional[List[str]], embeddings: Optional[List[Union[List[float], np.ndarray]]], load: bool):\n    if load and os.path.exists(os.path.join(self.save_path, self.name)):\n        self.load_index()\n\n    elif input_values and embeddings and len(input_values) == len(embeddings):\n        # Check that input_values and embeddings are the same length\n        unique_dict = collections.defaultdict(list)\n        for val, emb in zip(input_values, embeddings):\n            unique_dict[val].append(emb)\n\n        # Ensure that all embeddings for each value are identical\n        for val in unique_dict.keys():\n            if len(unique_dict[val]) > 1 and not all(np.array_equal(unique_dict[val][0], emb) for emb in unique_dict[val]):\n                raise ValueError(f'Different embeddings for the same value \"{val}\" found.')\n\n        self.add(list(unique_dict.keys()), [unique_dict[val][0] for val in unique_dict.keys()])\n        self.save_index()\n\n    elif input_values:\n        # Embed the input_values\n        self.add(list(set(input_values)))\n        self.save_index()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], output_type: str = \"value\") -> Union[int, str, np.ndarray, Dict[str, Union[int, str, np.ndarray]]]:\n\n    index = self.identify_input(identifier)\n    # Define output types\n    output_types = {\n        'index': index,\n        'value': self.values[index],\n        'embedding': self.embeddings[index],\n        'all': {\n            'index': index,\n            'value': self.values[index],\n            'embedding': self.embeddings[index]}\n    }\n\n    # Check output type is valid\n    if output_type not in output_types:\n        raise ValueError(\"Invalid output_type. Expected 'index', 'value', or 'embedding'.\")\n\n    return output_types[output_type]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef validate_value_length(self, value: str, tokens: List[int]) -> Union[bool, str]:\n    if not isinstance(value, str):\n        raise TypeError(\"Value must be a string.\")\n    token_len = len(tokens)\n    overflow_check = token_len > MAX_CONTEXT_LENGTH\n    match self.token_overflow_strategy:\n        case \"ignore\":\n            if overflow_check:\n                return False, value\n            else:\n                return True, value\n        case \"truncate\":\n            if overflow_check:\n                return True, TOKENIZER.decode(tokens[:MAX_CONTEXT_LENGTH])\n            else:\n                return True, value\n        case \"error\":\n            if overflow_check:\n                raise ValueError(f\" The input is too long for OpenAI, num tokens is {token_len}, instead of {MAX_CONTEXT_LENGTH}\")\n            else:\n                return True, value\n        case _:\n            raise ValueError(\"Invalid token_overflow_strategy. Expected 'ignore', 'truncate', or 'error'.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef add(self, values: List[str], embeddings: Optional[List[Union[List[float], np.ndarray]]] = None):\n\n    if embeddings is not None and len(values) != len(embeddings):\n        raise ValueError(\"values and embeddings must be the same length\")\n\n    # Check for duplicates and only add unique values\n    unique_values = []\n    unique_embeddings = []\n    token_batch = TOKENIZER.encode_batch(values, allowed_special=\"all\")\n    #extract the max value in old_ids consider that each old_ids is a list take the max of the list itself\n    for i, (val, tokens) in enumerate(zip(values, token_batch)):\n        is_valid, value = self.validate_value_length(val, tokens)\n        if not is_valid:\n            print(f\"Value '{value[:50]}...' is too long and will be ignored.\")\n            continue\n        if value not in self.index_set:\n            unique_values.append(value)\n            self.index_set.add(value)\n\n            self.old_ids[value] = [i]\n            if embeddings is not None:\n                unique_embeddings.append(embeddings[i])\n        else:\n            self.old_ids[value].append(i)\n\n    if not unique_values:\n        print(\"All values already exist in the index. No values were added.\")\n        return\n    if embeddings is None:\n        unique_embeddings = self.embedder.embed(unique_values)\n\n    # Add unique values to the set\n    self.index_set.update(unique_values)\n\n    # If embeddings array is not yet created, initialize it, else append to it\n    if self.embeddings is None:\n        logger.info(\"Initializing embeddings array\")\n        self.embeddings = np.vstack(unique_embeddings)\n    else:\n        self.embeddings = np.vstack((self.embeddings, unique_embeddings))\n\n    self.values.extend(unique_values)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef identify_input(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Union[int, str, np.ndarray]:\n    if isinstance(identifier, int):  # if given an index\n        if identifier < len(self.values):  # if valid index\n            index = identifier\n        else:\n            raise ValueError(\"Invalid index given.\")\n    elif isinstance(identifier, str):  # if given a value\n        if identifier in self.values:\n            index = self.values.index(identifier)\n        else:\n            raise ValueError(\"Value not found.\")\n    elif isinstance(identifier, np.ndarray):  # if given an embedding\n        indices = np.where(self.compare_embeddings(identifier, self.embeddings))[0]\n        if len(indices) == 0:\n            raise ValueError(\"Embedding not found.\")\n        index = indices[0]\n    else:\n        raise TypeError(\"Invalid identifier type. Expected int, str, np.ndarray, or list of these types\")\n\n    return index\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef remove(self, identifier: Union[int, str, np.ndarray, List[Union[str, np.ndarray]]]) -> None:\n\n    if isinstance(identifier, list):\n        if all(isinstance(i, type(identifier[0])) for i in identifier) and not isinstance(identifier[0], list) and not isinstance(identifier[0], int):\n            for i in identifier:\n                self.remove(i)\n        else:\n            raise TypeError(\"All elements in the list must be of the same type.\")\n        return\n    id = self.identify_input(identifier)\n    value = self.values[id]\n    self.index_set.remove(value)\n    self.old_ids.pop(value)\n    self.values.pop(id)\n    self.embeddings = np.delete(self.embeddings, [id], axis=0)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef update(self, old_identifier: Union[int, str, np.ndarray, List[Union[str, np.ndarray]]], new_value: Union[str, List[str]], new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n    if isinstance(new_value,str) and new_value in self.index_set:\n        raise ValueError(\"new_value already exists in the index. Please remove it first.\")\n    elif isinstance(new_value, list) and any(v in self.index_set for v in new_value):\n        raise ValueError(\"One or more new_value already exists in the index. Please remove them first.\")\n    if isinstance(old_identifier, list) and not isinstance(old_identifier[0], list) and not isinstance(old_identifier[0], int):\n        if not isinstance(new_value, list) or len(old_identifier) != len(new_value):\n            raise ValueError(\"For list inputs, old_identifier and new_value must all be lists of the same length.\")\n        if new_embedding is not None:\n            if not isinstance(new_embedding, list) or len(old_identifier) != len(new_embedding):\n                raise ValueError(\"new_embedding must be a list of same length as old_identifier and new_value.\")\n            # If new_embedding is a 2D array or list of lists\n            if isinstance(new_embedding[0], list) or isinstance(new_embedding[0], np.ndarray):\n                if len(new_embedding[0]) != len(self.embeddings[0]):\n                    raise ValueError(\"Each row in new_embedding must have the same dimension as the original embeddings.\")\n        else:\n            new_embedding = self.embedder.embed(new_value)\n        for old_id, new_val, new_emb in zip(old_identifier, new_value, new_embedding):\n            self.update(old_id, new_val, new_emb)\n        return\n    old_id = self.identify_input(old_identifier)\n    self.index_set.remove(self.values[old_id])\n    self.old_ids[new_value] = self.old_ids.pop(self.values[old_id])\n    self.values[old_id] = new_value\n    self.index_set.add(new_value)\n    self.embeddings[old_id] = self.embedder.embed([new_value]) if new_embedding is None else new_embedding\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef search(self, query: Optional[str] = None, query_embedding: Optional[np.ndarray] = None, top_k: int = 10, metric: str = \"cosine\", filter_mask: Optional[np.ndarray] = None) -> Tuple[List[str], Optional[List[float]], List[int]]:\n\n    # create a 2D numpy array from the embeddings list\n    embeddings_array = self.embeddings\n\n    # if no query or query embedding is provided, return random samples\n    if query is None and query_embedding is None:\n        indices = np.random.choice(len(self.values), size=top_k, replace=False)\n        return [self.values[i] for i in indices],None, indices  # return indices with dummy scores\n\n    # if the query is in the queries set, use the stored embedding\n    if query in self.queries_set:\n        print(\"Query is in queries set.\")\n        query_embedding = self.queries_embeddings[self.queries.index(query)]\n    # else if a query string is provided but not in queries set, compute its embedding\n    elif query is not None:\n        print(\"Query is not in queries set. Computing embedding...\")\n        query_embedding = self.embedder.embed([query])\n        # print(query_embedding)\n        self.queries_set.add(query)\n        self.queries.append(query)\n        # If queries_embeddings array is not yet created, initialize it, else append to it\n        if self.queries_embeddings is None:\n            self.queries_embeddings = np.array([query_embedding])\n        else:\n            self.queries_embeddings = np.vstack((self.queries_embeddings, query_embedding))\n\n    # compute distances or similarities\n    if metric == \"l2\":\n        scores = self.batched_l2_distance(query_embedding, embeddings_array, filter_mask)\n    elif metric == \"cosine\":\n        scores = self.batched_cosine_similarity(query_embedding, embeddings_array, filter_mask)\n    else:\n        raise ValueError(\"Invalid metric. Expected 'l2' or 'cosine'.\")\n\n    # sort by scores\n    sorted_indices = np.argsort(scores)\n\n    # for L2 distance, closer vectors are better (smaller distance)\n    # for cosine similarity, further vectors are better (larger similarity)\n    top_k = min(top_k, len(self.values))\n    top_indices = sorted_indices[:top_k] if metric == \"l2\" else sorted_indices[-top_k:][::-1]\n\n    # return indices and scores\n    return  [self.values[i] for i in top_indices],[scores[i] for i in top_indices], [i for i in top_indices]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"class FifoChat(FifoThread, Chat):\n    \"\"\"\n    A chatbot class that combines FIFO Memory Thread, BaseChat, and Prompter. The oldest messages are removed first\n    when reaching the max_memory limit. The memory is defined in terms of tokens, and outs are passed to the\n    longterm_memory. The lucid_memory is a redundant memory that stores all the messages.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Optional[str] = None,\n        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n        system_prompt: Optional[str] = None,\n        user_prompt: Optional[str] = None,\n        name: str = \"fifo_memory\",\n        max_index_memory: int = 400,\n        max_fifo_memory: int = 2048,\n        max_output_tokens: int = 1000,\n        longterm_thread: Optional[BaseThread] = None,\n    ):\n\n        FifoThread.__init__(\n            self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread\n        )\n        Chat.__init__(\n            self,\n            model=model,\n            index_dict=index_dict,\n            max_output_tokens=max_output_tokens,\n            max_index_memory=max_index_memory,\n            system_prompt=system_prompt,\n            user_prompt=user_prompt,\n            name=name,\n        )\n\n        self.prompt_func = self.fifo_memory_prompt\n\n    def fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\n        \"\"\"\n        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\n\n        :param message: A string representing the user message.\n        :return: A tuple containing a list of strings as the prompt and the marked question.\n        \"\"\"\n        marked_question = mark_question(self.user_prompt(message))\n        prompt = (\n            [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\n        )\n        return prompt, marked_question\n\n    def query(self, question: str, verbose: bool = True, stream: bool = False) -> Union[Generator,str]:\n        \"\"\"\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n        and added to the memory.\n\n        :param question: A string representing the user question.\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n        :return: A string representing the chatbot's response.\n        \"\"\"\n        marked_question = mark_question(question)\n        self.add_message(marked_question)\n        answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n        if stream:\n            return answer\n        else:\n            self.add_message(answer)\n            return answer\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(\n    self,\n    model: Optional[str] = None,\n    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n    system_prompt: Optional[str] = None,\n    user_prompt: Optional[str] = None,\n    name: str = \"fifo_memory\",\n    max_index_memory: int = 400,\n    max_fifo_memory: int = 2048,\n    max_output_tokens: int = 1000,\n    longterm_thread: Optional[BaseThread] = None,\n):\n\n    FifoThread.__init__(\n        self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread\n    )\n    Chat.__init__(\n        self,\n        model=model,\n        index_dict=index_dict,\n        max_output_tokens=max_output_tokens,\n        max_index_memory=max_index_memory,\n        system_prompt=system_prompt,\n        user_prompt=user_prompt,\n        name=name,\n    )\n\n    self.prompt_func = self.fifo_memory_prompt\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\n    \"\"\"\n        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\n\n        :param message: A string representing the user message.\n        :return: A tuple containing a list of strings as the prompt and the marked question.\n        \"\"\"\n    marked_question = mark_question(self.user_prompt(message))\n    prompt = (\n        [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\n    )\n    return prompt, marked_question\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef query(self, question: str, verbose: bool = True, stream: bool = False) -> Union[Generator,str]:\n    \"\"\"\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n        and added to the memory.\n\n        :param question: A string representing the user question.\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n        :return: A string representing the chatbot's response.\n        \"\"\"\n    marked_question = mark_question(question)\n    self.add_message(marked_question)\n    answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n    if stream:\n        return answer\n    else:\n        self.add_message(answer)\n        return answer\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass VectorChat(VectorThread, Chat):\n    \"\"\"\n    A chatbot class that combines Vector Memory Thread, BaseChat, and Prompter. Memory prompt is constructed by\n    filling the memory with the k most similar messages to the question until the max prompt memory tokens are reached.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Optional[str] = None,\n        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n        name: str = \"vector_memory\",\n        max_index_memory: int = 400,\n        max_vector_memory: int = 2048,\n        max_output_tokens: int = 1000,\n        system_prompt: str = None,\n        user_prompt: str = None,\n    \n    ):\n        VectorThread.__init__(self, name=name, max_context=max_vector_memory)\n        Chat.__init__(\n            self,\n            model=model,\n            index_dict=index_dict,\n            max_output_tokens=max_output_tokens,\n            max_index_memory=max_index_memory,\n            system_prompt=system_prompt,\n            user_prompt=user_prompt,\n            name=name,\n        )\n        self.max_vector_memory = self.max_context\n        self.prompt_func = self.vector_memory_prompt\n\n    def vector_memory_prompt(\n        self, message: str, k: int = 10\n    ) -> Tuple[List[dict], dict]:\n        \"\"\"\n        Combine system prompt, k most similar messages to the question, and the user prompt.\n\n        :param question: A string representing the user question.\n        :param k: The number of most similar messages to include in the prompt.\n        :return: A tuple containing a list of strings as the prompt and the marked question.\n        \"\"\"\n        sorted_messages, sorted_scores, sorted_indices = self.sorted_query(\n            message, k=k, max_tokens=self.max_vector_memory, reverse=True\n        )\n        marked_question = mark_question(self.user_prompt(message))\n        prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\n        return prompt, marked_question\n\n    def weighted_memory_prompt(\n        self,\n        message: str,\n        k: int = 10,\n        decay_factor: float = 0.1,\n        temporal_weight: float = 0.5,\n    ) -> Tuple[List[dict], dict]:\n        \"\"\"\n        Combine system prompt, weighted k most similar messages to the question, and the user prompt.\n\n        :param question: A string representing the user question.\n        :param k: The number of most similar messages to include in the prompt.\n        :param decay_factor: A float representing the decay factor for weighting.\n        :param temporal_weight: A float representing the weight of the temporal aspect.\n        :return: A tuple containing a list of strings as the prompt and the marked question.\n        \"\"\"\n        weighted_messages, weighted_scores, weighted_indices = self.weighted_query(\n            message,\n            k=k,\n            max_tokens=self.max_vector_memory,\n            decay_factor=decay_factor,\n            temporal_weight=temporal_weight,\n            order_by=\"chronological\",\n            reverse=True,\n        )\n        marked_question = mark_question(self.user_prompt(message))\n        prompt = (\n            [mark_system(self.system_prompt)] + weighted_messages + [marked_question]\n        )\n        return prompt, marked_question\n\n    def query(self, question: str, verbose: bool = False, stream:bool = False) -> Union[Generator,str]:\n        \"\"\"\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n        and added to the memory.\n\n        :param question: A string representing the user question.\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n        :return: A string representing the chatbot's response.\n        \"\"\"\n                \n        marked_question = mark_question(question)\n        self.add_message(marked_question)\n        answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n        if stream:\n            return answer\n        else:\n            self.add_message(answer)\n            return answer\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(\n    self,\n    model: Optional[str] = None,\n    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n    name: str = \"vector_memory\",\n    max_index_memory: int = 400,\n    max_vector_memory: int = 2048,\n    max_output_tokens: int = 1000,\n    system_prompt: str = None,\n    user_prompt: str = None,\n\n):\n    VectorThread.__init__(self, name=name, max_context=max_vector_memory)\n    Chat.__init__(\n        self,\n        model=model,\n        index_dict=index_dict,\n        max_output_tokens=max_output_tokens,\n        max_index_memory=max_index_memory,\n        system_prompt=system_prompt,\n        user_prompt=user_prompt,\n        name=name,\n    )\n    self.max_vector_memory = self.max_context\n    self.prompt_func = self.vector_memory_prompt\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef vector_memory_prompt(\n    self, message: str, k: int = 10\n) -> Tuple[List[dict], dict]:\n    \"\"\"\n        Combine system prompt, k most similar messages to the question, and the user prompt.\n\n        :param question: A string representing the user question.\n        :param k: The number of most similar messages to include in the prompt.\n        :return: A tuple containing a list of strings as the prompt and the marked question.\n        \"\"\"\n    sorted_messages, sorted_scores, sorted_indices = self.sorted_query(\n        message, k=k, max_tokens=self.max_vector_memory, reverse=True\n    )\n    marked_question = mark_question(self.user_prompt(message))\n    prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\n    return prompt, marked_question\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef weighted_memory_prompt(\n    self,\n    message: str,\n    k: int = 10,\n    decay_factor: float = 0.1,\n    temporal_weight: float = 0.5,\n) -> Tuple[List[dict], dict]:\n    \"\"\"\n        Combine system prompt, weighted k most similar messages to the question, and the user prompt.\n\n        :param question: A string representing the user question.\n        :param k: The number of most similar messages to include in the prompt.\n        :param decay_factor: A float representing the decay factor for weighting.\n        :param temporal_weight: A float representing the weight of the temporal aspect.\n        :return: A tuple containing a list of strings as the prompt and the marked question.\n        \"\"\"\n    weighted_messages, weighted_scores, weighted_indices = self.weighted_query(\n        message,\n        k=k,\n        max_tokens=self.max_vector_memory,\n        decay_factor=decay_factor,\n        temporal_weight=temporal_weight,\n        order_by=\"chronological\",\n        reverse=True,\n    )\n    marked_question = mark_question(self.user_prompt(message))\n    prompt = (\n        [mark_system(self.system_prompt)] + weighted_messages + [marked_question]\n    )\n    return prompt, marked_question\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef query(self, question: str, verbose: bool = False, stream:bool = False) -> Union[Generator,str]:\n    \"\"\"\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n        and added to the memory.\n\n        :param question: A string representing the user question.\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n        :return: A string representing the chatbot's response.\n        \"\"\"\n            \n    marked_question = mark_question(question)\n    self.add_message(marked_question)\n    answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n    if stream:\n        return answer\n    else:\n        self.add_message(answer)\n        return answer\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass FifoVectorChat(FifoThread, Chat):\n    \"\"\"\n    A chatbot class that combines FIFO Memory Thread, Vector Memory Thread, BaseChat, and Prompter.\n    The memory prompt is constructed by including both FIFO memory and Vector memory.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = None,\n        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n        system_prompt: str = None,\n        user_prompt: str = None,\n        name: str = \"fifo_vector_memory\",\n        max_memory: int = 2048,\n        max_index_memory: int = 400,\n        max_output_tokens: int = 1000,\n        longterm_thread: Optional[VectorThread] = None,\n        longterm_frac: float = 0.5,\n    ):\n        self.total_max_memory = max_memory\n\n        self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n        FifoThread.__init__(\n            self,\n            name=name,\n            max_memory=self.max_fifo_memory,\n            longterm_thread=self.longterm_thread,\n        )\n        Chat.__init__(\n            self,\n            model=model,\n            index_dict=index_dict,\n            max_output_tokens=max_output_tokens,\n            max_index_memory=max_index_memory,\n            system_prompt=system_prompt,\n            user_prompt=user_prompt,\n            name=name,\n        )\n        \n        self.prompt_func = self.fifovector_memory_prompt\n        self.prompt_list = []\n\n    def setup_longterm_memory(\n        self,\n        longterm_thread: Optional[VectorThread],\n        max_memory: int,\n        longterm_frac: float,\n    ):\n        \"\"\"\n        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n\n        :param longterm_thread: An optional VectorThread for long-term memory.\n        :param max_memory: The maximum amount of memory for the chatbot.\n        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n        \"\"\"\n        if longterm_thread is None:\n            self.longterm_frac = longterm_frac\n            self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n            self.max_vector_memory = max_memory - self.max_fifo_memory\n            self.longterm_thread = VectorThread(\n                name=\"longterm_memory\", max_context=self.max_vector_memory\n            )\n        else:\n            self.longterm_thread = longterm_thread\n            self.max_vector_memory = self.longterm_thread.max_context\n            self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n            self.longterm_frac = self.max_vector_memory / self.total_max_memory\n\n    def fifovector_memory_prompt(\n        self, message: str, k: int = 10\n    ) -> Tuple[List[dict], dict]:\n        \"\"\"\n        Combine the system prompt, long-term memory (vector memory), short-term memory (FIFO memory), and the user prompt.\n\n        :param question: A string representing the user question.\n        :param k: The number of most similar messages to include from the long-term memory.\n        :return: A tuple containing a list of strings as the prompt and the marked question.\n        \"\"\"\n        prompt = [mark_system(self.system_prompt)]\n        if (\n            len(self.longterm_thread.memory_thread) > 0\n            and self.longterm_thread.total_tokens <= self.max_vector_memory\n        ):\n            prompt += self.longterm_thread.memory_thread\n        elif (\n            len(self.longterm_thread.memory_thread) > 0\n            and self.longterm_thread.total_tokens > self.max_vector_memory\n        ):\n            (\n                sorted_messages,\n                sorted_scores,\n                sorted_indices,\n            ) = self.longterm_thread.sorted_query(\n                message, k=k, max_tokens=self.max_vector_memory, reverse=True\n            )\n            prompt += sorted_messages\n\n        prompt += self.memory_thread\n        marked_question = mark_question(self.user_prompt(message))\n        prompt += [marked_question]\n        return prompt, marked_question\n\n    def query(self, question: str, verbose: bool = False, stream:bool = False) -> Union[Generator,str]:\n        \"\"\"\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n        and added to the memory.\n\n        :param question: A string representing the user question.\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n        :return: A string representing the chatbot's response.\n        \"\"\"\n        prompt, marked_question = self.fifovector_memory_prompt(question)\n\n        self.add_message(marked_question)\n\n        answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n        if stream:\n            return answer\n        else:\n            self.add_message(answer)\n            return answer\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(\n    self,\n    model: str = None,\n    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n    system_prompt: str = None,\n    user_prompt: str = None,\n    name: str = \"fifo_vector_memory\",\n    max_memory: int = 2048,\n    max_index_memory: int = 400,\n    max_output_tokens: int = 1000,\n    longterm_thread: Optional[VectorThread] = None,\n    longterm_frac: float = 0.5,\n):\n    self.total_max_memory = max_memory\n\n    self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n    FifoThread.__init__(\n        self,\n        name=name,\n        max_memory=self.max_fifo_memory,\n        longterm_thread=self.longterm_thread,\n    )\n    Chat.__init__(\n        self,\n        model=model,\n        index_dict=index_dict,\n        max_output_tokens=max_output_tokens,\n        max_index_memory=max_index_memory,\n        system_prompt=system_prompt,\n        user_prompt=user_prompt,\n        name=name,\n    )\n    \n    self.prompt_func = self.fifovector_memory_prompt\n    self.prompt_list = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef setup_longterm_memory(\n    self,\n    longterm_thread: Optional[VectorThread],\n    max_memory: int,\n    longterm_frac: float,\n):\n    \"\"\"\n        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n\n        :param longterm_thread: An optional VectorThread for long-term memory.\n        :param max_memory: The maximum amount of memory for the chatbot.\n        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n        \"\"\"\n    if longterm_thread is None:\n        self.longterm_frac = longterm_frac\n        self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n        self.max_vector_memory = max_memory - self.max_fifo_memory\n        self.longterm_thread = VectorThread(\n            name=\"longterm_memory\", max_context=self.max_vector_memory\n        )\n    else:\n        self.longterm_thread = longterm_thread\n        self.max_vector_memory = self.longterm_thread.max_context\n        self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n        self.longterm_frac = self.max_vector_memory / self.total_max_memory\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef fifovector_memory_prompt(\n    self, message: str, k: int = 10\n) -> Tuple[List[dict], dict]:\n    \"\"\"\n        Combine the system prompt, long-term memory (vector memory), short-term memory (FIFO memory), and the user prompt.\n\n        :param question: A string representing the user question.\n        :param k: The number of most similar messages to include from the long-term memory.\n        :return: A tuple containing a list of strings as the prompt and the marked question.\n        \"\"\"\n    prompt = [mark_system(self.system_prompt)]\n    if (\n        len(self.longterm_thread.memory_thread) > 0\n        and self.longterm_thread.total_tokens <= self.max_vector_memory\n    ):\n        prompt += self.longterm_thread.memory_thread\n    elif (\n        len(self.longterm_thread.memory_thread) > 0\n        and self.longterm_thread.total_tokens > self.max_vector_memory\n    ):\n        (\n            sorted_messages,\n            sorted_scores,\n            sorted_indices,\n        ) = self.longterm_thread.sorted_query(\n            message, k=k, max_tokens=self.max_vector_memory, reverse=True\n        )\n        prompt += sorted_messages\n\n    prompt += self.memory_thread\n    marked_question = mark_question(self.user_prompt(message))\n    prompt += [marked_question]\n    return prompt, marked_question\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef query(self, question: str, verbose: bool = False, stream:bool = False) -> Union[Generator,str]:\n    \"\"\"\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n        and added to the memory.\n\n        :param question: A string representing the user question.\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n        :return: A string representing the chatbot's response.\n        \"\"\"\n    prompt, marked_question = self.fifovector_memory_prompt(question)\n\n    self.add_message(marked_question)\n\n    answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n    if stream:\n        return answer\n    else:\n        self.add_message(answer)\n        return answer\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass ContextManagedFifoVectorChat(FifoThread, Chat):\n    def __init__(\n        self,\n        model: str = None,\n        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n        system_prompt: str = None,\n        user_prompt: str = None,\n        name: str = \"fifo_vector_memory\",\n        max_memory: int = 2048,\n        max_index_memory: int = 400,\n        max_output_tokens: int = 1000,\n        longterm_thread: Optional[VectorThread] = None,\n        longterm_frac: float = 0.5,\n    ):\n        self.total_max_memory = max_memory\n        \n\n        self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n        FifoThread.__init__(\n            self,\n            name=name,\n            max_memory=self.max_fifo_memory,\n            longterm_thread=self.longterm_thread,\n        )\n        Chat.__init__(\n            self,\n            model=model,\n            index_dict=index_dict,\n            max_output_tokens=max_output_tokens,\n            max_index_memory=max_index_memory,\n            system_prompt=system_prompt,\n            user_prompt=user_prompt,\n            name=name,\n        )\n        #self.context_manager = ContextManager(index_dict)\n        #deep copy of index_dict\n        '''\n        deep_copied_index_dict = copy.deepcopy(self.index_dict)\n        \n        self.multi_kernel = CreateMultiKernel(deep_copied_index_dict).create_multi_kernel()\n        self.memory_kernel_dict = self.multi_kernel.memory_kernel_dict\n        self.path_group = self.multi_kernel.path_group\n        \n        self.prompt_func = self.fifovector_memory_prompt\n        '''\n        self.prompt_list = []\n    \n    def setup_longterm_memory(\n        self,\n        longterm_thread: Optional[VectorThread],\n        max_memory: int,\n        longterm_frac: float,\n    ):\n        \"\"\"\n        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n\n        :param longterm_thread: An optional VectorThread for long-term memory.\n        :param max_memory: The maximum amount of memory for the chatbot.\n        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n        \"\"\"\n        #TODO preload longterm memory with index summaries\n        if longterm_thread is None:\n            self.longterm_frac = longterm_frac\n            self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n            self.max_vector_memory = max_memory - self.max_fifo_memory\n            self.longterm_thread = VectorThread(\n                name=\"longterm_memory\", max_context=self.max_vector_memory\n            )\n        else:\n            self.longterm_thread = longterm_thread\n            self.max_vector_memory = self.longterm_thread.max_context\n            self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n            self.longterm_frac = self.max_vector_memory / self.total_max_memory\n\n    def calculate_stability(self, boundaries, adjacency_matrix_with_message):\n        boundary_stability = np.zeros(len(boundaries) - 1)\n        for i in range(len(boundaries) - 1):\n            boundary_connections = adjacency_matrix_with_message[boundaries[i]:boundaries[i+1], :][:, boundaries[i]:boundaries[i+1]]\n            boundary_stability[i] = np.mean(boundary_connections) - np.std(boundary_connections)**2\n        logging.info(f\"Boundary Stability: {boundary_stability}\")\n        return boundary_stability\n\n    def query_hints(self, message, index, k):\n        top_k, _, indices = index.token_bound_query(message, k=k, max_tokens=5000)\n        top_k_embeddings = [index.embeddings[i] for i in indices]\n        return top_k, top_k_embeddings\n\n    def query_mk_hints(self, message, index_key, index, k):\n        top_k, _, indices = index.token_bound_query(message, k=k, max_tokens=3000)\n        top_k_embeddings = [self.memory_kernel_dict[index_key].node_embeddings[i] for i in indices]\n        return top_k, top_k_embeddings\n\n\n    def heat_trajectory(self, message, k=20):\n        \"\"\"\n        This function gets the top k embeddings from all indexes including longterm, merges into numpy matrix,\n        record boundaries, computes kernel adj matrix, and sums all connections in each index section.\n        It returns the heat trajectory.\n        \"\"\"\n        embeddings = []\n        boundaries = [0]  # Start boundary\n        top_k_hints = {}\n        # Gather top k embeddings from each index\n        logging.info(\"Computing Trajectory for the current state of memory.\")\n        for index_key, index in self.index_dict.items():\n            top_k, top_k_embeddings = self.query_hints(message, index, k)\n            embeddings.extend(top_k_embeddings)\n            boundaries.append(boundaries[-1] + k)\n            top_k_hints[index_key] = top_k\n\n\n        if len(self.longterm_thread.values) >= k:\n            if len(self.longterm_thread.embeddings) != len(self.longterm_thread.values):\n                self.longterm_thread.compute_embeddings()\n            top_k, top_k_embeddings = self.query_hints(message, index, k)\n            embeddings.extend(top_k_embeddings)\n            boundaries.append(boundaries[-1] + k)\n            top_k_hints['longterm_thread'] = top_k\n        mean = np.sum(embeddings, axis=1)\n        mean = mean / np.linalg.norm(mean)\n        #should be shape (,1536) currently (4, 1536)\n        mean = np.sum(mean, axis=0)\n\n\n        logging.info(f\"Mean: {mean.shape}\")\n        embeddings_matrix = np.vstack(embeddings)\n        adjacency_matrix = cosine_similarity(embeddings_matrix)\n        message_embedding = EMBEDDER.embed(data=message)\n        #get residuals\n        message_embedding = message_embedding - mean\n        top_k_embeddings_with_message = embeddings_matrix.copy()\n        top_k_embeddings_with_message[:-1] += message_embedding\n        norms = np.linalg.norm(top_k_embeddings_with_message, axis=1)\n        normalized_embeddings = top_k_embeddings_with_message / norms[:, np.newaxis]\n        adjacency_matrix_with_message = cosine_similarity(normalized_embeddings)\n\n        adjacency_matrix_with_message = adjacency_matrix_with_message - adjacency_matrix\n        adjacency_matrix_with_message =  adjacency_matrix_with_message**3\n\n        degrees = np.sum(adjacency_matrix_with_message, axis=1)\n        heat_trajectory = [np.sum(degrees[boundaries[i]:boundaries[i + 1]]) for i in range(len(boundaries) - 1)]\n        logging.info(f\"Heat Trajectory: {heat_trajectory}\")\n        heat_dict = dict(zip(list(self.index_dict.keys()) + ['longterm_thread'], heat_trajectory))\n        sum_of_vals = sum(heat_dict.values())\n        heat_dict = {k: v / sum_of_vals for k, v in heat_dict.items()}\n        hdict = {k: v for k, v in sorted(heat_dict.items(), key=lambda item: item[1])}\n        logging.info(f\"Heat dict: {heat_dict}\")\n        return hdict, top_k_hints\n\n\n    def fifovector_memory_prompt(\n        self, message: str, k: int = 5\n    ) -> Tuple[List[dict], dict]:\n        hdict, top_k_hint_dict = self.heat_trajectory(message)\n        min_heat_index = list(hdict.keys())[0]\n        second_min_heat_index = list(hdict.keys())[1]\n        if len(hdict) > 2:\n            third_min_heat_index = list(hdict.keys())[2]\n        if min_heat_index == 'longterm_thread':\n            logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from long-term memory.\")\n            logging.info(f\"Number of values in Index: {len(self.longterm_thread.values)}\")\n            if len(hdict) > 2:\n                if hdict[second_min_heat_index] - hdict[third_min_heat_index] < 0.20:\n                    top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-2] + top_k_hint_dict[third_min_heat_index][:1]\n                else:\n                    top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-1]\n            else:\n                top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-1]\n\n            logging.info(f\"Top K Hint: {top_k_hint}\")\n            prompt =f'[LONG TERM MEMORY]{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n        elif min_heat_index in self.index_dict.keys():\n            # if the difference between min and second min is less than 0.25, merge hints from both indexes\n            if hdict[min_heat_index] - hdict[second_min_heat_index] < 0.25:\n                logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from index.\")\n                logging.info(f\"Number of values in Index {self.index_dict[min_heat_index].name}: {len(self.index_dict[min_heat_index].values)}\")\n                #take 2 from first index topk and 1 from second index topk\n                top_k_hint = top_k_hint_dict[min_heat_index][:k-2] + top_k_hint_dict[second_min_heat_index][:2]\n                logging.info(f\"Top K Hint: {top_k_hint}\")\n                prompt =f'{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n            else:\n                logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from index.\")\n                logging.info(f\"Number of values in Index {self.index_dict[min_heat_index].name}: {len(self.index_dict[min_heat_index].values)}\")\n                top_k_hint = top_k_hint_dict[min_heat_index][:k]\n                logging.info(f\"Top K Hint: {top_k_hint}\")\n                prompt =f'{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n        else:\n            raise ValueError(\"The provided index name is not available.\")\n\n        return prompt\n\n    def context_query(self, question: str, verbose: bool = False, stream: bool = False) -> Union[Generator, str]:\n        prompt = self.fifovector_memory_prompt(question)\n        modified_question = mark_question(prompt)\n        self.add_message(modified_question)\n\n        answer = BaseChat.query(self, message=prompt, verbose=verbose, stream=stream)\n\n        if stream:\n            return answer\n        else:\n            self.add_message(answer)\n            return answer\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    model: str = None,\n    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n    system_prompt: str = None,\n    user_prompt: str = None,\n    name: str = \"fifo_vector_memory\",\n    max_memory: int = 2048,\n    max_index_memory: int = 400,\n    max_output_tokens: int = 1000,\n    longterm_thread: Optional[VectorThread] = None,\n    longterm_frac: float = 0.5,\n):\n    self.total_max_memory = max_memory\n    \n\n    self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n    FifoThread.__init__(\n        self,\n        name=name,\n        max_memory=self.max_fifo_memory,\n        longterm_thread=self.longterm_thread,\n    )\n    Chat.__init__(\n        self,\n        model=model,\n        index_dict=index_dict,\n        max_output_tokens=max_output_tokens,\n        max_index_memory=max_index_memory,\n        system_prompt=system_prompt,\n        user_prompt=user_prompt,\n        name=name,\n    )\n    #self.context_manager = ContextManager(index_dict)\n    #deep copy of index_dict\n    '''\n        deep_copied_index_dict = copy.deepcopy(self.index_dict)\n        \n        self.multi_kernel = CreateMultiKernel(deep_copied_index_dict).create_multi_kernel()\n        self.memory_kernel_dict = self.multi_kernel.memory_kernel_dict\n        self.path_group = self.multi_kernel.path_group\n        \n        self.prompt_func = self.fifovector_memory_prompt\n        '''\n    self.prompt_list = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef setup_longterm_memory(\n    self,\n    longterm_thread: Optional[VectorThread],\n    max_memory: int,\n    longterm_frac: float,\n):\n    \"\"\"\n        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n\n        :param longterm_thread: An optional VectorThread for long-term memory.\n        :param max_memory: The maximum amount of memory for the chatbot.\n        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n        \"\"\"\n    #TODO preload longterm memory with index summaries\n    if longterm_thread is None:\n        self.longterm_frac = longterm_frac\n        self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n        self.max_vector_memory = max_memory - self.max_fifo_memory\n        self.longterm_thread = VectorThread(\n            name=\"longterm_memory\", max_context=self.max_vector_memory\n        )\n    else:\n        self.longterm_thread = longterm_thread\n        self.max_vector_memory = self.longterm_thread.max_context\n        self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n        self.longterm_frac = self.max_vector_memory / self.total_max_memory\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef calculate_stability(self, boundaries, adjacency_matrix_with_message):\n    boundary_stability = np.zeros(len(boundaries) - 1)\n    for i in range(len(boundaries) - 1):\n        boundary_connections = adjacency_matrix_with_message[boundaries[i]:boundaries[i+1], :][:, boundaries[i]:boundaries[i+1]]\n        boundary_stability[i] = np.mean(boundary_connections) - np.std(boundary_connections)**2\n    logging.info(f\"Boundary Stability: {boundary_stability}\")\n    return boundary_stability\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef query_hints(self, message, index, k):\n    top_k, _, indices = index.token_bound_query(message, k=k, max_tokens=5000)\n    top_k_embeddings = [index.embeddings[i] for i in indices]\n    return top_k, top_k_embeddings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef query_mk_hints(self, message, index_key, index, k):\n    top_k, _, indices = index.token_bound_query(message, k=k, max_tokens=3000)\n    top_k_embeddings = [self.memory_kernel_dict[index_key].node_embeddings[i] for i in indices]\n    return top_k, top_k_embeddings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef heat_trajectory(self, message, k=20):\n    \"\"\"\n        This function gets the top k embeddings from all indexes including longterm, merges into numpy matrix,\n        record boundaries, computes kernel adj matrix, and sums all connections in each index section.\n        It returns the heat trajectory.\n        \"\"\"\n    embeddings = []\n    boundaries = [0]  # Start boundary\n    top_k_hints = {}\n    # Gather top k embeddings from each index\n    logging.info(\"Computing Trajectory for the current state of memory.\")\n    for index_key, index in self.index_dict.items():\n        top_k, top_k_embeddings = self.query_hints(message, index, k)\n        embeddings.extend(top_k_embeddings)\n        boundaries.append(boundaries[-1] + k)\n        top_k_hints[index_key] = top_k\n\n\n    if len(self.longterm_thread.values) >= k:\n        if len(self.longterm_thread.embeddings) != len(self.longterm_thread.values):\n            self.longterm_thread.compute_embeddings()\n        top_k, top_k_embeddings = self.query_hints(message, index, k)\n        embeddings.extend(top_k_embeddings)\n        boundaries.append(boundaries[-1] + k)\n        top_k_hints['longterm_thread'] = top_k\n    mean = np.sum(embeddings, axis=1)\n    mean = mean / np.linalg.norm(mean)\n    #should be shape (,1536) currently (4, 1536)\n    mean = np.sum(mean, axis=0)\n\n\n    logging.info(f\"Mean: {mean.shape}\")\n    embeddings_matrix = np.vstack(embeddings)\n    adjacency_matrix = cosine_similarity(embeddings_matrix)\n    message_embedding = EMBEDDER.embed(data=message)\n    #get residuals\n    message_embedding = message_embedding - mean\n    top_k_embeddings_with_message = embeddings_matrix.copy()\n    top_k_embeddings_with_message[:-1] += message_embedding\n    norms = np.linalg.norm(top_k_embeddings_with_message, axis=1)\n    normalized_embeddings = top_k_embeddings_with_message / norms[:, np.newaxis]\n    adjacency_matrix_with_message = cosine_similarity(normalized_embeddings)\n\n    adjacency_matrix_with_message = adjacency_matrix_with_message - adjacency_matrix\n    adjacency_matrix_with_message =  adjacency_matrix_with_message**3\n\n    degrees = np.sum(adjacency_matrix_with_message, axis=1)\n    heat_trajectory = [np.sum(degrees[boundaries[i]:boundaries[i + 1]]) for i in range(len(boundaries) - 1)]\n    logging.info(f\"Heat Trajectory: {heat_trajectory}\")\n    heat_dict = dict(zip(list(self.index_dict.keys()) + ['longterm_thread'], heat_trajectory))\n    sum_of_vals = sum(heat_dict.values())\n    heat_dict = {k: v / sum_of_vals for k, v in heat_dict.items()}\n    hdict = {k: v for k, v in sorted(heat_dict.items(), key=lambda item: item[1])}\n    logging.info(f\"Heat dict: {heat_dict}\")\n    return hdict, top_k_hints\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef fifovector_memory_prompt(\n    self, message: str, k: int = 5\n) -> Tuple[List[dict], dict]:\n    hdict, top_k_hint_dict = self.heat_trajectory(message)\n    min_heat_index = list(hdict.keys())[0]\n    second_min_heat_index = list(hdict.keys())[1]\n    if len(hdict) > 2:\n        third_min_heat_index = list(hdict.keys())[2]\n    if min_heat_index == 'longterm_thread':\n        logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from long-term memory.\")\n        logging.info(f\"Number of values in Index: {len(self.longterm_thread.values)}\")\n        if len(hdict) > 2:\n            if hdict[second_min_heat_index] - hdict[third_min_heat_index] < 0.20:\n                top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-2] + top_k_hint_dict[third_min_heat_index][:1]\n            else:\n                top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-1]\n        else:\n            top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-1]\n\n        logging.info(f\"Top K Hint: {top_k_hint}\")\n        prompt =f'[LONG TERM MEMORY]{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n    elif min_heat_index in self.index_dict.keys():\n        # if the difference between min and second min is less than 0.25, merge hints from both indexes\n        if hdict[min_heat_index] - hdict[second_min_heat_index] < 0.25:\n            logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from index.\")\n            logging.info(f\"Number of values in Index {self.index_dict[min_heat_index].name}: {len(self.index_dict[min_heat_index].values)}\")\n            #take 2 from first index topk and 1 from second index topk\n            top_k_hint = top_k_hint_dict[min_heat_index][:k-2] + top_k_hint_dict[second_min_heat_index][:2]\n            logging.info(f\"Top K Hint: {top_k_hint}\")\n            prompt =f'{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n        else:\n            logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from index.\")\n            logging.info(f\"Number of values in Index {self.index_dict[min_heat_index].name}: {len(self.index_dict[min_heat_index].values)}\")\n            top_k_hint = top_k_hint_dict[min_heat_index][:k]\n            logging.info(f\"Top K Hint: {top_k_hint}\")\n            prompt =f'{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n    else:\n        raise ValueError(\"The provided index name is not available.\")\n\n    return prompt\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef context_query(self, question: str, verbose: bool = False, stream: bool = False) -> Union[Generator, str]:\n    prompt = self.fifovector_memory_prompt(question)\n    modified_question = mark_question(prompt)\n    self.add_message(modified_question)\n\n    answer = BaseChat.query(self, message=prompt, verbose=verbose, stream=stream)\n\n    if stream:\n        return answer\n    else:\n        self.add_message(answer)\n        return answer\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Set the variables\n\n\nasync def process_chat_requests(\n    request_data: List[Dict],\n    save_filepath: str,\n    request_url: str,\n    api_key: str,\n    max_requests_per_minute: float,\n    max_tokens_per_minute: float,\n    token_encoding_name: str,\n    max_attempts: int,\n    logging_level: int,\n):\n    \"\"\"Processes chat requests in parallel, throttling to stay under rate limits.\"\"\"\n    # constants\n    seconds_to_pause_after_rate_limit_error = 15\n    seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n\n    # initialize logging\n    logging.basicConfig(level=logging_level)\n    logging.debug(f\"Logging initialized at level {logging_level}\")\n\n    # infer API endpoint and construct request header\n    api_endpoint = api_endpoint_from_url(request_url)\n    request_header = {\"Authorization\": f\"Bearer {api_key}\"}\n\n    # initialize trackers\n    queue_of_requests_to_retry = asyncio.Queue()\n    task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...\n    status_tracker = StatusTracker()  # single instance to track a collection of variables\n    next_request = None  # variable to hold the next request to call\n\n    # initialize available capacity counts\n    available_request_capacity = max_requests_per_minute\n    available_token_capacity = max_tokens_per_minute\n    last_update_time = time.time()\n\n    # initialize flags\n    file_not_finished = True  # after file is empty, we'll skip reading it\n    logging.debug(f\"Initialization complete.\")\n\n    # `requests` will provide requests one at a time\n    requests = iter(request_data)\n    logging.debug(f\"List initialized. Entering main loop\")\n\n    while True:\n        # get next request (if one is not already waiting for capacity)\n        if next_request is None:\n            if not queue_of_requests_to_retry.empty():\n                next_request = queue_of_requests_to_retry.get_nowait()\n                logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")\n            elif file_not_finished:\n                try:\n                    # get new request\n                    request_json = next(requests)\n                    next_request = APIRequest(\n                        task_id=next(task_id_generator),\n                        request_json=request_json,\n                        token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),\n                        attempts_left=max_attempts,\n                        metadata=request_json.pop(\"metadata\", None)\n                    )\n                    status_tracker.num_tasks_started += 1\n                    status_tracker.num_tasks_in_progress += 1\n                    logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")\n                except StopIteration:\n                    # if file runs out, set flag to stop reading it\n                    logging.debug(\"Read file exhausted\")\n                    file_not_finished = False\n\n        # update available capacity\n        current_time = time.time()\n        seconds_since_update = current_time - last_update_time\n        available_request_capacity = min(\n            available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,\n            max_requests_per_minute,\n        )\n        available_token_capacity = min(\n            available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,\n            max_tokens_per_minute,\n        )\n        last_update_time = current_time\n\n        # if enough capacity available, call API\n        if next_request:\n            next_request_tokens = next_request.token_consumption\n            if (\n                available_request_capacity >= 1\n                and available_token_capacity >= next_request_tokens\n            ):\n                # update counters\n                available_request_capacity -= 1\n                available_token_capacity -= next_request_tokens\n                next_request.attempts_left -= 1\n\n                # call API\n                asyncio.create_task(\n                    next_request.call_api(\n                        request_url=request_url,\n                        request_header=request_header,\n                        retry_queue=queue_of_requests_to_retry,\n                        save_filepath=save_filepath,\n                        status_tracker=status_tracker,\n                    )\n                )\n                next_request = None  # reset next_request to empty\n\n        # if all tasks are finished, break\n        if status_tracker.num_tasks_in_progress == 0:\n            break\n\n        # main loop sleeps briefly so concurrent tasks can run\n        await asyncio.sleep(seconds_to_sleep_each_loop)\n\n        # if a rate limit error was hit recently, pause to cool down\n        seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)\n        if seconds_since_rate_limit_error < seconds_to_pause_after_rate_limit_error:\n            remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n            await asyncio.sleep(remaining_seconds_to_pause)\n            # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n            logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")\n\n    # after finishing, log final status\n    logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")\n    if status_tracker.num_tasks_failed > 0:\n        logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")\n    if status_tracker.num_rate_limit_errors > 0:\n        logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass BatchChat(BaseChat):\n    def __init__(self, model: str = \"gpt-3.5-turbo\", max_output_tokens: int = 200, save_filepath: str = None, max_attempts: int = 5, api_key: str = None ):\n        #super init\n        super().__init__(model=model, max_output_tokens=max_output_tokens)\n        # Set the variables\n        if save_filepath is None:\n            self.save_filepath = \"results.jsonl\"  # Default filename if not using input file\n        else:\n            self.save_filepath = save_filepath\n        self.request_url = \"https://api.openai.com/v1/chat/completions\" # URL for the chat endpoint\n        self.max_requests_per_minute = 3_000 * 0.5\n        self.max_tokens_per_minute = 250_000 * 0.5\n        self.token_encoding_name = \"cl100k_base\"\n        self.api_key = api_key\n        self.max_attempts = max_attempts\n        self.logging_level = logging.INFO\n\n    async def batch_query(self, messages: List[str], system_prompts:List[str]) -> List[str]:\n        request_data = []\n        for message, system_prompt in zip(messages, system_prompts):\n            prompt, _ = self.prompt_func(message)\n            print(prompt)\n            request_data.append({\n                \"model\": self.model,\n                \"max_tokens\": self.max_output_tokens,\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": message}\n                ]\n            })\n        responses = await process_chat_requests(request_data=request_data, request_url=self.request_url, api_key=self.api_key, save_filepath=self.save_filepath, max_requests_per_minute=self.max_requests_per_minute, max_tokens_per_minute=self.max_tokens_per_minute, token_encoding_name=self.token_encoding_name, max_attempts=self.max_attempts, logging_level=self.logging_level)\n        return responses\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, model: str = \"gpt-3.5-turbo\", max_output_tokens: int = 200, save_filepath: str = None, max_attempts: int = 5, api_key: str = None ):\n    #super init\n    super().__init__(model=model, max_output_tokens=max_output_tokens)\n    # Set the variables\n    if save_filepath is None:\n        self.save_filepath = \"results.jsonl\"  # Default filename if not using input file\n    else:\n        self.save_filepath = save_filepath\n    self.request_url = \"https://api.openai.com/v1/chat/completions\" # URL for the chat endpoint\n    self.max_requests_per_minute = 3_000 * 0.5\n    self.max_tokens_per_minute = 250_000 * 0.5\n    self.token_encoding_name = \"cl100k_base\"\n    self.api_key = api_key\n    self.max_attempts = max_attempts\n    self.logging_level = logging.INFO\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nasync def batch_query(self, messages: List[str], system_prompts:List[str]) -> List[str]:\n    request_data = []\n    for message, system_prompt in zip(messages, system_prompts):\n        prompt, _ = self.prompt_func(message)\n        print(prompt)\n        request_data.append({\n            \"model\": self.model,\n            \"max_tokens\": self.max_output_tokens,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": message}\n            ]\n        })\n    responses = await process_chat_requests(request_data=request_data, request_url=self.request_url, api_key=self.api_key, save_filepath=self.save_filepath, max_requests_per_minute=self.max_requests_per_minute, max_tokens_per_minute=self.max_tokens_per_minute, token_encoding_name=self.token_encoding_name, max_attempts=self.max_attempts, logging_level=self.logging_level)\n    return responses\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass ContextManager:\n    \"\"\"\n    A class that manages the context of user interactions and maps them to the appropriate memory index or thread.\n    \"\"\"\n    def __init__(\n        self, \n        index_dict: Dict[str, MemoryIndex], \n        longterm_thread_keywords: List[str] = [\"long ago\", \"in the past\", \"long term\", \"long-term\", \"longterm\"]\n    ) -> None:\n        \"\"\"\n        Initialize the ContextManager with the available indexes.\n\n        :param index_dict: A dictionary mapping index names to MemoryIndex instances.\n        :param fifo_thread_keywords: A list of keywords indicating user reference to recent conversation context.\n        :param longterm_thread_keywords: A list of keywords indicating user reference to long-term conversation context.\n        \"\"\"\n        self.index_dict = index_dict\n        self.longterm_thread_keywords = longterm_thread_keywords\n        self.keyword_to_index_map = self.create_keyword_to_index_map()\n\n    def create_keyword_to_index_map(self) -> Dict[str, str]:\n        \"\"\"\n        Create a mapping from keywords to index names.\n\n        :return: A dictionary mapping keywords to index names or threads.\n        \"\"\"\n        keyword_to_index_map = {index_name.split('_')[0]: index_name for index_name in self.index_dict.keys()}\n        for keyword in self.longterm_thread_keywords:\n            keyword_to_index_map[keyword] = \"longterm_thread\"\n        return keyword_to_index_map\n\n    def get_context_for_user_input(self, user_input: str) -> str:\n        \"\"\"\n        Get the appropriate context (index or thread) for the given user input.\n\n        :param user_input: A string representing the user's input.\n        :return: The name of the appropriate context (index or thread), or None if no specific context is appropriate.\n        \"\"\"\n        user_input_str = str(user_input)  # Ensure user_input is a string\n        for keyword, context in self.keyword_to_index_map.items():\n            if re.search(r'\\b' + keyword + r'\\b', user_input_str, re.I):\n                return context\n        return None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self, \n    index_dict: Dict[str, MemoryIndex], \n    longterm_thread_keywords: List[str] = [\"long ago\", \"in the past\", \"long term\", \"long-term\", \"longterm\"]\n) -> None:\n    \"\"\"\n        Initialize the ContextManager with the available indexes.\n\n        :param index_dict: A dictionary mapping index names to MemoryIndex instances.\n        :param fifo_thread_keywords: A list of keywords indicating user reference to recent conversation context.\n        :param longterm_thread_keywords: A list of keywords indicating user reference to long-term conversation context.\n        \"\"\"\n    self.index_dict = index_dict\n    self.longterm_thread_keywords = longterm_thread_keywords\n    self.keyword_to_index_map = self.create_keyword_to_index_map()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef create_keyword_to_index_map(self) -> Dict[str, str]:\n    \"\"\"\n        Create a mapping from keywords to index names.\n\n        :return: A dictionary mapping keywords to index names or threads.\n        \"\"\"\n    keyword_to_index_map = {index_name.split('_')[0]: index_name for index_name in self.index_dict.keys()}\n    for keyword in self.longterm_thread_keywords:\n        keyword_to_index_map[keyword] = \"longterm_thread\"\n    return keyword_to_index_map\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_context_for_user_input(self, user_input: str) -> str:\n    \"\"\"\n        Get the appropriate context (index or thread) for the given user input.\n\n        :param user_input: A string representing the user's input.\n        :return: The name of the appropriate context (index or thread), or None if no specific context is appropriate.\n        \"\"\"\n    user_input_str = str(user_input)  # Ensure user_input is a string\n    for keyword, context in self.keyword_to_index_map.items():\n        if re.search(r'\\b' + keyword + r'\\b', user_input_str, re.I):\n            return context\n    return None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass Chat(BaseChat, Prompter):\n    \"\"\"\n    This class combines the BaseChat and Prompter classes to create a oneshot chatbot with a system and user prompt,\n    and the ability to handle queries to multiple MemoryIndex through the index_dict.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = None,\n        max_output_tokens: int = 1000,\n        system_prompt: str = None,\n        user_prompt: str = None,\n        index_dict: Optional[Dict[str, MemoryIndex]] = None,\n        max_index_memory: int = 1000,\n        name: str = \"Chat\",\n    ) -> None:\n        BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\n        Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\n        self.index_dict = index_dict\n        self.setup_indices(max_index_memory)\n        self.name = name\n    \n    def add_user_defined_ids(self, id_dict: Dict[str, list]):\n        self.user_defined_ids.append(id_dict)\n        self.use_user_defined_ids = True\n        self.setup_index_prompts()\n\n\n    def setup_index_prompts(self):\n        if self.current_index is not None or self.use_user_defined_ids:\n            print(\"Index is available so using index prompts\")\n            self.system_prompt = (\n                INDEX_SYSTEM_PROMPT \n                if self.user_defined_system_prompt is None \n                else self.user_defined_system_prompt\n            )\n            self.user_prompt = (\n                self.get_index_hints\n                if self.user_defined_user_prompt is None\n                else self.user_defined_user_prompt\n            )\n        else:\n            if self.user_defined_system_prompt is None:\n                print(\"No user defined system prompt defaulting to default prompts\")\n                self.set_default_prompts()\n            else:\n                print(\"User defined system prompt and default user prompt\")\n                self.system_prompt = self.user_defined_system_prompt\n                self.user_prompt = self.default_user_prompt\n\n\n                \n\n    def setup_indices(self, max_index_memory):\n        \"\"\"setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index.\"\"\"\n        if self.index_dict is not None:\n            self.max_index_memory = max_index_memory\n            # set the last index to be the current index\n            self.current_index = list(self.index_dict.keys())[-1]\n            self.setup_index_prompts()    \n        else:\n            self.current_index = None\n\n    def get_index_hints(\n        self, question: str, k: int = 10, max_tokens: int = None\n    ) -> str:\n        \"\"\"\n        Get hints from the current index for the given question.\n\n        :param question: A string representing the user question.\n        :param k: The number of most similar messages to include from the index.\n        :param max_tokens: The maximum number of tokens to be retrieved from the index.\n        :return: A string representing the hint prompt with the question.\n        \"\"\"\n        if max_tokens is None:\n            max_tokens = self.max_index_memory\n        hints = []\n        if self.use_user_defined_ids is True:\n            user_defined_id = self.user_defined_ids[-1]\n            for index, ids in user_defined_id.items():\n                for i in ids:\n                    hints.append(self.index_dict[index].values[i])\n            self.use_user_defined_ids = False\n            self.setup_index_prompts()\n        elif self.current_index is not None:\n            index_instance = self.index_dict[self.current_index]\n            if isinstance(index_instance, MemoryIndex):\n                hints, _, _ = index_instance.token_bound_query(\n                    question, k=k, max_tokens=max_tokens\n                )\n            else:\n                raise ValueError(\"The current index is not a valid index instance.\")\n        if len(hints) == 0:\n            return question\n        else:\n            hints_string = \"\\n\".join(hints)\n            hint_prompt = INDEX_HINT_PROMPT\n            question_intro = QUESTION_INTRO\n            return hint_prompt.format(\n                hints_string=hints_string\n            ) + question_intro.format(question=question)\n\n    def set_current_index(self, index_name: Optional[str]) -> None:\n        \"\"\"\n        Set the current index to be used for hints.\n\n        :param index_name: A string representing the index name or None to clear the current index.\n        :raise ValueError: If the provided index name is not available.\n        \"\"\"\n        if self.index_dict is None:\n            raise ValueError(\"No index_dict are available.\")\n        elif index_name in self.index_dict:\n            self.current_index = index_name\n        elif index_name is None:\n            self.current_index = None\n        else:\n            raise ValueError(\"The provided index name is not available.\")\n        self.setup_index_prompts()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(\n    self,\n    model: str = None,\n    max_output_tokens: int = 1000,\n    system_prompt: str = None,\n    user_prompt: str = None,\n    index_dict: Optional[Dict[str, MemoryIndex]] = None,\n    max_index_memory: int = 1000,\n    name: str = \"Chat\",\n) -> None:\n    BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\n    Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\n    self.index_dict = index_dict\n    self.setup_indices(max_index_memory)\n    self.name = name\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef add_user_defined_ids(self, id_dict: Dict[str, list]):\n    self.user_defined_ids.append(id_dict)\n    self.use_user_defined_ids = True\n    self.setup_index_prompts()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef setup_index_prompts(self):\n    if self.current_index is not None or self.use_user_defined_ids:\n        print(\"Index is available so using index prompts\")\n        self.system_prompt = (\n            INDEX_SYSTEM_PROMPT \n            if self.user_defined_system_prompt is None \n            else self.user_defined_system_prompt\n        )\n        self.user_prompt = (\n            self.get_index_hints\n            if self.user_defined_user_prompt is None\n            else self.user_defined_user_prompt\n        )\n    else:\n        if self.user_defined_system_prompt is None:\n            print(\"No user defined system prompt defaulting to default prompts\")\n            self.set_default_prompts()\n        else:\n            print(\"User defined system prompt and default user prompt\")\n            self.system_prompt = self.user_defined_system_prompt\n            self.user_prompt = self.default_user_prompt\n\n\n            \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef setup_indices(self, max_index_memory):\n    \"\"\"setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index.\"\"\"\n    if self.index_dict is not None:\n        self.max_index_memory = max_index_memory\n        # set the last index to be the current index\n        self.current_index = list(self.index_dict.keys())[-1]\n        self.setup_index_prompts()    \n    else:\n        self.current_index = None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_index_hints(\n    self, question: str, k: int = 10, max_tokens: int = None\n) -> str:\n    \"\"\"\n        Get hints from the current index for the given question.\n\n        :param question: A string representing the user question.\n        :param k: The number of most similar messages to include from the index.\n        :param max_tokens: The maximum number of tokens to be retrieved from the index.\n        :return: A string representing the hint prompt with the question.\n        \"\"\"\n    if max_tokens is None:\n        max_tokens = self.max_index_memory\n    hints = []\n    if self.use_user_defined_ids is True:\n        user_defined_id = self.user_defined_ids[-1]\n        for index, ids in user_defined_id.items():\n            for i in ids:\n                hints.append(self.index_dict[index].values[i])\n        self.use_user_defined_ids = False\n        self.setup_index_prompts()\n    elif self.current_index is not None:\n        index_instance = self.index_dict[self.current_index]\n        if isinstance(index_instance, MemoryIndex):\n            hints, _, _ = index_instance.token_bound_query(\n                question, k=k, max_tokens=max_tokens\n            )\n        else:\n            raise ValueError(\"The current index is not a valid index instance.\")\n    if len(hints) == 0:\n        return question\n    else:\n        hints_string = \"\\n\".join(hints)\n        hint_prompt = INDEX_HINT_PROMPT\n        question_intro = QUESTION_INTRO\n        return hint_prompt.format(\n            hints_string=hints_string\n        ) + question_intro.format(question=question)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef set_current_index(self, index_name: Optional[str]) -> None:\n    \"\"\"\n        Set the current index to be used for hints.\n\n        :param index_name: A string representing the index name or None to clear the current index.\n        :raise ValueError: If the provided index name is not available.\n        \"\"\"\n    if self.index_dict is None:\n        raise ValueError(\"No index_dict are available.\")\n    elif index_name in self.index_dict:\n        self.current_index = index_name\n    elif index_name is None:\n        self.current_index = None\n    else:\n        raise ValueError(\"The provided index name is not available.\")\n    self.setup_index_prompts()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass Prompter:\n    \"\"\"\n    This class handles the system and user prompts and the prompt_func. By subclassing and overriding the\n    prompt_func, you can change the way the prompts are composed.\n    \"\"\"\n\n    def __init__(self, system_prompt: Union[str,None] = None, user_prompt: Union[str,None] = None):\n        \"\"\"\n        Initialize the Prompter with system and user prompts.\n\n        :param system_prompt: A string representing the system prompt.\n        :param user_prompt: A string representing the user prompt.\n        \"\"\"\n        \n        if system_prompt is None:\n            self.system_prompt = DEFAULT_SYSTEM_PROMPT\n            self.user_defined_system_prompt = None\n        else:\n            self.system_prompt = system_prompt\n            self.user_defined_system_prompt = system_prompt\n        if user_prompt is None:\n            self.user_prompt = self.default_user_prompt\n            self.user_defined_user_prompt = None\n        else:\n            self.user_prompt = user_prompt\n            self.user_defined_user_prompt = user_prompt\n\n        self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\n        self.user_defined_ids = []\n        self.user_defined_values = []\n        self.use_user_defined_ids = False\n\n    def set_default_prompts(self):\n        self.system_prompt = DEFAULT_SYSTEM_PROMPT\n        self.user_prompt = self.default_user_prompt\n\n\n    def default_user_prompt(self, message: str) -> str:\n        return DEFAULT_USER_PROMPT.format(question=message)\n\n    def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\n        \"\"\"\n        Compose the prompt for the chat-gpt API.\n\n        :param message: A string representing the user message.\n        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\n        \"\"\"\n        marked_question = mark_question(self.user_prompt(message))\n        prompt = [mark_system(self.system_prompt)] + [marked_question]\n        return prompt, marked_question\n\n    def update_system_prompt(self, new_prompt: str) -> None:\n        \"\"\"\n        Update the system prompt.\n\n        :param new_prompt: A string representing the new system prompt.\n        \"\"\"\n        self.system_prompt = new_prompt\n\n    def update_user_prompt(self, new_prompt ) -> None:\n        \"\"\"\n        Update the user prompt.\n\n        :param new_prompt: A string representing the new user prompt.\n        \"\"\"\n        self.user_prompt = new_prompt\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(self, system_prompt: Union[str,None] = None, user_prompt: Union[str,None] = None):\n    \"\"\"\n        Initialize the Prompter with system and user prompts.\n\n        :param system_prompt: A string representing the system prompt.\n        :param user_prompt: A string representing the user prompt.\n        \"\"\"\n    \n    if system_prompt is None:\n        self.system_prompt = DEFAULT_SYSTEM_PROMPT\n        self.user_defined_system_prompt = None\n    else:\n        self.system_prompt = system_prompt\n        self.user_defined_system_prompt = system_prompt\n    if user_prompt is None:\n        self.user_prompt = self.default_user_prompt\n        self.user_defined_user_prompt = None\n    else:\n        self.user_prompt = user_prompt\n        self.user_defined_user_prompt = user_prompt\n\n    self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\n    self.user_defined_ids = []\n    self.user_defined_values = []\n    self.use_user_defined_ids = False\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef set_default_prompts(self):\n    self.system_prompt = DEFAULT_SYSTEM_PROMPT\n    self.user_prompt = self.default_user_prompt\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef default_user_prompt(self, message: str) -> str:\n    return DEFAULT_USER_PROMPT.format(question=message)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\n    \"\"\"\n        Compose the prompt for the chat-gpt API.\n\n        :param message: A string representing the user message.\n        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\n        \"\"\"\n    marked_question = mark_question(self.user_prompt(message))\n    prompt = [mark_system(self.system_prompt)] + [marked_question]\n    return prompt, marked_question\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef update_system_prompt(self, new_prompt: str) -> None:\n    \"\"\"\n        Update the system prompt.\n\n        :param new_prompt: A string representing the new system prompt.\n        \"\"\"\n    self.system_prompt = new_prompt\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef update_user_prompt(self, new_prompt ) -> None:\n    \"\"\"\n        Update the user prompt.\n\n        :param new_prompt: A string representing the new user prompt.\n        \"\"\"\n    self.user_prompt = new_prompt\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass BaseChat:\n    \"\"\"\n    This is the base class for chatbots, defining the basic functions that a chatbot should have, mainly the calls to\n    chat-gpt API, and a basic Gradio interface. It has a prompt_func that acts as a placeholder for a call to chat-gpt\n    API without any additional messages. It can be overridden by subclasses to add additional messages to the prompt.\n    \"\"\"\n\n    def __init__(self, model: Union[str,None] = None, max_output_tokens: int = 200):\n        \"\"\"\n        Initialize the BaseChat with a model and max_output_tokens.\n\n        :param model: A string representing the chat model to be used.\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\n        \"\"\"\n        if model is None:\n            self.model = \"gpt-3.5-turbo\"\n        else:\n            self.model = model\n        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n        self.max_output_tokens = max_output_tokens\n        self.failed_responses = []\n        self.outputs = []\n        self.inputs = []\n        self.prompts = []\n        self.prompt_func = self.identity_prompter\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        # Remove the tokenizer attribute from the state\n        del state[\"tokenizer\"]\n        return state\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        # Reinitialize the tokenizer attribute after unpickling\n        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n\n    def identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\n        \"\"\"\n        A simple identity prompter that takes a message and returns the message marked as a question.\n\n        :param message: A string representing the user message.\n        :return: A tuple containing the marked question and the original message.\n        \"\"\"\n        return [mark_question(message)], mark_question(message)\n\n    def chat_response( self, prompt: List[dict], max_tokens: Union[int,None] = None, stream:bool = False ) -> Union[Generator,Tuple[Dict, bool]]:\n        if max_tokens is None:\n            max_tokens = self.max_output_tokens\n        if \"gpt\" in self.model:\n            logging.info(prompt)\n            response, status = chatgpt_response(prompt=prompt,model=self.model, max_tokens = 1000, stream=stream)\n            if status:\n                return response, True\n            else:\n                self.failed_responses.append(response)\n                return response, False\n\n        elif \"command\" in self.model:\n            response, status = cohere_response(prompt=prompt,model=self.model, max_tokens = 1000)  \n            if status:\n                return response, True\n            else:\n                self.failed_responses.append(response)\n                return response, False\n        else:\n            return {}, False \n\n    def reply(self, message: str, verbose: bool = True, stream: bool = False) -> Union[Generator, str]:\n        \"\"\"\n        Reply to a given message using the chatbot.\n\n        :param message: A string representing the user message.\n        :return: A string representing the chatbot's response.\n        \"\"\"\n        if stream:\n            return self.query(message, verbose, stream)\n        else:\n            return self.query(message, verbose)[\"content\"]\n\n    def query(self, message: str, verbose: bool = True, stream: bool = False) -> Union[Generator, str]:\n        \"\"\"\n        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\n\n        :param message: A string representing the user message.\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n        :return: A string representing the chatbot's response.\n        \"\"\"\n\n        prompt, _ = self.prompt_func(message)\n\n        if stream:\n            return self.chat_response(prompt=prompt, stream=stream)\n\n        response, success = self.chat_response(prompt)\n        if verbose:\n            display(Markdown(\"#### Question: \\n {question}\".format(question=message)))\n        if success:\n            answer = get_mark_from_response(response, self.model)\n            self.outputs.append(answer)\n            self.inputs.append(message)\n            self.prompts.append(prompt)\n            if verbose:\n                display(\n                    Markdown(\n                        \" #### Anwser: \\n {answer}\".format(\n                            answer=get_str_from_response(response, self.model)\n                        )\n                    )\n                )\n            return answer\n        else:\n            raise Exception(\"OpenAI API Error inside query function\")\n\n    def reset_logs(self):\n        \"\"\"\n        Reset the chatbot's memory.\n        \"\"\"\n        self.outputs = []\n        self.inputs = []\n        self.prompts = []\n\n    def get_logs(self):\n        \"\"\"\n        Get the chatbot's memory.\n\n        :return: A tuple containing the chatbot's memory as three lists of strings.\n        \"\"\"\n        return self.inputs, self.outputs, self.prompts\n\n    def run_text(\n        self, text: str, state: List[Tuple[str, str]]\n    ) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\n        \"\"\"\n        Process the user's text input and update the chat state.\n\n        :param text: A string representing the user input.\n        :param state: A list of tuples representing the current chat state.\n        :return: A tuple containing the updated chat state as two lists of tuples.\n        \"\"\"\n        print(\"===============Running run_text =============\")\n        print(\"Inputs:\", text)\n        try:\n            print(\"======>Current memory:\\n %s\" % self.memory_thread)\n        except:\n            print(\"======>No memory\")\n        print(\"failed here\")\n        response = self.reply(text)\n        state = state + [(text, response)]\n        print(\"Outputs:\", state)\n        return state, state\n\n    def gradio(self):\n        \"\"\"\n        Create and launch a Gradio interface for the chatbot.\n        \"\"\"\n        with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n            chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\n            state = gr.State([])\n            with gr.Row():\n                with gr.Column(scale=1):\n                    txt = gr.Textbox(\n                        show_label=False,\n                        placeholder=\"Enter text and press enter, or upload an image\",\n                    ).style(container=False)\n                with gr.Column(scale=0.15, min_width=0):\n                    clear = gr.Button(\"Clear\")\n\n            txt.submit(lambda text, state: self.run_text(text, state), [txt, state], [chatbot, state])\n            txt.submit(lambda: \"\", None, txt)\n            demo.launch(server_name=\"localhost\", server_port=7860)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(self, model: Union[str,None] = None, max_output_tokens: int = 200):\n    \"\"\"\n        Initialize the BaseChat with a model and max_output_tokens.\n\n        :param model: A string representing the chat model to be used.\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\n        \"\"\"\n    if model is None:\n        self.model = \"gpt-3.5-turbo\"\n    else:\n        self.model = model\n    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n    self.max_output_tokens = max_output_tokens\n    self.failed_responses = []\n    self.outputs = []\n    self.inputs = []\n    self.prompts = []\n    self.prompt_func = self.identity_prompter\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __getstate__(self):\n    state = self.__dict__.copy()\n    # Remove the tokenizer attribute from the state\n    del state[\"tokenizer\"]\n    return state\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __setstate__(self, state):\n    self.__dict__.update(state)\n    # Reinitialize the tokenizer attribute after unpickling\n    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\n    \"\"\"\n        A simple identity prompter that takes a message and returns the message marked as a question.\n\n        :param message: A string representing the user message.\n        :return: A tuple containing the marked question and the original message.\n        \"\"\"\n    return [mark_question(message)], mark_question(message)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef chat_response( self, prompt: List[dict], max_tokens: Union[int,None] = None, stream:bool = False ) -> Union[Generator,Tuple[Dict, bool]]:\n    if max_tokens is None:\n        max_tokens = self.max_output_tokens\n    if \"gpt\" in self.model:\n        logging.info(prompt)\n        response, status = chatgpt_response(prompt=prompt,model=self.model, max_tokens = 1000, stream=stream)\n        if status:\n            return response, True\n        else:\n            self.failed_responses.append(response)\n            return response, False\n\n    elif \"command\" in self.model:\n        response, status = cohere_response(prompt=prompt,model=self.model, max_tokens = 1000)  \n        if status:\n            return response, True\n        else:\n            self.failed_responses.append(response)\n            return response, False\n    else:\n        return {}, False \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef reply(self, message: str, verbose: bool = True, stream: bool = False) -> Union[Generator, str]:\n    \"\"\"\n        Reply to a given message using the chatbot.\n\n        :param message: A string representing the user message.\n        :return: A string representing the chatbot's response.\n        \"\"\"\n    if stream:\n        return self.query(message, verbose, stream)\n    else:\n        return self.query(message, verbose)[\"content\"]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef query(self, message: str, verbose: bool = True, stream: bool = False) -> Union[Generator, str]:\n    \"\"\"\n        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\n\n        :param message: A string representing the user message.\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n        :return: A string representing the chatbot's response.\n        \"\"\"\n\n    prompt, _ = self.prompt_func(message)\n\n    if stream:\n        return self.chat_response(prompt=prompt, stream=stream)\n\n    response, success = self.chat_response(prompt)\n    if verbose:\n        display(Markdown(\"#### Question: \\n {question}\".format(question=message)))\n    if success:\n        answer = get_mark_from_response(response, self.model)\n        self.outputs.append(answer)\n        self.inputs.append(message)\n        self.prompts.append(prompt)\n        if verbose:\n            display(\n                Markdown(\n                    \" #### Anwser: \\n {answer}\".format(\n                        answer=get_str_from_response(response, self.model)\n                    )\n                )\n            )\n        return answer\n    else:\n        raise Exception(\"OpenAI API Error inside query function\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef reset_logs(self):\n    \"\"\"\n        Reset the chatbot's memory.\n        \"\"\"\n    self.outputs = []\n    self.inputs = []\n    self.prompts = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_logs(self):\n    \"\"\"\n        Get the chatbot's memory.\n\n        :return: A tuple containing the chatbot's memory as three lists of strings.\n        \"\"\"\n    return self.inputs, self.outputs, self.prompts\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef run_text(\n    self, text: str, state: List[Tuple[str, str]]\n) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\n    \"\"\"\n        Process the user's text input and update the chat state.\n\n        :param text: A string representing the user input.\n        :param state: A list of tuples representing the current chat state.\n        :return: A tuple containing the updated chat state as two lists of tuples.\n        \"\"\"\n    print(\"===============Running run_text =============\")\n    print(\"Inputs:\", text)\n    try:\n        print(\"======>Current memory:\\n %s\" % self.memory_thread)\n    except:\n        print(\"======>No memory\")\n    print(\"failed here\")\n    response = self.reply(text)\n    state = state + [(text, response)]\n    print(\"Outputs:\", state)\n    return state, state\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef gradio(self):\n    \"\"\"\n        Create and launch a Gradio interface for the chatbot.\n        \"\"\"\n    with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n        chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\n        state = gr.State([])\n        with gr.Row():\n            with gr.Column(scale=1):\n                txt = gr.Textbox(\n                    show_label=False,\n                    placeholder=\"Enter text and press enter, or upload an image\",\n                ).style(container=False)\n            with gr.Column(scale=0.15, min_width=0):\n                clear = gr.Button(\"Clear\")\n\n        txt.submit(lambda text, state: self.run_text(text, state), [txt, state], [chatbot, state])\n        txt.submit(lambda: \"\", None, txt)\n        demo.launch(server_name=\"localhost\", server_port=7860)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef list_subjects_and_perspective():\n    # read from the file\n    prompts = json.load(open(filename))\n    subject_and_perspective = list(prompts.keys())\n    subjects = set()\n    perspectives = set()\n    for item in subject_and_perspective:\n        subject, perspective = item.split('\\\\')\n        subjects.add(subject)\n        perspectives.add(perspective)\n    return subjects, perspectives\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef list_subjects():\n    # read from the file\n    prompts = json.load(open(filename))\n    subject_and_perspective = list(prompts.keys())\n    subjects = set()\n    for item in subject_and_perspective:\n        subject, perspective = item.split('\\\\')\n        subjects.add(subject)\n    return subjects\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef list_perspectives():\n    # read from the file\n    prompts = json.load(open(filename))\n    subject_and_perspective = list(prompts.keys())\n    perspectives = set()\n    for item in subject_and_perspective:\n        subject, perspective = item.split('\\\\')\n        perspectives.add(perspective)\n    return perspectives\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_perspective_prompt(subject, perspective):\n    # read from the file\n    prompts = json.load(open(filename))\n    key = subject + '\\\\' + perspective\n    if key in prompts:\n        return prompts[key]\n    else:\n        raise Exception('No prompt found for subject: ' + subject + ' and perspective: ' + perspective +' use list_subjects() and list_perspectives() to see available prompts')\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_random_perspective_prompt():\n    # read from the file\n    prompts = json.load(open(filename))\n    key = random.choice(list(prompts.keys()))\n    subject = key.split('\\\\')[0]\n    perspective = key.split('\\\\')[1]\n    return subject, perspective, prompts[key]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef extract_values_and_embeddings(\n        data_frame: pd.DataFrame,\n        columns: Union[str, List[str]],\n        embeddings_col: Optional[str],\n    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\n    \"\"\"\n        Extract values and embeddings from a pandas DataFrame.\n\n        Args:\n            data_frame: The DataFrame to extract values and embeddings from.\n            columns: The columns of the DataFrame to use as values.\n            embeddings_col: The column name containing the embeddings.\n\n        Returns:\n            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n        \"\"\"\n\n    if isinstance(columns, list) and len(columns) > 1:\n        data_frame[\"values_combined\"] = data_frame[columns].apply(\n            lambda x: \" \".join(x), axis=1\n        )\n        columns = \"values_combined\"\n    elif isinstance(columns, list) and len(columns) == 1:\n        columns = columns[0]\n    elif not isinstance(columns, str):\n        raise ValueError(\"The columns are not valid\")\n\n    values = []\n    embeddings = []\n\n    for _, row in data_frame.iterrows():\n        value = row[columns]\n        values.append(value)\n\n        if embeddings_col is not None:\n            embedding = row[embeddings_col]\n            embeddings.append(embedding)\n\n    return values, embeddings if embeddings_col is not None else None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef convert_mark_to_str_prompt(messages: List[dict], prompt: str = \"\") -> str:\n    prompt = \"\"\n    for message in messages:\n        role = message[\"role\"].upper()\n        content = message[\"content\"]\n        prompt += f\" #{role}: {content}\"\n\n    return prompt\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef mark_system(system_prompt):\n    return {\"role\": \"system\", \"content\": system_prompt}\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef mark_answer(answer):\n    return {\"role\": \"assistant\", \"content\": answer}\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef mark_question(question):\n    return {\"role\": \"user\", \"content\": question}\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef check_dict(message_dict):\n    if (\n        type(message_dict) is list\n        and len(message_dict) == 1\n        and type(message_dict[0]) is dict\n    ):\n        message_dict = message_dict[0]\n    elif type(message_dict) is not dict:\n        raise Exception(\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\n            message_dict,\n            type(message_dict),\n        )\n    return message_dict\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef get_mark_from_response(response, model = \"gpt\"):\n    # return the answer from the response\n    if \"gpt\" in model:\n        role = response[\"choices\"][0][\"message\"][\"role\"]\n        message = response[\"choices\"][0][\"message\"][\"content\"]\n    elif \"command\" in model:\n        role = \"assistant\"\n        message = response[0].text \n    else:\n        raise Exception(\"Unknown model type\")\n    return {\"role\": role, \"content\": message}\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef get_str_from_response(response, model = \"gpt\"):\n    # return the answer from the response\n    if \"gpt\" in model:\n        return response[\"choices\"][0][\"message\"][\"content\"]\n    elif \"command\" in model:\n        text = response[0].text\n        text_without_assistant = text.replace(\"#ASSISTANT\", \"\")\n        return text_without_assistant\n    else:\n        raise Exception(\"Unknown model type\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef apply_sigmoid(matrix:np.ndarray):\n    \"\"\"\n    This function applies a sigmoid non-linearity to the matrix elements.\n    The sigmoid function maps any value to a value between 0 and 1.\n    \"\"\"\n    return expit(matrix)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef apply_threshold(matrix, threshold=0.5):\n    \"\"\"\n    This function applies a threshold to the matrix elements.\n    All values above the threshold are set to 1, all values below or equal to the threshold are set to 0.\n    \"\"\"\n    matrix[matrix > threshold] = 1\n    matrix[matrix <= threshold] = 0\n    return matrix\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef concat_columns(example, index=None):\n    column1='title'\n    column2='text'\n    example['merged_column'] = example[column1] +\" - \" + example[column2]\n    return example\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef extract_values_and_embeddings_hf(\n    dataset: datasets.Dataset,\n    value_column: Union[str, List[str]],\n    embeddings_column: Optional[str],\n) -> Tuple[List[str], Optional[List[np.ndarray]]]:\n    \"\"\"\n    Extract values and embeddings from a Hugging Face dataset.\n\n    Args:\n        dataset: The Hugging Face dataset to extract values and embeddings from.\n        value_column: The column(s) of the dataset to use as values.\n        embeddings_column: The column name containing the embeddings.\n\n    Returns:\n        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n    \"\"\"\n    if isinstance(value_column, str):\n        value_column = [value_column]\n    print(\"Merging values: Start\")\n    merged_docs = dataset.map(concat_columns, with_indices=True)\n    print(\"Merging values: Done\")\n    values = merged_docs['merged_column']\n    embeddings = dataset[embeddings_column]\n    return values, embeddings if embeddings_column is not None else None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef extract_values_hf(dataset: datasets.Dataset, value_column: Union[str, List[str]]) -> List[str]:\n    \"\"\"\n    Extract values from a Hugging Face dataset.\n\n    Args:\n        dataset: The Hugging Face dataset to extract values from.\n        value_column: The column(s) of the dataset to use as values.\n\n    Returns:\n        A list with the extracted values.\n    \"\"\"\n    if isinstance(value_column, str):\n        value_column = [value_column]\n    if len(value_column) == 1:\n        return dataset[value_column[0]]\n    elif len(value_column) > 1:\n        merged_docs = dataset.map(concat_columns)\n        return merged_docs\n    else:\n        raise ValueError(\"No value column specified.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef extract_values_and_embeddings_python(\n    directory_path: str,\n    minify_code: bool = False,\n    remove_docstrings: bool = False,\n    resolution: str = \"function\"\n) -> Tuple[List[str], List[Dict[str, str]]]:\n    values = []\n    context = []\n\n    parser = PythonParser(\n        directory_path=directory_path,\n        minify_code=minify_code,\n        remove_docstrings=remove_docstrings\n    )\n\n    results_dict = parser.process_directory()\n\n    if resolution == \"function\":\n        source_codes = results_dict['function_source_codes']\n        nodes = results_dict['function_nodes']\n    elif resolution == \"class\":\n        source_codes = results_dict['class_source_codes']\n        nodes = results_dict['class_nodes']\n    elif resolution == \"both\":\n        source_codes = results_dict['full_source']\n        nodes = results_dict['full_nodes']\n    else:\n        raise ValueError(f\"Invalid resolution: {resolution}\")\n    if resolution in ['function', 'class']:\n        for source_code, node in zip(source_codes, nodes):\n            values.append(source_code)\n            context.append({\"libcst tree\": node})\n    elif resolution == \"both\":\n        for source_code, node, filename in zip(source_codes, nodes, results_dict['file_map']):\n            values.append(source_code)\n            context.append({\"libcst tree\": str(node), \"filename\": filename})\n    else:\n        raise ValueError(f\"Invalid resolution: {resolution}\")\n    return values, context\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nasync def process_api_requests_from_file(\n    requests_filepath: str,\n    save_filepath: str,\n    request_url: str,\n    api_key: str,\n    max_requests_per_minute: float,\n    max_tokens_per_minute: float,\n    token_encoding_name: str,\n    max_attempts: int,\n    logging_level: int,\n):\n    \"\"\"Processes API requests in parallel, throttling to stay under rate limits.\"\"\"\n    # constants\n    seconds_to_pause_after_rate_limit_error = 15\n    seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n\n    # initialize logging\n    logging.basicConfig(level=logging_level)\n    logging.debug(f\"Logging initialized at level {logging_level}\")\n\n    # infer API endpoint and construct request header\n    api_endpoint = api_endpoint_from_url(request_url)\n    request_header = {\"Authorization\": f\"Bearer {api_key}\"}\n\n    # initialize trackers\n    queue_of_requests_to_retry = asyncio.Queue()\n    task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...\n    status_tracker = StatusTracker()  # single instance to track a collection of variables\n    next_request = None  # variable to hold the next request to call\n\n    # initialize available capacity counts\n    available_request_capacity = max_requests_per_minute\n    available_token_capacity = max_tokens_per_minute\n    last_update_time = time.time()\n\n    # initialize flags\n    file_not_finished = True  # after file is empty, we'll skip reading it\n    logging.debug(f\"Initialization complete.\")\n\n    # initialize file reading\n    with open(requests_filepath) as file:\n        # `requests` will provide requests one at a time\n        requests = file.__iter__()\n        logging.debug(f\"File opened. Entering main loop\")\n\n        while True:\n            # get next request (if one is not already waiting for capacity)\n            if next_request is None:\n                if not queue_of_requests_to_retry.empty():\n                    next_request = queue_of_requests_to_retry.get_nowait()\n                    logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")\n                elif file_not_finished:\n                    try:\n                        # get new request\n                        request_json = json.loads(next(requests))\n                        next_request = APIRequest(\n                            task_id=next(task_id_generator),\n                            request_json=request_json,\n                            token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),\n                            attempts_left=max_attempts,\n                            metadata=request_json.pop(\"metadata\", None)\n                        )\n                        status_tracker.num_tasks_started += 1\n                        status_tracker.num_tasks_in_progress += 1\n                        logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")\n                    except StopIteration:\n                        # if file runs out, set flag to stop reading it\n                        logging.debug(\"Read file exhausted\")\n                        file_not_finished = False\n\n            # update available capacity\n            current_time = time.time()\n            seconds_since_update = current_time - last_update_time\n            available_request_capacity = min(\n                available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,\n                max_requests_per_minute,\n            )\n            available_token_capacity = min(\n                available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,\n                max_tokens_per_minute,\n            )\n            last_update_time = current_time\n\n            # if enough capacity available, call API\n            if next_request:\n                next_request_tokens = next_request.token_consumption\n                if (\n                    available_request_capacity >= 1\n                    and available_token_capacity >= next_request_tokens\n                ):\n                    # update counters\n                    available_request_capacity -= 1\n                    available_token_capacity -= next_request_tokens\n                    next_request.attempts_left -= 1\n\n                    # call API\n                    asyncio.create_task(\n                        next_request.call_api(\n                            request_url=request_url,\n                            request_header=request_header,\n                            retry_queue=queue_of_requests_to_retry,\n                            save_filepath=save_filepath,\n                            status_tracker=status_tracker,\n                        )\n                    )\n                    next_request = None  # reset next_request to empty\n\n            # if all tasks are finished, break\n            if status_tracker.num_tasks_in_progress == 0:\n                break\n\n            # main loop sleeps briefly so concurrent tasks can run\n            await asyncio.sleep(seconds_to_sleep_each_loop)\n\n            # if a rate limit error was hit recently, pause to cool down\n            seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)\n            if seconds_since_rate_limit_error < seconds_to_pause_after_rate_limit_error:\n                remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n                await asyncio.sleep(remaining_seconds_to_pause)\n                # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n                logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")\n\n        # after finishing, log final status\n        logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")\n        if status_tracker.num_tasks_failed > 0:\n            logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")\n        if status_tracker.num_rate_limit_errors > 0:\n            logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# dataclasses\n\n\n@dataclass\nclass StatusTracker:\n    \"\"\"Stores metadata about the script's progress. Only one instance is created.\"\"\"\n\n    num_tasks_started: int = 0\n    num_tasks_in_progress: int = 0  # script ends when this reaches 0\n    num_tasks_succeeded: int = 0\n    num_tasks_failed: int = 0\n    num_rate_limit_errors: int = 0\n    num_api_errors: int = 0  # excluding rate limit errors, counted above\n    num_other_errors: int = 0\n    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n@dataclass\nclass APIRequest:\n    \"\"\"Stores an API request's inputs, outputs, and other metadata. Contains a method to make an API call.\"\"\"\n\n    task_id: int\n    request_json: dict\n    token_consumption: int\n    attempts_left: int\n    metadata: dict\n    result: list = field(default_factory=list)\n\n    async def call_api(\n        self,\n        request_url: str,\n        request_header: dict,\n        retry_queue: asyncio.Queue,\n        save_filepath: str,\n        status_tracker: StatusTracker,\n    ):\n        \"\"\"Calls the OpenAI API and saves results.\"\"\"\n        logging.info(f\"Starting request #{self.task_id}\")\n        error = None\n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    url=request_url, headers=request_header, json=self.request_json\n                ) as response:\n                    response = await response.json()\n            if \"error\" in response:\n                logging.warning(\n                    f\"Request {self.task_id} failed with error {response['error']}\"\n                )\n                status_tracker.num_api_errors += 1\n                error = response\n                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n                    status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \n                    status_tracker.num_rate_limit_errors += 1\n                    status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n\n        except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n            logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n            status_tracker.num_other_errors += 1\n            error = e\n        if error:\n            self.result.append(error)\n            if self.attempts_left:\n                retry_queue.put_nowait(self)\n            else:\n                logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n                data = (\n                    [self.request_json, [str(e) for e in self.result], self.metadata]\n                    if self.metadata\n                    else [self.request_json, [str(e) for e in self.result]]\n                )\n                append_to_jsonl(data, save_filepath)\n                status_tracker.num_tasks_in_progress -= 1\n                status_tracker.num_tasks_failed += 1\n        else:\n            data = (\n                [self.request_json, response, self.metadata]\n                if self.metadata\n                else [self.request_json, response]\n            )\n            append_to_jsonl(data, save_filepath)\n            status_tracker.num_tasks_in_progress -= 1\n            status_tracker.num_tasks_succeeded += 1\n            logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nasync def call_api(\n    self,\n    request_url: str,\n    request_header: dict,\n    retry_queue: asyncio.Queue,\n    save_filepath: str,\n    status_tracker: StatusTracker,\n):\n    \"\"\"Calls the OpenAI API and saves results.\"\"\"\n    logging.info(f\"Starting request #{self.task_id}\")\n    error = None\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                url=request_url, headers=request_header, json=self.request_json\n            ) as response:\n                response = await response.json()\n        if \"error\" in response:\n            logging.warning(\n                f\"Request {self.task_id} failed with error {response['error']}\"\n            )\n            status_tracker.num_api_errors += 1\n            error = response\n            if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n                status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \n                status_tracker.num_rate_limit_errors += 1\n                status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n\n    except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n        logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n        status_tracker.num_other_errors += 1\n        error = e\n    if error:\n        self.result.append(error)\n        if self.attempts_left:\n            retry_queue.put_nowait(self)\n        else:\n            logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n            data = (\n                [self.request_json, [str(e) for e in self.result], self.metadata]\n                if self.metadata\n                else [self.request_json, [str(e) for e in self.result]]\n            )\n            append_to_jsonl(data, save_filepath)\n            status_tracker.num_tasks_in_progress -= 1\n            status_tracker.num_tasks_failed += 1\n    else:\n        data = (\n            [self.request_json, response, self.metadata]\n            if self.metadata\n            else [self.request_json, response]\n        )\n        append_to_jsonl(data, save_filepath)\n        status_tracker.num_tasks_in_progress -= 1\n        status_tracker.num_tasks_succeeded += 1\n        logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# functions\n\n\ndef api_endpoint_from_url(request_url):\n    \"\"\"Extract the API endpoint from the request URL.\"\"\"\n    match = re.search('^https://[^/]+/v\\\\d+/(.+)$', request_url)\n    return match[1]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef append_to_jsonl(data, filename: str) -> None:\n    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\n    json_string = json.dumps(data)\n    with open(filename, \"a\") as f:\n        f.write(json_string + \"\\n\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef num_tokens_consumed_from_request(\n    request_json: dict,\n    api_endpoint: str,\n    token_encoding_name: str,\n):\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\n    encoding = tiktoken.get_encoding(token_encoding_name)\n    # if completions request, tokens = prompt + n * max_tokens\n    if api_endpoint.endswith(\"completions\"):\n        max_tokens = request_json.get(\"max_tokens\", 15)\n        n = request_json.get(\"n\", 1)\n        completion_tokens = n * max_tokens\n\n        # chat completions\n        if api_endpoint.startswith(\"chat/\"):\n            num_tokens = 0\n            for message in request_json[\"messages\"]:\n                num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n                for key, value in message.items():\n                    num_tokens += len(encoding.encode(value))\n                    if key == \"name\":  # if there's a name, the role is omitted\n                        num_tokens -= 1  # role is always required and always 1 token\n            num_tokens += 2  # every reply is primed with <im_start>assistant\n            return num_tokens + completion_tokens\n        # normal completions\n        else:\n            prompt = request_json[\"prompt\"]\n            if isinstance(prompt, str):  # single prompt\n                prompt_tokens = len(encoding.encode(prompt))\n                num_tokens = prompt_tokens + completion_tokens\n                return num_tokens\n            elif isinstance(prompt, list):  # multiple prompts\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\n                return num_tokens\n            else:\n                raise TypeError('Expecting either string or list of strings for \"prompt\" field in completion request')\n    # if embeddings request, tokens = input tokens\n    elif api_endpoint == \"embeddings\":\n        input = request_json[\"input\"]\n        if isinstance(input, str):  # single input\n            num_tokens = len(encoding.encode(input))\n            return num_tokens\n        elif isinstance(input, list):  # multiple inputs\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\n            return num_tokens\n        else:\n            raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\n    else:\n        raise NotImplementedError(f'API endpoint \"{api_endpoint}\" not implemented in this script')\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef task_id_generator_function():\n    \"\"\"Generate integers 0, 1, 2, and so on.\"\"\"\n    task_id = 0\n    while True:\n        yield task_id\n        task_id += 1\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef extract_values_and_embeddings_pd(\n        data_frame: pd.DataFrame,\n        value_column: str,\n        embeddings_col: Optional[str],\n    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\n    \"\"\"\n        Extract values and embeddings from a pandas DataFrame.\n\n        Args:\n            data_frame: The DataFrame to extract values and embeddings from.\n            value_column: The column of the DataFrame to use as values.\n            embeddings_col: The column name containing the embeddings.\n\n        Returns:\n            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n        \"\"\"\n    values = data_frame[value_column].tolist()\n    embeddings = data_frame[embeddings_col].tolist() if embeddings_col else None\n\n    return values, embeddings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef extract_values_and_embeddings_hf(\n    dataset: datasets.Dataset,\n    value_column: str,\n    embeddings_column: Optional[str],\n) -> Tuple[List[str], Optional[List[np.ndarray]]]:\n    \"\"\"\n    Extract values and embeddings from a Hugging Face dataset.\n\n    Args:\n        dataset: The Hugging Face dataset to extract values and embeddings from.\n        value_column: The column of the dataset to use as values.\n        embeddings_column: The column name containing the embeddings.\n\n    Returns:\n        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n    \"\"\"\n    values = dataset[value_column]\n    embeddings = dataset[embeddings_column] if embeddings_column else None\n\n    return values, embeddings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef extract_values_and_embeddings_polars(\n        data_frame: pl.DataFrame,\n        value_column: str,\n        embeddings_column: Optional[str]\n    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\n    \"\"\"\n    Extract values and embeddings from a Polars DataFrame.\n\n    Args:\n        data_frame: The DataFrame to extract values and embeddings from.\n        value_column: The column of the DataFrame to use as values.\n        embeddings_column: The column name containing the embeddings.\n\n    Returns:\n        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n    \"\"\"\n    values = data_frame[value_column].to_list()\n    embeddings = data_frame[embeddings_column].to_list() if embeddings_column else None\n\n    return values, embeddings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_context_from_pandas(\n          data_frame: pd.DataFrame,\n          context_columns: List[str]):\n    \"\"\"Extract context information from a pandas DataFrame.\"\"\"\n    # This function will convert the row of the DataFrame into a dictionary where the keys are the column names.\n    def row_to_dict(row: pd.Series) -> Dict[str, Any]:\n        return {column: row[column] for column in context_columns}\n\n    # Apply the function to each row of the DataFrame and convert the result into a list.\n    context = data_frame.apply(row_to_dict, axis=1).tolist()\n\n    return context\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"# This function will convert the row of the DataFrame into a dictionary where the keys are the column names.\ndef row_to_dict(row: pd.Series) -> Dict[str, Any]:\n    return {column: row[column] for column in context_columns}\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_context_from_hf(\n          data_frame: datasets.Dataset,\n          context_columns: List[str]):\n    \"\"\"return a list dictionaries with the keys the column name and value the context columns values\"\"\"\n    context_data = {column: data_frame[column] for column in context_columns}\n    context = []\n    data_frame_len = len(data_frame)\n    for row in range(data_frame_len):\n         context.append({column: context_data[column][row] for column in context_columns})\n    return context\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_context_from_polars(\n          data_frame: pl.DataFrame,\n          context_columns: List[str]) -> List[Dict[str, Any]]:\n    \"\"\"Extract context information from a Polars DataFrame.\"\"\"\n\n    context = []\n    \n    # Convert each row to a dictionary\n    for i in range(len(data_frame)):\n        row_dict = data_frame[i]\n        context.append({column: row_dict[column] for column in context_columns})\n\n    return context\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass EmbeddingAnalysis:\n    def __init__(self, embedding_matrix: np.ndarray, kernel_matrix: np.ndarray) -> None:\n        \"\"\"\n        Initializes an instance of the EmbeddingAnalysis class.\n\n        Parameters:\n        kernel_matrix (np.ndarray): A square kernel matrix.\n\n        Raises:\n        ValueError: If the input kernel matrix is not square or symmetric.\n        \"\"\"\n        if kernel_matrix.shape[0] != kernel_matrix.shape[1]:\n            raise ValueError(\"The input kernel matrix must be square.\")\n        if not np.allclose(kernel_matrix, kernel_matrix.T, atol=1e-8):\n            raise ValueError(\"The input kernel matrix must be symmetric.\")\n        self.embedding_matrix = embedding_matrix\n        self.kernel_matrix = kernel_matrix\n        self.eigenvalues = np.linalg.eigvalsh(kernel_matrix)\n\n    def mean_center(self, A):\n        # A.shape = (embedding dim, num of articles)\n        avg_doc = np.mean(A, axis=1)\n        # avg_doc.shape = (embedding dim, 1)\n        # Compute eigenfaces on mean-subtracted training data\n        X = A - np.tile(avg_doc,(A.shape[1],1)).T\n        # X.shape = A.shape\n        return X, avg_doc\n\n    def eigen_topic(self, A, r1 = 5, r2=55):\n        X, avg_doc = self.mean_center(A)\n        # SVD on Mean-centered articles\n        U, _, _ = np.linalg.svd(X)\n        # U.shape = (embedding dim, embedding dim)\n        # np.diag(S.shape) = (embedding dim, embedding dim)\n        # VT.shape = (num of articles, num of articles)\n        econ_UT = U[:, r1:r2].T\n        # econ_UT.shape = (r2-r1, emmbedding dim)\n        transformed_matrix = econ_UT @ X\n        # transformed_matrix.shape = (r2-r1, num of articles)\n        return transformed_matrix, avg_doc, econ_UT\n\n    def check_symmetry(self) -> bool:\n        \"\"\"\n        Checks if the kernel matrix is symmetric.\n\n        Returns:\n        bool: True if the kernel matrix is symmetric, False otherwise.\n        \"\"\"\n        return np.allclose(self.kernel_matrix, self.kernel_matrix.T, atol=1e-8)\n\n    def is_positive_definite(self) -> bool:\n        \"\"\"\n        Checks if the kernel matrix is positive definite.\n\n        Returns:\n        bool: True if all eigenvalues are positive, False otherwise.\n        \"\"\"\n        return np.all(self.eigenvalues > 0)\n\n    def is_positive_semi_definite(self) -> bool:\n        \"\"\"\n        Checks if the kernel matrix is positive semi-definite.\n\n        Returns:\n        bool: True if all eigenvalues are non-negative, False otherwise.\n        \"\"\"\n        return np.all(self.eigenvalues >= 0)\n\n    def is_negative_definite(self) -> bool:\n        \"\"\"\n        Checks if the kernel matrix is negative definite.\n\n        Returns:\n        bool: True if all eigenvalues are negative, False otherwise.\n        \"\"\"\n        return np.all(self.eigenvalues < 0)\n\n    def is_negative_semi_definite(self) -> bool:\n        \"\"\"\n        Checks if the kernel matrix is negative semi-definite.\n\n        Returns:\n        bool: True if all eigenvalues are non-positive, False otherwise.\n        \"\"\"\n        return np.all(self.eigenvalues <= 0)\n\n    def is_indefinite(self) -> bool:\n        \"\"\"\n        Checks if the kernel matrix is indefinite.\n\n        Returns:\n        bool: True if the matrix has both positive and negative eigenvalues, False otherwise.\n        \"\"\"\n        has_negative = np.any(self.eigenvalues < 0)\n        has_non_negative = np.any(self.eigenvalues >= 0)\n        return has_negative and has_non_negative\n\n    def check_definiteness(self, num_random_vectors: int = 1000) -> Dict[str, bool]:\n        \"\"\"\n        Checks the definiteness of the kernel matrix using random vectors.\n\n        num_random_vectors: Number of random vectors to use for checking.\n\n        Returns:\n        Dict[str, bool]: A dictionary with the results of the analysis.\n        \"\"\"\n        n = self.kernel_matrix.shape[0]\n        is_positive_definite = True\n        is_negative_definite = True\n        for _ in range(num_random_vectors):\n            x = np.random.randn(n)\n            xTMx = x.T @ self.kernel_matrix @ x\n            if xTMx <= 0:\n                is_positive_definite = False\n            if xTMx >= 0:\n                is_negative_definite = False\n        return {\n            \"is_positive_definite\": is_positive_definite,\n            \"is_negative_definite\": is_negative_definite,\n        }\n\n    def run_analysis(self) -> Dict[str, bool]:\n        return {\n            \"is_symmetric\": self.check_symmetry(),\n            \"is_positive_semi_definite\": self.is_positive_semi_definite(),\n            \"is_negative_semi_definite\": self.is_negative_semi_definite(),\n            \"is_indefinite\": self.is_indefinite(),\n            **self.check_definiteness()\n        }\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, embedding_matrix: np.ndarray, kernel_matrix: np.ndarray) -> None:\n    \"\"\"\n        Initializes an instance of the EmbeddingAnalysis class.\n\n        Parameters:\n        kernel_matrix (np.ndarray): A square kernel matrix.\n\n        Raises:\n        ValueError: If the input kernel matrix is not square or symmetric.\n        \"\"\"\n    if kernel_matrix.shape[0] != kernel_matrix.shape[1]:\n        raise ValueError(\"The input kernel matrix must be square.\")\n    if not np.allclose(kernel_matrix, kernel_matrix.T, atol=1e-8):\n        raise ValueError(\"The input kernel matrix must be symmetric.\")\n    self.embedding_matrix = embedding_matrix\n    self.kernel_matrix = kernel_matrix\n    self.eigenvalues = np.linalg.eigvalsh(kernel_matrix)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef mean_center(self, A):\n    # A.shape = (embedding dim, num of articles)\n    avg_doc = np.mean(A, axis=1)\n    # avg_doc.shape = (embedding dim, 1)\n    # Compute eigenfaces on mean-subtracted training data\n    X = A - np.tile(avg_doc,(A.shape[1],1)).T\n    # X.shape = A.shape\n    return X, avg_doc\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef eigen_topic(self, A, r1 = 5, r2=55):\n    X, avg_doc = self.mean_center(A)\n    # SVD on Mean-centered articles\n    U, _, _ = np.linalg.svd(X)\n    # U.shape = (embedding dim, embedding dim)\n    # np.diag(S.shape) = (embedding dim, embedding dim)\n    # VT.shape = (num of articles, num of articles)\n    econ_UT = U[:, r1:r2].T\n    # econ_UT.shape = (r2-r1, emmbedding dim)\n    transformed_matrix = econ_UT @ X\n    # transformed_matrix.shape = (r2-r1, num of articles)\n    return transformed_matrix, avg_doc, econ_UT\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef check_symmetry(self) -> bool:\n    \"\"\"\n        Checks if the kernel matrix is symmetric.\n\n        Returns:\n        bool: True if the kernel matrix is symmetric, False otherwise.\n        \"\"\"\n    return np.allclose(self.kernel_matrix, self.kernel_matrix.T, atol=1e-8)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef is_positive_definite(self) -> bool:\n    \"\"\"\n        Checks if the kernel matrix is positive definite.\n\n        Returns:\n        bool: True if all eigenvalues are positive, False otherwise.\n        \"\"\"\n    return np.all(self.eigenvalues > 0)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef is_positive_semi_definite(self) -> bool:\n    \"\"\"\n        Checks if the kernel matrix is positive semi-definite.\n\n        Returns:\n        bool: True if all eigenvalues are non-negative, False otherwise.\n        \"\"\"\n    return np.all(self.eigenvalues >= 0)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef is_negative_definite(self) -> bool:\n    \"\"\"\n        Checks if the kernel matrix is negative definite.\n\n        Returns:\n        bool: True if all eigenvalues are negative, False otherwise.\n        \"\"\"\n    return np.all(self.eigenvalues < 0)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef is_negative_semi_definite(self) -> bool:\n    \"\"\"\n        Checks if the kernel matrix is negative semi-definite.\n\n        Returns:\n        bool: True if all eigenvalues are non-positive, False otherwise.\n        \"\"\"\n    return np.all(self.eigenvalues <= 0)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef is_indefinite(self) -> bool:\n    \"\"\"\n        Checks if the kernel matrix is indefinite.\n\n        Returns:\n        bool: True if the matrix has both positive and negative eigenvalues, False otherwise.\n        \"\"\"\n    has_negative = np.any(self.eigenvalues < 0)\n    has_non_negative = np.any(self.eigenvalues >= 0)\n    return has_negative and has_non_negative\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef check_definiteness(self, num_random_vectors: int = 1000) -> Dict[str, bool]:\n    \"\"\"\n        Checks the definiteness of the kernel matrix using random vectors.\n\n        num_random_vectors: Number of random vectors to use for checking.\n\n        Returns:\n        Dict[str, bool]: A dictionary with the results of the analysis.\n        \"\"\"\n    n = self.kernel_matrix.shape[0]\n    is_positive_definite = True\n    is_negative_definite = True\n    for _ in range(num_random_vectors):\n        x = np.random.randn(n)\n        xTMx = x.T @ self.kernel_matrix @ x\n        if xTMx <= 0:\n            is_positive_definite = False\n        if xTMx >= 0:\n            is_negative_definite = False\n    return {\n        \"is_positive_definite\": is_positive_definite,\n        \"is_negative_definite\": is_negative_definite,\n    }\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef run_analysis(self) -> Dict[str, bool]:\n    return {\n        \"is_symmetric\": self.check_symmetry(),\n        \"is_positive_semi_definite\": self.is_positive_semi_definite(),\n        \"is_negative_semi_definite\": self.is_negative_semi_definite(),\n        \"is_indefinite\": self.is_indefinite(),\n        **self.check_definiteness()\n    }\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass RateLimiter:\n    def __init__(self, calls_per_minute: int, verbose: bool = False):\n        self.calls_per_minute = calls_per_minute\n        self.interval = 60 / calls_per_minute\n        self.lock = Lock()\n        self.last_call_time = None\n        self.verbose = verbose\n\n    def __call__(self, func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with self.lock:\n                if self.last_call_time is not None:\n                    time_since_last_call = time.time() - self.last_call_time\n                    if time_since_last_call < self.interval:\n                        time_to_wait = self.interval - time_since_last_call\n                        if self.verbose:\n                            print(\n                                f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\n                            )\n                        time.sleep(time_to_wait)\n                    elif self.verbose:\n                        print(\n                            f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\n                        )\n                else:\n                    if self.verbose:\n                        print(\"RateLimiter: This is the first call, no wait required.\")\n                self.last_call_time = time.time()\n            return func(*args, **kwargs)\n\n        return wrapper\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, calls_per_minute: int, verbose: bool = False):\n    self.calls_per_minute = calls_per_minute\n    self.interval = 60 / calls_per_minute\n    self.lock = Lock()\n    self.last_call_time = None\n    self.verbose = verbose\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __call__(self, func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        with self.lock:\n            if self.last_call_time is not None:\n                time_since_last_call = time.time() - self.last_call_time\n                if time_since_last_call < self.interval:\n                    time_to_wait = self.interval - time_since_last_call\n                    if self.verbose:\n                        print(\n                            f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\n                        )\n                    time.sleep(time_to_wait)\n                elif self.verbose:\n                    print(\n                        f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\n                    )\n            else:\n                if self.verbose:\n                    print(\"RateLimiter: This is the first call, no wait required.\")\n            self.last_call_time = time.time()\n        return func(*args, **kwargs)\n\n    return wrapper\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"@functools.wraps(func)\ndef wrapper(*args, **kwargs):\n    with self.lock:\n        if self.last_call_time is not None:\n            time_since_last_call = time.time() - self.last_call_time\n            if time_since_last_call < self.interval:\n                time_to_wait = self.interval - time_since_last_call\n                if self.verbose:\n                    print(\n                        f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\n                    )\n                time.sleep(time_to_wait)\n            elif self.verbose:\n                print(\n                    f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\n                )\n        else:\n            if self.verbose:\n                print(\"RateLimiter: This is the first call, no wait required.\")\n        self.last_call_time = time.time()\n    return func(*args, **kwargs)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass RateLimitedThreadPoolExecutor(ThreadPoolExecutor):\n    def __init__(self, max_workers=None, *args, **kwargs):\n        super().__init__(max_workers)\n        self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\n\n    def submit(self, fn, *args, **kwargs):\n        rate_limited_fn = self.rate_limiter(fn)\n        return super().submit(rate_limited_fn, *args, **kwargs)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, max_workers=None, *args, **kwargs):\n    super().__init__(max_workers)\n    self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef submit(self, fn, *args, **kwargs):\n    rate_limited_fn = self.rate_limiter(fn)\n    return super().submit(rate_limited_fn, *args, **kwargs)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"@dataclass\nclass StatusTracker:\n    \"\"\"Stores metadata about the script's progress. Only one instance is created.\"\"\"\n\n    num_tasks_started: int = 0\n    num_tasks_in_progress: int = 0  # script ends when this reaches 0\n    num_tasks_succeeded: int = 0\n    num_tasks_failed: int = 0\n    num_rate_limit_errors: int = 0\n    num_api_errors: int = 0  # excluding rate limit errors, counted above\n    num_other_errors: int = 0\n    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n@dataclass\nclass APIRequest:\n    \"\"\"Stores an API request's inputs, outputs, and other metadata. Contains a method to make an API call.\"\"\"\n\n    task_id: int\n    request_json: dict\n    token_consumption: int\n    attempts_left: int\n    metadata: dict\n    result: list = field(default_factory=list)\n\n    async def call_api(\n        self,\n        request_url: str,\n        request_header: dict,\n        retry_queue: asyncio.Queue,\n        save_filepath: str,\n        status_tracker: StatusTracker,\n    ):\n        \"\"\"Calls the OpenAI API and saves results.\"\"\"\n        logging.info(f\"Starting request #{self.task_id}\")\n        error = None\n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    url=request_url, headers=request_header, json=self.request_json\n                ) as response:\n                    response = await response.json()\n            if \"error\" in response:\n                logging.warning(\n                    f\"Request {self.task_id} failed with error {response['error']}\"\n                )\n                status_tracker.num_api_errors += 1\n                error = response\n                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n                    status_tracker.time_of_last_rate_limit_error = time.time()\n                    status_tracker.num_rate_limit_errors += 1\n                    status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n\n        except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n            logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n            status_tracker.num_other_errors += 1\n            error = e\n        if error:\n            self.result.append(error)\n            if self.attempts_left:\n                retry_queue.put_nowait(self)\n            else:\n                logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n                data = (\n                    [self.request_json, [str(e) for e in self.result], self.metadata]\n                    if self.metadata\n                    else [self.request_json, [str(e) for e in self.result]]\n                )\n                append_to_jsonl(data, save_filepath)\n                status_tracker.num_tasks_in_progress -= 1\n                status_tracker.num_tasks_failed += 1\n        else:\n            data = (\n                [self.request_json, response, self.metadata]\n                if self.metadata\n                else [self.request_json, response]\n            )\n            append_to_jsonl(data, save_filepath)\n            status_tracker.num_tasks_in_progress -= 1\n            status_tracker.num_tasks_succeeded += 1\n            logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nasync def call_api(\n    self,\n    request_url: str,\n    request_header: dict,\n    retry_queue: asyncio.Queue,\n    save_filepath: str,\n    status_tracker: StatusTracker,\n):\n    \"\"\"Calls the OpenAI API and saves results.\"\"\"\n    logging.info(f\"Starting request #{self.task_id}\")\n    error = None\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                url=request_url, headers=request_header, json=self.request_json\n            ) as response:\n                response = await response.json()\n        if \"error\" in response:\n            logging.warning(\n                f\"Request {self.task_id} failed with error {response['error']}\"\n            )\n            status_tracker.num_api_errors += 1\n            error = response\n            if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n                status_tracker.time_of_last_rate_limit_error = time.time()\n                status_tracker.num_rate_limit_errors += 1\n                status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n\n    except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n        logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n        status_tracker.num_other_errors += 1\n        error = e\n    if error:\n        self.result.append(error)\n        if self.attempts_left:\n            retry_queue.put_nowait(self)\n        else:\n            logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n            data = (\n                [self.request_json, [str(e) for e in self.result], self.metadata]\n                if self.metadata\n                else [self.request_json, [str(e) for e in self.result]]\n            )\n            append_to_jsonl(data, save_filepath)\n            status_tracker.num_tasks_in_progress -= 1\n            status_tracker.num_tasks_failed += 1\n    else:\n        data = (\n            [self.request_json, response, self.metadata]\n            if self.metadata\n            else [self.request_json, response]\n        )\n        append_to_jsonl(data, save_filepath)\n        status_tracker.num_tasks_in_progress -= 1\n        status_tracker.num_tasks_succeeded += 1\n        logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# functions\n\n\ndef api_endpoint_from_url(request_url):\n    \"\"\"Extract the API endpoint from the request URL.\"\"\"\n    match = re.search('^https://[^/]+/v\\\\d+/(.+)$', request_url)\n    return match[1]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef append_to_jsonl(data, filename: str) -> None:\n    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\n    json_string = json.dumps(data)\n    with open(filename, \"a\") as f:\n        f.write(json_string + \"\\n\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef num_tokens_consumed_from_request(\n    request_json: dict,\n    api_endpoint: str,\n    token_encoding_name: str,\n):\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\n    encoding = tiktoken.get_encoding(token_encoding_name)\n    # if completions request, tokens = prompt + n * max_tokens\n    if api_endpoint.endswith(\"completions\"):\n        max_tokens = request_json.get(\"max_tokens\", 15)\n        n = request_json.get(\"n\", 1)\n        completion_tokens = n * max_tokens\n\n        # chat completions\n        if api_endpoint.startswith(\"chat/\"):\n            num_tokens = 0\n            for message in request_json[\"messages\"]:\n                num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n                for key, value in message.items():\n                    num_tokens += len(encoding.encode(value))\n                    if key == \"name\":  # if there's a name, the role is omitted\n                        num_tokens -= 1  # role is always required and always 1 token\n            num_tokens += 2  # every reply is primed with <im_start>assistant\n            return num_tokens + completion_tokens\n        # normal completions\n        else:\n            prompt = request_json[\"prompt\"]\n            if isinstance(prompt, str):  # single prompt\n                prompt_tokens = len(encoding.encode(prompt))\n                num_tokens = prompt_tokens + completion_tokens\n                return num_tokens\n            elif isinstance(prompt, list):  # multiple prompts\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\n                return num_tokens\n            else:\n                raise TypeError('Expecting either string or list of strings for \"prompt\" field in completion request')\n    # if embeddings request, tokens = input tokens\n    elif api_endpoint == \"embeddings\":\n        input = request_json[\"input\"]\n        if isinstance(input, str):  # single input\n            num_tokens = len(encoding.encode(input))\n            return num_tokens\n        elif isinstance(input, list):  # multiple inputs\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\n            return num_tokens\n        else:\n            raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\n    else:\n        raise NotImplementedError(f'API endpoint \"{api_endpoint}\" not implemented in this script')\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef task_id_generator_function():\n    \"\"\"Generate integers 0, 1, 2, and so on.\"\"\"\n    task_id = 0\n    while True:\n        yield task_id\n        task_id += 1\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nasync def process_chat_requests(\n    request_data: List[Dict],\n    save_filepath: str,\n    request_url: str,\n    api_key: str,\n    max_requests_per_minute: float,\n    max_tokens_per_minute: float,\n    token_encoding_name: str,\n    max_attempts: int,\n    logging_level: int,\n):\n    \"\"\"Processes chat requests in parallel, throttling to stay under rate limits.\"\"\"\n    # constants\n    seconds_to_pause_after_rate_limit_error = 15\n    seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n\n    # initialize logging\n    logging.basicConfig(level=logging_level)\n    logging.debug(f\"Logging initialized at level {logging_level}\")\n\n    # infer API endpoint and construct request header\n    api_endpoint = api_endpoint_from_url(request_url)\n    request_header = {\"Authorization\": f\"Bearer {api_key}\"}\n\n    # initialize trackers\n    queue_of_requests_to_retry = asyncio.Queue()\n    task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...\n    status_tracker = StatusTracker()  # single instance to track a collection of variables\n    next_request = None  # variable to hold the next request to call\n\n    # initialize available capacity counts\n    available_request_capacity = max_requests_per_minute\n    available_token_capacity = max_tokens_per_minute\n    last_update_time = time.time()\n\n    # initialize flags\n    file_not_finished = True  # after file is empty, we'll skip reading it\n    logging.debug(f\"Initialization complete.\")\n\n    # `requests` will provide requests one at a time\n    requests = iter(request_data)\n    logging.debug(f\"List initialized. Entering main loop\")\n\n    while True:\n        # get next request (if one is not already waiting for capacity)\n        if next_request is None:\n            if not queue_of_requests_to_retry.empty():\n                next_request = queue_of_requests_to_retry.get_nowait()\n                logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")\n            elif file_not_finished:\n                try:\n                    # get new request\n                    request_json = next(requests)\n                    next_request = APIRequest(\n                        task_id=next(task_id_generator),\n                        request_json=request_json,\n                        token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),\n                        attempts_left=max_attempts,\n                        metadata=request_json.pop(\"metadata\", None)\n                    )\n                    status_tracker.num_tasks_started += 1\n                    status_tracker.num_tasks_in_progress += 1\n                    logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")\n                except StopIteration:\n                    # if file runs out, set flag to stop reading it\n                    logging.debug(\"Read file exhausted\")\n                    file_not_finished = False\n\n        # update available capacity\n        current_time = time.time()\n        seconds_since_update = current_time - last_update_time\n        available_request_capacity = min(\n            available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,\n            max_requests_per_minute,\n        )\n        available_token_capacity = min(\n            available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,\n            max_tokens_per_minute,\n        )\n        last_update_time = current_time\n\n        # if enough capacity available, call API\n        if next_request:\n            next_request_tokens = next_request.token_consumption\n            if (\n                available_request_capacity >= 1\n                and available_token_capacity >= next_request_tokens\n            ):\n                # update counters\n                available_request_capacity -= 1\n                available_token_capacity -= next_request_tokens\n                next_request.attempts_left -= 1\n\n                # call API\n                asyncio.create_task(\n                    next_request.call_api(\n                        request_url=request_url,\n                        request_header=request_header,\n                        retry_queue=queue_of_requests_to_retry,\n                        save_filepath=save_filepath,\n                        status_tracker=status_tracker,\n                    )\n                )\n                next_request = None  # reset next_request to empty\n\n        # if all tasks are finished, break\n        if status_tracker.num_tasks_in_progress == 0:\n            break\n\n        # main loop sleeps briefly so concurrent tasks can run\n        await asyncio.sleep(seconds_to_sleep_each_loop)\n\n        # if a rate limit error was hit recently, pause to cool down\n        seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)\n        if seconds_since_rate_limit_error < seconds_to_pause_after_rate_limit_error:\n            remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n            await asyncio.sleep(remaining_seconds_to_pause)\n            # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n            logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")\n\n    # after finishing, log final status\n    logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")\n    if status_tracker.num_tasks_failed > 0:\n        logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")\n    if status_tracker.num_rate_limit_errors > 0:\n        logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass OpenAiEmbedder:\n\n    def get_embedding_size(self):\n        return ADA_EMBEDDING_SIZE\n\n    def embed(self, data, verbose=False):\n        if isinstance(data, list) and len(data) > 1:\n            logger.info(\"Batch embedding\")\n            return self.batch_embed(data)\n        elif isinstance(data, list) and len(data) == 1:\n            logger.info(\"Serial embedding\")\n            data = data[0]\n\n        if isinstance(data, dict) and \"content\" in data:\n            if verbose:\n                logger.info(\"Embedding without mark\", data[\"content\"])\n            out = openai.Embedding.create(\n                input=data[\"content\"], engine=\"text-embedding-ada-002\"\n            )\n        else:\n            if len(TOKENIZER.encode(data)) > MAX_CONTEXT_LENGTH:\n                raise ValueError(f\" The input is too long for OpenAI, num tokens is {len(TOKENIZER.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\n            if verbose:\n                logger.info(f\"Embedding without preprocessing the input {data}\")\n            out = openai.Embedding.create(\n                input=str(data), engine=\"text-embedding-ada-002\"\n            )\n        return out.data[0].embedding\n\n    def batch_embed(self, data: List[str], batch_size: int = 1000):\n        if isinstance(data, dict) and \"content\" in data:\n            raise ValueError(\"Batch embedding not supported for dictionaries\")\n        elif isinstance(data, str):\n            raise ValueError(\"Batch embedding not supported for strings use embed() instead\")\n        elif isinstance(data, list):\n            batch = []\n            embeddings = []\n            i = 1\n            total_number_of_batches = len(data)//batch_size + 1 if len(data) % batch_size > 0 else len(data)//batch_size\n            for value in data:\n                batch.append(value)\n                if len(batch) == batch_size:\n                    start = time.time()\n                    out = openai.Embedding.create(\n                        input=batch, engine=\"text-embedding-ada-002\"\n                    )\n                    for embedding in out.data:\n                        embeddings.append(embedding.embedding)\n                    logger.info(f\"Batch {i} of {total_number_of_batches}\")\n                    logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\n                    i += 1\n                    batch = []\n            if len(batch) > 0:\n                start = time.time()\n                out = openai.Embedding.create(\n                    input=batch, engine=\"text-embedding-ada-002\"\n                )\n                for embedding in out.data:\n                    embeddings.append(embedding.embedding)\n                logger.info(f\"Batch {i} of {total_number_of_batches}\")\n                logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\n            logger.info(f'Total number of embeddings {len(embeddings)}')\n\n            if len(embeddings) != len(data):\n                raise ValueError(\"The number of embeddings is different from the number of values an error occured in OpenAI API during the embedding process\")\n            return embeddings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_embedding_size(self):\n    return ADA_EMBEDDING_SIZE\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef embed(self, data, verbose=False):\n    if isinstance(data, list) and len(data) > 1:\n        logger.info(\"Batch embedding\")\n        return self.batch_embed(data)\n    elif isinstance(data, list) and len(data) == 1:\n        logger.info(\"Serial embedding\")\n        data = data[0]\n\n    if isinstance(data, dict) and \"content\" in data:\n        if verbose:\n            logger.info(\"Embedding without mark\", data[\"content\"])\n        out = openai.Embedding.create(\n            input=data[\"content\"], engine=\"text-embedding-ada-002\"\n        )\n    else:\n        if len(TOKENIZER.encode(data)) > MAX_CONTEXT_LENGTH:\n            raise ValueError(f\" The input is too long for OpenAI, num tokens is {len(TOKENIZER.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\n        if verbose:\n            logger.info(f\"Embedding without preprocessing the input {data}\")\n        out = openai.Embedding.create(\n            input=str(data), engine=\"text-embedding-ada-002\"\n        )\n    return out.data[0].embedding\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef batch_embed(self, data: List[str], batch_size: int = 1000):\n    if isinstance(data, dict) and \"content\" in data:\n        raise ValueError(\"Batch embedding not supported for dictionaries\")\n    elif isinstance(data, str):\n        raise ValueError(\"Batch embedding not supported for strings use embed() instead\")\n    elif isinstance(data, list):\n        batch = []\n        embeddings = []\n        i = 1\n        total_number_of_batches = len(data)//batch_size + 1 if len(data) % batch_size > 0 else len(data)//batch_size\n        for value in data:\n            batch.append(value)\n            if len(batch) == batch_size:\n                start = time.time()\n                out = openai.Embedding.create(\n                    input=batch, engine=\"text-embedding-ada-002\"\n                )\n                for embedding in out.data:\n                    embeddings.append(embedding.embedding)\n                logger.info(f\"Batch {i} of {total_number_of_batches}\")\n                logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\n                i += 1\n                batch = []\n        if len(batch) > 0:\n            start = time.time()\n            out = openai.Embedding.create(\n                input=batch, engine=\"text-embedding-ada-002\"\n            )\n            for embedding in out.data:\n                embeddings.append(embedding.embedding)\n            logger.info(f\"Batch {i} of {total_number_of_batches}\")\n            logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\n        logger.info(f'Total number of embeddings {len(embeddings)}')\n\n        if len(embeddings) != len(data):\n            raise ValueError(\"The number of embeddings is different from the number of values an error occured in OpenAI API during the embedding process\")\n        return embeddings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n\ndef parse_and_embed_functions(input_str: str) -> List[np.ndarray]:\n    # Parse the input string with libcst\n    module = cst.parse_module(input_str)\n\n    # Find all the functions in the module and embed them separately\n    embeddings = []\n    for node in module.body:\n\n        if isinstance(node, cst.FunctionDef) or isinstance(node, cst.ClassDef):\n            func_str = cst.Module(body=[node]).code\n            print(\"Function string\", func_str)\n            embedding = openai.Embedding.create(\n                input=str(func_str)[:MAX_CONTEXT_LENGTH],\n                engine=\"text-embedding-ada-002\",\n            )\n            if embedding is not None:\n                embeddings.append(embedding.data[0].embedding)\n\n    avg_embedding = avg_embeddings(embeddings)\n    print(avg_embedding.shape)\n    return avg_embedding\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef avg_embeddings(embeddings: List[np.ndarray]) -> np.ndarray:\n    print(\"Embeddings len\", len(embeddings))\n    # convert embeddings to numpy array\n    embeddings = np.array(embeddings)\n    print(\"Embedding Matrix Shape\", embeddings.shape)\n    return np.array([np.sum(embeddings.T, axis=1)]).astype(np.float32)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass SBERTEmbedder:\n    def get_embedding_size(self):\n        return SBERT_EMBEDDING_SIZE\n\n    def embed(self,\n        data,\n        key=\"content\",\n        model_name=\"all-MiniLM-L6-v2\",\n        batch_size=128,\n    ):\n        \"\"\"\n        Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n        \"\"\"\n        print(\"Embedding data\")\n        model = SentenceTransformer(model_name)\n        print(\"Model loaded\")\n        if isinstance(data, dict):\n            sentences = data[key].tolist()\n            unique_sentences = data[key].unique()\n        elif isinstance(data, str):\n            #breal the string into sentences based on . or ? or !\n            sentences = re.split('[.!?]', data)\n            sentences = [s.strip() for s in sentences if s.strip()]  #\n            #filter empty sentences\n            sentences = list(filter(lambda x: len(x) > 0, sentences))\n            unique_sentences = list(set(sentences))\n        else:\n            raise ValueError(f\"Data must be a dictionary with attribute {key} or a string, but got {type(data)} instead\")\n        \n        print(\"Unique sentences\", len(unique_sentences))\n        self.unique_sentences = unique_sentences\n        for sentence in unique_sentences:\n            tokens = tokenizer.encode(sentence)\n            if len(tokens) > MAX_CONTEXT_LENGTH:\n                raise ValueError(f\" The input subsentence is too long for SBERT, num tokens is {len(tokens)}, instead of {MAX_CONTEXT_LENGTH}\")\n\n       \n        embeddings = model.encode(\n                unique_sentences, show_progress_bar=True, batch_size=batch_size\n            )\n\n        print(\"Embeddings computed\")\n\n        mapping = {\n            sentence: embedding\n            for sentence, embedding in zip(unique_sentences, embeddings)\n        }\n        embeddings = np.array([mapping[sentence] for sentence in sentences])\n\n        return np.mean(embeddings, axis=0).tolist()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def get_embedding_size(self):\n    return SBERT_EMBEDDING_SIZE\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef embed(self,\n    data,\n    key=\"content\",\n    model_name=\"all-MiniLM-L6-v2\",\n    batch_size=128,\n):\n    \"\"\"\n        Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n        \"\"\"\n    print(\"Embedding data\")\n    model = SentenceTransformer(model_name)\n    print(\"Model loaded\")\n    if isinstance(data, dict):\n        sentences = data[key].tolist()\n        unique_sentences = data[key].unique()\n    elif isinstance(data, str):\n        #breal the string into sentences based on . or ? or !\n        sentences = re.split('[.!?]', data)\n        sentences = [s.strip() for s in sentences if s.strip()]  #\n        #filter empty sentences\n        sentences = list(filter(lambda x: len(x) > 0, sentences))\n        unique_sentences = list(set(sentences))\n    else:\n        raise ValueError(f\"Data must be a dictionary with attribute {key} or a string, but got {type(data)} instead\")\n    \n    print(\"Unique sentences\", len(unique_sentences))\n    self.unique_sentences = unique_sentences\n    for sentence in unique_sentences:\n        tokens = tokenizer.encode(sentence)\n        if len(tokens) > MAX_CONTEXT_LENGTH:\n            raise ValueError(f\" The input subsentence is too long for SBERT, num tokens is {len(tokens)}, instead of {MAX_CONTEXT_LENGTH}\")\n\n       \n    embeddings = model.encode(\n            unique_sentences, show_progress_bar=True, batch_size=batch_size\n        )\n\n    print(\"Embeddings computed\")\n\n    mapping = {\n        sentence: embedding\n        for sentence, embedding in zip(unique_sentences, embeddings)\n    }\n    embeddings = np.array([mapping[sentence] for sentence in sentences])\n\n    return np.mean(embeddings, axis=0).tolist()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass CohereEmbedder:\n    def get_embedding_size(self):\n        return COHERE_EMBEDDING_SIZE\n\n    def embed(self, data, verbose=False):\n        if type(data) is dict and \"content\" in data:\n            if verbose is True:\n                print(\"Embedding from dictionary\", data[\"content\"])\n                response = co.embed(texts= data[\"content\"],model='multilingual-22-12')\n        else:\n            if len(tokenizer.encode(data)) > MAX_CONTEXT_LENGTH:\n                raise ValueError(f\" The input is too long for Cohere, num tokens is {len(tokenizer.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\n            if verbose is True:\n                print(\"Embedding without preprocessing the input\", data)\n            response = co.embed(texts=[str(data)],model='multilingual-22-12')\n        return response.embeddings[0]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def get_embedding_size(self):\n    return COHERE_EMBEDDING_SIZE\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef embed(self, data, verbose=False):\n    if type(data) is dict and \"content\" in data:\n        if verbose is True:\n            print(\"Embedding from dictionary\", data[\"content\"])\n            response = co.embed(texts= data[\"content\"],model='multilingual-22-12')\n    else:\n        if len(tokenizer.encode(data)) > MAX_CONTEXT_LENGTH:\n            raise ValueError(f\" The input is too long for Cohere, num tokens is {len(tokenizer.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\n        if verbose is True:\n            print(\"Embedding without preprocessing the input\", data)\n        response = co.embed(texts=[str(data)],model='multilingual-22-12')\n    return response.embeddings[0]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass PolarsGenerator:\n\n    def __init__(\n        self,\n        input_df: Union[pl.DataFrame,str] = 'noinput',\n        name: str = \"summarizer\",\n        tokenizer: Optional[Any] = None,\n        save_path: str = 'batch_generator',\n        logging_level: int = 10,\n    ) -> None:\n        #creave save path if it does not exist\n        if not os.path.exists(save_path):\n            os.makedirs(save_path)\n        if isinstance(input_df, pl.DataFrame):\n            self.load_path = f\"{save_path}/{name}.ndjson\" ## pyright: ignore\n            input_df.write_ndjson(self.load_path)\n        elif input_df == 'noinput':\n            raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n        elif isinstance(input_df, str):\n            self.load_path = input_df\n        else:\n            raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n        \n\n        # Settings\n\n        self.name = name\n        self.save_path =f\"{save_path}/{self.name}_output.ndjson\"\n        self.error_path = f\"{save_path}/{self.name}_errors.ndjson\"\n        self.log_path = f\"{save_path}/{self.name}_log.ndjson\"\n\n        self.logging_level = logging_level  \n        \n        if tokenizer is None:\n            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n        else:\n            self.tokenizer = tokenizer\n\n\n        logging.basicConfig(level=logging_level)\n        logging.debug(f\"Logging initialized at level {logging_level}\")\n\n\n        # Status Tracker - polars\n        self.st_model = StatusTrackerModel(name=name)\n        self.st: pl.DataFrame = pl.DataFrame(self.st_model.model_dump())\n        \n        # loads the frame in advance for usefull checks \n        self.frame = pl.read_ndjson(self.load_path)\n\n        # queues\n        self.requests_queue = asyncio.Queue()\n        self.retries_queue = asyncio.Queue()\n        self.errors_queue = asyncio.Queue()\n\n\n        # api authentication\n        self.api_key =  os.getenv(\"OPENAI_API_KEY\")\n        self.request_header = {\"Authorization\": f\"Bearer {self.api_key}\"}\n    \n        logging.debug(f\"Initialization complete.\")\n\n    def enqueue_objects(self):\n        id = 0\n        with open(self.load_path, 'r') as jsonl_file:\n            for line in jsonl_file:\n                id += 1\n                line = line.strip()\n                if not line:\n                    continue\n                json_obj = json.loads(line)\n                request = OpenaiRequest(**json_obj)\n                self.requests_queue.put_nowait((id,request))\n                self.len_queue = self.requests_queue.qsize()\n\n\n    async def process_objects(self):\n        while True:\n                    next_request = None                 \n                    if not self.retries_queue.empty():\n                            next_request = self.retries_queue.get_nowait()\n\n                            self.st.replace(\"num_tasks_started\", self.st['num_tasks_started']+1)\n                            logging.debug(f\"Retrying request: {next_request[0]}\")\n                            source_queue = self.retries_queue \n                    elif not self.requests_queue.empty():                       \n                            logging.debug(f\"Trying to retrieve next request\")\n                            next_request = self.requests_queue.get_nowait()\n\n                            self.st.replace(\"num_tasks_started\", self.st['num_tasks_started']+1)\n                            source_queue = self.requests_queue\n                            logging.info(f'Next request is {next_request[0]} of {self.len_queue}')\n                            logging.info(f'Respects token limit? {next_request[1].respect_token_limit}')\n                            if next_request[1].respect_token_limit:\n                                logging.debug(f\"Reading request: {next_request[0]}\")\n                            else:\n                                self.errors_queue.put_nowait(next_request)\n                    else:\n                        logging.info(\"Exiting the loop\")\n                        break\n\n                    current_time = time.time()\n                    seconds_since_update = current_time - self.st['last_update_time'][0]\n\n                    if next_request is not None:\n\n                        available_request_capacity = pl.Series([min([\n                            self.st['available_request_capacity'][0] + next_request[1].max_requests_per_minute * seconds_since_update / 60.0,\n                            next_request[1].max_requests_per_minute\n                        ])])\n                        self.st.replace(\"available_request_capacity\", available_request_capacity)\n                        logging.info(f\"Task number {next_request[0]} available_request_capacity: {self.st['available_request_capacity'][0]}\")\n\n\n                        available_token_capacity = pl.Series([min([\n                            self.st['available_token_capacity'][0] + next_request[1].max_tokens_per_minute * seconds_since_update / 60.0,\n                            next_request[1].max_tokens_per_minute\n                        ])])\n                        self.st.replace(\"available_token_capacity\", available_token_capacity)\n                        logging.info(f\"Task number {next_request[0]} available_token_capacity: {self.st['available_token_capacity'][0]}\")\n\n                        next_request_tokens = next_request[1].total_tokens\n\n\n                        if (\n                            self.st['available_request_capacity'][0] >= 1\n                            and self.st['available_token_capacity'][0] >= next_request_tokens\n                        ):\n                            # update counters\n                            self.st.replace(\"available_request_capacity\", available_request_capacity-1)\n                            self.st.replace(\"available_token_capacity\", available_token_capacity-next_request_tokens)\n                        \n\n                            # call API\n                            try:\n                                logging.info(f\"Calling Api for {next_request[0]}\")\n                                start_time = int(time.time())\n                                async with aiohttp.ClientSession() as session:\n                                    async with session.post(\n                                    url=next_request[1].url, headers=self.request_header, json=next_request[1].body\n                                    ) as response:\n                                        response = await response.json()\n                                if \"error\" in response:\n                                    logging.warning(\n                                        f\"Request {next_request[0]} failed with error {response['error']['message']}\"\n                                    )\n                                    self.st.replace(\"num_api_errors\", self.st['num_api_errors']+1)\n                                    if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n                                        self.st.replace(\"time_of_last_rate_limit_error\", pl.Series([time.time()]))\n                                        self.st.replace(\"num_rate_limit_errors\", self.st['num_rate_limit_errors']+1)\n                                        self.st.replace(\"num_api_errors\", self.st['num_api_errors']-1)\n\n                                        self.retries_queue.put_nowait(next_request)\n                                    else:\n                                        json_string = json.dumps(response)\n                                        with open(self.error_path, \"a\") as f:\n                                            f.write(json_string + \"\\n\") \n                                else:\n                                    output = ''\n                                    if next_request[1].request_type == 'chat':\n                                        output = {\n                                            'start_time': start_time,\n                                            'output': response['choices'][0]['message']['content'],\n                                            'prompt_tokens': response['usage']['prompt_tokens'],\n                                            'completion_tokens': response['usage']['completion_tokens'],\n                                            'total_tokens': response['usage']['total_tokens'],\n                                            'end_time': response['created']\n                                        }   \n                                    elif next_request[1].request_type == 'embedding':\n\n                                        output = {\n                                            'start_time': start_time,\n                                            'output': response['data'][0]['embedding'],\n                                            'prompt_tokens': response['usage']['prompt_tokens'],\n                                            'total_tokens': response['usage']['total_tokens'],\n                                            'end_time': time.time()\n                                        }\n\n                                    json_string = json.dumps(output)\n\n                                    with open(self.save_path, \"a\") as f:\n                                        f.write(json_string + \"\\n\")\n\n                                    total_tokens = response['usage']['total_tokens']\n                                    self.st.replace(\"available_token_capacity\", available_token_capacity+total_tokens)\n                                    self.st.replace(\"available_request_capacity\", available_request_capacity+1)\n                                                                      \n                            except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n                                logging.warning(f\"Request {next_request[0]} failed with Exception {e}\")\n                                self.st.replace(\"num_other_errors\", self.st['num_other_errors']+1)\n                                self.st.replace(\"available_token_capacity\", available_token_capacity+next_request_tokens)\n                                self.st.replace(\"available_request_capacity\", available_request_capacity+1)\n                                json_string = json.dumps(str(e))\n                                with open(self.error_path, \"a\") as f:\n                                    f.write(json_string + \"\\n\")\n\n                            finally:\n                                if source_queue is not None:\n                                    source_queue.task_done()\n\n\n                    \n                    await asyncio.sleep(self.st['seconds_to_sleep_each_loop'][0])\n\n                    # if a rate limit error was hit recently, pause to cool down\n                    seconds_since_rate_limit_error = (time.time() - self.st['time_of_last_rate_limit_error'][0])\n                    if seconds_since_rate_limit_error < self.st['seconds_to_pause_after_rate_limit_error'][0]:\n                        remaining_seconds_to_pause = (self.st['seconds_to_pause_after_rate_limit_error'][0] - seconds_since_rate_limit_error)\n                        await asyncio.sleep(remaining_seconds_to_pause)\n                        # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n                        logging.warn(f\"Pausing to cool down until {time.ctime(self.st['time_of_last_rate_limit_error'][0] + self.st['seconds_to_pause_after_rate_limit_error'][0])}\")\n\n\n\n        logging.info(f\"\"\"Parallel processing complete. Results saved to {self.save_path}\"\"\")\n        if self.st['num_tasks_failed'][0] > 0:\n            logging.warning(f\"{self.st['num_tasks_failed'][0]} / {self.st['num_tasks_started'][0]} requests failed. Errors logged to {self.log_path}.\")\n        if self.st['num_rate_limit_errors'][0] > 0:\n            logging.warning(f\"{self.st['num_rate_limit_errors'][0]} rate limit errors received. Consider running at a lower rate.\")\n        \n        self.st.write_ndjson(self.log_path)\n        print(self.st)\n\n                    \n\n\n\n    async def main(self):\n        logging.debug(f\"Entering main loop\")\n        self.enqueue_objects()\n        consumers = [asyncio.create_task(self.process_objects()) for _ in range(5)]\n        await self.requests_queue.join()\n        await self.retries_queue.join()\n        for consumer in consumers:\n            consumer.cancel()\n\n    def execute(self):\n        asyncio.run(self.main())\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(\n    self,\n    input_df: Union[pl.DataFrame,str] = 'noinput',\n    name: str = \"summarizer\",\n    tokenizer: Optional[Any] = None,\n    save_path: str = 'batch_generator',\n    logging_level: int = 10,\n) -> None:\n    #creave save path if it does not exist\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    if isinstance(input_df, pl.DataFrame):\n        self.load_path = f\"{save_path}/{name}.ndjson\" ## pyright: ignore\n        input_df.write_ndjson(self.load_path)\n    elif input_df == 'noinput':\n        raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n    elif isinstance(input_df, str):\n        self.load_path = input_df\n    else:\n        raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n    \n\n    # Settings\n\n    self.name = name\n    self.save_path =f\"{save_path}/{self.name}_output.ndjson\"\n    self.error_path = f\"{save_path}/{self.name}_errors.ndjson\"\n    self.log_path = f\"{save_path}/{self.name}_log.ndjson\"\n\n    self.logging_level = logging_level  \n    \n    if tokenizer is None:\n        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n    else:\n        self.tokenizer = tokenizer\n\n\n    logging.basicConfig(level=logging_level)\n    logging.debug(f\"Logging initialized at level {logging_level}\")\n\n\n    # Status Tracker - polars\n    self.st_model = StatusTrackerModel(name=name)\n    self.st: pl.DataFrame = pl.DataFrame(self.st_model.model_dump())\n    \n    # loads the frame in advance for usefull checks \n    self.frame = pl.read_ndjson(self.load_path)\n\n    # queues\n    self.requests_queue = asyncio.Queue()\n    self.retries_queue = asyncio.Queue()\n    self.errors_queue = asyncio.Queue()\n\n\n    # api authentication\n    self.api_key =  os.getenv(\"OPENAI_API_KEY\")\n    self.request_header = {\"Authorization\": f\"Bearer {self.api_key}\"}\n    \n    logging.debug(f\"Initialization complete.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef enqueue_objects(self):\n    id = 0\n    with open(self.load_path, 'r') as jsonl_file:\n        for line in jsonl_file:\n            id += 1\n            line = line.strip()\n            if not line:\n                continue\n            json_obj = json.loads(line)\n            request = OpenaiRequest(**json_obj)\n            self.requests_queue.put_nowait((id,request))\n            self.len_queue = self.requests_queue.qsize()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nasync def process_objects(self):\n    while True:\n                next_request = None                 \n                if not self.retries_queue.empty():\n                        next_request = self.retries_queue.get_nowait()\n\n                        self.st.replace(\"num_tasks_started\", self.st['num_tasks_started']+1)\n                        logging.debug(f\"Retrying request: {next_request[0]}\")\n                        source_queue = self.retries_queue \n                elif not self.requests_queue.empty():                       \n                        logging.debug(f\"Trying to retrieve next request\")\n                        next_request = self.requests_queue.get_nowait()\n\n                        self.st.replace(\"num_tasks_started\", self.st['num_tasks_started']+1)\n                        source_queue = self.requests_queue\n                        logging.info(f'Next request is {next_request[0]} of {self.len_queue}')\n                        logging.info(f'Respects token limit? {next_request[1].respect_token_limit}')\n                        if next_request[1].respect_token_limit:\n                            logging.debug(f\"Reading request: {next_request[0]}\")\n                        else:\n                            self.errors_queue.put_nowait(next_request)\n                else:\n                    logging.info(\"Exiting the loop\")\n                    break\n\n                current_time = time.time()\n                seconds_since_update = current_time - self.st['last_update_time'][0]\n\n                if next_request is not None:\n\n                    available_request_capacity = pl.Series([min([\n                        self.st['available_request_capacity'][0] + next_request[1].max_requests_per_minute * seconds_since_update / 60.0,\n                        next_request[1].max_requests_per_minute\n                    ])])\n                    self.st.replace(\"available_request_capacity\", available_request_capacity)\n                    logging.info(f\"Task number {next_request[0]} available_request_capacity: {self.st['available_request_capacity'][0]}\")\n\n\n                    available_token_capacity = pl.Series([min([\n                        self.st['available_token_capacity'][0] + next_request[1].max_tokens_per_minute * seconds_since_update / 60.0,\n                        next_request[1].max_tokens_per_minute\n                    ])])\n                    self.st.replace(\"available_token_capacity\", available_token_capacity)\n                    logging.info(f\"Task number {next_request[0]} available_token_capacity: {self.st['available_token_capacity'][0]}\")\n\n                    next_request_tokens = next_request[1].total_tokens\n\n\n                    if (\n                        self.st['available_request_capacity'][0] >= 1\n                        and self.st['available_token_capacity'][0] >= next_request_tokens\n                    ):\n                        # update counters\n                        self.st.replace(\"available_request_capacity\", available_request_capacity-1)\n                        self.st.replace(\"available_token_capacity\", available_token_capacity-next_request_tokens)\n                        \n\n                        # call API\n                        try:\n                            logging.info(f\"Calling Api for {next_request[0]}\")\n                            start_time = int(time.time())\n                            async with aiohttp.ClientSession() as session:\n                                async with session.post(\n                                url=next_request[1].url, headers=self.request_header, json=next_request[1].body\n                                ) as response:\n                                    response = await response.json()\n                            if \"error\" in response:\n                                logging.warning(\n                                    f\"Request {next_request[0]} failed with error {response['error']['message']}\"\n                                )\n                                self.st.replace(\"num_api_errors\", self.st['num_api_errors']+1)\n                                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n                                    self.st.replace(\"time_of_last_rate_limit_error\", pl.Series([time.time()]))\n                                    self.st.replace(\"num_rate_limit_errors\", self.st['num_rate_limit_errors']+1)\n                                    self.st.replace(\"num_api_errors\", self.st['num_api_errors']-1)\n\n                                    self.retries_queue.put_nowait(next_request)\n                                else:\n                                    json_string = json.dumps(response)\n                                    with open(self.error_path, \"a\") as f:\n                                        f.write(json_string + \"\\n\") \n                            else:\n                                output = ''\n                                if next_request[1].request_type == 'chat':\n                                    output = {\n                                        'start_time': start_time,\n                                        'output': response['choices'][0]['message']['content'],\n                                        'prompt_tokens': response['usage']['prompt_tokens'],\n                                        'completion_tokens': response['usage']['completion_tokens'],\n                                        'total_tokens': response['usage']['total_tokens'],\n                                        'end_time': response['created']\n                                    }   \n                                elif next_request[1].request_type == 'embedding':\n\n                                    output = {\n                                        'start_time': start_time,\n                                        'output': response['data'][0]['embedding'],\n                                        'prompt_tokens': response['usage']['prompt_tokens'],\n                                        'total_tokens': response['usage']['total_tokens'],\n                                        'end_time': time.time()\n                                    }\n\n                                json_string = json.dumps(output)\n\n                                with open(self.save_path, \"a\") as f:\n                                    f.write(json_string + \"\\n\")\n\n                                total_tokens = response['usage']['total_tokens']\n                                self.st.replace(\"available_token_capacity\", available_token_capacity+total_tokens)\n                                self.st.replace(\"available_request_capacity\", available_request_capacity+1)\n                                                                  \n                        except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n                            logging.warning(f\"Request {next_request[0]} failed with Exception {e}\")\n                            self.st.replace(\"num_other_errors\", self.st['num_other_errors']+1)\n                            self.st.replace(\"available_token_capacity\", available_token_capacity+next_request_tokens)\n                            self.st.replace(\"available_request_capacity\", available_request_capacity+1)\n                            json_string = json.dumps(str(e))\n                            with open(self.error_path, \"a\") as f:\n                                f.write(json_string + \"\\n\")\n\n                        finally:\n                            if source_queue is not None:\n                                source_queue.task_done()\n\n\n                \n                await asyncio.sleep(self.st['seconds_to_sleep_each_loop'][0])\n\n                # if a rate limit error was hit recently, pause to cool down\n                seconds_since_rate_limit_error = (time.time() - self.st['time_of_last_rate_limit_error'][0])\n                if seconds_since_rate_limit_error < self.st['seconds_to_pause_after_rate_limit_error'][0]:\n                    remaining_seconds_to_pause = (self.st['seconds_to_pause_after_rate_limit_error'][0] - seconds_since_rate_limit_error)\n                    await asyncio.sleep(remaining_seconds_to_pause)\n                    # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n                    logging.warn(f\"Pausing to cool down until {time.ctime(self.st['time_of_last_rate_limit_error'][0] + self.st['seconds_to_pause_after_rate_limit_error'][0])}\")\n\n\n\n    logging.info(f\"\"\"Parallel processing complete. Results saved to {self.save_path}\"\"\")\n    if self.st['num_tasks_failed'][0] > 0:\n        logging.warning(f\"{self.st['num_tasks_failed'][0]} / {self.st['num_tasks_started'][0]} requests failed. Errors logged to {self.log_path}.\")\n    if self.st['num_rate_limit_errors'][0] > 0:\n        logging.warning(f\"{self.st['num_rate_limit_errors'][0]} rate limit errors received. Consider running at a lower rate.\")\n    \n    self.st.write_ndjson(self.log_path)\n    print(self.st)\n\n                \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n\nasync def main(self):\n    logging.debug(f\"Entering main loop\")\n    self.enqueue_objects()\n    consumers = [asyncio.create_task(self.process_objects()) for _ in range(5)]\n    await self.requests_queue.join()\n    await self.retries_queue.join()\n    for consumer in consumers:\n        consumer.cancel()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef execute(self):\n    asyncio.run(self.main())\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass OpenaiRequest:\n    def __init__(\n        self, \n        **parameters\n        ) -> None:\n\n        if ('input' in parameters):  \n            self.request_type = 'embedding'\n        else:\n            self.request_type = 'chat'\n        \n        self.body = OpenaiRequestModel(\n                model=parameters.get('model','gpt-3.5-turbo'),\n                input=parameters.get('input'),\n                messages=parameters.get('messages', [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]) ,\n                function=parameters.get('function'),\n                function_call=parameters.get('function_call'),\n                temperature=parameters.get('temperature'),\n                top_p=parameters.get('top_p'),\n                n=parameters.get('n',1),\n                stream=parameters.get('stream'),\n                stop=parameters.get('stop'),\n                max_tokens=parameters.get('max_tokens',4000),\n                presence_penalty=parameters.get('presence_penalty'),\n                frequency_penalty=parameters.get('frequency_penalty'),\n                logit_bias=parameters.get('logit_bias'),\n                user=parameters.get('user')\n        )\n\n        self.body = {k: v for k, v in self.body.model_dump().items() if v is not None}\n        if self.request_type == 'embedding':\n            keys_to_remove = [\"n\", \"max_tokens\"]\n            self.body = {k: v for k, v in self.body.items() if k not in keys_to_remove}\n\n        \"\"\"Count the number of tokens in the request. Only supports chat and embedding requests.\"\"\"\n        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n        self.total_tokens = 0\n        self.respect_token_limit = False\n        self.max_tokens_per_minute = 0\n        self.max_requests_per_minute = 0\n        self.url = ''\n\n        # if completions request, tokens = prompt + n * max_tokens\n        if self.request_type == 'chat':\n            max_tokens = self.body['max_tokens']  \n            n = self.body['n'] # pyright: ignore\n            completion_tokens = n * max_tokens # pyright: ignore\n            for message in self.body[\"messages\"]: # pyright: ignore\n                self.total_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n                for key, value in message.items():\n                    self.total_tokens += len(encoding.encode(value))\n                    if key == \"name\":  # if there's a name, the role is omitted\n                        self.total_tokens -= 1  # role is always required and always 1 token\n                    self.total_tokens += 2  # every reply is primed with <im_start>assistant\n                self.total_tokens =  self.total_tokens + completion_tokens\n            \n        # if embeddings request, tokens = input tokens\n        elif self.request_type == \"embedding\":\n            if isinstance(self.body['input'], str):  # single input # pyright: ignore\n                self.total_tokens = len(encoding.encode(self.body[\"input\"]))  # pyright: ignore\n            elif isinstance(input, list):  # multiple inputs\n                self.total_tokens = sum([len(encoding.encode(i)) for i in self.body[\"input\"]]) # pyright: ignore\n            else:\n                raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n\n\n        match self.body['model']: # pyright: ignore\n            case 'gpt-3.5-turbo':\n                if self.total_tokens < 10000:\n                    self.respect_token_limit = True\n                    self.max_requests_per_minute = 3000\n                    self.max_tokens_per_minute= 1000000\n                    self.url = 'https://api.openai.com/v1/chat/completions'\n                else:\n                    self.respect_token_limit = False\n                    self.max_requests_per_minute = 0\n                    self.max_tokens_per_minute= 0\n\n            case 'gpt-4':\n                if self.total_tokens < 16000:\n                    self.respect_token_limit = True\n                    self.max_requests_per_minute = 3000\n                    self.max_tokens_per_minute= 1000000\n                    self.url = 'https://api.openai.com/v1/chat/completions'\n                else:\n                    self.respect_token_limit = False\n                    self.max_requests_per_minute = 0\n                    self.max_tokens_per_minute= 0\n\n\n            case 'text-embedding-ada-002':\n                if self.total_tokens < 10000:\n                    self.respect_token_limit = True\n                    self.max_tokens_per_minute = 1000000\n                    self.max_requests_per_minute = 3000\n                    self.url = 'https://api.openai.com/v1/embeddings'\n                else:\n                    self.respect_token_limit = False              \n                    self.max_requests_per_minute = 0\n                    self.max_tokens_per_minute= 0\n\n\n                \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self, \n    **parameters\n    ) -> None:\n\n    if ('input' in parameters):  \n        self.request_type = 'embedding'\n    else:\n        self.request_type = 'chat'\n    \n    self.body = OpenaiRequestModel(\n            model=parameters.get('model','gpt-3.5-turbo'),\n            input=parameters.get('input'),\n            messages=parameters.get('messages', [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]) ,\n            function=parameters.get('function'),\n            function_call=parameters.get('function_call'),\n            temperature=parameters.get('temperature'),\n            top_p=parameters.get('top_p'),\n            n=parameters.get('n',1),\n            stream=parameters.get('stream'),\n            stop=parameters.get('stop'),\n            max_tokens=parameters.get('max_tokens',4000),\n            presence_penalty=parameters.get('presence_penalty'),\n            frequency_penalty=parameters.get('frequency_penalty'),\n            logit_bias=parameters.get('logit_bias'),\n            user=parameters.get('user')\n    )\n\n    self.body = {k: v for k, v in self.body.model_dump().items() if v is not None}\n    if self.request_type == 'embedding':\n        keys_to_remove = [\"n\", \"max_tokens\"]\n        self.body = {k: v for k, v in self.body.items() if k not in keys_to_remove}\n\n    \"\"\"Count the number of tokens in the request. Only supports chat and embedding requests.\"\"\"\n    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n    self.total_tokens = 0\n    self.respect_token_limit = False\n    self.max_tokens_per_minute = 0\n    self.max_requests_per_minute = 0\n    self.url = ''\n\n    # if completions request, tokens = prompt + n * max_tokens\n    if self.request_type == 'chat':\n        max_tokens = self.body['max_tokens']  \n        n = self.body['n'] # pyright: ignore\n        completion_tokens = n * max_tokens # pyright: ignore\n        for message in self.body[\"messages\"]: # pyright: ignore\n            self.total_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n            for key, value in message.items():\n                self.total_tokens += len(encoding.encode(value))\n                if key == \"name\":  # if there's a name, the role is omitted\n                    self.total_tokens -= 1  # role is always required and always 1 token\n                self.total_tokens += 2  # every reply is primed with <im_start>assistant\n            self.total_tokens =  self.total_tokens + completion_tokens\n        \n    # if embeddings request, tokens = input tokens\n    elif self.request_type == \"embedding\":\n        if isinstance(self.body['input'], str):  # single input # pyright: ignore\n            self.total_tokens = len(encoding.encode(self.body[\"input\"]))  # pyright: ignore\n        elif isinstance(input, list):  # multiple inputs\n            self.total_tokens = sum([len(encoding.encode(i)) for i in self.body[\"input\"]]) # pyright: ignore\n        else:\n            raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n\n\n    match self.body['model']: # pyright: ignore\n        case 'gpt-3.5-turbo':\n            if self.total_tokens < 10000:\n                self.respect_token_limit = True\n                self.max_requests_per_minute = 3000\n                self.max_tokens_per_minute= 1000000\n                self.url = 'https://api.openai.com/v1/chat/completions'\n            else:\n                self.respect_token_limit = False\n                self.max_requests_per_minute = 0\n                self.max_tokens_per_minute= 0\n\n        case 'gpt-4':\n            if self.total_tokens < 16000:\n                self.respect_token_limit = True\n                self.max_requests_per_minute = 3000\n                self.max_tokens_per_minute= 1000000\n                self.url = 'https://api.openai.com/v1/chat/completions'\n            else:\n                self.respect_token_limit = False\n                self.max_requests_per_minute = 0\n                self.max_tokens_per_minute= 0\n\n\n        case 'text-embedding-ada-002':\n            if self.total_tokens < 10000:\n                self.respect_token_limit = True\n                self.max_tokens_per_minute = 1000000\n                self.max_requests_per_minute = 3000\n                self.url = 'https://api.openai.com/v1/embeddings'\n            else:\n                self.respect_token_limit = False              \n                self.max_requests_per_minute = 0\n                self.max_tokens_per_minute= 0\n\n\n            \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass BatchGenerator:\n\n    def __init__(\n        self,\n        input_df: Union[pl.DataFrame,str] = 'noinput',\n        task: str = 'chat',\n        name: str = \"summarizer\",\n        tokenizer: Optional[Any] = None,\n        save_path: str = 'batch_generator',\n        logging_level: int = 10,\n    ) -> None:\n\n        if isinstance(input_df, pl.DataFrame):\n            self.load_path = f\"{save_path}/{name}.ndjson\" ## pyright: ignore\n            input_df.write_ndjson(self.load_path)\n        elif input_df == 'noinput':\n            raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n        else:\n            self.load_path = input_df\n\n        self.name = name\n        self.task = task\n        self.save_path = save_path\n        self.logging_level = logging_level    \n\n        if tokenizer is None:\n            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n        else:\n            self.tokenizer = tokenizer\n\n        # debug loads the frame in advance for usefull checks \n        self.frame = pl.read_ndjson(self.load_path)\n\n        # queues\n        self.requests_queue = asyncio.Queue()\n        self.retries_queue = asyncio.Queue()\n        self.errors_queue = asyncio.Queue()\n\n        # constants\n        self.max_attempts = 5\n        self.seconds_to_pause_after_rate_limit_error = 15\n        self.seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n\n        # initialize logging\n        logging.basicConfig(level=logging_level)\n        logging.debug(f\"Logging initialized at level {logging_level}\")\n        \n        # initialize available capacity counts\n        self.available_request_capacity = 1500\n        self.available_token_capacity = 6250000\n        self.last_update_time = time.time()\n\n        # api authentication\n        self.api_key =  os.getenv(\"OPENAI_API_KEY\")\n        self.request_header = {\"Authorization\": f\"Bearer {self.api_key}\"}\n    \n        @dataclass\n        class StatusTracker:\n            num_tasks_started: int = 0\n            num_tasks_in_progress: int = 0  # script ends when this reaches 0\n            num_tasks_succeeded: int = 0\n            num_tasks_failed: int = 0\n            num_rate_limit_errors: int = 0\n            num_api_errors: int = 0  # excluding rate limit errors, counted above\n            num_other_errors: int = 0\n            time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n\n        self.status_tracker = StatusTracker()\n\n        self.finished = False  # after file is empty, we'll skip reading it\n        logging.debug(f\"Initialization complete.\")\n\n    def enqueue_objects(self):\n        with open(self.load_path, 'r') as jsonl_file:\n            for line in jsonl_file:\n                line = line.strip()\n                if not line:\n                    continue\n                json_obj = json.loads(line)                \n                self.requests_queue.put_nowait(json_obj)  \n\n\n    async def process_objects(self, queue):\n        next_request = None  # variable to hold the next request to call\n        while True:\n                    # get next request (if one is not already waiting for capacity)\n                    if not self.retries_queue.empty():\n                            next_request = self.retries_queue.get_nowait()\n                            logging.debug(f\"Retrying request: {next_request}\")\n                    elif not self.requests_queue.empty():\n                            try:\n                                logging.debug(f\"Trying to retrieve next request\")\n                                next_request = self.requests_queue.get_nowait()     \n                                logging.info(f'Next request is {next_request}')\n                                if next_request.respect_token_limit:\n                                    self.status_tracker.num_tasks_started += 1\n                                    self.status_tracker.num_tasks_in_progress += 1\n                                    logging.debug(f\"Reading request: {next_request}\")\n                                else:\n                                    self.errors_queue.put_nowait(next_request)\n                            except StopIteration:\n                                # if file runs out, set flag to stop reading it\n                                logging.debug(\"Read file exhausted\")\n                                self.finished = True\n\n                    # update available capacity\n                    current_time = time.time()\n                    seconds_since_update = current_time - self.last_update_time\n                    self.available_request_capacity = min(\n                    self.available_request_capacity + next_request.max_requests_per_minute * seconds_since_update / 60.0, ## pyright: ignore\n                        next_request.max_requests_per_minute, ## pyright: ignore\n                    )\n                    self.available_token_capacity = min(\n                        self.available_token_capacity + next_request.max_tokens_per_minute * seconds_since_update / 60.0, ## pyright: ignore\n                        next_request.max_tokens_per_minute,  ## pyright: ignore\n                    )\n                    self.last_update_time = current_time\n\n                    # if enough capacity available, call API\n                    if next_request:\n                        next_request_tokens = next_request.total_tokens\n                        if (\n                            self.available_request_capacity >= 1\n                            and self.available_token_capacity >= next_request_tokens\n                        ):\n                            # update counters\n                            self.available_request_capacity -= 1\n                            self.available_token_capacity -= next_request_tokens\n                            \n\n                            # call API\n                            try:\n                                logging.info(f\"Calling Api for {next_request.body}\")\n                                async with aiohttp.ClientSession() as session:\n                                    async with session.post(\n                                    url=next_request.url, headers=self.request_header, json=next_request.body\n                                    ) as response:\n                                        response = await response.json()\n                                if \"error\" in response:\n                                    logging.warning(\n                                        \"\"\" f\"Request {self.task_id} failed with error {response['error']}\" \"\"\"\n                                    )\n                                    self.status_tracker.num_api_errors += 1\n                                    if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n                                        self.status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \n                                        self.status_tracker.num_rate_limit_errors += 1\n                                        self.status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n                                        self.retries_queue.put_nowait(next_request)\n                                    else:\n                                        self.errors_queue.put_nowait(next_request)\n                                else:\n                                        print(response)\n                                        \n\n                            except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n                                logging.warning(f\"Request {next_request} failed with Exception {e}\")\n                                self.status_tracker.num_other_errors += 1\n                            queue.task_done() \n                            next_request = None  # reset next_request to empty\n\n                    # if all tasks are finished, break\n                    if self.status_tracker.num_tasks_in_progress == 0:\n                        break\n\n                    # main loop sleeps briefly so concurrent tasks can run\n                    await asyncio.sleep(self.seconds_to_sleep_each_loop)\n\n                    # if a rate limit error was hit recently, pause to cool down\n                    seconds_since_rate_limit_error = (time.time() - self.status_tracker.time_of_last_rate_limit_error)\n                    if seconds_since_rate_limit_error < self.seconds_to_pause_after_rate_limit_error:\n                        remaining_seconds_to_pause = (self.seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n                        await asyncio.sleep(remaining_seconds_to_pause)\n                        # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n                        logging.warn(f\"Pausing to cool down until {time.ctime(self.status_tracker.time_of_last_rate_limit_error + self.seconds_to_pause_after_rate_limit_error)}\")\n\n        # after finishing, log final status\n        logging.info(f\"\"\"Parallel processing complete. Results saved to {self.save_path}\"\"\")\n        if self.status_tracker.num_tasks_failed > 0:\n            logging.warning(f\"{self.status_tracker.num_tasks_failed} / {self.status_tracker.num_tasks_started} requests failed. Errors logged to {self.save_path}.\")\n        if self.status_tracker.num_rate_limit_errors > 0:\n            logging.warning(f\"{self.status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n\n\n\n    async def main(self):\n        logging.debug(f\"Entering main loop\")\n        self.enqueue_objects()\n        consumers = [asyncio.create_task(self.process_objects(self.requests_queue)) for _ in range(5)]\n        await self.requests_queue.join()\n        await self.retries_queue.join()\n        for consumer in consumers:\n            consumer.cancel()\n\n    def execute(self):\n        asyncio.run(self.main())\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef __init__(\n    self,\n    input_df: Union[pl.DataFrame,str] = 'noinput',\n    task: str = 'chat',\n    name: str = \"summarizer\",\n    tokenizer: Optional[Any] = None,\n    save_path: str = 'batch_generator',\n    logging_level: int = 10,\n) -> None:\n\n    if isinstance(input_df, pl.DataFrame):\n        self.load_path = f\"{save_path}/{name}.ndjson\" ## pyright: ignore\n        input_df.write_ndjson(self.load_path)\n    elif input_df == 'noinput':\n        raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n    else:\n        self.load_path = input_df\n\n    self.name = name\n    self.task = task\n    self.save_path = save_path\n    self.logging_level = logging_level    \n\n    if tokenizer is None:\n        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n    else:\n        self.tokenizer = tokenizer\n\n    # debug loads the frame in advance for usefull checks \n    self.frame = pl.read_ndjson(self.load_path)\n\n    # queues\n    self.requests_queue = asyncio.Queue()\n    self.retries_queue = asyncio.Queue()\n    self.errors_queue = asyncio.Queue()\n\n    # constants\n    self.max_attempts = 5\n    self.seconds_to_pause_after_rate_limit_error = 15\n    self.seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n\n    # initialize logging\n    logging.basicConfig(level=logging_level)\n    logging.debug(f\"Logging initialized at level {logging_level}\")\n    \n    # initialize available capacity counts\n    self.available_request_capacity = 1500\n    self.available_token_capacity = 6250000\n    self.last_update_time = time.time()\n\n    # api authentication\n    self.api_key =  os.getenv(\"OPENAI_API_KEY\")\n    self.request_header = {\"Authorization\": f\"Bearer {self.api_key}\"}\n    \n    @dataclass\n    class StatusTracker:\n        num_tasks_started: int = 0\n        num_tasks_in_progress: int = 0  # script ends when this reaches 0\n        num_tasks_succeeded: int = 0\n        num_tasks_failed: int = 0\n        num_rate_limit_errors: int = 0\n        num_api_errors: int = 0  # excluding rate limit errors, counted above\n        num_other_errors: int = 0\n        time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n\n    self.status_tracker = StatusTracker()\n\n    self.finished = False  # after file is empty, we'll skip reading it\n    logging.debug(f\"Initialization complete.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"    \n@dataclass\nclass StatusTracker:\n    num_tasks_started: int = 0\n    num_tasks_in_progress: int = 0  # script ends when this reaches 0\n    num_tasks_succeeded: int = 0\n    num_tasks_failed: int = 0\n    num_rate_limit_errors: int = 0\n    num_api_errors: int = 0  # excluding rate limit errors, counted above\n    num_other_errors: int = 0\n    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef enqueue_objects(self):\n    with open(self.load_path, 'r') as jsonl_file:\n        for line in jsonl_file:\n            line = line.strip()\n            if not line:\n                continue\n            json_obj = json.loads(line)                \n            self.requests_queue.put_nowait(json_obj)  \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nasync def process_objects(self, queue):\n    next_request = None  # variable to hold the next request to call\n    while True:\n                # get next request (if one is not already waiting for capacity)\n                if not self.retries_queue.empty():\n                        next_request = self.retries_queue.get_nowait()\n                        logging.debug(f\"Retrying request: {next_request}\")\n                elif not self.requests_queue.empty():\n                        try:\n                            logging.debug(f\"Trying to retrieve next request\")\n                            next_request = self.requests_queue.get_nowait()     \n                            logging.info(f'Next request is {next_request}')\n                            if next_request.respect_token_limit:\n                                self.status_tracker.num_tasks_started += 1\n                                self.status_tracker.num_tasks_in_progress += 1\n                                logging.debug(f\"Reading request: {next_request}\")\n                            else:\n                                self.errors_queue.put_nowait(next_request)\n                        except StopIteration:\n                            # if file runs out, set flag to stop reading it\n                            logging.debug(\"Read file exhausted\")\n                            self.finished = True\n\n                # update available capacity\n                current_time = time.time()\n                seconds_since_update = current_time - self.last_update_time\n                self.available_request_capacity = min(\n                self.available_request_capacity + next_request.max_requests_per_minute * seconds_since_update / 60.0, ## pyright: ignore\n                    next_request.max_requests_per_minute, ## pyright: ignore\n                )\n                self.available_token_capacity = min(\n                    self.available_token_capacity + next_request.max_tokens_per_minute * seconds_since_update / 60.0, ## pyright: ignore\n                    next_request.max_tokens_per_minute,  ## pyright: ignore\n                )\n                self.last_update_time = current_time\n\n                # if enough capacity available, call API\n                if next_request:\n                    next_request_tokens = next_request.total_tokens\n                    if (\n                        self.available_request_capacity >= 1\n                        and self.available_token_capacity >= next_request_tokens\n                    ):\n                        # update counters\n                        self.available_request_capacity -= 1\n                        self.available_token_capacity -= next_request_tokens\n                        \n\n                        # call API\n                        try:\n                            logging.info(f\"Calling Api for {next_request.body}\")\n                            async with aiohttp.ClientSession() as session:\n                                async with session.post(\n                                url=next_request.url, headers=self.request_header, json=next_request.body\n                                ) as response:\n                                    response = await response.json()\n                            if \"error\" in response:\n                                logging.warning(\n                                    \"\"\" f\"Request {self.task_id} failed with error {response['error']}\" \"\"\"\n                                )\n                                self.status_tracker.num_api_errors += 1\n                                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n                                    self.status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \n                                    self.status_tracker.num_rate_limit_errors += 1\n                                    self.status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n                                    self.retries_queue.put_nowait(next_request)\n                                else:\n                                    self.errors_queue.put_nowait(next_request)\n                            else:\n                                    print(response)\n                                    \n\n                        except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n                            logging.warning(f\"Request {next_request} failed with Exception {e}\")\n                            self.status_tracker.num_other_errors += 1\n                        queue.task_done() \n                        next_request = None  # reset next_request to empty\n\n                # if all tasks are finished, break\n                if self.status_tracker.num_tasks_in_progress == 0:\n                    break\n\n                # main loop sleeps briefly so concurrent tasks can run\n                await asyncio.sleep(self.seconds_to_sleep_each_loop)\n\n                # if a rate limit error was hit recently, pause to cool down\n                seconds_since_rate_limit_error = (time.time() - self.status_tracker.time_of_last_rate_limit_error)\n                if seconds_since_rate_limit_error < self.seconds_to_pause_after_rate_limit_error:\n                    remaining_seconds_to_pause = (self.seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n                    await asyncio.sleep(remaining_seconds_to_pause)\n                    # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n                    logging.warn(f\"Pausing to cool down until {time.ctime(self.status_tracker.time_of_last_rate_limit_error + self.seconds_to_pause_after_rate_limit_error)}\")\n\n    # after finishing, log final status\n    logging.info(f\"\"\"Parallel processing complete. Results saved to {self.save_path}\"\"\")\n    if self.status_tracker.num_tasks_failed > 0:\n        logging.warning(f\"{self.status_tracker.num_tasks_failed} / {self.status_tracker.num_tasks_started} requests failed. Errors logged to {self.save_path}.\")\n    if self.status_tracker.num_rate_limit_errors > 0:\n        logging.warning(f\"{self.status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n\nasync def main(self):\n    logging.debug(f\"Entering main loop\")\n    self.enqueue_objects()\n    consumers = [asyncio.create_task(self.process_objects(self.requests_queue)) for _ in range(5)]\n    await self.requests_queue.join()\n    await self.retries_queue.join()\n    for consumer in consumers:\n        consumer.cancel()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef execute(self):\n    asyncio.run(self.main())\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass ApiRequest:\n    def __init__(\n        self, \n        **parameters\n        ) -> None:\n\n        if ('input' in parameters) or ('input' in parameters.get('body', {})):  \n            self.request_type = 'embedding'\n        else:\n            self.request_type = 'chat'\n        \n\n        if parameters.get('body') is not None:\n            self.body = parameters.get('body')\n\n        else:\n            self.body = {\n                \"model\": parameters.get('model'),\n                \"input\": parameters.get('input'),\n                \"messages\": parameters.get('messages'),\n                \"function\": parameters.get('function'),\n                \"function_call\": parameters.get('function_call'),\n                \"temperature\": parameters.get('temperature'),\n                \"top_p\": parameters.get('top_p'),\n                \"n\": parameters.get('n'),\n                \"stream\": parameters.get('stream'),\n                \"stop\": parameters.get('stop'),\n                \"max_tokens\": parameters.get('max_tokens'),\n                \"presence_penalty\": parameters.get('presence_penalty'),\n                \"frequency_penalty\": parameters.get('frequency_penalty'),\n                \"logit_bias\": parameters.get('logit_bias'),\n                \"user\": parameters.get('user')\n            }\n\n        # Remove keys with None values\n        if self.body is not None:\n            self.body = {k: v for k, v in self.body.items() if v is not None}\n\n        \"\"\"Count the number of tokens in the request. Only supports chat and embedding requests.\"\"\"\n        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n\n        # if completions request, tokens = prompt + n * max_tokens\n        if self.request_type == 'chat':\n            max_tokens = self.body['max_tokens'] # pyright: ignore\n            n = self.body['n'] # pyright: ignore\n            completion_tokens = n * max_tokens # pyright: ignore\n            for message in self.body[\"messages\"]: # pyright: ignore\n                self.total_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n                for key, value in message.items():\n                    self.total_tokens += len(encoding.encode(value))\n                    if key == \"name\":  # if there's a name, the role is omitted\n                        self.total_tokens -= 1  # role is always required and always 1 token\n                    self.total_tokens += 2  # every reply is primed with <im_start>assistant\n                self.total_tokens =  self.total_tokens + completion_tokens\n            \n        # if embeddings request, tokens = input tokens\n        elif self.request_type == \"embedding\":\n            if isinstance(self.body['input'], str):  # single input # pyright: ignore\n                self.total_tokens = len(encoding.encode(self.body[\"input\"]))  # pyright: ignore\n            elif isinstance(input, list):  # multiple inputs\n                self.total_tokens = sum([len(encoding.encode(i)) for i in self.body[\"input\"]]) # pyright: ignore\n            else:\n                raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n\n        match self.body['model']: # pyright: ignore\n            case 'gpt-3.5-turbo':\n                if self.total_tokens < 4000:\n                    self.respect_token_limit = True\n                    self.max_requests_per_minute = 3000\n                    self.max_tokens_per_minute= 1000000\n                    self.url = 'https://api.openai.com/v1/chat/completions'\n                else:\n                    self.respect_token_limit = False\n            case 'gpt-4':\n                if self.total_tokens < 16000:\n                    self.respect_token_limit = True\n                    self.max_requests_per_minute = 3000\n                    self.max_tokens_per_minute= 1000000\n                    self.url = 'https://api.openai.com/v1/chat/completions'\n                else:\n                    self.respect_token_limit = False\n\n            case 'ext-embedding-ada-002':\n                if self.total_tokens < 4000:\n                    self.respect_token_limit = True\n                    self.max_tokens_per_minute = 1000000\n                    self.max_requests_per_minute = 3000\n                    self.url = 'https://api.openai.com/v1/embeddings'\n                else:\n                    self.respect_token_limit = False                \n\n                \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self, \n    **parameters\n    ) -> None:\n\n    if ('input' in parameters) or ('input' in parameters.get('body', {})):  \n        self.request_type = 'embedding'\n    else:\n        self.request_type = 'chat'\n    \n\n    if parameters.get('body') is not None:\n        self.body = parameters.get('body')\n\n    else:\n        self.body = {\n            \"model\": parameters.get('model'),\n            \"input\": parameters.get('input'),\n            \"messages\": parameters.get('messages'),\n            \"function\": parameters.get('function'),\n            \"function_call\": parameters.get('function_call'),\n            \"temperature\": parameters.get('temperature'),\n            \"top_p\": parameters.get('top_p'),\n            \"n\": parameters.get('n'),\n            \"stream\": parameters.get('stream'),\n            \"stop\": parameters.get('stop'),\n            \"max_tokens\": parameters.get('max_tokens'),\n            \"presence_penalty\": parameters.get('presence_penalty'),\n            \"frequency_penalty\": parameters.get('frequency_penalty'),\n            \"logit_bias\": parameters.get('logit_bias'),\n            \"user\": parameters.get('user')\n        }\n\n    # Remove keys with None values\n    if self.body is not None:\n        self.body = {k: v for k, v in self.body.items() if v is not None}\n\n    \"\"\"Count the number of tokens in the request. Only supports chat and embedding requests.\"\"\"\n    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n\n    # if completions request, tokens = prompt + n * max_tokens\n    if self.request_type == 'chat':\n        max_tokens = self.body['max_tokens'] # pyright: ignore\n        n = self.body['n'] # pyright: ignore\n        completion_tokens = n * max_tokens # pyright: ignore\n        for message in self.body[\"messages\"]: # pyright: ignore\n            self.total_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n            for key, value in message.items():\n                self.total_tokens += len(encoding.encode(value))\n                if key == \"name\":  # if there's a name, the role is omitted\n                    self.total_tokens -= 1  # role is always required and always 1 token\n                self.total_tokens += 2  # every reply is primed with <im_start>assistant\n            self.total_tokens =  self.total_tokens + completion_tokens\n        \n    # if embeddings request, tokens = input tokens\n    elif self.request_type == \"embedding\":\n        if isinstance(self.body['input'], str):  # single input # pyright: ignore\n            self.total_tokens = len(encoding.encode(self.body[\"input\"]))  # pyright: ignore\n        elif isinstance(input, list):  # multiple inputs\n            self.total_tokens = sum([len(encoding.encode(i)) for i in self.body[\"input\"]]) # pyright: ignore\n        else:\n            raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n\n    match self.body['model']: # pyright: ignore\n        case 'gpt-3.5-turbo':\n            if self.total_tokens < 4000:\n                self.respect_token_limit = True\n                self.max_requests_per_minute = 3000\n                self.max_tokens_per_minute= 1000000\n                self.url = 'https://api.openai.com/v1/chat/completions'\n            else:\n                self.respect_token_limit = False\n        case 'gpt-4':\n            if self.total_tokens < 16000:\n                self.respect_token_limit = True\n                self.max_requests_per_minute = 3000\n                self.max_tokens_per_minute= 1000000\n                self.url = 'https://api.openai.com/v1/chat/completions'\n            else:\n                self.respect_token_limit = False\n\n        case 'ext-embedding-ada-002':\n            if self.total_tokens < 4000:\n                self.respect_token_limit = True\n                self.max_tokens_per_minute = 1000000\n                self.max_requests_per_minute = 3000\n                self.url = 'https://api.openai.com/v1/embeddings'\n            else:\n                self.respect_token_limit = False                \n\n            \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef chatgpt_response(\n     \n    prompt: List[Dict[str, Union[str, Dict[str, str]]]], \n    model: str = \"gpt-3.5-turbo\", \n    max_tokens: int = 1000, \n    stream: bool = False\n) -> Union[Generator,Tuple]:\n\n    try:\n        print(\"Trying to call OpenAI API...\")\n        response = openai.ChatCompletion.create(\n            model=model,\n            messages=prompt,\n            max_tokens=max_tokens,\n            stream=stream\n        )\n        return response, True\n\n    except openai.APIError as e:\n        logging.error(f\"Unexpected error in openai call: {e}\") \n        fail_response = {\n            \"choices\": [\n                {\n                    \"message\": {\n                        \"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"\n                    }\n                }\n            ]\n        }\n        return fail_response, False\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass StatusTrackerModel(BaseModel):\n    name: str\n    max_attempts: int = 5\n    seconds_to_pause_after_rate_limit_error: int = 15\n    seconds_to_sleep_each_loop: float  = 0.001\n    available_request_capacity: int = 1500\n    available_token_capacity: int = 625000\n    last_update_time: float = now()\n    num_rate_limit_errors: int = 0\n    time_of_last_rate_limit_error: float = 0.0\n    num_tasks_failed: int = 0\n    num_tasks_started: int = 0\n    num_api_errors: int = 0\n    num_other_errors: int = 0 \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n\nclass OpenaiRequestModel(BaseModel):\n    model: str\n    input: Union[str,List,None]\n    messages: Union[List,None]\n    function: Union[List,None]\n    function_call: Union[str,Any,None]\n    temperature: Union[float,None]\n    top_p: Union[float,None]\n    n: Union[int,None]\n    stream: Union[bool,None]\n    stop: Union[str,List,None]\n    max_tokens: Union[int,None]\n    presence_penalty: Union[float,None]\n    frequency_penalty: Union[float,None]\n    logit_bias: Union[Mapping,None]\n    user: Union[str,None]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n\ndef cohere_response(prompt: List[dict],model: str = \"command\", max_tokens: int = 1000\n    ) -> Tuple[Dict, bool]:\n    \"\"\"\n        Call the Cohere API with the given prompt and maximum number of output tokens.\n        \n\n        :param prompt: A list of strings representing the prompt to send to the API.\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\n        :param model: A string representing the model to use. either command or command-nightly\n        :return: A tuple containing the API response cohere object and a boolean indicating success.\n        \"\"\"             \n    try:\n        prompt= convert_mark_to_str_prompt(prompt)\n        print(\"Trying to call Cohere API... using model:\", model)\n        response = co.generate(\n        model= model,\n        prompt= prompt,\n        max_tokens=max_tokens,\n        #end_sequences=['#SYSTEM:', '#USER:'],\n        )\n        return response, True\n\n    except:\n          return None, False\n    \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef cohere_summarize(prompt: str, model: str = \"summarize-xlarge\", length: str = \"medium\", extractiveness: str = \"medium\", format: str = \"bullets\", additional_command: str = None) -> str:\n    response = co.summarize( \n    text=prompt,model=model, \n    length=length,\n    extractiveness=extractiveness,\n    format=format,\n    additional_command=additional_command\n    )\n\n    summary = response.summary\n    return summary\n              \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass OsProcessor:\n    def __init__(self, directory_path: str):\n        self.directory_path = directory_path\n\n    def get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\n        \"\"\"Returns a list of all files in a directory\"\"\"\n        if directory_path is None:\n            directory_path = self.directory_path\n\n        all_files = []\n        for root, _, files in os.walk(directory_path):\n            for file in files:\n                all_files.append(os.path.join(root, file))\n\n        return all_files\n\n    def get_files_with_extension(\n        self, extension: str, directory_path: Optional[str] = None\n    ) -> List[str]:\n        \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\n        if directory_path is None:\n            directory_path = self.directory_path\n\n        all_files = self.get_all_files(directory_path)\n        files_with_extension = [file for file in all_files if file.endswith(extension)]\n\n        return files_with_extension\n\n    def get_file_extension(self, file_path: str) -> str:\n        \"\"\"Returns the extension of a file\"\"\"\n        return Path(file_path).suffix\n\n    def get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\n        \"\"\"Returns a list of all subdirectories in a directory\"\"\"\n        if directory_path is None:\n            directory_path = self.directory_path\n\n        subdirectories = [\n            os.path.join(directory_path, d)\n            for d in os.listdir(directory_path)\n            if os.path.isdir(os.path.join(directory_path, d))\n        ]\n\n        return subdirectories\n\n    def create_directory(self, directory_path: str) -> None:\n        \"\"\"Creates a directory if it does not exist\"\"\"\n        if not os.path.exists(directory_path):\n            os.makedirs(directory_path)\n\n    def delete_directory(self, directory_path: str) -> None:\n        \"\"\"Deletes a directory if it exists\"\"\"\n        if os.path.exists(directory_path):\n            shutil.rmtree(directory_path)\n\n    def copy_file(self, source_path: str, destination_path: str) -> None:\n        \"\"\"Copies a file from one location to another\"\"\"\n        shutil.copy2(source_path, destination_path)\n\n    def move_file(self, source_path: str, destination_path: str) -> None:\n        \"\"\"Moves a file from one location to another\"\"\"\n        shutil.move(source_path, destination_path)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, directory_path: str):\n    self.directory_path = directory_path\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\n    \"\"\"Returns a list of all files in a directory\"\"\"\n    if directory_path is None:\n        directory_path = self.directory_path\n\n    all_files = []\n    for root, _, files in os.walk(directory_path):\n        for file in files:\n            all_files.append(os.path.join(root, file))\n\n    return all_files\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_files_with_extension(\n    self, extension: str, directory_path: Optional[str] = None\n) -> List[str]:\n    \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\n    if directory_path is None:\n        directory_path = self.directory_path\n\n    all_files = self.get_all_files(directory_path)\n    files_with_extension = [file for file in all_files if file.endswith(extension)]\n\n    return files_with_extension\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_file_extension(self, file_path: str) -> str:\n    \"\"\"Returns the extension of a file\"\"\"\n    return Path(file_path).suffix\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\n    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\n    if directory_path is None:\n        directory_path = self.directory_path\n\n    subdirectories = [\n        os.path.join(directory_path, d)\n        for d in os.listdir(directory_path)\n        if os.path.isdir(os.path.join(directory_path, d))\n    ]\n\n    return subdirectories\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef create_directory(self, directory_path: str) -> None:\n    \"\"\"Creates a directory if it does not exist\"\"\"\n    if not os.path.exists(directory_path):\n        os.makedirs(directory_path)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef delete_directory(self, directory_path: str) -> None:\n    \"\"\"Deletes a directory if it exists\"\"\"\n    if os.path.exists(directory_path):\n        shutil.rmtree(directory_path)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef copy_file(self, source_path: str, destination_path: str) -> None:\n    \"\"\"Copies a file from one location to another\"\"\"\n    shutil.copy2(source_path, destination_path)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef move_file(self, source_path: str, destination_path: str) -> None:\n    \"\"\"Moves a file from one location to another\"\"\"\n    shutil.move(source_path, destination_path)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass GithubProcessor(OsProcessor):\n    def __init__(\n        self,\n        base_directory: str,\n        username=None,\n        repo_name=None,\n        code_parsers=None,\n        minify_code: bool = False,\n        remove_docstrings: bool = False,\n    ):\n        self.username = username\n        self.repo_name = repo_name\n        self.base_directory = base_directory\n        self.github = Github()\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\n        repo_path = self.clone_repo(self.repo.clone_url)\n\n        OsProcessor.__init__(self, repo_path)\n        self.code_parsers = code_parsers or [\n            PythonParser(\n                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\n            )\n        ]\n\n    def get_public_repos(self):\n        \"\"\"Returns a list of all public repos for the user.\"\"\"\n        user = self.github.get_user(self.username)\n        return user.get_repos()\n\n    def clone_repo(self, repo_url: str):\n        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n        target_directory = os.path.join(self.base_directory, repo_name)\n\n        if os.path.exists(target_directory):\n            shutil.rmtree(target_directory)\n\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n\n        return target_directory\n\n    def process_repo(self, repo_path=None):\n        \"\"\"Processes the repo at the specified path.\n        If no path is specified, the repo at self.directory_path is processed.\n        Returns the list of parsed functions and classes.\"\"\"\n        if repo_path is None:\n            repo_path = self.directory_path\n\n        for code_parser in self.code_parsers:\n            code_parser.directory_path = repo_path\n            code_parser.process_directory(repo_path)\n\n    def process_repos(self):\n        \"\"\"Processes all public repos for the user.\"\"\"\n        for repo in self.get_public_repos():\n            if not repo.private:\n                print(f\"Processing repo: {repo.name}\")\n                repo_path = self.clone_repo(repo.clone_url)\n                self.process_repo(repo_path)\n                shutil.rmtree(repo_path)\n\n    def get_repo(self, repo_name):\n        \"\"\"Returns the repo with the specified name.\"\"\"\n        user = self.github.get_user(self.username)\n        return user.get_repo(repo_name)\n\n    def process_single_repo(self):\n\n        repo = self.get_repo(self.repo_name)\n        print(f\"Processing repo: {self.repo_name}\")\n        repo_path = self.clone_repo(repo.clone_url)\n        self.process_repo(repo_path)\n        shutil.rmtree(repo_path)\n\n    def get_issues(self, state=\"open\"):\n        \"\"\"\n        Returns a list of all issues in the repo with the specified state.\n        \"\"\"\n        issues = []\n        for issue in self.repo.get_issues(state=state):\n            issues.append(issue)\n        return issues\n\n    def parse_issues(self, state=\"open\"):\n        \"\"\"\n        Parses all issues in the repo with the specified state and returns a list of dicts.\n        Each dict contains the issue number, title, body, and labels.\n        \"\"\"\n        parsed_issues = []\n        issues = self.get_issues(state=state)\n        for issue in issues:\n            parsed_issue = {\n                \"number\": issue.number,\n                \"title\": issue.title,\n                \"body\": issue.body,\n                \"labels\": [label.name for label in issue.labels],\n            }\n            parsed_issues.append(parsed_issue)\n        return parsed_issues\n\n    def get_commits(self):\n        \"\"\"\n        Returns a list of all commits in the main branch of the repository.\n        \"\"\"\n        commits = []\n        branch = self.repo.get_branch(\"main\")\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\n            commits.append(commit)\n        return commits\n\n    def parse_commits(self):\n        \"\"\"\n        Parses all commits in the main branch of the repository and returns a list of dicts.\n        Each dict contains the commit sha, commit message, and author information.\n        \"\"\"\n        parsed_commits = []\n        commits = self.get_commits()\n        for commit in commits:\n            parsed_commit = {\n                \"sha\": commit.sha,\n                \"message\": commit.commit.message,\n                \"author\": {\n                    \"name\": commit.commit.author.name,\n                    \"email\": commit.commit.author.email,\n                    \"date\": commit.commit.author.date,\n                },\n            }\n            parsed_commits.append(parsed_commit)\n        return parsed_commits\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    base_directory: str,\n    username=None,\n    repo_name=None,\n    code_parsers=None,\n    minify_code: bool = False,\n    remove_docstrings: bool = False,\n):\n    self.username = username\n    self.repo_name = repo_name\n    self.base_directory = base_directory\n    self.github = Github()\n    self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\n    repo_path = self.clone_repo(self.repo.clone_url)\n\n    OsProcessor.__init__(self, repo_path)\n    self.code_parsers = code_parsers or [\n        PythonParser(\n            repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\n        )\n    ]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_public_repos(self):\n    \"\"\"Returns a list of all public repos for the user.\"\"\"\n    user = self.github.get_user(self.username)\n    return user.get_repos()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef clone_repo(self, repo_url: str):\n    \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\n    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n    target_directory = os.path.join(self.base_directory, repo_name)\n\n    if os.path.exists(target_directory):\n        shutil.rmtree(target_directory)\n\n    subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n\n    return target_directory\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef process_repo(self, repo_path=None):\n    \"\"\"Processes the repo at the specified path.\n        If no path is specified, the repo at self.directory_path is processed.\n        Returns the list of parsed functions and classes.\"\"\"\n    if repo_path is None:\n        repo_path = self.directory_path\n\n    for code_parser in self.code_parsers:\n        code_parser.directory_path = repo_path\n        code_parser.process_directory(repo_path)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef process_repos(self):\n    \"\"\"Processes all public repos for the user.\"\"\"\n    for repo in self.get_public_repos():\n        if not repo.private:\n            print(f\"Processing repo: {repo.name}\")\n            repo_path = self.clone_repo(repo.clone_url)\n            self.process_repo(repo_path)\n            shutil.rmtree(repo_path)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_repo(self, repo_name):\n    \"\"\"Returns the repo with the specified name.\"\"\"\n    user = self.github.get_user(self.username)\n    return user.get_repo(repo_name)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef process_single_repo(self):\n\n    repo = self.get_repo(self.repo_name)\n    print(f\"Processing repo: {self.repo_name}\")\n    repo_path = self.clone_repo(repo.clone_url)\n    self.process_repo(repo_path)\n    shutil.rmtree(repo_path)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_issues(self, state=\"open\"):\n    \"\"\"\n        Returns a list of all issues in the repo with the specified state.\n        \"\"\"\n    issues = []\n    for issue in self.repo.get_issues(state=state):\n        issues.append(issue)\n    return issues\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef parse_issues(self, state=\"open\"):\n    \"\"\"\n        Parses all issues in the repo with the specified state and returns a list of dicts.\n        Each dict contains the issue number, title, body, and labels.\n        \"\"\"\n    parsed_issues = []\n    issues = self.get_issues(state=state)\n    for issue in issues:\n        parsed_issue = {\n            \"number\": issue.number,\n            \"title\": issue.title,\n            \"body\": issue.body,\n            \"labels\": [label.name for label in issue.labels],\n        }\n        parsed_issues.append(parsed_issue)\n    return parsed_issues\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_commits(self):\n    \"\"\"\n        Returns a list of all commits in the main branch of the repository.\n        \"\"\"\n    commits = []\n    branch = self.repo.get_branch(\"main\")\n    for commit in self.repo.get_commits(sha=branch.commit.sha):\n        commits.append(commit)\n    return commits\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef parse_commits(self):\n    \"\"\"\n        Parses all commits in the main branch of the repository and returns a list of dicts.\n        Each dict contains the commit sha, commit message, and author information.\n        \"\"\"\n    parsed_commits = []\n    commits = self.get_commits()\n    for commit in commits:\n        parsed_commit = {\n            \"sha\": commit.sha,\n            \"message\": commit.commit.message,\n            \"author\": {\n                \"name\": commit.commit.author.name,\n                \"email\": commit.commit.author.email,\n                \"date\": commit.commit.author.date,\n            },\n        }\n        parsed_commits.append(parsed_commit)\n    return parsed_commits\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\n# A custom visitor to find function calls and their arguments\nclass FunctionCallFinder(cst.CSTVisitor):\n    METADATA_DEPENDENCIES = (PositionProvider,)\n\n    def visit_Call(self, node: cst.Call) -> None:\n        function_name = None\n        if isinstance(node.func, cst.Name):\n            function_name = node.func.value\n\n        if function_name:\n            pos = self.get_metadata(PositionProvider, node).start\n            print(\n                f\"Function '{function_name}' called at line {pos.line}, column {pos.column} with arguments:\"\n            )\n\n            for arg in node.args:\n                arg_start_pos = self.get_metadata(PositionProvider, arg).start\n                arg_value = arg.value\n                if isinstance(arg_value, cst.SimpleString):\n                    arg_value = arg_value.evaluated_value\n                print(\n                    f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\n                )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Call(self, node: cst.Call) -> None:\n    function_name = None\n    if isinstance(node.func, cst.Name):\n        function_name = node.func.value\n\n    if function_name:\n        pos = self.get_metadata(PositionProvider, node).start\n        print(\n            f\"Function '{function_name}' called at line {pos.line}, column {pos.column} with arguments:\"\n        )\n\n        for arg in node.args:\n            arg_start_pos = self.get_metadata(PositionProvider, arg).start\n            arg_value = arg.value\n            if isinstance(arg_value, cst.SimpleString):\n                arg_value = arg_value.evaluated_value\n            print(\n                f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\n            )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass MultiplicationCounterVisitor(cst.CSTVisitor):\n    def __init__(self):\n        self.count = 0\n        self.functions_with_operation_dict = {}\n\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n        self.current_function = node\n        self.functions_with_operation_dict[node.name] = []\n\n    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n        self.current_function = None\n\n    def visit_BinaryOperation(self, node: cst.BinaryOperation) -> None:\n        if isinstance(node.operator, cst.Multiply) or isinstance(\n            node.operator, cst.BitAnd\n        ):\n            self.count += 1\n            if self.current_function:\n                self.functions_with_operation_dict[self.current_function.name].append(\n                    cst.Module([]).code_for_node(node)\n                )\n\n    def visit_Call(self, node: cst.Call) -> None:\n        if m.matches(node, m.Call(func=m.Attribute(attr=m.Name(\"dot\")))) or m.matches(\n            node, m.Call(func=m.Name(\"dot\"), args=[m.Arg(), m.Arg()])\n        ):\n            self.count += 1\n            if self.current_function:\n                self.functions_with_operation_dict[self.current_function.name].append(\n                    cst.Module([]).code_for_node(node)\n                )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self):\n    self.count = 0\n    self.functions_with_operation_dict = {}\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n    self.current_function = node\n    self.functions_with_operation_dict[node.name] = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n    self.current_function = None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_BinaryOperation(self, node: cst.BinaryOperation) -> None:\n    if isinstance(node.operator, cst.Multiply) or isinstance(\n        node.operator, cst.BitAnd\n    ):\n        self.count += 1\n        if self.current_function:\n            self.functions_with_operation_dict[self.current_function.name].append(\n                cst.Module([]).code_for_node(node)\n            )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_Call(self, node: cst.Call) -> None:\n    if m.matches(node, m.Call(func=m.Attribute(attr=m.Name(\"dot\")))) or m.matches(\n        node, m.Call(func=m.Name(\"dot\"), args=[m.Arg(), m.Arg()])\n    ):\n        self.count += 1\n        if self.current_function:\n            self.functions_with_operation_dict[self.current_function.name].append(\n                cst.Module([]).code_for_node(node)\n            )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass FunctionAndClassVisitor(cst.CSTVisitor):\n    def __init__(self):\n        self.function_source_codes = []\n        self.function_nodes = []\n        self.class_source_codes = []\n        self.class_nodes = []\n\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n        function_source_code = cst.Module([]).code_for_node(node)\n        # add in place summary and code mod\n        self.function_nodes.append(node)\n        self.function_source_codes.append(function_source_code)\n\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\n        class_source_code = cst.Module([]).code_for_node(node)\n        # add in place summary and code mod\n        self.class_nodes.append(node)\n        self.class_source_codes.append(class_source_code)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self):\n    self.function_source_codes = []\n    self.function_nodes = []\n    self.class_source_codes = []\n    self.class_nodes = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n    function_source_code = cst.Module([]).code_for_node(node)\n    # add in place summary and code mod\n    self.function_nodes.append(node)\n    self.function_source_codes.append(function_source_code)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ClassDef(self, node: cst.ClassDef) -> None:\n    class_source_code = cst.Module([]).code_for_node(node)\n    # add in place summary and code mod\n    self.class_nodes.append(node)\n    self.class_source_codes.append(class_source_code)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass TypingCollector(cst.CSTVisitor):\n    def __init__(self):\n        # stack for storing the canonical name of the current function\n        self.stack: List[Tuple[str, ...]] = []\n        # store the annotations\n        self.annotations: Dict[\n            Tuple[str, ...],  # key: tuple of canonical class/function name\n            Tuple[cst.Parameters, Optional[cst.Annotation]],  # value: (params, returns)\n        ] = {}\n\n    def visit_ClassDef(self, node: cst.ClassDef) -> Optional[bool]:\n        self.stack.append(node.name.value)\n\n    def leave_ClassDef(self, node: cst.ClassDef) -> None:\n        self.stack.pop()\n\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> Optional[bool]:\n        self.stack.append(node.name.value)\n        self.annotations[tuple(self.stack)] = (node.params, node.returns)\n        return False  # pyi files don't support inner functions, return False to stop the traversal.\n\n    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n        self.stack.pop()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self):\n    # stack for storing the canonical name of the current function\n    self.stack: List[Tuple[str, ...]] = []\n    # store the annotations\n    self.annotations: Dict[\n        Tuple[str, ...],  # key: tuple of canonical class/function name\n        Tuple[cst.Parameters, Optional[cst.Annotation]],  # value: (params, returns)\n    ] = {}\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ClassDef(self, node: cst.ClassDef) -> Optional[bool]:\n    self.stack.append(node.name.value)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef leave_ClassDef(self, node: cst.ClassDef) -> None:\n    self.stack.pop()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> Optional[bool]:\n    self.stack.append(node.name.value)\n    self.annotations[tuple(self.stack)] = (node.params, node.returns)\n    return False  # pyi files don't support inner functions, return False to stop the traversal.\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n    self.stack.pop()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass PythonMinifier:\n    def __init__(self, code: str = None):\n\n        self.code = code\n        self.output_code = None\n\n    def minify(self):\n        if self.code:\n            self.output_code = self.minify_code(self.code)\n\n    def get_minified_code(self):\n        if not self.output_code:\n            self.minify()\n        return self.output_code\n\n    @staticmethod\n    def minify_code(code: str) -> str:\n        return minify(code)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, code: str = None):\n\n    self.code = code\n    self.output_code = None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef minify(self):\n    if self.code:\n        self.output_code = self.minify_code(self.code)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_minified_code(self):\n    if not self.output_code:\n        self.minify()\n    return self.output_code\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n@staticmethod\ndef minify_code(code: str) -> str:\n    return minify(code)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass PythonDocstringExtractor:\n    @staticmethod\n    def extract_docstring(function_def: cst.FunctionDef) -> str:\n        docstring = None\n\n        for stmt in function_def.body.body:\n            if isinstance(stmt, cst.SimpleStatementLine):\n                for expr in stmt.body:\n                    if isinstance(expr, cst.Expr) and isinstance(\n                        expr.value, cst.SimpleString\n                    ):\n                        docstring = expr.value.value.strip('\"').strip(\"'\")\n                        break\n            if docstring is not None:\n                break\n\n        if docstring is not None:\n            return docstring.strip()\n        else:\n            function_name = function_def.name.value\n            return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"@staticmethod\ndef extract_docstring(function_def: cst.FunctionDef) -> str:\n    docstring = None\n\n    for stmt in function_def.body.body:\n        if isinstance(stmt, cst.SimpleStatementLine):\n            for expr in stmt.body:\n                if isinstance(expr, cst.Expr) and isinstance(\n                    expr.value, cst.SimpleString\n                ):\n                    docstring = expr.value.value.strip('\"').strip(\"'\")\n                    break\n        if docstring is not None:\n            break\n\n    if docstring is not None:\n        return docstring.strip()\n    else:\n        function_name = function_def.name.value\n        return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass FunctionAndClassVisitor(cst.CSTVisitor):\n    def __init__(self):\n        self.function_source_codes = []\n        self.function_nodes = []\n        self.function_count = 0\n        self.class_source_codes = []\n        self.class_nodes = []\n        self.class_count = 0\n        self.filename_map = []\n        self.full_source_list = []\n        self.full_node_list = []\n\n\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n        \"\"\"This method is called for every FunctionDef node in the tree.\n        and it does the following:\n        1. Gets the source code for the node\n        2. Adds the node to the list of function nodes\n        3. Adds the source code to the list of function source codes\n        4. Increments the function count\n        \"\"\"\n        function_source_code = cst.Module([]).code_for_node(node)\n        self.function_nodes.append(node)\n        self.function_source_codes.append(function_source_code)\n        self.full_node_list.append(node)\n        self.full_source_list.append(function_source_code)\n        self.function_count += 1\n\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\n        \"\"\"This method is called for every ClassDef node in the tree.\n        and it does the following:\n        1. Gets the source code for the node\n        2. Adds the node to the list of class nodes\n        3. Adds the source code to the list of class source codes\n        4. Increments the class count\n        \"\"\"\n        class_source_code = cst.Module([]).code_for_node(node)\n        self.class_nodes.append(node)\n        self.class_source_codes.append(class_source_code)\n        self.full_node_list.append(node)\n        self.full_source_list.append(class_source_code)\n        self.class_count += 1\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self):\n    self.function_source_codes = []\n    self.function_nodes = []\n    self.function_count = 0\n    self.class_source_codes = []\n    self.class_nodes = []\n    self.class_count = 0\n    self.filename_map = []\n    self.full_source_list = []\n    self.full_node_list = []\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n    \"\"\"This method is called for every FunctionDef node in the tree.\n        and it does the following:\n        1. Gets the source code for the node\n        2. Adds the node to the list of function nodes\n        3. Adds the source code to the list of function source codes\n        4. Increments the function count\n        \"\"\"\n    function_source_code = cst.Module([]).code_for_node(node)\n    self.function_nodes.append(node)\n    self.function_source_codes.append(function_source_code)\n    self.full_node_list.append(node)\n    self.full_source_list.append(function_source_code)\n    self.function_count += 1\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef visit_ClassDef(self, node: cst.ClassDef) -> None:\n    \"\"\"This method is called for every ClassDef node in the tree.\n        and it does the following:\n        1. Gets the source code for the node\n        2. Adds the node to the list of class nodes\n        3. Adds the source code to the list of class source codes\n        4. Increments the class count\n        \"\"\"\n    class_source_code = cst.Module([]).code_for_node(node)\n    self.class_nodes.append(node)\n    self.class_source_codes.append(class_source_code)\n    self.full_node_list.append(node)\n    self.full_source_list.append(class_source_code)\n    self.class_count += 1\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass PythonParser(OsProcessor):\n    def __init__(\n        self,\n        directory_path: str,\n        visitor: Optional[FunctionAndClassVisitor] = None,\n        minify_code: bool = False,\n        remove_docstrings: bool = False,\n    ):\n        super().__init__(directory_path)\n        self.visitor = visitor if visitor else FunctionAndClassVisitor()\n        self.minify_code = minify_code\n        self.remove_docstrings = remove_docstrings\n\n    def remove_docstring(self, tree: cst.Module) -> str:\n        \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\n\n        # Remove docstrings using a transformer\n        class DocstringRemover(cst.CSTTransformer):\n            def leave_FunctionDef(\n                self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n            ) -> cst.FunctionDef:\n                docstring = PythonDocstringExtractor.extract_docstring(original_node)\n                if docstring.startswith(\"No docstring\"):\n                    return updated_node\n\n                return updated_node.with_changes(\n                    body=updated_node.body.with_changes(\n                        body=[\n                            stmt\n                            for stmt in updated_node.body.body\n                            if not (\n                                isinstance(stmt, cst.SimpleStatementLine)\n                                and any(\n                                    isinstance(expr, cst.Expr)\n                                    and isinstance(expr.value, cst.SimpleString)\n                                    for expr in stmt.body\n                                )\n                            )\n                        ]\n                    )\n                )\n\n        tree = tree.visit(DocstringRemover())\n        return tree.code\n\n    def _process_file(self, file_path: str):\n        \"\"\"This method is called for every file in the directory.\n        It does the following:\n        1. Reads the file\n        2. Parses the file\n        3. Visits the file with the visitor\n        \"\"\"\n        #get current number of nodes in visitor\n        current_node_count = self.visitor.function_count + self.visitor.class_count\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            source_code = file.read()\n\n        try:\n            tree = cst.parse_module(source_code)\n        except cst.ParserSyntaxError:\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\n            return\n\n        tree.visit(self.visitor)\n        #calculate how many new nodes were added\n        new_node_counter = self.visitor.function_count + self.visitor.class_count - current_node_count\n        self.visitor.filename_map.extend([file_path]*new_node_counter)\n        # Remove docstrings if specified\n        if self.remove_docstrings:\n            source_code = self.remove_docstring(source_code, tree)\n\n        # Minify the code if specified\n        if self.minify_code:\n            minifier = PythonMinifier(source_code)\n            source_code = minifier.get_minified_code()\n\n        # Add the processed code to the corresponding list in the visitor\n        self.visitor.function_source_codes.append(source_code)\n\n    def process_file(self, file_path: str):\n        \"\"\"This method is called for every file in the directory.\n        It does the following:\n        1. Runs flake8 on the file\n        if flake8 returns a non-zero exit code, it means the file has a syntax error\n        2. Reads the file\n        3. Parses the file\n        4. Visits the file with the visitor\n\n        \"\"\"\n        result = subprocess.run(\n            [\"flake8\", \"--select=E999\", file_path], capture_output=True\n        )\n\n        if result.returncode != 0:\n            print(f\"Skipping file with syntax error: {file_path}\")\n            print(result.stderr.decode(\"utf-8\"))\n            return\n\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            source_code = f.read()\n\n        try:\n            tree = cst.parse_module(source_code)\n            tree.visit(self.visitor)\n        except cst.ParserSyntaxError as e:\n            print(f\"Syntax error: {e}\")\n            print(f\"Skipping file with syntax error: {file_path}\")\n\n    def process_directory(\n        self,\n    ) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\n        \"\"\"This method is called for every directory.\n        It does the following:\n        1. Gets all the python files in the directory\n        2. Processes each file\n        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\n        \"\"\"\n\n        python_files = self.get_files_with_extension(\".py\")\n\n        for file_path in python_files:\n            self._process_file(file_path)\n\n        result_dict = {\n            'function_source_codes': self.visitor.function_source_codes,\n            'function_nodes': self.visitor.function_nodes,\n            'class_source_codes': self.visitor.class_source_codes,\n            'class_nodes': self.visitor.class_nodes,\n            'file_map': self.visitor.filename_map,\n            'full_nodes': self.visitor.full_node_list,\n            'full_source': self.visitor.full_source_list\n        }\n\n\n        return result_dict\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self,\n    directory_path: str,\n    visitor: Optional[FunctionAndClassVisitor] = None,\n    minify_code: bool = False,\n    remove_docstrings: bool = False,\n):\n    super().__init__(directory_path)\n    self.visitor = visitor if visitor else FunctionAndClassVisitor()\n    self.minify_code = minify_code\n    self.remove_docstrings = remove_docstrings\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef remove_docstring(self, tree: cst.Module) -> str:\n    \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\n\n    # Remove docstrings using a transformer\n    class DocstringRemover(cst.CSTTransformer):\n        def leave_FunctionDef(\n            self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n        ) -> cst.FunctionDef:\n            docstring = PythonDocstringExtractor.extract_docstring(original_node)\n            if docstring.startswith(\"No docstring\"):\n                return updated_node\n\n            return updated_node.with_changes(\n                body=updated_node.body.with_changes(\n                    body=[\n                        stmt\n                        for stmt in updated_node.body.body\n                        if not (\n                            isinstance(stmt, cst.SimpleStatementLine)\n                            and any(\n                                isinstance(expr, cst.Expr)\n                                and isinstance(expr.value, cst.SimpleString)\n                                for expr in stmt.body\n                            )\n                        )\n                    ]\n                )\n            )\n\n    tree = tree.visit(DocstringRemover())\n    return tree.code\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n# Remove docstrings using a transformer\nclass DocstringRemover(cst.CSTTransformer):\n    def leave_FunctionDef(\n        self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n    ) -> cst.FunctionDef:\n        docstring = PythonDocstringExtractor.extract_docstring(original_node)\n        if docstring.startswith(\"No docstring\"):\n            return updated_node\n\n        return updated_node.with_changes(\n            body=updated_node.body.with_changes(\n                body=[\n                    stmt\n                    for stmt in updated_node.body.body\n                    if not (\n                        isinstance(stmt, cst.SimpleStatementLine)\n                        and any(\n                            isinstance(expr, cst.Expr)\n                            and isinstance(expr.value, cst.SimpleString)\n                            for expr in stmt.body\n                        )\n                    )\n                ]\n            )\n        )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def leave_FunctionDef(\n    self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n) -> cst.FunctionDef:\n    docstring = PythonDocstringExtractor.extract_docstring(original_node)\n    if docstring.startswith(\"No docstring\"):\n        return updated_node\n\n    return updated_node.with_changes(\n        body=updated_node.body.with_changes(\n            body=[\n                stmt\n                for stmt in updated_node.body.body\n                if not (\n                    isinstance(stmt, cst.SimpleStatementLine)\n                    and any(\n                        isinstance(expr, cst.Expr)\n                        and isinstance(expr.value, cst.SimpleString)\n                        for expr in stmt.body\n                    )\n                )\n            ]\n        )\n    )\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _process_file(self, file_path: str):\n    \"\"\"This method is called for every file in the directory.\n        It does the following:\n        1. Reads the file\n        2. Parses the file\n        3. Visits the file with the visitor\n        \"\"\"\n    #get current number of nodes in visitor\n    current_node_count = self.visitor.function_count + self.visitor.class_count\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        source_code = file.read()\n\n    try:\n        tree = cst.parse_module(source_code)\n    except cst.ParserSyntaxError:\n        print(f\"Skipping file {file_path}: Failed to parse syntax\")\n        return\n\n    tree.visit(self.visitor)\n    #calculate how many new nodes were added\n    new_node_counter = self.visitor.function_count + self.visitor.class_count - current_node_count\n    self.visitor.filename_map.extend([file_path]*new_node_counter)\n    # Remove docstrings if specified\n    if self.remove_docstrings:\n        source_code = self.remove_docstring(source_code, tree)\n\n    # Minify the code if specified\n    if self.minify_code:\n        minifier = PythonMinifier(source_code)\n        source_code = minifier.get_minified_code()\n\n    # Add the processed code to the corresponding list in the visitor\n    self.visitor.function_source_codes.append(source_code)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef process_file(self, file_path: str):\n    \"\"\"This method is called for every file in the directory.\n        It does the following:\n        1. Runs flake8 on the file\n        if flake8 returns a non-zero exit code, it means the file has a syntax error\n        2. Reads the file\n        3. Parses the file\n        4. Visits the file with the visitor\n\n        \"\"\"\n    result = subprocess.run(\n        [\"flake8\", \"--select=E999\", file_path], capture_output=True\n    )\n\n    if result.returncode != 0:\n        print(f\"Skipping file with syntax error: {file_path}\")\n        print(result.stderr.decode(\"utf-8\"))\n        return\n\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        source_code = f.read()\n\n    try:\n        tree = cst.parse_module(source_code)\n        tree.visit(self.visitor)\n    except cst.ParserSyntaxError as e:\n        print(f\"Syntax error: {e}\")\n        print(f\"Skipping file with syntax error: {file_path}\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef process_directory(\n    self,\n) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\n    \"\"\"This method is called for every directory.\n        It does the following:\n        1. Gets all the python files in the directory\n        2. Processes each file\n        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\n        \"\"\"\n\n    python_files = self.get_files_with_extension(\".py\")\n\n    for file_path in python_files:\n        self._process_file(file_path)\n\n    result_dict = {\n        'function_source_codes': self.visitor.function_source_codes,\n        'function_nodes': self.visitor.function_nodes,\n        'class_source_codes': self.visitor.class_source_codes,\n        'class_nodes': self.visitor.class_nodes,\n        'file_map': self.visitor.filename_map,\n        'full_nodes': self.visitor.full_node_list,\n        'full_source': self.visitor.full_source_list\n    }\n\n\n    return result_dict\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass IssueParser:\n    def __init__(self, repo_name):\n        self.g = Github()\n        self.repo = self.g.get_repo(repo_name)\n\n    def get_issues(self, state=\"open\"):\n        \"\"\"\n        Returns a list of all issues in the repo with the specified state.\n        \"\"\"\n        issues = []\n        for issue in self.repo.get_issues(state=state):\n            issues.append(issue)\n        return issues\n\n    def parse_issues(self, state=\"open\"):\n        \"\"\"\n        Parses all issues in the repo with the specified state and returns a list of dicts.\n        Each dict contains the issue number, title, body, and labels.\n        \"\"\"\n        parsed_issues = []\n        issues = self.get_issues(state=state)\n        for issue in issues:\n            parsed_issue = {\n                \"number\": issue.number,\n                \"title\": issue.title,\n                \"body\": issue.body,\n                \"labels\": [label.name for label in issue.labels],\n            }\n            parsed_issues.append(parsed_issue)\n        return parsed_issues\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, repo_name):\n    self.g = Github()\n    self.repo = self.g.get_repo(repo_name)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_issues(self, state=\"open\"):\n    \"\"\"\n        Returns a list of all issues in the repo with the specified state.\n        \"\"\"\n    issues = []\n    for issue in self.repo.get_issues(state=state):\n        issues.append(issue)\n    return issues\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef parse_issues(self, state=\"open\"):\n    \"\"\"\n        Parses all issues in the repo with the specified state and returns a list of dicts.\n        Each dict contains the issue number, title, body, and labels.\n        \"\"\"\n    parsed_issues = []\n    issues = self.get_issues(state=state)\n    for issue in issues:\n        parsed_issue = {\n            \"number\": issue.number,\n            \"title\": issue.title,\n            \"body\": issue.body,\n            \"labels\": [label.name for label in issue.labels],\n        }\n        parsed_issues.append(parsed_issue)\n    return parsed_issues\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass CommitParser:\n    def __init__(self, repo_name):\n        self.g = Github()\n        self.repo = self.g.get_repo(repo_name)\n\n    def get_commits(self):\n        \"\"\"\n        Returns a list of all commits in the main branch of the repository.\n        \"\"\"\n        commits = []\n        branch = self.repo.get_branch(\"main\")\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\n            commits.append(commit)\n        return commits\n\n    def parse_commits(self):\n        \"\"\"\n        Parses all commits in the main branch of the repository and returns a list of dicts.\n        Each dict contains the commit sha, commit message, and author information.\n        \"\"\"\n        parsed_commits = []\n        commits = self.get_commits()\n        for commit in commits:\n            parsed_commit = {\n                \"sha\": commit.sha,\n                \"message\": commit.commit.message,\n                \"author\": {\n                    \"name\": commit.commit.author.name,\n                    \"email\": commit.commit.author.email,\n                    \"date\": commit.commit.author.date,\n                },\n            }\n            parsed_commits.append(parsed_commit)\n        return parsed_commits\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, repo_name):\n    self.g = Github()\n    self.repo = self.g.get_repo(repo_name)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_commits(self):\n    \"\"\"\n        Returns a list of all commits in the main branch of the repository.\n        \"\"\"\n    commits = []\n    branch = self.repo.get_branch(\"main\")\n    for commit in self.repo.get_commits(sha=branch.commit.sha):\n        commits.append(commit)\n    return commits\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef parse_commits(self):\n    \"\"\"\n        Parses all commits in the main branch of the repository and returns a list of dicts.\n        Each dict contains the commit sha, commit message, and author information.\n        \"\"\"\n    parsed_commits = []\n    commits = self.get_commits()\n    for commit in commits:\n        parsed_commit = {\n            \"sha\": commit.sha,\n            \"message\": commit.commit.message,\n            \"author\": {\n                \"name\": commit.commit.author.name,\n                \"email\": commit.commit.author.email,\n                \"date\": commit.commit.author.date,\n            },\n        }\n        parsed_commits.append(parsed_commit)\n    return parsed_commits\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass DirectoryProcessor:\n    def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\n        self.directory_path = directory_path\n        self.visitor = visitor\n\n    def _process_file(self, file_path: str):\n        with open(file_path, \"r\") as file:\n            source_code = file.read()\n\n        try:\n            tree = cst.parse_module(source_code)\n        except cst.ParserSyntaxError:\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\n            return\n\n        tree.visit(self.visitor)\n\n    def process_file(self, file_path: str):\n        # Run flake8 on the file\n        result = subprocess.run(\n            [\"flake8\", \"--select=E999\", file_path], capture_output=True\n        )\n\n        if result.returncode != 0:\n            print(f\"Skipping file with syntax error: {file_path}\")\n            print(result.stderr.decode(\"utf-8\"))\n            return\n\n        with open(file_path, \"r\") as f:\n            source_code = f.read()\n\n        try:\n            tree = cst.parse_module(source_code)\n            tree.visit(self.visitor)\n        except cst.ParserSyntaxError as e:\n            print(f\"Syntax error: {e}\")\n            print(f\"Skipping file with syntax error: {file_path}\")\n\n    def process_directory(self) -> List[str]:\n        function_source_codes = []\n        class_source_codes = []\n\n        for root, _, files in os.walk(self.directory_path):\n            for file in files:\n                if file.endswith(\".py\"):\n                    file_path = os.path.join(root, file)\n                    self._process_file(file_path)\n\n        function_source_codes = self.visitor.function_source_codes\n        function_nodes = self.visitor.function_nodes\n        class_source_codes = self.visitor.class_source_codes\n        class_nodes = self.visitor.class_nodes\n\n        return function_source_codes, class_source_codes, function_nodes, class_nodes\n\n    def clone_repo(self, repo_url):\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n        target_directory = os.path.join(self.directory_path, repo_name)\n\n        if os.path.exists(target_directory):\n            shutil.rmtree(target_directory)\n\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n\n        return target_directory\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\n    self.directory_path = directory_path\n    self.visitor = visitor\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _process_file(self, file_path: str):\n    with open(file_path, \"r\") as file:\n        source_code = file.read()\n\n    try:\n        tree = cst.parse_module(source_code)\n    except cst.ParserSyntaxError:\n        print(f\"Skipping file {file_path}: Failed to parse syntax\")\n        return\n\n    tree.visit(self.visitor)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef process_file(self, file_path: str):\n    # Run flake8 on the file\n    result = subprocess.run(\n        [\"flake8\", \"--select=E999\", file_path], capture_output=True\n    )\n\n    if result.returncode != 0:\n        print(f\"Skipping file with syntax error: {file_path}\")\n        print(result.stderr.decode(\"utf-8\"))\n        return\n\n    with open(file_path, \"r\") as f:\n        source_code = f.read()\n\n    try:\n        tree = cst.parse_module(source_code)\n        tree.visit(self.visitor)\n    except cst.ParserSyntaxError as e:\n        print(f\"Syntax error: {e}\")\n        print(f\"Skipping file with syntax error: {file_path}\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef process_directory(self) -> List[str]:\n    function_source_codes = []\n    class_source_codes = []\n\n    for root, _, files in os.walk(self.directory_path):\n        for file in files:\n            if file.endswith(\".py\"):\n                file_path = os.path.join(root, file)\n                self._process_file(file_path)\n\n    function_source_codes = self.visitor.function_source_codes\n    function_nodes = self.visitor.function_nodes\n    class_source_codes = self.visitor.class_source_codes\n    class_nodes = self.visitor.class_nodes\n\n    return function_source_codes, class_source_codes, function_nodes, class_nodes\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef clone_repo(self, repo_url):\n    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n    target_directory = os.path.join(self.directory_path, repo_name)\n\n    if os.path.exists(target_directory):\n        shutil.rmtree(target_directory)\n\n    subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n\n    return target_directory\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass GitHubUserProcessor:\n    def __init__(\n        self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n    ):\n        self.username = username\n        self.repo_name = repo_name\n        self.github = Github()\n        self.directory_processor = None\n        self.function_source_codes = []\n        self.class_source_codes = []\n        self.visitor = visitor\n\n    def get_public_repos(self):\n        user = self.github.get_user(self.username)\n        return user.get_repos()\n\n    def process_repos(self, base_directory):\n        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n        for repo in self.get_public_repos():\n            if not repo.private:\n                print(f\"Processing repo: {repo.name}\")\n                repo_path = self.directory_processor.clone_repo(repo.clone_url)\n                (\n                    function_source_codes,\n                    class_source_codes,\n                ) = self.directory_processor.process_directory()\n                self.function_source_codes.extend(function_source_codes)\n                self.class_source_codes.extend(class_source_codes)\n                shutil.rmtree(repo_path)\n\n        return self.directory_processor\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n):\n    self.username = username\n    self.repo_name = repo_name\n    self.github = Github()\n    self.directory_processor = None\n    self.function_source_codes = []\n    self.class_source_codes = []\n    self.visitor = visitor\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_public_repos(self):\n    user = self.github.get_user(self.username)\n    return user.get_repos()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef process_repos(self, base_directory):\n    self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n    for repo in self.get_public_repos():\n        if not repo.private:\n            print(f\"Processing repo: {repo.name}\")\n            repo_path = self.directory_processor.clone_repo(repo.clone_url)\n            (\n                function_source_codes,\n                class_source_codes,\n            ) = self.directory_processor.process_directory()\n            self.function_source_codes.extend(function_source_codes)\n            self.class_source_codes.extend(class_source_codes)\n            shutil.rmtree(repo_path)\n\n    return self.directory_processor\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass GitHubRepoProcessor:\n    def __init__(\n        self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n    ):\n        self.username = username\n        self.repo_name = repo_name\n        self.github = Github()\n        self.directory_processor = None\n        self.function_source_codes = []\n        self.function_nodes = []\n        self.class_source_codes = []\n        self.class_nodes = []\n        self.visitor = visitor\n\n    def get_repo(self, repo_name):\n        user = self.github.get_user(self.username)\n        return user.get_repo(repo_name)\n\n    def process_repo(self, base_directory):\n        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n        repo = self.get_repo(self.repo_name)\n        print(f\"Processing repo: {self.repo_name}\")\n        repo_path = self.directory_processor.clone_repo(repo.clone_url)\n        (\n            function_source_codes,\n            class_source_codes,\n            function_nodes,\n            class_nodes,\n        ) = self.directory_processor.process_directory()\n        self.function_source_codes.extend(function_source_codes)\n        self.function_nodes.extend(function_nodes)\n        self.class_source_codes.extend(class_source_codes)\n        self.class_nodes.extend(class_nodes)\n        shutil.rmtree(repo_path)\n        return self.directory_processor\n\n    def get_values(self):\n        # concatenate the function and class source codes\n        self.function_source_codes.extend(self.class_source_codes)\n        self.function_nodes.extend(self.class_nodes)\n        return self.function_source_codes, self.function_nodes\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(\n    self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n):\n    self.username = username\n    self.repo_name = repo_name\n    self.github = Github()\n    self.directory_processor = None\n    self.function_source_codes = []\n    self.function_nodes = []\n    self.class_source_codes = []\n    self.class_nodes = []\n    self.visitor = visitor\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_repo(self, repo_name):\n    user = self.github.get_user(self.username)\n    return user.get_repo(repo_name)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef process_repo(self, base_directory):\n    self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n    repo = self.get_repo(self.repo_name)\n    print(f\"Processing repo: {self.repo_name}\")\n    repo_path = self.directory_processor.clone_repo(repo.clone_url)\n    (\n        function_source_codes,\n        class_source_codes,\n        function_nodes,\n        class_nodes,\n    ) = self.directory_processor.process_directory()\n    self.function_source_codes.extend(function_source_codes)\n    self.function_nodes.extend(function_nodes)\n    self.class_source_codes.extend(class_source_codes)\n    self.class_nodes.extend(class_nodes)\n    shutil.rmtree(repo_path)\n    return self.directory_processor\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef get_values(self):\n    # concatenate the function and class source codes\n    self.function_source_codes.extend(self.class_source_codes)\n    self.function_nodes.extend(self.class_nodes)\n    return self.function_source_codes, self.function_nodes\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass PubmedAPI:\n    def __init__(self):\n        self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n\n    def search(self, query, max_results=10):\n        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n        record = Entrez.read(handle)\n        handle.close()\n        return record[\"IdList\"]\n\n    def fetch_abstract(self, pubmed_id):\n        handle = Entrez.efetch(\n            db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\"\n        )\n        abstract = handle.read()\n        handle.close()\n        return abstract\n\n    def fetch_pmc_full_text(self, pubmed_id):\n        # Get the PMC ID for the PubMed ID\n        handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\n        record = Entrez.read(handle)\n        handle.close()\n        pmc_id = None\n        for link in record[0][\"LinkSetDb\"]:\n            if link[\"DbTo\"] == \"pmc\":\n                pmc_id = link[\"Link\"][0][\"Id\"]\n                break\n\n        if not pmc_id:\n            return None\n\n        # Fetch the PMC article XML\n        handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\n        xml_content = handle.read()\n        handle.close()\n\n        # Parse the XML and extract the full text\n        soup = BeautifulSoup(xml_content, \"xml\")\n        full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n\n        return full_text\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self):\n    self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef search(self, query, max_results=10):\n    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n    record = Entrez.read(handle)\n    handle.close()\n    return record[\"IdList\"]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef fetch_abstract(self, pubmed_id):\n    handle = Entrez.efetch(\n        db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\"\n    )\n    abstract = handle.read()\n    handle.close()\n    return abstract\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef fetch_pmc_full_text(self, pubmed_id):\n    # Get the PMC ID for the PubMed ID\n    handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\n    record = Entrez.read(handle)\n    handle.close()\n    pmc_id = None\n    for link in record[0][\"LinkSetDb\"]:\n        if link[\"DbTo\"] == \"pmc\":\n            pmc_id = link[\"Link\"][0][\"Id\"]\n            break\n\n    if not pmc_id:\n        return None\n\n    # Fetch the PMC article XML\n    handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\n    xml_content = handle.read()\n    handle.close()\n\n    # Parse the XML and extract the full text\n    soup = BeautifulSoup(xml_content, \"xml\")\n    full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n\n    return full_text\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass PubmedParser:\n    def __init__(self):\n        self.api = PubmedAPI()\n\n    def parse_papers(self, query, max_results=10):\n        pubmed_ids = self.api.search(query, max_results)\n        paper_list = []\n        for pubmed_id in pubmed_ids:\n            paper_dict = {}\n            paper_dict[\"pubmed_id\"] = pubmed_id\n            paper_dict[\"abstract\"] = self.api.fetch_abstract(pubmed_id)\n            paper_dict[\"content\"] = self.api.fetch_pmc_full_text(pubmed_id)\n            paper_list.append(paper_dict)\n        return paper_list\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self):\n    self.api = PubmedAPI()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef parse_papers(self, query, max_results=10):\n    pubmed_ids = self.api.search(query, max_results)\n    paper_list = []\n    for pubmed_id in pubmed_ids:\n        paper_dict = {}\n        paper_dict[\"pubmed_id\"] = pubmed_id\n        paper_dict[\"abstract\"] = self.api.fetch_abstract(pubmed_id)\n        paper_dict[\"content\"] = self.api.fetch_pmc_full_text(pubmed_id)\n        paper_list.append(paper_dict)\n    return paper_list\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass ArxivVanityParser:\n    def __init__(self):\n        self.base_url = \"https://www.arxiv-vanity.com/\"\n\n    def _get_vanity_url(self, arxiv_id):\n        return urljoin(self.base_url, \"papers/\" + arxiv_id)\n\n    def _fetch_html(self, url):\n        response = requests.get(url)\n        if response.status_code == 200:\n            return response.text\n        else:\n            return None\n\n    def _extract_main_content(self, html):\n        soup = BeautifulSoup(html, \"html.parser\")\n        paragraphs = soup.find_all(\"div\", {\"class\": \"ltx_para\"})\n        content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\n        return content\n\n    def parse_paper(self, arxiv_id):\n        vanity_url = self._get_vanity_url(arxiv_id)\n        html = self._fetch_html(vanity_url)\n        if html is not None:\n            return self._extract_main_content(html)\n        else:\n            return None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self):\n    self.base_url = \"https://www.arxiv-vanity.com/\"\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _get_vanity_url(self, arxiv_id):\n    return urljoin(self.base_url, \"papers/\" + arxiv_id)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _fetch_html(self, url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.text\n    else:\n        return None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _extract_main_content(self, html):\n    soup = BeautifulSoup(html, \"html.parser\")\n    paragraphs = soup.find_all(\"div\", {\"class\": \"ltx_para\"})\n    content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\n    return content\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef parse_paper(self, arxiv_id):\n    vanity_url = self._get_vanity_url(arxiv_id)\n    html = self._fetch_html(vanity_url)\n    if html is not None:\n        return self._extract_main_content(html)\n    else:\n        return None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass ArxivAPI:\n    def __init__(self):\n        self.base_url = \"http://export.arxiv.org/api/query?\"\n        self.pdf_download_url = \"https://arxiv.org/pdf/\"\n\n    def search(self, query, max_results=10):\n        url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\n        response = requests.get(url)\n        if response.status_code == 200:\n            return response.text\n        else:\n            return None\n\n    def download_pdf(self, paper_key, save_directory=\"./\"):\n        pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\n        response = requests.get(pdf_url)\n        if response.status_code == 200:\n            with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\n                f.write(response.content)\n            print(f\"PDF for {paper_key} downloaded successfully.\")\n        else:\n            print(f\"Error downloading PDF for {paper_key}.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self):\n    self.base_url = \"http://export.arxiv.org/api/query?\"\n    self.pdf_download_url = \"https://arxiv.org/pdf/\"\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef search(self, query, max_results=10):\n    url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.text\n    else:\n        return None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef download_pdf(self, paper_key, save_directory=\"./\"):\n    pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\n    response = requests.get(pdf_url)\n    if response.status_code == 200:\n        with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\n            f.write(response.content)\n        print(f\"PDF for {paper_key} downloaded successfully.\")\n    else:\n        print(f\"Error downloading PDF for {paper_key}.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass ArxivParser:\n    def __init__(self):\n        self.api = ArxivAPI()\n        self.vanity_parser = ArxivVanityParser()\n\n    def _parse_arxiv_id(self, url):\n        return url.split(\"/\")[-1]\n\n    def parse_papers(self, query, max_results=10):\n        search_results = self.api.search(query, max_results)\n        if search_results is not None:\n            soup = BeautifulSoup(search_results, \"html.parser\")\n            entries = soup.find_all(\"entry\")\n            paper_list = []\n            for entry in entries:\n                paper_dict = {}\n                arxiv_id = self._parse_arxiv_id(entry.id.string)\n                paper_dict[\"arxiv_id\"] = arxiv_id\n                paper_dict[\"title\"] = entry.title.string\n                paper_dict[\"summary\"] = entry.summary.string\n                paper_dict[\"content\"] = self.vanity_parser.parse_paper(str(arxiv_id))\n                if paper_dict[\"content\"] == None:\n                    continue\n                paper_list.append(paper_dict)\n            return paper_list\n        else:\n            return None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self):\n    self.api = ArxivAPI()\n    self.vanity_parser = ArxivVanityParser()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _parse_arxiv_id(self, url):\n    return url.split(\"/\")[-1]\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef parse_papers(self, query, max_results=10):\n    search_results = self.api.search(query, max_results)\n    if search_results is not None:\n        soup = BeautifulSoup(search_results, \"html.parser\")\n        entries = soup.find_all(\"entry\")\n        paper_list = []\n        for entry in entries:\n            paper_dict = {}\n            arxiv_id = self._parse_arxiv_id(entry.id.string)\n            paper_dict[\"arxiv_id\"] = arxiv_id\n            paper_dict[\"title\"] = entry.title.string\n            paper_dict[\"summary\"] = entry.summary.string\n            paper_dict[\"content\"] = self.vanity_parser.parse_paper(str(arxiv_id))\n            if paper_dict[\"content\"] == None:\n                continue\n            paper_list.append(paper_dict)\n        return paper_list\n    else:\n        return None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass SubjectPerspectiveAnalyzer:\n    def __init__(self, chatbot: 'Chat'):\n        self.chatbot = chatbot\n\n    def analyze_subject_perspective(self, user_subject: str, user_perspective: str) -> dict:\n        prompts = [\n            f\"Generate ideas and concepts that explore the connection between {user_subject} and {user_perspective}, considering both traditional and unconventional approaches.\",\n            f\"List key concepts or topics that would help analyze {user_subject} through the lens of {user_perspective}, including relevant principles, theories, or models.\",\n            f\"Identify potential areas or research topics where {user_subject} and {user_perspective} intersect, highlighting intriguing or innovative perspectives.\"\n        ]\n\n        output = {}\n\n        with RateLimitedThreadPoolExecutor(max_workers=3, calls_per_minute=20, verbose = False) as executor:\n            future_to_prompt = {executor.submit(self._analyze_prompt, prompt): prompt for prompt in prompts}\n            for future in as_completed(future_to_prompt):\n                prompt = future_to_prompt[future]\n                try:\n                    output[prompt] = future.result()\n                except Exception as exc:\n                    counter = 0\n                    print(f'An exception occurred while analyzing prompt \"{prompt}\": {exc}')\n                    while counter < 3:\n                        try:\n                            output[prompt] = self._analyze_prompt(prompt)\n                            break\n                        except Exception as exc:\n                            counter += 1\n                            print(f'An exception occurred while analyzing prompt \"{prompt}\": {exc}')\n                    if counter == 3:\n                        output[prompt] = []\n\n        return output\n\n    def _analyze_prompt(self, prompt: str) -> list:\n        response = self.chatbot.reply(prompt, verbose=False)\n        return self._format_response(response)\n\n    def _format_response(self, response: str) -> list:\n        formatted_response = response.strip().split('\\n')\n        return formatted_response\n    \n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, chatbot: 'Chat'):\n    self.chatbot = chatbot\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef analyze_subject_perspective(self, user_subject: str, user_perspective: str) -> dict:\n    prompts = [\n        f\"Generate ideas and concepts that explore the connection between {user_subject} and {user_perspective}, considering both traditional and unconventional approaches.\",\n        f\"List key concepts or topics that would help analyze {user_subject} through the lens of {user_perspective}, including relevant principles, theories, or models.\",\n        f\"Identify potential areas or research topics where {user_subject} and {user_perspective} intersect, highlighting intriguing or innovative perspectives.\"\n    ]\n\n    output = {}\n\n    with RateLimitedThreadPoolExecutor(max_workers=3, calls_per_minute=20, verbose = False) as executor:\n        future_to_prompt = {executor.submit(self._analyze_prompt, prompt): prompt for prompt in prompts}\n        for future in as_completed(future_to_prompt):\n            prompt = future_to_prompt[future]\n            try:\n                output[prompt] = future.result()\n            except Exception as exc:\n                counter = 0\n                print(f'An exception occurred while analyzing prompt \"{prompt}\": {exc}')\n                while counter < 3:\n                    try:\n                        output[prompt] = self._analyze_prompt(prompt)\n                        break\n                    except Exception as exc:\n                        counter += 1\n                        print(f'An exception occurred while analyzing prompt \"{prompt}\": {exc}')\n                if counter == 3:\n                    output[prompt] = []\n\n    return output\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _analyze_prompt(self, prompt: str) -> list:\n    response = self.chatbot.reply(prompt, verbose=False)\n    return self._format_response(response)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _format_response(self, response: str) -> list:\n    formatted_response = response.strip().split('\\n')\n    return formatted_response\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass Ideation:\n    def __init__(self, memory_index: MemoryIndex):\n        self.memory_index = memory_index\n\n    def retrieve_ideas(self, queries: Dict, k: int = 30, max_tokens: int = 10000):\n        \"\"\"\n        Generate ideas based on the given list of queries.\n\n        Args:\n            queries: The list of queries for generating ideas.\n            k: The number of top search results to consider.\n            max_tokens: The maximum number of tokens to return.\n\n        Returns:\n            A list of ideas generated based on the queries.\n        \"\"\"\n        ideas = []\n        for key, queries in queries.items():\n            for query in queries:\n                if query is None or len(query) < 10:\n                    continue\n                top_k_hints, scores, indices = self.memory_index.token_bound_query(\n                    query, k=k, max_tokens=max_tokens\n                )\n                last_query = self.memory_index.query_history[-1]\n                hints_tokens = last_query[\"hints_tokens\"]\n                returned_tokens = last_query[\"returned_tokens\"]\n\n                ideas.append({\"key_task\": key, \"query\": query, \"hints\": top_k_hints, \"scores\": scores, \"hints_tokens\": hints_tokens, \"returned_tokens\": returned_tokens})\n\n        return ideas\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, memory_index: MemoryIndex):\n    self.memory_index = memory_index\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef retrieve_ideas(self, queries: Dict, k: int = 30, max_tokens: int = 10000):\n    \"\"\"\n        Generate ideas based on the given list of queries.\n\n        Args:\n            queries: The list of queries for generating ideas.\n            k: The number of top search results to consider.\n            max_tokens: The maximum number of tokens to return.\n\n        Returns:\n            A list of ideas generated based on the queries.\n        \"\"\"\n    ideas = []\n    for key, queries in queries.items():\n        for query in queries:\n            if query is None or len(query) < 10:\n                continue\n            top_k_hints, scores, indices = self.memory_index.token_bound_query(\n                query, k=k, max_tokens=max_tokens\n            )\n            last_query = self.memory_index.query_history[-1]\n            hints_tokens = last_query[\"hints_tokens\"]\n            returned_tokens = last_query[\"returned_tokens\"]\n\n            ideas.append({\"key_task\": key, \"query\": query, \"hints\": top_k_hints, \"scores\": scores, \"hints_tokens\": hints_tokens, \"returned_tokens\": returned_tokens})\n\n    return ideas\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass IdeaCluster:\n    def __init__(self, ideas: list, max_tokens_per_cluster: int):\n        self.ideas = ideas\n        self.max_tokens_per_cluster = max_tokens_per_cluster\n        self.idea_index = self.create_idea_index()\n        self.cluster_labels = None\n\n    def create_idea_index(self):\n        gathered_docs = []\n        for idea in self.ideas:\n            for hint in idea[\"hints\"]:\n                gathered_docs.append(hint)\n        self.gathered_docs = set(gathered_docs)\n        idea_index = MemoryIndex(values=self.gathered_docs, is_batched=True, name=\"ideas\")\n        return idea_index\n\n    def cluster_embeddings(self, n_neighbors: int = 10, min_cluster_size: int = 5):\n        reducer = umap.UMAP(n_neighbors=n_neighbors)\n        reduced_embeddings = reducer.fit_transform(self.idea_index.get_all_embeddings())\n\n        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n        labels = clusterer.fit_predict(reduced_embeddings)\n\n        token_count_per_cluster = self.count_tokens_per_cluster(labels)\n        print(token_count_per_cluster)\n        if max(token_count_per_cluster.values()) <= self.max_tokens_per_cluster:\n            self.cluster_labels = labels\n            print(\"Clusters created successfully.\")\n        else:\n            print(\"Clusters exceed the maximum token count.\")\n\n    def count_tokens_per_cluster(self, labels):\n        token_count_per_cluster = {}\n\n        for label, doc in zip(labels, self.gathered_docs):\n            if label not in token_count_per_cluster:\n                token_count_per_cluster[label] = len(tokenizer.encode(doc))\n            else:\n                token_count_per_cluster[label] += len(tokenizer.encode(doc))\n        return token_count_per_cluster\n\n    def create_minimum_spanning_paths(self):\n        if self.cluster_labels is None:\n            raise ValueError(\"You must run cluster_embeddings() before creating minimum spanning paths.\")\n\n        unique_labels = np.unique(self.cluster_labels)\n        min_span_paths = []\n\n        for label in unique_labels:\n\n\n            # Get the indices of the current cluster\n            cluster_indices = np.where(self.cluster_labels == label)[0]\n\n            # Calculate the pairwise distances between embeddings in the cluster\n            cluster_embeddings = self.idea_index.embeddings[cluster_indices]\n            dist_matrix = squareform(pdist(cluster_embeddings))\n\n            # Create a graph from the distance matrix\n            graph = nx.from_numpy_array(dist_matrix)\n\n            # Compute the minimum spanning tree of the graph\n            min_span_tree = nx.minimum_spanning_tree(graph)\n\n            # Get the minimum spanning paths\n            min_span_paths_cluster = []\n            visited = set()\n            for u, v in min_span_tree.edges():\n                if u not in visited and v not in visited:\n                    orig_u = cluster_indices[u]\n                    orig_v = cluster_indices[v]\n                    min_span_paths_cluster.append(orig_u)\n                    visited.add(u)\n                    visited.add(v)\n            # Add the last node to complete the path\n            min_span_paths_cluster.append(orig_v)\n\n            min_span_paths.append(min_span_paths_cluster)\n\n        self.min_span_paths = min_span_paths\n\n    \n    def plot_embeddings_with_path(self):\n        paths = self.min_span_paths\n        embeddings = self.idea_index.embeddings\n        title = \"Minimum Spanning Paths\"\n        tsne = TSNE(n_components=2, random_state=42)\n        reduced_embeddings = tsne.fit_transform(embeddings)\n\n        plt.figure(figsize=(10, 8))\n        colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n        for i, path in enumerate(paths):\n            path_embeddings = reduced_embeddings[path]\n            plt.scatter(\n                path_embeddings[:, 0],\n                path_embeddings[:, 1],\n                color=colors[i],\n                label=f\"Cluster {i}\",\n            )\n            for j in range(len(path) - 1):\n                plt.plot(\n                    [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n                    [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n                    color=colors[i],\n                )\n        plt.title(title)\n        plt.legend()\n        plt.show()\n    def get_clustered_ideas(self):\n        if self.cluster_labels is None:\n            raise ValueError(\"You must run cluster_embeddings() before getting clustered ideas.\")\n        \n        clustered_ideas = {}\n        for label, idea in zip(self.cluster_labels, self.idea_index.values):\n            if label not in clustered_ideas:\n                clustered_ideas[label] = [idea]\n            else:\n                clustered_ideas[label].append(idea)\n        \n        # Convert the dictionary to a list of lists (each list corresponds to a cluster)\n        return list(clustered_ideas.values())\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, ideas: list, max_tokens_per_cluster: int):\n    self.ideas = ideas\n    self.max_tokens_per_cluster = max_tokens_per_cluster\n    self.idea_index = self.create_idea_index()\n    self.cluster_labels = None\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef create_idea_index(self):\n    gathered_docs = []\n    for idea in self.ideas:\n        for hint in idea[\"hints\"]:\n            gathered_docs.append(hint)\n    self.gathered_docs = set(gathered_docs)\n    idea_index = MemoryIndex(values=self.gathered_docs, is_batched=True, name=\"ideas\")\n    return idea_index\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef cluster_embeddings(self, n_neighbors: int = 10, min_cluster_size: int = 5):\n    reducer = umap.UMAP(n_neighbors=n_neighbors)\n    reduced_embeddings = reducer.fit_transform(self.idea_index.get_all_embeddings())\n\n    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n    labels = clusterer.fit_predict(reduced_embeddings)\n\n    token_count_per_cluster = self.count_tokens_per_cluster(labels)\n    print(token_count_per_cluster)\n    if max(token_count_per_cluster.values()) <= self.max_tokens_per_cluster:\n        self.cluster_labels = labels\n        print(\"Clusters created successfully.\")\n    else:\n        print(\"Clusters exceed the maximum token count.\")\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef count_tokens_per_cluster(self, labels):\n    token_count_per_cluster = {}\n\n    for label, doc in zip(labels, self.gathered_docs):\n        if label not in token_count_per_cluster:\n            token_count_per_cluster[label] = len(tokenizer.encode(doc))\n        else:\n            token_count_per_cluster[label] += len(tokenizer.encode(doc))\n    return token_count_per_cluster\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef create_minimum_spanning_paths(self):\n    if self.cluster_labels is None:\n        raise ValueError(\"You must run cluster_embeddings() before creating minimum spanning paths.\")\n\n    unique_labels = np.unique(self.cluster_labels)\n    min_span_paths = []\n\n    for label in unique_labels:\n\n\n        # Get the indices of the current cluster\n        cluster_indices = np.where(self.cluster_labels == label)[0]\n\n        # Calculate the pairwise distances between embeddings in the cluster\n        cluster_embeddings = self.idea_index.embeddings[cluster_indices]\n        dist_matrix = squareform(pdist(cluster_embeddings))\n\n        # Create a graph from the distance matrix\n        graph = nx.from_numpy_array(dist_matrix)\n\n        # Compute the minimum spanning tree of the graph\n        min_span_tree = nx.minimum_spanning_tree(graph)\n\n        # Get the minimum spanning paths\n        min_span_paths_cluster = []\n        visited = set()\n        for u, v in min_span_tree.edges():\n            if u not in visited and v not in visited:\n                orig_u = cluster_indices[u]\n                orig_v = cluster_indices[v]\n                min_span_paths_cluster.append(orig_u)\n                visited.add(u)\n                visited.add(v)\n        # Add the last node to complete the path\n        min_span_paths_cluster.append(orig_v)\n\n        min_span_paths.append(min_span_paths_cluster)\n\n    self.min_span_paths = min_span_paths\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef plot_embeddings_with_path(self):\n    paths = self.min_span_paths\n    embeddings = self.idea_index.embeddings\n    title = \"Minimum Spanning Paths\"\n    tsne = TSNE(n_components=2, random_state=42)\n    reduced_embeddings = tsne.fit_transform(embeddings)\n\n    plt.figure(figsize=(10, 8))\n    colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n    for i, path in enumerate(paths):\n        path_embeddings = reduced_embeddings[path]\n        plt.scatter(\n            path_embeddings[:, 0],\n            path_embeddings[:, 1],\n            color=colors[i],\n            label=f\"Cluster {i}\",\n        )\n        for j in range(len(path) - 1):\n            plt.plot(\n                [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n                [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n                color=colors[i],\n            )\n    plt.title(title)\n    plt.legend()\n    plt.show()\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def get_clustered_ideas(self):\n    if self.cluster_labels is None:\n        raise ValueError(\"You must run cluster_embeddings() before getting clustered ideas.\")\n    \n    clustered_ideas = {}\n    for label, idea in zip(self.cluster_labels, self.idea_index.values):\n        if label not in clustered_ideas:\n            clustered_ideas[label] = [idea]\n        else:\n            clustered_ideas[label].append(idea)\n    \n    # Convert the dictionary to a list of lists (each list corresponds to a cluster)\n    return list(clustered_ideas.values())\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\nclass Summarizer:\n    def __init__(self, texts: list):\n        self.texts = texts\n\n    def summarize_texts(self) -> dict:\n        output = {}\n\n        with RateLimitedThreadPoolExecutor(max_workers=12, calls_per_minute=200, verbose = False) as executor:\n            future_to_text = {executor.submit(self._summarize_text, text): text for text in self.texts}\n            for future in as_completed(future_to_text):\n                text = future_to_text[future]\n                try:\n                    output[text] = future.result()\n                except Exception as exc:\n                    print(f'An exception occurred while summarizing text: {exc}')\n\n        return output\n\n    def _summarize_text(self, text: str) -> str:\n        summary = cohere_summarize(text, model=\"summarize-xlarge\", length=\"auto\", extractiveness=\"low\", format=\"auto\")\n        return summary\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, texts: list):\n    self.texts = texts\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef summarize_texts(self) -> dict:\n    output = {}\n\n    with RateLimitedThreadPoolExecutor(max_workers=12, calls_per_minute=200, verbose = False) as executor:\n        future_to_text = {executor.submit(self._summarize_text, text): text for text in self.texts}\n        for future in as_completed(future_to_text):\n            text = future_to_text[future]\n            try:\n                output[text] = future.result()\n            except Exception as exc:\n                print(f'An exception occurred while summarizing text: {exc}')\n\n    return output\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef _summarize_text(self, text: str) -> str:\n    summary = cohere_summarize(text, model=\"summarize-xlarge\", length=\"auto\", extractiveness=\"low\", format=\"auto\")\n    return summary\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\n\ndef generate_perspective_prompt(user_subject, user_perspective, seed_model = \"gpt-3.5-turbo\"):\n    start = perf_counter()\n    chat_instance = Chat(model=seed_model)\n    analyzer = SubjectPerspectiveAnalyzer(chat_instance)\n    output = analyzer.analyze_subject_perspective(user_subject, user_perspective)\n    end = perf_counter()\n    print(\"Time to analyze_perspective: \", end - start)\n\n    dataset_url = \"Cohere/wikipedia-22-12-simple-embeddings\"\n    start = perf_counter()\n    index = MemoryIndex(name=\"wiki_index\", load=True, is_batched=True,embedder=CohereEmbedder)\n    if len(index.values)>0:\n        loaded = True\n    else:\n        loaded = False\n\n    if not loaded:\n        print(\"Index not found, creating new index\")\n        index = MemoryIndex.from_hf_dataset(dataset_url, [\"title\", \"text\"],embeddings_column= \"emb\", name=\"wiki_index\", is_batched=True,embedder=CohereEmbedder)\n    end = perf_counter()\n    print(\"Time to index: \", end - start)\n\n    start = perf_counter()\n    ideation = Ideation(memory_index=index)\n    ideas = ideation.retrieve_ideas(output, k=40, max_tokens=10000)\n    token_count = 0\n    for idea in ideas:\n        token_count += idea[\"returned_tokens\"]\n    end = perf_counter()\n    print(\"Time to retrieve ideas: \", end - start)\n    print(\"Number of tokens: \", token_count)\n\n    start = perf_counter()\n    max_tokens_per_cluster = 20000\n    idea_cluster = IdeaCluster(ideas, max_tokens_per_cluster)\n    idea_cluster.cluster_embeddings()\n    time.sleep(0.5)\n    ideas = idea_cluster.get_clustered_ideas()\n    end = perf_counter()\n    combined_idea = []\n    ideas_tokens = []\n    for idea in ideas:\n        combined_idea.append(\"\\n\".join(idea))\n        ideas_tokens.append(tokenizer.encode(combined_idea[-1]))\n    print(\"Time to cluster ideas: \", end - start)\n    print(\"Number of clusters: \", len(ideas))\n    print(\"Number of tokens in each cluster: \", [len(tokens) for tokens in ideas_tokens])\n    start = perf_counter()\n    summarizer = Summarizer(combined_idea)\n    summaries = summarizer.summarize_texts()\n    end = perf_counter()\n    print(\"Time to summarize: \", end - start)\n    print(\"Number of summaries: \", len(summaries))\n    print(\"Number of tokens in each summary: \", [len(tokenizer.encode(summary)) for summary in summaries.values()])\n    start = perf_counter()\n    system_prompt = f\"\"\"With the summarized information from Cohere about the essential ideas, concenpts, priciples, and intersection points between {user_subject} and {user_perspective}, construct an appealing and context-aware chatbot prompt that impels the chatbot to respond with insights and perspectives born from the synergy of these two domains. Use the tips below to help you create an effective chatbot prompt: 1. Begin with a concise introduction: Initiate the chatbot prompt by setting the context, encompassing the user's specified subject and perspective. 2. Accentuate the intersection: Secure that the chatbot prompt underlines the connection between the user_subject and user_perspective, leading to more pertinent and perceptive responses. 3. Foster exploration: Ensure the chatbot prompt provokes the chatbot to delve into the main ideas, principles, and concepts from the summaries with a thoughtful and reflective approach. 4. Pose open-ended questions: Incorporate open-ended queries in the chatbot prompt, stimulating the chatbot to contemplate beyond the summaries and offer comprehensive responses. 5. Prioritize simplicity: Maintain the chatbot prompt's clarity and brevity, assisting the chatbot in comprehending the context and reacting suitably. 6. Steer the conversation: Craft the chatbot prompt in a manner that subtly directs the chatbot's answers, confirming they consistently focus on the user_subject and user_perspective. By leveraging these tips, develop a chatbot prompt that generates responses illustrative of the user's specified subject and perspective, culminating in a customized and significant interaction. Remember to conclude the prompt with 7 content pillars that will help the chatbot use the perspective at the best of its capacity. \"\"\".format(user_subject=user_subject, user_perspective=user_perspective)\n    for i, summary in enumerate(summaries.values()):\n        system_prompt += summary + \"\\n\\n\"\n    prompt_generator = Chat(name= \"prompt generator\",system_prompt=system_prompt, max_output_tokens= 2000, model = \"gpt-4\")\n    perspective_prompt = prompt_generator.reply(\"\", verbose=False)\n    additional_prompt =  \"\"\" \\n\\n Bear in mind that while engaging with the user, questions may arise that seem unrelated to the initial prompt. It's crucial to maintain flexibility and creativity, interpreting and addressing these topics through the unique prism provided by the initial prompt.\"\"\"\n    perspective_prompt += additional_prompt\n    end = perf_counter()\n    print(\"Time to generate prompt: \", end - start)\n    return perspective_prompt\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\nclass PerspectivePromptGenerator:\n    def __init__(self, subjects, perspectives, max_workers=10, calls_per_minute=20, base_filename=\"prompt_\"):\n        self.subjects = subjects\n        self.perspectives = perspectives\n        self.executor = RateLimitedThreadPoolExecutor(\n            max_workers=max_workers, \n            calls_per_minute=calls_per_minute\n        )\n        self.prompts = []\n        self.base_filename = base_filename\n        self.lock = threading.Lock()  # create a lock\n\n    def handle_future(self, future):\n        try:\n            result = future.result()\n            self.prompts.append(result)\n            complete_filename = self.base_filename + \"results.json\"\n            with self.lock:  # acquire the lock before writing to the file\n                self.save_prompts_to_json(complete_filename)\n        except Exception as e:\n            error_report = {\"error\": str(e), \"traceback\": traceback.format_exc()}\n            self.prompts.append(error_report)\n            complete_filename = self.base_filename + \"errors.json\"\n            with self.lock:  # acquire the lock before writing to the file\n                self.save_prompts_to_json(complete_filename)\n    \n    def generate_prompts(self):\n        for subject in self.subjects:\n            for perspective in self.perspectives:\n                future = self.executor.submit(\n                    generate_perspective_prompt, \n                    subject, \n                    perspective\n                )\n                future.add_done_callback(self.handle_future)\n        self.executor.shutdown(wait=True)\n        return self.prompts\n\n    def save_prompts_to_json(self, filename):\n        with open(filename, 'w') as f:\n            json.dump(self.prompts, f)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"def __init__(self, subjects, perspectives, max_workers=10, calls_per_minute=20, base_filename=\"prompt_\"):\n    self.subjects = subjects\n    self.perspectives = perspectives\n    self.executor = RateLimitedThreadPoolExecutor(\n        max_workers=max_workers, \n        calls_per_minute=calls_per_minute\n    )\n    self.prompts = []\n    self.base_filename = base_filename\n    self.lock = threading.Lock()  # create a lock\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef handle_future(self, future):\n    try:\n        result = future.result()\n        self.prompts.append(result)\n        complete_filename = self.base_filename + \"results.json\"\n        with self.lock:  # acquire the lock before writing to the file\n            self.save_prompts_to_json(complete_filename)\n    except Exception as e:\n        error_report = {\"error\": str(e), \"traceback\": traceback.format_exc()}\n        self.prompts.append(error_report)\n        complete_filename = self.base_filename + \"errors.json\"\n        with self.lock:  # acquire the lock before writing to the file\n            self.save_prompts_to_json(complete_filename)\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef generate_prompts(self):\n    for subject in self.subjects:\n        for perspective in self.perspectives:\n            future = self.executor.submit(\n                generate_perspective_prompt, \n                subject, \n                perspective\n            )\n            future.add_done_callback(self.handle_future)\n    self.executor.shutdown(wait=True)\n    return self.prompts\n"}]}
{"model":"gpt-3.5-turbo-16k-0613","messages":[{"role":"system","content":"You are a helpful Summarizer. Please summarize the meaning of the code I am gonna show you."},{"role":"user","content":"\ndef save_prompts_to_json(self, filename):\n    with open(filename, 'w') as f:\n        json.dump(self.prompts, f)\n"}]}
