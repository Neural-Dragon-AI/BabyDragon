{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import Union, List, Optional, Dict\n",
    "import datetime\n",
    "import uuid\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "# Base class for all bd_types\n",
    "class BDType(BaseModel):\n",
    "    source: str = Field(\"babydragon\", description=\"The source of the data.\")\n",
    "    timestamp: Optional[datetime.datetime] = Field(None, description=\"When the data was collected or created. If not provided, the current time is used.\")\n",
    "    id: uuid.UUID = Field(default_factory=uuid.uuid4, description=\"Unique identifier of the data.\")\n",
    "    data_name: Optional[str] = Field(None, description=\"Name of the data.\")\n",
    "    elements_name: Optional[List[str]] = Field(None, description=\"Names of the elements if the data is a list.\")\n",
    "\n",
    "    @field_validator(\"timestamp\")\n",
    "    def set_timestamp(cls, v):\n",
    "        return v or datetime.datetime.now()\n",
    "\n",
    "    @field_validator(\"id\")\n",
    "    def set_id(cls, values, **kwargs):\n",
    "        if \"id\" not in values:\n",
    "            values[\"id\"] = uuid.uuid4()\n",
    "        return values\n",
    "\n",
    "\n",
    "class NaturalLanguageSingle(BDType):\n",
    "    text: str = Field(..., description=\"The natural language text. It should be less than or equal to `max_tokens` in length when tokenized.\")\n",
    "    max_tokens: int = Field(8000, description=\"The maximum allowed length of the text in tokens. The default value is 8000.\")\n",
    "    \n",
    "    @field_validator(\"text\")\n",
    "    def validate_text(cls, v, info):\n",
    "        try:\n",
    "            # Tokenize the text and get the token count\n",
    "            token_count = len(tokenizer.encode(v))\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Failed to tokenize text.\") from e\n",
    "\n",
    "        # Get max_tokens from info.data, if not available, default to 8000\n",
    "        max_tokens = info.data.get(\"max_tokens\", 8000)\n",
    "\n",
    "        if token_count > max_tokens:\n",
    "            raise ValueError(f\"Text is longer than {max_tokens} tokens.\")\n",
    "\n",
    "        return v\n",
    "\n",
    "\n",
    "class NaturalLanguageList(BDType):\n",
    "    texts: List[NaturalLanguageSingle] = Field(..., description=\"A list of `NaturalLanguageSingle` objects. Each object should pass the validation requirements of the `NaturalLanguageSingle` class.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "\n",
    "# Function to generate a list of strings of specific length\n",
    "def generate_strings(num_strings, tokens_per_string):\n",
    "    return [string.ascii_lowercase * (tokens_per_string // 26) for _ in range(num_strings)]\n",
    "\n",
    "# Lengths of the lists to generate\n",
    "list_lengths = [10, 100, 1000, 10000, 100000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation time for a NaturalLanguageSingle list of length 10: 0.05450248718261719 seconds\n",
      "Initialization time for a NaturalLanguageList with a list of length 10: 0.0005013942718505859 seconds\n",
      "---\n",
      "Creation time for a NaturalLanguageSingle list of length 100: 0.504584789276123 seconds\n",
      "Initialization time for a NaturalLanguageList with a list of length 100: 0.0 seconds\n",
      "---\n",
      "Creation time for a NaturalLanguageSingle list of length 1000: 4.906872034072876 seconds\n",
      "Initialization time for a NaturalLanguageList with a list of length 1000: 0.0 seconds\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokens per string\n",
    "tokens_per_string = 4000\n",
    "\n",
    "for length in list_lengths[:-1]:\n",
    "    # Generate a list of strings\n",
    "    strings = generate_strings(length, tokens_per_string)\n",
    "    \n",
    "    # Start timer for NaturalLanguageSingle list creation\n",
    "    start_time_nls_list = time.time()\n",
    "    \n",
    "    # Create a NaturalLanguageSingle list\n",
    "    nls_list = [NaturalLanguageSingle(source='source', text=text) for text in strings]\n",
    "    \n",
    "    # End timer for NaturalLanguageSingle list creation\n",
    "    end_time_nls_list = time.time()\n",
    "    \n",
    "    # Start timer for NaturalLanguageList initialization\n",
    "    start_time_nll = time.time()\n",
    "    \n",
    "    # Initialize a NaturalLanguageList instance\n",
    "    nll = NaturalLanguageList(source='source', texts=nls_list)\n",
    "    \n",
    "    # End timer for NaturalLanguageList initialization\n",
    "    end_time_nll = time.time()\n",
    "    \n",
    "    # Print the creation and initialization times\n",
    "    print(f\"Creation time for a NaturalLanguageSingle list of length {length}: {end_time_nls_list - start_time_nls_list} seconds\")\n",
    "    print(f\"Initialization time for a NaturalLanguageList with a list of length {length}: {end_time_nll - start_time_nll} seconds\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to generate strings for size 10: 0.0 seconds\n",
      "Time to create DataFrame for size 10: 0.004997730255126953 seconds\n",
      "Time to create NaturalLanguageSingle instances using map for size 10: 0.08402609825134277 seconds\n",
      "Time to create NaturalLanguageList for size 10: 0.0010006427764892578 seconds\n",
      "Time to generate strings for size 100: 0.0005011558532714844 seconds\n",
      "Time to create DataFrame for size 100: 0.0005004405975341797 seconds\n",
      "Time to create NaturalLanguageSingle instances using map for size 100: 0.5065701007843018 seconds\n",
      "Time to create NaturalLanguageList for size 100: 0.0 seconds\n",
      "Time to generate strings for size 1000: 0.0009996891021728516 seconds\n",
      "Time to create DataFrame for size 1000: 0.005500316619873047 seconds\n",
      "Time to create NaturalLanguageSingle instances using map for size 1000: 4.95569920539856 seconds\n",
      "Time to create NaturalLanguageList for size 1000: 0.0 seconds\n",
      "Time to generate strings for size 10000: 0.015499353408813477 seconds\n",
      "Time to create DataFrame for size 10000: 0.04850292205810547 seconds\n",
      "Time to create NaturalLanguageSingle instances using map for size 10000: 49.702903032302856 seconds\n",
      "Time to create NaturalLanguageList for size 10000: 0.0009982585906982422 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import polars as pl\n",
    "import string\n",
    "\n",
    "def create_nls(text):\n",
    "    return NaturalLanguageSingle(source='source', text=text)\n",
    "    \n",
    "def generate_strings(num_strings, tokens_per_string):\n",
    "    return [string.ascii_lowercase * (tokens_per_string // 26) for _ in range(num_strings)]\n",
    "\n",
    "# For different sizes of the DataFrame\n",
    "for size in list_lengths[:-1]:\n",
    "    # Create a DataFrame with 'size' number of strings\n",
    "    start_time = time.time()\n",
    "    text = generate_strings(size, 4000)\n",
    "    print(f\"Time to generate strings for size {size}: {time.time() - start_time} seconds\")\n",
    "    start_time = time.time()\n",
    "    df = pl.DataFrame({\n",
    "        \"text\": text\n",
    "    })\n",
    "    print(f\"Time to create DataFrame for size {size}: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Create NaturalLanguageSingle instances and measure the time\n",
    "    start_time = time.time()\n",
    "    df = df.with_columns(pl.col(\"text\").apply(create_nls).alias(\"nls\"))\n",
    "    print(f\"Time to create NaturalLanguageSingle instances using map for size {size}: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Create a NaturalLanguageList and measure the time\n",
    "    start_time = time.time()\n",
    "    nll = NaturalLanguageList(source='source', texts=df['nls'].to_list())\n",
    "    print(f\"Time to create NaturalLanguageList for size {size}: {time.time() - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to generate strings for size 10: 0.003497600555419922 seconds\n",
      "Time to create DataFrame for size 10: 0.003002643585205078 seconds\n",
      "Time to create NaturalLanguageSingle instances for size 10: 0.006508588790893555 seconds\n",
      "Time to create NaturalLanguageList for size 10: 0.007002353668212891 seconds\n",
      "Time to generate strings for size 100: 0.0 seconds\n",
      "Time to create DataFrame for size 100: 0.0 seconds\n",
      "Time to create NaturalLanguageSingle instances for size 100: 0.000997781753540039 seconds\n",
      "Time to create NaturalLanguageList for size 100: 0.0 seconds\n",
      "Time to generate strings for size 1000: 0.0009996891021728516 seconds\n",
      "Time to create DataFrame for size 1000: 0.0030012130737304688 seconds\n",
      "Time to create NaturalLanguageSingle instances for size 1000: 0.006499290466308594 seconds\n",
      "Time to create NaturalLanguageList for size 1000: 0.0005011558532714844 seconds\n",
      "Time to generate strings for size 10000: 0.012498617172241211 seconds\n",
      "Time to create DataFrame for size 10000: 0.049504995346069336 seconds\n",
      "Time to create NaturalLanguageSingle instances for size 10000: 0.06700825691223145 seconds\n",
      "Time to create NaturalLanguageList for size 10000: 0.000499725341796875 seconds\n",
      "Time to generate strings for size 100000: 0.13251399993896484 seconds\n",
      "Time to create DataFrame for size 100000: 0.5060732364654541 seconds\n",
      "Time to create NaturalLanguageSingle instances for size 100000: 0.7060976028442383 seconds\n",
      "Time to create NaturalLanguageList for size 100000: 0.005010843276977539 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def generate_strings(num_strings, tokens_per_string):\n",
    "    return [string.ascii_lowercase * (tokens_per_string // 26) for _ in range(num_strings)]\n",
    "\n",
    "@lru_cache\n",
    "def create_nls(text):\n",
    "    return NaturalLanguageSingle(source='source', text=text)\n",
    "\n",
    "# For different sizes of the DataFrame\n",
    "for size in list_lengths:\n",
    "    # Create a DataFrame with 'size' number of strings\n",
    "    start_time = time.time()\n",
    "    text = generate_strings(size, 4000)\n",
    "    print(f\"Time to generate strings for size {size}: {time.time() - start_time} seconds\")\n",
    "    start_time = time.time()\n",
    "    df = pl.DataFrame({\n",
    "        \"text\": text\n",
    "    })\n",
    "    print(f\"Time to create DataFrame for size {size}: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Create NaturalLanguageSingle instances and measure the time\n",
    "    start_time = time.time()\n",
    "    df = df.with_columns(pl.col(\"text\").apply(create_nls).alias(\"nls\"))\n",
    "    print(f\"Time to create NaturalLanguageSingle instances for size {size}: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Create a NaturalLanguageList and measure the time\n",
    "    start_time = time.time()\n",
    "    nll = NaturalLanguageList(source='source', texts=df['nls'].to_list())\n",
    "    print(f\"Time to create NaturalLanguageList for size {size}: {time.time() - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to generate strings for size 10: 0.03099799156188965 seconds\n",
      "Time to create DataFrame for size 10: 0.01900005340576172 seconds\n",
      "Time to create NaturalLanguageSingle with Threading instances for size 10: 0.021015167236328125 seconds\n",
      "Time to create NaturalLanguageList for size 10: 0.0 seconds\n",
      "Time to generate strings for size 100: 0.000499725341796875 seconds\n",
      "Time to create DataFrame for size 100: 0.0004999637603759766 seconds\n",
      "Time to create NaturalLanguageSingle with Threading instances for size 100: 0.0709998607635498 seconds\n",
      "Time to create NaturalLanguageList for size 100: 0.0 seconds\n",
      "Time to generate strings for size 1000: 0.0005009174346923828 seconds\n",
      "Time to create DataFrame for size 1000: 0.003999233245849609 seconds\n",
      "Time to create NaturalLanguageSingle with Threading instances for size 1000: 0.645089864730835 seconds\n",
      "Time to create NaturalLanguageList for size 1000: 0.0 seconds\n",
      "Time to generate strings for size 10000: 0.020998477935791016 seconds\n",
      "Time to create DataFrame for size 10000: 0.05150580406188965 seconds\n",
      "Time to create NaturalLanguageSingle with Threading instances for size 10000: 6.326817274093628 seconds\n",
      "Time to create NaturalLanguageList for size 10000: 0.0010001659393310547 seconds\n",
      "Time to generate strings for size 100000: 0.1375107765197754 seconds\n",
      "Time to create DataFrame for size 100000: 0.4920785427093506 seconds\n",
      "Time to create NaturalLanguageSingle with Threading instances for size 100000: 63.56087875366211 seconds\n",
      "Time to create NaturalLanguageList for size 100000: 0.01900029182434082 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def generate_strings(num_strings, tokens_per_string):\n",
    "    return [string.ascii_lowercase * (tokens_per_string // 26) for _ in range(num_strings)]\n",
    "def create_nls(text):\n",
    "    return NaturalLanguageSingle(source='source', text=text)\n",
    "# For different sizes of the DataFrame\n",
    "for size in list_lengths:\n",
    "    # Create a DataFrame with 'size' number of strings\n",
    "    start_time = time.time()\n",
    "    text = generate_strings(size, 4000)\n",
    "    print(f\"Time to generate strings for size {size}: {time.time() - start_time} seconds\")\n",
    "    start_time = time.time()\n",
    "    df = pl.DataFrame({\n",
    "        \"text\": text\n",
    "    })\n",
    "    print(f\"Time to create DataFrame for size {size}: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Create NaturalLanguageSingle instances and measure the time\n",
    "    start_time = time.time()\n",
    "    df = df.with_columns(pl.col(\"text\").apply(create_nls, strategy= 'threading').alias(\"nls\"))\n",
    "    print(f\"Time to create NaturalLanguageSingle with Threading instances for size {size}: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Create a NaturalLanguageList and measure the time\n",
    "    start_time = time.time()\n",
    "    nll = NaturalLanguageList(source='source', texts=df['nls'].to_list())\n",
    "    print(f\"Time to create NaturalLanguageList for size {size}: {time.time() - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
