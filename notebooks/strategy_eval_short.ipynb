{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/notebooks/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon\n",
      "Loading index from /Users/danielhug/neuraldragon/gitensor/BabyDragon/notebooks/storage/openai_index_parallel\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/notebooks/storage/openai_index_parallel/openai_index_parallel_embeddings.npz\n",
      "Loading index from /Users/danielhug/neuraldragon/gitensor/BabyDragon/notebooks/storage/babydragon_code_memory_module_06_11_22\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/notebooks/storage/babydragon_code_memory_module_06_11_22/babydragon_code_memory_module_06_11_22_embeddings.npz\n",
      "Loading index from /Users/danielhug/neuraldragon/gitensor/BabyDragon/notebooks/storage/libcst_index_parallel\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/notebooks/storage/libcst_index_parallel/libcst_index_parallel_embeddings.npz\n",
      "Loading index from /Users/danielhug/neuraldragon/gitensor/BabyDragon/notebooks/storage/fastapi_index_parallel\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/notebooks/storage/fastapi_index_parallel/fastapi_index_parallel_embeddings.npz\n",
      "468 415\n",
      "Loading index from /Users/danielhug/neuraldragon/gitensor/BabyDragon/notebooks/storage/four_index_test_index\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/notebooks/storage/four_index_test_index/four_index_test_index_embeddings.npz\n",
      "Creating a new index from a faiss index and values list\n",
      "163  values in the index\n",
      "163  embeddings in the index\n",
      "Computing the adjacency matrix\n",
      "Embeddings shape:  (163, 1536)\n",
      "Computing the k-hop adjacency matrix and aggregated features\n",
      "Compute the k-hop adjacency matrix\n",
      "Aggregate the messages from the k-hop neighborhood:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 429.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the memory index\n",
      "Creating a new index\n",
      "0  values in the index\n",
      "0  embeddings in the index\n",
      "Creating a new index from a list of embeddings and values\n",
      "163  values in the index\n",
      "163  embeddings in the index\n",
      "Loading index from storage/four_index_test_question_kernel\n",
      "storage/four_index_test_question_kernel/four_index_test_question_kernel_embeddings.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import babydragon\n",
    "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
    "from babydragon.memory.indexes.python_index import PythonIndex\n",
    "from babydragon.memory.kernels.memory_kernel import MemoryKernel\n",
    "from babydragon.memory.kernels.multi_kernel import SpectralClusteringMultiKernel, HDBSCANMultiKernel\n",
    "from babydragon.memory.kernels.multi_kernel_visualization import MultiKernelVisualization\n",
    "from babydragon.chat.chat import Chat\n",
    "from babydragon.tasks.multi_kernel_task import MultiKernelTask\n",
    "import openai\n",
    "\n",
    "\n",
    "openai.api_key =  \"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\"\n",
    "\n",
    "babydragon_path = f'{os.path.dirname(os.path.abspath(babydragon.__file__))}'\n",
    "print(babydragon_path)\n",
    "\n",
    "\n",
    "save_path = \"/Users/danielhug/neuraldragon/gitensor/BabyDragon/notebooks/storage\"\n",
    "index = PythonIndex(directory_path=babydragon_path, name=\"openai_index_parallel\", save_path=save_path, load=True)\n",
    "index2 = PythonIndex(directory_path=babydragon_path,name=\"babydragon_code_memory_module_06_11_22\", save_path=save_path, load=True)\n",
    "index3 = PythonIndex(directory_path=babydragon_path,name=\"libcst_index_parallel\", save_path=save_path, load=True)\n",
    "index4 = PythonIndex(directory_path=babydragon_path,name=\"fastapi_index_parallel\", save_path=save_path, load=True)\n",
    "\n",
    "\n",
    "print(len(index.values), len(index2.values))\n",
    "\n",
    "\n",
    "test_index = MemoryIndex( name=\"four_index_test_index\", save_path=save_path, load=True)\n",
    "\n",
    "test_multi_kernel_dict = {\n",
    "    \"test_index_kernel\": MemoryKernel(mem_index = test_index, name=\"four_index_test_index_kernel\"),\n",
    "}\n",
    "\n",
    "vis = MultiKernelVisualization(HDBSCANMultiKernel(test_multi_kernel_dict))\n",
    "#vis.visualize_paths()\n",
    "\n",
    "test_question = MemoryIndex(name=\"four_index_test_question_kernel\", save_path=\"storage\", load=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new index\n",
      "0  values in the index\n",
      "0  embeddings in the index\n",
      "Index is available so using index prompts\n"
     ]
    }
   ],
   "source": [
    "from babydragon.chat.memory_chat import ContextManagedFifoVectorChat, VectorChat, FifoChat\n",
    "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def dummy_user_prompt(\n",
    "        question, k: int = 10, max_tokens: int = None\n",
    "    ) -> str:\n",
    "    return question\n",
    "\n",
    "index_dict = {\"openai_index_parallel\": index, \"babydragon_code_memory_module_06_11_22\": index2, \"libcst_index_parallel\": index3, \"fastapi_index_parallel\": index4}\n",
    "fifo_vector_chatbot = ContextManagedFifoVectorChat(model=\"gpt-3.5-turbo-16k\", index_dict=index_dict, user_prompt=dummy_user_prompt, max_memory=10000, max_index_memory=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the purpose of the `parallel_embeddings` function in the given code and what does it return?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_question.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing Trajectory for the current state of memory.\n",
      "INFO:root:Mean: (1536,)\n",
      "INFO:root:Heat Trajectory: [-21.152412, -19.682482, -22.222788, -92.46245]\n",
      "INFO:root:Heat dict: {'openai_index_parallel': 0.13601076846536245, 'babydragon_code_memory_module_06_11_22': 0.12655906181288382, 'libcst_index_parallel': 0.14289332084890974, 'fastapi_index_parallel': 0.594536848872844}\n",
      "INFO:root:Chosen Index: babydragon_code_memory_module_06_11_22 - Retrieving prompt from index.\n",
      "INFO:root:Number of values in Index babydragon_code_memory_module_06_11_22: 415\n",
      "INFO:root:Top K Hint: ['from typing import Any, List\\n\\nfrom babydragon.models.embedders.ada2 import OpenAiEmbedder\\nfrom babydragon.tasks.base_task import BaseTask\\n\\n\\nclass EmbeddingTask(BaseTask):\\n    def __init__(\\n        self,\\n        embedder: OpenAiEmbedder,\\n        values: List[Any],\\n        path: List[List[int]],\\n        max_workers: int = 1,\\n        task_id: str = \"task\",\\n        calls_per_minute: int = 1500,\\n        backup: bool = True,\\n    ):\\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup)\\n        self.embedder = embedder\\n        self.values = values\\n\\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\\n        # expected to work with a lig of a single element\\n        if len(sub_path) != 1:\\n            raise ValueError(\\n                \"Embedding task expected to work with a list of a single element\"\\n            )\\n        sub_results = {}\\n        for i in sub_path:\\n            embedded_value = self.embedder.embed(self.values[i])\\n            sub_results[i] = embedded_value\\n        return sub_results\\n\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings', '\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n', 'from typing import List\\n\\nimport libcst as cst\\nimport numpy as np\\nimport openai\\nimport tiktoken\\n\\nADA_EMBEDDING_SIZE = 1536\\nMAX_CONTEXT_LENGTH = 8100\\n\\ntokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n\\nclass OpenAiEmbedder:\\n    def get_embedding_size(self):\\n        return ADA_EMBEDDING_SIZE\\n\\n    def embed(self, data, verbose=False):\\n\\n        if type(data) is dict and \"content\" in data:\\n            if verbose is True:\\n                print(\"Embedding without mark\", data[\"content\"])\\n            out = openai.Embedding.create(\\n                input=data[\"content\"], engine=\"text-embedding-ada-002\"\\n            )\\n        else:\\n            if len(tokenizer.encode(data)) > MAX_CONTEXT_LENGTH:\\n                raise ValueError(f\" The input is too long for OpenAI, num tokens is {len(tokenizer.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\\n            if verbose is True:\\n                print(\"Embedding without preprocessing the input\", data)\\n            out = openai.Embedding.create(\\n                input=str(data), engine=\"text-embedding-ada-002\"\\n            )\\n        return out.data[0].embedding\\n\\n    def batch_embed(self, data: List[str]):\\n        if type(data) is dict and \"content\" in data:\\n            raise ValueError(\"Batch embedding not supported for dictionaries\")\\n        elif type(data) is str:\\n            return self.embed(data)\\n        elif type(data) is list:\\n            out = openai.Embedding.create(\\n                input=data, engine=\"text-embedding-ada-002\"\\n            )\\n            embeddings = []\\n            for embedding in out.data:\\n                embeddings.append(embedding.embedding)\\n            return embeddings\\n\\n\\n\\ndef parse_and_embed_functions(input_str: str) -> List[np.ndarray]:\\n    # Parse the input string with libcst\\n    module = cst.parse_module(input_str)\\n\\n    # Find all the functions in the module and embed them separately\\n    embeddings = []\\n    for node in module.body:\\n\\n        if isinstance(node, cst.FunctionDef) or isinstance(node, cst.ClassDef):\\n            func_str = cst.Module(body=[node]).code\\n            print(\"Function string\", func_str)\\n            embedding = openai.Embedding.create(\\n                input=str(func_str)[:MAX_CONTEXT_LENGTH],\\n                engine=\"text-embedding-ada-002\",\\n            )\\n            if embedding is not None:\\n                embeddings.append(embedding.data[0].embedding)\\n\\n    avg_embedding = avg_embeddings(embeddings)\\n    print(avg_embedding.shape)\\n    return avg_embedding\\n\\n\\ndef avg_embeddings(embeddings: List[np.ndarray]) -> np.ndarray:\\n    print(\"Embeddings len\", len(embeddings))\\n    # convert embeddings to numpy array\\n    embeddings = np.array(embeddings)\\n    print(\"Embedding Matrix Shape\", embeddings.shape)\\n    return np.array([np.sum(embeddings.T, axis=1)]).astype(np.float32)\\n', 'import textwrap as tr\\nfrom typing import List, Optional\\n\\nimport matplotlib.pyplot as plt\\nimport plotly.express as px\\nfrom scipy import spatial\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.metrics import average_precision_score, precision_recall_curve\\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\\n\\nimport openai\\nfrom openai.datalib.numpy_helper import numpy as np\\nfrom openai.datalib.pandas_helper import pandas as pd\\n\\n\\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\\ndef get_embedding(text: str, engine=\"text-similarity-davinci-001\", **kwargs) -> List[float]:\\n\\n    # replace newlines, which can negatively affect performance.\\n    text = text.replace(\"\\\\n\", \" \")\\n\\n    return openai.Embedding.create(input=[text], engine=engine, **kwargs)[\"data\"][0][\"embedding\"]\\n\\n\\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\\nasync def aget_embedding(\\n    text: str, engine=\"text-similarity-davinci-001\", **kwargs\\n) -> List[float]:\\n\\n    # replace newlines, which can negatively affect performance.\\n    text = text.replace(\"\\\\n\", \" \")\\n\\n    return (await openai.Embedding.acreate(input=[text], engine=engine, **kwargs))[\"data\"][0][\\n        \"embedding\"\\n    ]\\n\\n\\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\\ndef get_embeddings(\\n    list_of_text: List[str], engine=\"text-similarity-babbage-001\", **kwargs\\n) -> List[List[float]]:\\n    assert len(list_of_text) <= 2048, \"The batch size should not be larger than 2048.\"\\n\\n    # replace newlines, which can negatively affect performance.\\n    list_of_text = [text.replace(\"\\\\n\", \" \") for text in list_of_text]\\n\\n    data = openai.Embedding.create(input=list_of_text, engine=engine, **kwargs).data\\n    return [d[\"embedding\"] for d in data]\\n\\n\\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\\nasync def aget_embeddings(\\n    list_of_text: List[str], engine=\"text-similarity-babbage-001\", **kwargs\\n) -> List[List[float]]:\\n    assert len(list_of_text) <= 2048, \"The batch size should not be larger than 2048.\"\\n\\n    # replace newlines, which can negatively affect performance.\\n    list_of_text = [text.replace(\"\\\\n\", \" \") for text in list_of_text]\\n\\n    data = (await openai.Embedding.acreate(input=list_of_text, engine=engine, **kwargs)).data\\n    return [d[\"embedding\"] for d in data]\\n\\n\\ndef cosine_similarity(a, b):\\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\\n\\n\\ndef plot_multiclass_precision_recall(\\n    y_score, y_true_untransformed, class_list, classifier_name\\n):\\n    \"\"\"\\n    Precision-Recall plotting for a multiclass problem. It plots average precision-recall, per class precision recall and reference f1 contours.\\n\\n    Code slightly modified, but heavily based on https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\\n    \"\"\"\\n    n_classes = len(class_list)\\n    y_true = pd.concat(\\n        [(y_true_untransformed == class_list[i]) for i in range(n_classes)], axis=1\\n    ).values\\n\\n    # For each class\\n    precision = dict()\\n    recall = dict()\\n    average_precision = dict()\\n    for i in range(n_classes):\\n        precision[i], recall[i], _ = precision_recall_curve(y_true[:, i], y_score[:, i])\\n        average_precision[i] = average_precision_score(y_true[:, i], y_score[:, i])\\n\\n    # A \"micro-average\": quantifying score on all classes jointly\\n    precision_micro, recall_micro, _ = precision_recall_curve(\\n        y_true.ravel(), y_score.ravel()\\n    )\\n    average_precision_micro = average_precision_score(y_true, y_score, average=\"micro\")\\n    print(\\n        str(classifier_name)\\n        + \" - Average precision score over all classes: {0:0.2f}\".format(\\n            average_precision_micro\\n        )\\n    )\\n\\n    # setup plot details\\n    plt.figure(figsize=(9, 10))\\n    f_scores = np.linspace(0.2, 0.8, num=4)\\n    lines = []\\n    labels = []\\n    for f_score in f_scores:\\n        x = np.linspace(0.01, 1)\\n        y = f_score * x / (2 * x - f_score)\\n        (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\\n        plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\\n\\n    lines.append(l)\\n    labels.append(\"iso-f1 curves\")\\n    (l,) = plt.plot(recall_micro, precision_micro, color=\"gold\", lw=2)\\n    lines.append(l)\\n    labels.append(\\n        \"average Precision-recall (auprc = {0:0.2f})\" \"\".format(average_precision_micro)\\n    )\\n\\n    for i in range(n_classes):\\n        (l,) = plt.plot(recall[i], precision[i], lw=2)\\n        lines.append(l)\\n        labels.append(\\n            \"Precision-recall for class `{0}` (auprc = {1:0.2f})\"\\n            \"\".format(class_list[i], average_precision[i])\\n        )\\n\\n    fig = plt.gcf()\\n    fig.subplots_adjust(bottom=0.25)\\n    plt.xlim([0.0, 1.0])\\n    plt.ylim([0.0, 1.05])\\n    plt.xlabel(\"Recall\")\\n    plt.ylabel(\"Precision\")\\n    plt.title(f\"{classifier_name}: Precision-Recall curve for each class\")\\n    plt.legend(lines, labels)\\n\\n\\ndef distances_from_embeddings(\\n    query_embedding: List[float],\\n    embeddings: List[List[float]],\\n    distance_metric=\"cosine\",\\n) -> List[List]:\\n    \"\"\"Return the distances between a query embedding and a list of embeddings.\"\"\"\\n    distance_metrics = {\\n        \"cosine\": spatial.distance.cosine,\\n        \"L1\": spatial.distance.cityblock,\\n        \"L2\": spatial.distance.euclidean,\\n        \"Linf\": spatial.distance.chebyshev,\\n    }\\n    distances = [\\n        distance_metrics[distance_metric](query_embedding, embedding)\\n        for embedding in embeddings\\n    ]\\n    return distances\\n\\n\\ndef indices_of_nearest_neighbors_from_distances(distances) -> np.ndarray:\\n    \"\"\"Return a list of indices of nearest neighbors from a list of distances.\"\"\"\\n    return np.argsort(distances)\\n\\n\\ndef pca_components_from_embeddings(\\n    embeddings: List[List[float]], n_components=2\\n) -> np.ndarray:\\n    \"\"\"Return the PCA components of a list of embeddings.\"\"\"\\n    pca = PCA(n_components=n_components)\\n    array_of_embeddings = np.array(embeddings)\\n    return pca.fit_transform(array_of_embeddings)\\n\\n\\ndef tsne_components_from_embeddings(\\n    embeddings: List[List[float]], n_components=2, **kwargs\\n) -> np.ndarray:\\n    \"\"\"Returns t-SNE components of a list of embeddings.\"\"\"\\n    # use better defaults if not specified\\n    if \"init\" not in kwargs.keys():\\n        kwargs[\"init\"] = \"pca\"\\n    if \"learning_rate\" not in kwargs.keys():\\n        kwargs[\"learning_rate\"] = \"auto\"\\n    tsne = TSNE(n_components=n_components, **kwargs)\\n    array_of_embeddings = np.array(embeddings)\\n    return tsne.fit_transform(array_of_embeddings)\\n\\n\\ndef chart_from_components(\\n    components: np.ndarray,\\n    labels: Optional[List[str]] = None,\\n    strings: Optional[List[str]] = None,\\n    x_title=\"Component 0\",\\n    y_title=\"Component 1\",\\n    mark_size=5,\\n    **kwargs,\\n):\\n    \"\"\"Return an interactive 2D chart of embedding components.\"\"\"\\n    empty_list = [\"\" for _ in components]\\n    data = pd.DataFrame(\\n        {\\n            x_title: components[:, 0],\\n            y_title: components[:, 1],\\n            \"label\": labels if labels else empty_list,\\n            \"string\": [\"<br>\".join(tr.wrap(string, width=30)) for string in strings]\\n            if strings\\n            else empty_list,\\n        }\\n    )\\n    chart = px.scatter(\\n        data,\\n        x=x_title,\\n        y=y_title,\\n        color=\"label\" if labels else None,\\n        symbol=\"label\" if labels else None,\\n        hover_data=[\"string\"] if strings else None,\\n        **kwargs,\\n    ).update_traces(marker=dict(size=mark_size))\\n    return chart\\n\\n\\ndef chart_from_components_3D(\\n    components: np.ndarray,\\n    labels: Optional[List[str]] = None,\\n    strings: Optional[List[str]] = None,\\n    x_title: str = \"Component 0\",\\n    y_title: str = \"Component 1\",\\n    z_title: str = \"Compontent 2\",\\n    mark_size: int = 5,\\n    **kwargs,\\n):\\n    \"\"\"Return an interactive 3D chart of embedding components.\"\"\"\\n    empty_list = [\"\" for _ in components]\\n    data = pd.DataFrame(\\n        {\\n            x_title: components[:, 0],\\n            y_title: components[:, 1],\\n            z_title: components[:, 2],\\n            \"label\": labels if labels else empty_list,\\n            \"string\": [\"<br>\".join(tr.wrap(string, width=30)) for string in strings]\\n            if strings\\n            else empty_list,\\n        }\\n    )\\n    chart = px.scatter_3d(\\n        data,\\n        x=x_title,\\n        y=y_title,\\n        z=z_title,\\n        color=\"label\" if labels else None,\\n        symbol=\"label\" if labels else None,\\n        hover_data=[\"string\"] if strings else None,\\n        **kwargs,\\n    ).update_traces(marker=dict(size=mark_size))\\n    return chart\\n', 'import base64\\nimport time\\n\\nfrom openai import util\\nfrom openai.api_resources.abstract.engine_api_resource import EngineAPIResource\\nfrom openai.datalib.numpy_helper import assert_has_numpy\\nfrom openai.datalib.numpy_helper import numpy as np\\nfrom openai.error import TryAgain\\n\\n\\nclass Embedding(EngineAPIResource):\\n    OBJECT_NAME = \"embeddings\"\\n\\n    @classmethod\\n    def create(cls, *args, **kwargs):\\n        \"\"\"\\n        Creates a new embedding for the provided input and parameters.\\n\\n        See https://platform.openai.com/docs/api-reference/embeddings for a list\\n        of valid parameters.\\n        \"\"\"\\n        start = time.time()\\n        timeout = kwargs.pop(\"timeout\", None)\\n\\n        user_provided_encoding_format = kwargs.get(\"encoding_format\", None)\\n\\n        # If encoding format was not explicitly specified, we opaquely use base64 for performance\\n        if not user_provided_encoding_format:\\n            kwargs[\"encoding_format\"] = \"base64\"\\n\\n        while True:\\n            try:\\n                response = super().create(*args, **kwargs)\\n\\n                # If a user specifies base64, we\\'ll just return the encoded string.\\n                # This is only for the default case.\\n                if not user_provided_encoding_format:\\n                    for data in response.data:\\n\\n                        # If an engine isn\\'t using this optimization, don\\'t do anything\\n                        if type(data[\"embedding\"]) == str:\\n                            assert_has_numpy()\\n                            data[\"embedding\"] = np.frombuffer(\\n                                base64.b64decode(data[\"embedding\"]), dtype=\"float32\"\\n                            ).tolist()\\n\\n                return response\\n            except TryAgain as e:\\n                if timeout is not None and time.time() > start + timeout:\\n                    raise\\n\\n                util.log_info(\"Waiting for model to warm up\", error=e)\\n\\n    @classmethod\\n    async def acreate(cls, *args, **kwargs):\\n        \"\"\"\\n        Creates a new embedding for the provided input and parameters.\\n\\n        See https://platform.openai.com/docs/api-reference/embeddings for a list\\n        of valid parameters.\\n        \"\"\"\\n        start = time.time()\\n        timeout = kwargs.pop(\"timeout\", None)\\n\\n        user_provided_encoding_format = kwargs.get(\"encoding_format\", None)\\n\\n        # If encoding format was not explicitly specified, we opaquely use base64 for performance\\n        if not user_provided_encoding_format:\\n            kwargs[\"encoding_format\"] = \"base64\"\\n\\n        while True:\\n            try:\\n                response = await super().acreate(*args, **kwargs)\\n\\n                # If a user specifies base64, we\\'ll just return the encoded string.\\n                # This is only for the default case.\\n                if not user_provided_encoding_format:\\n                    for data in response.data:\\n\\n                        # If an engine isn\\'t using this optimization, don\\'t do anything\\n                        if type(data[\"embedding\"]) == str:\\n                            data[\"embedding\"] = np.frombuffer(\\n                                base64.b64decode(data[\"embedding\"]), dtype=\"float32\"\\n                            ).tolist()\\n\\n                return response\\n            except TryAgain as e:\\n                if timeout is not None and time.time() > start + timeout:\\n                    raise\\n\\n                util.log_info(\"Waiting for model to warm up\", error=e)\\n']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'The `parallel_embeddings` function in the given code is used to perform parallel embeddings of a list of values using an `EmbeddingTask`, which is a class that performs embedding of individual values. It takes the following parameters: `embedder` (an instance of the `OpenAiEmbedder` class), `values` (a list of values to be embedded), `max_workers` (the maximum number of workers to use for parallel execution), `backup` (a boolean value indicating whether to use backup embedding), and `name` (a string used to identify the task).\\n\\nThe function prepares the paths for the `EmbeddingTask` by creating a list of lists, where each inner list contains a single index from the `values` list. It then initializes an instance of the `EmbeddingTask` class with the given parameters and executes it using the `work()` method. The `work()` method is responsible for executing the task and returning the embeddings.\\n\\nFinally, the function sorts the embeddings based on their index and returns a list of embeddings.\\n\\nIn summary, the purpose of the `parallel_embeddings` function is to perform parallel embedding of a list of values using an `EmbeddingTask` and return the embeddings.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fifo_vector_chatbot.context_query(test_question.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Any, List\n",
    "\n",
    "from babydragon.chat.chat import Chat\n",
    "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
    "from babydragon.memory.threads.base_thread import BaseThread\n",
    "from babydragon.tasks.base_task import BaseTask\n",
    "from IPython.display import Markdown, display\n",
    "class LLMWriterMod(BaseTask):\n",
    "    def __init__(\n",
    "        self,\n",
    "        index: MemoryIndex,\n",
    "        path: List[List[int]],\n",
    "        chatbot: ContextManagedFifoVectorChat,\n",
    "        write_func=None,\n",
    "        context=None,\n",
    "        task_name=\"summary\",\n",
    "        max_workers: int = 1,\n",
    "        task_id: str = \"LLMWriteTask\",\n",
    "        calls_per_minute: int = 20,\n",
    "        backup: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a LLMWriteTask instance.\n",
    "\n",
    "        :param index: List of strings representing the queries.\n",
    "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
    "        :param chatbot: Chatbot instance used for executing queries.\n",
    "        :param max_workers: Maximum number of worker threads (default is 4).\n",
    "        \"\"\"\n",
    "        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\n",
    "        self.index = index\n",
    "        self.chatbot = chatbot\n",
    "        self.write_func = write_func if write_func else self.llm_response\n",
    "        self.new_index_name = self.index.name + f\"_{task_name}\"\n",
    "        self.context = context\n",
    "\n",
    "    def llm_response(self, chatbot: Chat, message: str, context=None, id=None):\n",
    "        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n",
    "        # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\n",
    "        #     return \"the message is too long to be processed\"\n",
    "        # moved the error catching to multi-threading but custom method could report the error here\n",
    "        return chatbot.context_query(message)\n",
    "\n",
    "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Execute a sub-task using a separate copy of the chatbot instance.\n",
    "\n",
    "        :param sub_path: List of indices representing the sub-task's sequence.\n",
    "        :return: List of strings representing the responses for each query in the sub-task.\n",
    "        \"\"\"\n",
    "        if self.parallel:\n",
    "            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
    "            chatbot_instance = copy.deepcopy(self.chatbot)\n",
    "        else:\n",
    "            chatbot_instance = self.chatbot\n",
    "        if isinstance(self.chatbot, BaseThread):\n",
    "            chatbot_instance.reset_memory()\n",
    "\n",
    "        sub_results = {}\n",
    "        for i in sub_path:\n",
    "            current_val = self.index.values[i]\n",
    "            display(Markdown(\"#### Question: \\n {question}\".format(question=current_val)))\n",
    "\n",
    "            response = self.write_func(\n",
    "                chatbot_instance, current_val, self.context, id=i\n",
    "            )\n",
    "            display(\n",
    "                    Markdown(\n",
    "                        \" #### Anwser: \\n {answer}\".format(\n",
    "                            answer=response['content']\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            sub_results[i] = response\n",
    "        return sub_results\n",
    "\n",
    "    def write(self):\n",
    "        content_to_write = self.work()\n",
    "        self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\n",
    "        self.new_index.save()\n",
    "        return self.new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up savepath\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'storage'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_index = test_question\n",
    "path = vis.memory_kernel_group.path_group['test_index_kernel']\n",
    "answer_eval_task = LLMWriterMod(\n",
    "    index=target_index,\n",
    "    path=path,\n",
    "    chatbot=fifo_vector_chatbot,\n",
    "    max_workers=1,\n",
    "    task_id=\"answer3\",\n",
    ")\n",
    "answer_eval_task.save_path = \"storage\"\n",
    "answer_eval_task.save_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " What is the purpose of the `parallel_embeddings` function in the given code and what does it return?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing Trajectory for the current state of memory.\n",
      "INFO:root:Mean: (1536,)\n",
      "INFO:root:Heat Trajectory: [-21.15241, -19.682482, -22.222788, -92.46245]\n",
      "INFO:root:Heat dict: {'openai_index_parallel': 0.13601075786912198, 'babydragon_code_memory_module_06_11_22': 0.12655906336504466, 'libcst_index_parallel': 0.14289332260139917, 'fastapi_index_parallel': 0.5945368561644342}\n",
      "INFO:root:Chosen Index: babydragon_code_memory_module_06_11_22 - Retrieving prompt from index.\n",
      "INFO:root:Number of values in Index babydragon_code_memory_module_06_11_22: 415\n",
      "INFO:root:Top K Hint: ['from typing import Any, List\\n\\nfrom babydragon.models.embedders.ada2 import OpenAiEmbedder\\nfrom babydragon.tasks.base_task import BaseTask\\n\\n\\nclass EmbeddingTask(BaseTask):\\n    def __init__(\\n        self,\\n        embedder: OpenAiEmbedder,\\n        values: List[Any],\\n        path: List[List[int]],\\n        max_workers: int = 1,\\n        task_id: str = \"task\",\\n        calls_per_minute: int = 1500,\\n        backup: bool = True,\\n    ):\\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup)\\n        self.embedder = embedder\\n        self.values = values\\n\\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\\n        # expected to work with a lig of a single element\\n        if len(sub_path) != 1:\\n            raise ValueError(\\n                \"Embedding task expected to work with a list of a single element\"\\n            )\\n        sub_results = {}\\n        for i in sub_path:\\n            embedded_value = self.embedder.embed(self.values[i])\\n            sub_results[i] = embedded_value\\n        return sub_results\\n\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings', '\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n', 'from typing import List\\n\\nimport libcst as cst\\nimport numpy as np\\nimport openai\\nimport tiktoken\\n\\nADA_EMBEDDING_SIZE = 1536\\nMAX_CONTEXT_LENGTH = 8100\\n\\ntokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n\\nclass OpenAiEmbedder:\\n    def get_embedding_size(self):\\n        return ADA_EMBEDDING_SIZE\\n\\n    def embed(self, data, verbose=False):\\n\\n        if type(data) is dict and \"content\" in data:\\n            if verbose is True:\\n                print(\"Embedding without mark\", data[\"content\"])\\n            out = openai.Embedding.create(\\n                input=data[\"content\"], engine=\"text-embedding-ada-002\"\\n            )\\n        else:\\n            if len(tokenizer.encode(data)) > MAX_CONTEXT_LENGTH:\\n                raise ValueError(f\" The input is too long for OpenAI, num tokens is {len(tokenizer.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\\n            if verbose is True:\\n                print(\"Embedding without preprocessing the input\", data)\\n            out = openai.Embedding.create(\\n                input=str(data), engine=\"text-embedding-ada-002\"\\n            )\\n        return out.data[0].embedding\\n\\n    def batch_embed(self, data: List[str]):\\n        if type(data) is dict and \"content\" in data:\\n            raise ValueError(\"Batch embedding not supported for dictionaries\")\\n        elif type(data) is str:\\n            return self.embed(data)\\n        elif type(data) is list:\\n            out = openai.Embedding.create(\\n                input=data, engine=\"text-embedding-ada-002\"\\n            )\\n            embeddings = []\\n            for embedding in out.data:\\n                embeddings.append(embedding.embedding)\\n            return embeddings\\n\\n\\n\\ndef parse_and_embed_functions(input_str: str) -> List[np.ndarray]:\\n    # Parse the input string with libcst\\n    module = cst.parse_module(input_str)\\n\\n    # Find all the functions in the module and embed them separately\\n    embeddings = []\\n    for node in module.body:\\n\\n        if isinstance(node, cst.FunctionDef) or isinstance(node, cst.ClassDef):\\n            func_str = cst.Module(body=[node]).code\\n            print(\"Function string\", func_str)\\n            embedding = openai.Embedding.create(\\n                input=str(func_str)[:MAX_CONTEXT_LENGTH],\\n                engine=\"text-embedding-ada-002\",\\n            )\\n            if embedding is not None:\\n                embeddings.append(embedding.data[0].embedding)\\n\\n    avg_embedding = avg_embeddings(embeddings)\\n    print(avg_embedding.shape)\\n    return avg_embedding\\n\\n\\ndef avg_embeddings(embeddings: List[np.ndarray]) -> np.ndarray:\\n    print(\"Embeddings len\", len(embeddings))\\n    # convert embeddings to numpy array\\n    embeddings = np.array(embeddings)\\n    print(\"Embedding Matrix Shape\", embeddings.shape)\\n    return np.array([np.sum(embeddings.T, axis=1)]).astype(np.float32)\\n', 'import textwrap as tr\\nfrom typing import List, Optional\\n\\nimport matplotlib.pyplot as plt\\nimport plotly.express as px\\nfrom scipy import spatial\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.metrics import average_precision_score, precision_recall_curve\\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\\n\\nimport openai\\nfrom openai.datalib.numpy_helper import numpy as np\\nfrom openai.datalib.pandas_helper import pandas as pd\\n\\n\\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\\ndef get_embedding(text: str, engine=\"text-similarity-davinci-001\", **kwargs) -> List[float]:\\n\\n    # replace newlines, which can negatively affect performance.\\n    text = text.replace(\"\\\\n\", \" \")\\n\\n    return openai.Embedding.create(input=[text], engine=engine, **kwargs)[\"data\"][0][\"embedding\"]\\n\\n\\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\\nasync def aget_embedding(\\n    text: str, engine=\"text-similarity-davinci-001\", **kwargs\\n) -> List[float]:\\n\\n    # replace newlines, which can negatively affect performance.\\n    text = text.replace(\"\\\\n\", \" \")\\n\\n    return (await openai.Embedding.acreate(input=[text], engine=engine, **kwargs))[\"data\"][0][\\n        \"embedding\"\\n    ]\\n\\n\\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\\ndef get_embeddings(\\n    list_of_text: List[str], engine=\"text-similarity-babbage-001\", **kwargs\\n) -> List[List[float]]:\\n    assert len(list_of_text) <= 2048, \"The batch size should not be larger than 2048.\"\\n\\n    # replace newlines, which can negatively affect performance.\\n    list_of_text = [text.replace(\"\\\\n\", \" \") for text in list_of_text]\\n\\n    data = openai.Embedding.create(input=list_of_text, engine=engine, **kwargs).data\\n    return [d[\"embedding\"] for d in data]\\n\\n\\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\\nasync def aget_embeddings(\\n    list_of_text: List[str], engine=\"text-similarity-babbage-001\", **kwargs\\n) -> List[List[float]]:\\n    assert len(list_of_text) <= 2048, \"The batch size should not be larger than 2048.\"\\n\\n    # replace newlines, which can negatively affect performance.\\n    list_of_text = [text.replace(\"\\\\n\", \" \") for text in list_of_text]\\n\\n    data = (await openai.Embedding.acreate(input=list_of_text, engine=engine, **kwargs)).data\\n    return [d[\"embedding\"] for d in data]\\n\\n\\ndef cosine_similarity(a, b):\\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\\n\\n\\ndef plot_multiclass_precision_recall(\\n    y_score, y_true_untransformed, class_list, classifier_name\\n):\\n    \"\"\"\\n    Precision-Recall plotting for a multiclass problem. It plots average precision-recall, per class precision recall and reference f1 contours.\\n\\n    Code slightly modified, but heavily based on https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\\n    \"\"\"\\n    n_classes = len(class_list)\\n    y_true = pd.concat(\\n        [(y_true_untransformed == class_list[i]) for i in range(n_classes)], axis=1\\n    ).values\\n\\n    # For each class\\n    precision = dict()\\n    recall = dict()\\n    average_precision = dict()\\n    for i in range(n_classes):\\n        precision[i], recall[i], _ = precision_recall_curve(y_true[:, i], y_score[:, i])\\n        average_precision[i] = average_precision_score(y_true[:, i], y_score[:, i])\\n\\n    # A \"micro-average\": quantifying score on all classes jointly\\n    precision_micro, recall_micro, _ = precision_recall_curve(\\n        y_true.ravel(), y_score.ravel()\\n    )\\n    average_precision_micro = average_precision_score(y_true, y_score, average=\"micro\")\\n    print(\\n        str(classifier_name)\\n        + \" - Average precision score over all classes: {0:0.2f}\".format(\\n            average_precision_micro\\n        )\\n    )\\n\\n    # setup plot details\\n    plt.figure(figsize=(9, 10))\\n    f_scores = np.linspace(0.2, 0.8, num=4)\\n    lines = []\\n    labels = []\\n    for f_score in f_scores:\\n        x = np.linspace(0.01, 1)\\n        y = f_score * x / (2 * x - f_score)\\n        (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\\n        plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\\n\\n    lines.append(l)\\n    labels.append(\"iso-f1 curves\")\\n    (l,) = plt.plot(recall_micro, precision_micro, color=\"gold\", lw=2)\\n    lines.append(l)\\n    labels.append(\\n        \"average Precision-recall (auprc = {0:0.2f})\" \"\".format(average_precision_micro)\\n    )\\n\\n    for i in range(n_classes):\\n        (l,) = plt.plot(recall[i], precision[i], lw=2)\\n        lines.append(l)\\n        labels.append(\\n            \"Precision-recall for class `{0}` (auprc = {1:0.2f})\"\\n            \"\".format(class_list[i], average_precision[i])\\n        )\\n\\n    fig = plt.gcf()\\n    fig.subplots_adjust(bottom=0.25)\\n    plt.xlim([0.0, 1.0])\\n    plt.ylim([0.0, 1.05])\\n    plt.xlabel(\"Recall\")\\n    plt.ylabel(\"Precision\")\\n    plt.title(f\"{classifier_name}: Precision-Recall curve for each class\")\\n    plt.legend(lines, labels)\\n\\n\\ndef distances_from_embeddings(\\n    query_embedding: List[float],\\n    embeddings: List[List[float]],\\n    distance_metric=\"cosine\",\\n) -> List[List]:\\n    \"\"\"Return the distances between a query embedding and a list of embeddings.\"\"\"\\n    distance_metrics = {\\n        \"cosine\": spatial.distance.cosine,\\n        \"L1\": spatial.distance.cityblock,\\n        \"L2\": spatial.distance.euclidean,\\n        \"Linf\": spatial.distance.chebyshev,\\n    }\\n    distances = [\\n        distance_metrics[distance_metric](query_embedding, embedding)\\n        for embedding in embeddings\\n    ]\\n    return distances\\n\\n\\ndef indices_of_nearest_neighbors_from_distances(distances) -> np.ndarray:\\n    \"\"\"Return a list of indices of nearest neighbors from a list of distances.\"\"\"\\n    return np.argsort(distances)\\n\\n\\ndef pca_components_from_embeddings(\\n    embeddings: List[List[float]], n_components=2\\n) -> np.ndarray:\\n    \"\"\"Return the PCA components of a list of embeddings.\"\"\"\\n    pca = PCA(n_components=n_components)\\n    array_of_embeddings = np.array(embeddings)\\n    return pca.fit_transform(array_of_embeddings)\\n\\n\\ndef tsne_components_from_embeddings(\\n    embeddings: List[List[float]], n_components=2, **kwargs\\n) -> np.ndarray:\\n    \"\"\"Returns t-SNE components of a list of embeddings.\"\"\"\\n    # use better defaults if not specified\\n    if \"init\" not in kwargs.keys():\\n        kwargs[\"init\"] = \"pca\"\\n    if \"learning_rate\" not in kwargs.keys():\\n        kwargs[\"learning_rate\"] = \"auto\"\\n    tsne = TSNE(n_components=n_components, **kwargs)\\n    array_of_embeddings = np.array(embeddings)\\n    return tsne.fit_transform(array_of_embeddings)\\n\\n\\ndef chart_from_components(\\n    components: np.ndarray,\\n    labels: Optional[List[str]] = None,\\n    strings: Optional[List[str]] = None,\\n    x_title=\"Component 0\",\\n    y_title=\"Component 1\",\\n    mark_size=5,\\n    **kwargs,\\n):\\n    \"\"\"Return an interactive 2D chart of embedding components.\"\"\"\\n    empty_list = [\"\" for _ in components]\\n    data = pd.DataFrame(\\n        {\\n            x_title: components[:, 0],\\n            y_title: components[:, 1],\\n            \"label\": labels if labels else empty_list,\\n            \"string\": [\"<br>\".join(tr.wrap(string, width=30)) for string in strings]\\n            if strings\\n            else empty_list,\\n        }\\n    )\\n    chart = px.scatter(\\n        data,\\n        x=x_title,\\n        y=y_title,\\n        color=\"label\" if labels else None,\\n        symbol=\"label\" if labels else None,\\n        hover_data=[\"string\"] if strings else None,\\n        **kwargs,\\n    ).update_traces(marker=dict(size=mark_size))\\n    return chart\\n\\n\\ndef chart_from_components_3D(\\n    components: np.ndarray,\\n    labels: Optional[List[str]] = None,\\n    strings: Optional[List[str]] = None,\\n    x_title: str = \"Component 0\",\\n    y_title: str = \"Component 1\",\\n    z_title: str = \"Compontent 2\",\\n    mark_size: int = 5,\\n    **kwargs,\\n):\\n    \"\"\"Return an interactive 3D chart of embedding components.\"\"\"\\n    empty_list = [\"\" for _ in components]\\n    data = pd.DataFrame(\\n        {\\n            x_title: components[:, 0],\\n            y_title: components[:, 1],\\n            z_title: components[:, 2],\\n            \"label\": labels if labels else empty_list,\\n            \"string\": [\"<br>\".join(tr.wrap(string, width=30)) for string in strings]\\n            if strings\\n            else empty_list,\\n        }\\n    )\\n    chart = px.scatter_3d(\\n        data,\\n        x=x_title,\\n        y=y_title,\\n        z=z_title,\\n        color=\"label\" if labels else None,\\n        symbol=\"label\" if labels else None,\\n        hover_data=[\"string\"] if strings else None,\\n        **kwargs,\\n    ).update_traces(marker=dict(size=mark_size))\\n    return chart\\n', 'import base64\\nimport time\\n\\nfrom openai import util\\nfrom openai.api_resources.abstract.engine_api_resource import EngineAPIResource\\nfrom openai.datalib.numpy_helper import assert_has_numpy\\nfrom openai.datalib.numpy_helper import numpy as np\\nfrom openai.error import TryAgain\\n\\n\\nclass Embedding(EngineAPIResource):\\n    OBJECT_NAME = \"embeddings\"\\n\\n    @classmethod\\n    def create(cls, *args, **kwargs):\\n        \"\"\"\\n        Creates a new embedding for the provided input and parameters.\\n\\n        See https://platform.openai.com/docs/api-reference/embeddings for a list\\n        of valid parameters.\\n        \"\"\"\\n        start = time.time()\\n        timeout = kwargs.pop(\"timeout\", None)\\n\\n        user_provided_encoding_format = kwargs.get(\"encoding_format\", None)\\n\\n        # If encoding format was not explicitly specified, we opaquely use base64 for performance\\n        if not user_provided_encoding_format:\\n            kwargs[\"encoding_format\"] = \"base64\"\\n\\n        while True:\\n            try:\\n                response = super().create(*args, **kwargs)\\n\\n                # If a user specifies base64, we\\'ll just return the encoded string.\\n                # This is only for the default case.\\n                if not user_provided_encoding_format:\\n                    for data in response.data:\\n\\n                        # If an engine isn\\'t using this optimization, don\\'t do anything\\n                        if type(data[\"embedding\"]) == str:\\n                            assert_has_numpy()\\n                            data[\"embedding\"] = np.frombuffer(\\n                                base64.b64decode(data[\"embedding\"]), dtype=\"float32\"\\n                            ).tolist()\\n\\n                return response\\n            except TryAgain as e:\\n                if timeout is not None and time.time() > start + timeout:\\n                    raise\\n\\n                util.log_info(\"Waiting for model to warm up\", error=e)\\n\\n    @classmethod\\n    async def acreate(cls, *args, **kwargs):\\n        \"\"\"\\n        Creates a new embedding for the provided input and parameters.\\n\\n        See https://platform.openai.com/docs/api-reference/embeddings for a list\\n        of valid parameters.\\n        \"\"\"\\n        start = time.time()\\n        timeout = kwargs.pop(\"timeout\", None)\\n\\n        user_provided_encoding_format = kwargs.get(\"encoding_format\", None)\\n\\n        # If encoding format was not explicitly specified, we opaquely use base64 for performance\\n        if not user_provided_encoding_format:\\n            kwargs[\"encoding_format\"] = \"base64\"\\n\\n        while True:\\n            try:\\n                response = await super().acreate(*args, **kwargs)\\n\\n                # If a user specifies base64, we\\'ll just return the encoded string.\\n                # This is only for the default case.\\n                if not user_provided_encoding_format:\\n                    for data in response.data:\\n\\n                        # If an engine isn\\'t using this optimization, don\\'t do anything\\n                        if type(data[\"embedding\"]) == str:\\n                            data[\"embedding\"] = np.frombuffer(\\n                                base64.b64decode(data[\"embedding\"]), dtype=\"float32\"\\n                            ).tolist()\\n\\n                return response\\n            except TryAgain as e:\\n                if timeout is not None and time.time() > start + timeout:\\n                    raise\\n\\n                util.log_info(\"Waiting for model to warm up\", error=e)\\n']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The memory BaseThread is full, the message with index 0 was moved to the longterm memory"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The purpose of the `parallel_embeddings` function in the given code is to perform parallel embedding of a list of values using the `EmbeddingTask` class. \n",
       "\n",
       "The function takes in several parameters: `embedder`, `values`, `max_workers`, `backup`, and `name`. \n",
       "- `embedder` is an instance of the `OpenAiEmbedder` class used for embedding the values.\n",
       "- `values` is the list of values to be embedded.\n",
       "- `max_workers` specifies the maximum number of worker processes to use for parallel execution.\n",
       "- `backup` is a boolean flag that determines whether to create a backup of the embedding task.\n",
       "- `name` is a string that is used to generate a unique task ID for the embedding task.\n",
       "\n",
       "The function starts by preparing the paths for the `EmbeddingTask` by creating a list of paths where each path contains a single index from the range of the number of values. \n",
       "\n",
       "Next, it initializes an instance of the `EmbeddingTask` class with the given parameters and executes it using the `work()` method. The `work()` method executes the embedding task in parallel, processing each sub-task in a separate worker process.\n",
       "\n",
       "Finally, the function gathers the results from the embedding task and returns a list of embeddings, sorted based on the original order of the values."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " What is the purpose of the `__init__` function in the given code?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing Trajectory for the current state of memory.\n",
      "INFO:root:Mean: (1536,)\n",
      "INFO:root:Heat Trajectory: [-31.086157, -29.855848, -33.301956, -124.13779]\n",
      "INFO:root:Heat dict: {'openai_index_parallel': 0.14234777906759713, 'babydragon_code_memory_module_06_11_22': 0.13671402742462047, 'libcst_index_parallel': 0.1524942283470543, 'fastapi_index_parallel': 0.568443965160728}\n",
      "INFO:root:Chosen Index: babydragon_code_memory_module_06_11_22 - Retrieving prompt from index.\n",
      "INFO:root:Number of values in Index babydragon_code_memory_module_06_11_22: 415\n",
      "INFO:root:Top K Hint: ['def __init__(self, code: str = None):\\n\\n    self.code = code\\n    self.output_code = None\\n', 'def __init__(self):\\n    self.function_source_codes = []\\n    self.function_nodes = []\\n    self.class_source_codes = []\\n    self.class_nodes = []\\n', '# This is the __init__.py file for the package.\\n', 'def __init__(\\n    self,\\n    key=None,\\n    api_base=None,\\n    api_type=None,\\n    api_version=None,\\n    organization=None,\\n):\\n    self.api_base = api_base or openai.api_base\\n    self.api_key = key or util.default_api_key()\\n    self.api_type = (\\n        ApiType.from_str(api_type)\\n        if api_type\\n        else ApiType.from_str(openai.api_type)\\n    )\\n    self.api_version = api_version or openai.api_version\\n    self.organization = organization or openai.organization\\n', '\\ndef __init__(self, engine: Optional[str] = None, **kwargs):\\n    super().__init__(engine=engine, **kwargs)\\n']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The purpose of the `__init__` function in the given code is to initialize the attributes of the class or object. It is called automatically when an object is created from the class. The code inside the `__init__` function is executed to set up the initial state of the object."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " What are the parameters and return type of the `create_paths` function in the `ClusterPaths` class and its subclasses `HDBSCANPaths` and `SpectralClusteringPaths`?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing Trajectory for the current state of memory.\n",
      "INFO:root:Mean: (1536,)\n",
      "INFO:root:Heat Trajectory: [-24.00449, -18.175482, -20.939137, -83.1896]\n",
      "INFO:root:Heat dict: {'openai_index_parallel': 0.16406740588736501, 'babydragon_code_memory_module_06_11_22': 0.12422693261355, 'libcst_index_parallel': 0.14311613462389317, 'fastapi_index_parallel': 0.5685895268751918}\n",
      "INFO:root:Chosen Index: babydragon_code_memory_module_06_11_22 - Retrieving prompt from index.\n",
      "INFO:root:Number of values in Index babydragon_code_memory_module_06_11_22: 415\n",
      "INFO:root:Top K Hint: ['from typing import List\\n\\nimport hdbscan\\nimport numpy as np\\nfrom sklearn.cluster import SpectralClustering\\n\\n\\nclass ClusterPaths:\\n    def create_paths(\\n        self, embeddings: np.ndarray, num_clusters: int\\n    ) -> List[List[int]]:\\n        raise NotImplementedError\\n\\n\\nclass HDBSCANPaths(ClusterPaths):\\n    def create_paths(\\n        self, embeddings: np.ndarray, num_clusters: int\\n    ) -> List[List[int]]:\\n        clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\\n        cluster_assignments = clusterer.fit_predict(embeddings)\\n        paths = [[] for _ in range(num_clusters)]\\n        for i, cluster in enumerate(cluster_assignments):\\n            paths[cluster].append(i)\\n        paths = [path for path in paths if path]\\n        return paths\\n\\n\\nclass SpectralClusteringPaths(ClusterPaths):\\n    def create_paths(\\n        self, A: np.ndarray, num_clusters: int\\n    ) -> List[List[int]]:\\n        n_samples = A.shape[0]\\n        n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\\n        spectral_clustering = SpectralClustering(\\n            n_clusters=num_clusters,\\n            affinity=\"precomputed\",\\n            n_neighbors=n_neighbors,\\n            random_state=42,\\n        )\\n        cluster_assignments = spectral_clustering.fit_predict(A)\\n        paths = [[] for _ in range(num_clusters)]\\n        for i, cluster in enumerate(cluster_assignments):\\n            paths[cluster].append(i)\\n        paths = [path for path in paths if path]\\n        return paths\\n', 'def create_paths(\\n    self, embeddings: np.ndarray, num_clusters: int\\n) -> List[List[int]]:\\n    raise NotImplementedError\\n', 'def create_paths(\\n    self, embeddings: np.ndarray, num_clusters: int\\n) -> List[List[int]]:\\n    clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\\n    cluster_assignments = clusterer.fit_predict(embeddings)\\n    paths = [[] for _ in range(num_clusters)]\\n    for i, cluster in enumerate(cluster_assignments):\\n        paths[cluster].append(i)\\n    paths = [path for path in paths if path]\\n    return paths\\n', '\\n\\n@add_slots\\n@dataclass(frozen=True)\\nclass Parameters(CSTNode):\\n    \"\"\"\\n    A function or lambda parameter list.\\n    \"\"\"\\n\\n    #: Positional parameters, with or without defaults. Positional parameters\\n    #: with defaults must all be after those without defaults.\\n    params: Sequence[Param] = ()\\n\\n    # Optional parameter that captures unspecified positional arguments or a sentinel\\n    # star that dictates parameters following are kwonly args.\\n    star_arg: Union[Param, ParamStar, MaybeSentinel] = MaybeSentinel.DEFAULT\\n\\n    #: Keyword-only params that may or may not have defaults.\\n    kwonly_params: Sequence[Param] = ()\\n\\n    #: Optional parameter that captures unspecified kwargs.\\n    star_kwarg: Optional[Param] = None\\n\\n    #: Positional-only parameters, with or without defaults. Positional-only\\n    #: parameters with defaults must all be after those without defaults.\\n    posonly_params: Sequence[Param] = ()\\n\\n    #: Optional sentinel that dictates parameters preceeding are positional-only\\n    #: args.\\n    posonly_ind: Union[ParamSlash, MaybeSentinel] = MaybeSentinel.DEFAULT\\n\\n    def _validate_stars_sequence(self, vals: Sequence[Param], *, section: str) -> None:\\n        if len(vals) == 0:\\n            return\\n        for val in vals:\\n            if isinstance(val.star, str) and val.star != \"\":\\n                raise CSTValidationError(\\n                    f\"Expecting a star prefix of \\'\\' for {section} Param.\"\\n                )\\n\\n    def _validate_posonly_ind(self) -> None:\\n        if isinstance(self.posonly_ind, ParamSlash) and len(self.posonly_params) == 0:\\n            raise CSTValidationError(\\n                \"Must have at least one posonly param if ParamSlash is used.\"\\n            )\\n\\n    def _validate_kwonly_star(self) -> None:\\n        if isinstance(self.star_arg, ParamStar) and len(self.kwonly_params) == 0:\\n            raise CSTValidationError(\\n                \"Must have at least one kwonly param if ParamStar is used.\"\\n            )\\n\\n    def _validate_defaults(self) -> None:\\n        seen_default = False\\n        # pyre-fixme[60]: Concatenation not yet support for multiple variadic\\n        #  tuples: `*self.posonly_params, *self.params`.\\n        for param in (*self.posonly_params, *self.params):\\n            if param.default:\\n                # Mark that we\\'ve moved onto defaults\\n                if not seen_default:\\n                    seen_default = True\\n            else:\\n                if seen_default:\\n                    # We accidentally included a non-default after a default arg!\\n                    raise CSTValidationError(\\n                        \"Cannot have param without defaults following a param with defaults.\"\\n                    )\\n        star_arg = self.star_arg\\n        if isinstance(star_arg, Param) and star_arg.default is not None:\\n            raise CSTValidationError(\"Cannot have default for star_arg.\")\\n        star_kwarg = self.star_kwarg\\n        if star_kwarg is not None and star_kwarg.default is not None:\\n            raise CSTValidationError(\"Cannot have default for star_kwarg.\")\\n\\n    def _validate_stars(self) -> None:\\n        if len(self.params) > 0:\\n            self._validate_stars_sequence(self.params, section=\"params\")\\n        if len(self.posonly_params) > 0:\\n            self._validate_stars_sequence(self.posonly_params, section=\"posonly_params\")\\n        star_arg = self.star_arg\\n        if (\\n            isinstance(star_arg, Param)\\n            and isinstance(star_arg.star, str)\\n            and star_arg.star != \"*\"\\n        ):\\n            raise CSTValidationError(\\n                \"Expecting a star prefix of \\'*\\' for star_arg Param.\"\\n            )\\n        if len(self.kwonly_params) > 0:\\n            self._validate_stars_sequence(self.kwonly_params, section=\"kwonly_params\")\\n        star_kwarg = self.star_kwarg\\n        if (\\n            star_kwarg is not None\\n            and isinstance(star_kwarg.star, str)\\n            and star_kwarg.star != \"**\"\\n        ):\\n            raise CSTValidationError(\\n                \"Expecting a star prefix of \\'**\\' for star_kwarg Param.\"\\n            )\\n\\n    def _validate(self) -> None:\\n        # Validate posonly_params slash placement semantics.\\n        self._validate_posonly_ind()\\n        # Validate kwonly_param star placement semantics.\\n        self._validate_kwonly_star()\\n        # Validate defaults semantics for params and star_arg/star_kwarg.\\n        self._validate_defaults()\\n        # Validate that we don\\'t have random stars on non star_kwarg.\\n        self._validate_stars()\\n\\n    def _visit_and_replace_children(self, visitor: CSTVisitorT) -> \"Parameters\":\\n        return Parameters(\\n            posonly_params=visit_sequence(\\n                self, \"posonly_params\", self.posonly_params, visitor\\n            ),\\n            posonly_ind=visit_sentinel(self, \"posonly_ind\", self.posonly_ind, visitor),\\n            params=visit_sequence(self, \"params\", self.params, visitor),\\n            star_arg=visit_sentinel(self, \"star_arg\", self.star_arg, visitor),\\n            kwonly_params=visit_sequence(\\n                self, \"kwonly_params\", self.kwonly_params, visitor\\n            ),\\n            star_kwarg=visit_optional(self, \"star_kwarg\", self.star_kwarg, visitor),\\n        )\\n\\n    def _safe_to_join_with_lambda(self) -> bool:\\n        \"\"\"\\n        Determine if Parameters need a space after the `lambda` keyword. Returns True\\n        iff it\\'s safe to omit the space between `lambda` and these Parameters.\\n\\n        See also `BaseExpression._safe_to_use_with_word_operator`.\\n\\n        For example: `lambda*_: pass`\\n        \"\"\"\\n        if len(self.posonly_params) != 0:\\n            return False\\n\\n        # posonly_ind can\\'t appear if above condition is false\\n\\n        if len(self.params) > 0 and self.params[0].star not in {\"*\", \"**\"}:\\n            return False\\n\\n        return True\\n\\n    def _codegen_impl(self, state: CodegenState) -> None:  # noqa: C901\\n        # Compute the star existence first so we can ask about whether\\n        # each element is the last in the list or not.\\n        star_arg = self.star_arg\\n        if isinstance(star_arg, MaybeSentinel):\\n            starincluded = len(self.kwonly_params) > 0\\n        elif isinstance(star_arg, (Param, ParamStar)):\\n            starincluded = True\\n        else:\\n            starincluded = False\\n        # Render out the positional-only params first. They will always have trailing\\n        # commas because in order to have positional-only params, there must be a\\n        # slash afterwards.\\n        for i, param in enumerate(self.posonly_params):\\n            param._codegen(state, default_star=\"\", default_comma=True)\\n        # Render out the positional-only indicator if necessary.\\n        more_values = (\\n            starincluded\\n            or len(self.params) > 0\\n            or len(self.kwonly_params) > 0\\n            or self.star_kwarg is not None\\n        )\\n        posonly_ind = self.posonly_ind\\n        if isinstance(posonly_ind, ParamSlash):\\n            # Its explicitly included, so render the version we have here which\\n            # might have spacing applied to its comma.\\n            posonly_ind._codegen(state, default_comma=more_values)\\n        elif len(self.posonly_params) > 0:\\n            if more_values:\\n                state.add_token(\"/, \")\\n            else:\\n                state.add_token(\"/\")\\n        # Render out the params next, computing necessary trailing commas.\\n        lastparam = len(self.params) - 1\\n        more_values = (\\n            starincluded or len(self.kwonly_params) > 0 or self.star_kwarg is not None\\n        )\\n        for i, param in enumerate(self.params):\\n            param._codegen(\\n                state, default_star=\"\", default_comma=(i < lastparam or more_values)\\n            )\\n        # Render out optional star sentinel if its explicitly included or\\n        # if we are inferring it from kwonly_params. Otherwise, render out the\\n        # optional star_arg.\\n        if isinstance(star_arg, MaybeSentinel):\\n            if starincluded:\\n                state.add_token(\"*, \")\\n        elif isinstance(star_arg, Param):\\n            more_values = len(self.kwonly_params) > 0 or self.star_kwarg is not None\\n            star_arg._codegen(state, default_star=\"*\", default_comma=more_values)\\n        elif isinstance(star_arg, ParamStar):\\n            star_arg._codegen(state)\\n        # Render out the kwonly_args next, computing necessary trailing commas.\\n        lastparam = len(self.kwonly_params) - 1\\n        more_values = self.star_kwarg is not None\\n        for i, param in enumerate(self.kwonly_params):\\n            param._codegen(\\n                state, default_star=\"\", default_comma=(i < lastparam or more_values)\\n            )\\n        # Finally, render out any optional star_kwarg\\n        star_kwarg = self.star_kwarg\\n        if star_kwarg is not None:\\n            star_kwarg._codegen(state, default_star=\"**\", default_comma=False)\\n', '# Copyright (c) Meta Platforms, Inc. and affiliates.\\n#\\n# This source code is licensed under the MIT license found in the\\n# LICENSE file in the root directory of this source tree.\\n\\n\\nfrom pathlib import PurePath\\nfrom typing import TYPE_CHECKING, TypeVar, Union\\n\\nif TYPE_CHECKING:\\n    from libcst._nodes.base import CSTNode  # noqa: F401\\n\\n\\nCSTNodeT = TypeVar(\"CSTNodeT\", bound=\"CSTNode\")\\nCSTNodeT_co = TypeVar(\"CSTNodeT_co\", bound=\"CSTNode\", covariant=True)\\nStrPath = Union[str, PurePath]\\n']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The `create_paths` function is defined in the `ClusterPaths` class and its subclasses `HDBSCANPaths` and `SpectralClusteringPaths`. It takes two parameters: `embeddings`, which is a numpy array or adjacency matrix A, and `num_clusters`, which is an integer representing the number of clusters. \n",
       "\n",
       "The `create_paths` function returns a list of lists, where each inner list represents a cluster and contains the indices of data points belonging to that cluster."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " What is the purpose of the function `plot_embeddings_with_path` in the given code?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing Trajectory for the current state of memory.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "out_data = []\n",
    "for val in test_question.values[:5]:\n",
    "    try:\n",
    "        if \"Error in sub_task for index\" in val:\n",
    "            continue\n",
    "        if \"__init__\" in val:\n",
    "            continue\n",
    "\n",
    "        display(Markdown(\"#### Question: \\n {question}\".format(question=val)))\n",
    "        answ = answer_eval_task.llm_response(chatbot=fifo_vector_chatbot, message=val)\n",
    "        display(\n",
    "            Markdown(\n",
    "                \" #### Anwser: \\n {answer}\".format(\n",
    "                    answer=answ['content']\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        out_data.append((val, answ))\n",
    "        time.sleep(3)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(out_data, columns=[\"question\", \"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"storage/answer_eval_task.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
