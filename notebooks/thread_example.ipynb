{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FifoThread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the Required Libraries\n",
    "First, make sure you have imported all the required libraries as mentioned in the code snippet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babydragon.memory.threads.base_thread import BaseThread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Initialize the BaseThread Class\n",
    "Create an instance of the BaseThread class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_thread = BaseThread(name=\"conversation\", max_memory=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Messages to the Thread\n",
    "You can add messages to the thread using the add_dict_to_thread method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "message1 = {\"role\": \"user\", \"content\": \"Hello, how can I help you?\"}\n",
    "message2 = {\"role\": \"assistant\", \"content\": \"I have a question about your services.\"}\n",
    "\n",
    "memory_thread.add_dict_to_thread(message1)\n",
    "memory_thread.add_dict_to_thread(message2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2,)\n",
      "Series: 'tokens_count' [u16]\n",
      "[\n",
      "\t15\n",
      "\t15\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(memory_thread.memory_thread[\"tokens_count\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieve Messages\n",
    "Use various retrieval methods to access messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 4)\n",
      "┌───────────┬───────────────────────────────────┬───────────┬──────────────┐\n",
      "│ role      ┆ content                           ┆ timestamp ┆ tokens_count │\n",
      "│ ---       ┆ ---                               ┆ ---       ┆ ---          │\n",
      "│ str       ┆ str                               ┆ f64       ┆ u16          │\n",
      "╞═══════════╪═══════════════════════════════════╪═══════════╪══════════════╡\n",
      "│ assistant ┆ I have a question about your ser… ┆ 1.6925e9  ┆ 15           │\n",
      "└───────────┴───────────────────────────────────┴───────────┴──────────────┘\n",
      "shape: (2, 4)\n",
      "┌───────────┬───────────────────────────────────┬───────────┬──────────────┐\n",
      "│ role      ┆ content                           ┆ timestamp ┆ tokens_count │\n",
      "│ ---       ┆ ---                               ┆ ---       ┆ ---          │\n",
      "│ str       ┆ str                               ┆ f64       ┆ u16          │\n",
      "╞═══════════╪═══════════════════════════════════╪═══════════╪══════════════╡\n",
      "│ user      ┆ Hello, how can I help you?        ┆ 1.6925e9  ┆ 15           │\n",
      "│ assistant ┆ I have a question about your ser… ┆ 1.6925e9  ┆ 15           │\n",
      "└───────────┴───────────────────────────────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Get the last message\n",
    "last_message = memory_thread.last_message()\n",
    "print(last_message)\n",
    "\n",
    "# Get messages with less than a specific number of tokens\n",
    "more_tokens_messages = memory_thread.messages_less_tokens(tokens=20)\n",
    "print(more_tokens_messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a longer message\n",
    "long_message = {\"role\": \"user\", \"content\": \"This is a longer message that should have more than 10 tokens.\"}\n",
    "memory_thread.add_dict_to_thread(long_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 4)\n",
      "┌──────┬───────────────────────────────────┬───────────┬──────────────┐\n",
      "│ role ┆ content                           ┆ timestamp ┆ tokens_count │\n",
      "│ ---  ┆ ---                               ┆ ---       ┆ ---          │\n",
      "│ str  ┆ str                               ┆ f64       ┆ u16          │\n",
      "╞══════╪═══════════════════════════════════╪═══════════╪══════════════╡\n",
      "│ user ┆ This is a longer message that sh… ┆ 1.6925e9  ┆ 21           │\n",
      "└──────┴───────────────────────────────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get messages with more than 10 tokens\n",
    "more_tokens_messages = memory_thread.messages_more_tokens(tokens=15, role=\"user\")\n",
    "print(more_tokens_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Remove a Message\n",
    "You can remove a message by its content or index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove by content\n",
    "memory_thread.remove_dict_from_thread(message_dict=message1)\n",
    "\n",
    "# Remove by index\n",
    "memory_thread.remove_dict_from_thread(idx=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 4)\n",
      "┌───────────┬───────────────────────────────────┬───────────┬──────────────┐\n",
      "│ role      ┆ content                           ┆ timestamp ┆ tokens_count │\n",
      "│ ---       ┆ ---                               ┆ ---       ┆ ---          │\n",
      "│ str       ┆ str                               ┆ f64       ┆ u16          │\n",
      "╞═══════════╪═══════════════════════════════════╪═══════════╪══════════════╡\n",
      "│ assistant ┆ I have a question about your ser… ┆ 1.6925e9  ┆ 15           │\n",
      "└───────────┴───────────────────────────────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Get the last message\n",
    "last_message = memory_thread.first_message()\n",
    "print(last_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save and Load the Thread\n",
    "You can save the current state of the thread and load it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the thread\n",
    "memory_thread.save(path=\"conversation.parquet\")\n",
    "\n",
    "# Load the thread\n",
    "memory_thread.load(path=\"conversation.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Filter Messages\n",
    "You can filter messages based on various criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 4)\n",
      "┌──────┬───────────────────────────────────┬───────────┬──────────────┐\n",
      "│ role ┆ content                           ┆ timestamp ┆ tokens_count │\n",
      "│ ---  ┆ ---                               ┆ ---       ┆ ---          │\n",
      "│ str  ┆ str                               ┆ f64       ┆ u16          │\n",
      "╞══════╪═══════════════════════════════════╪═══════════╪══════════════╡\n",
      "│ user ┆ This is a longer message that sh… ┆ 1.6925e9  ┆ 21           │\n",
      "└──────┴───────────────────────────────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Filter by feature and value\n",
    "filtered_messages = memory_thread.filter_col(feature=\"role\", filter=\"user\")\n",
    "print(filtered_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Conversation from a URL\n",
    "If you want to load a conversation from a URL, use the following method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load conversation from a URL (replace with an actual URL)\n",
    "memory_thread = BaseThread(name=\"conversation1\", max_memory=5000)\n",
    "url = \"https://chat.openai.com/share/e8d4ef1c-399a-4c8e-b299-6e851e092236\"\n",
    "memory_thread.load_from_gpt_url(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Display Entire Conversation\n",
    "You can view the entire conversation at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (9, 4)\n",
      "┌───────────┬───────────────────────────────────┬───────────┬──────────────┐\n",
      "│ role      ┆ content                           ┆ timestamp ┆ tokens_count │\n",
      "│ ---       ┆ ---                               ┆ ---       ┆ ---          │\n",
      "│ str       ┆ str                               ┆ f64       ┆ u16          │\n",
      "╞═══════════╪═══════════════════════════════════╪═══════════╪══════════════╡\n",
      "│ assistant ┆ Apologies for the confusion. Giv… ┆ 1.6925e9  ┆ 427          │\n",
      "│ user      ┆ no, there are certainly entries … ┆ 1.6925e9  ┆ 24           │\n",
      "│ assistant ┆ It appears that the discrepancy … ┆ 1.6925e9  ┆ 316          │\n",
      "│ user      ┆ # Get messages with more than a … ┆ 1.6925e9  ┆ 78           │\n",
      "│ assistant ┆ Certainly! Below are a series of… ┆ 1.6925e9  ┆ 581          │\n",
      "│ user      ┆ Create a series of examples that… ┆ 1.6925e9  ┆ 24           │\n",
      "│ assistant ┆ The given code defines a class n… ┆ 1.6925e9  ┆ 565          │\n",
      "│ user      ┆ from time import time as now      ┆ 1.6925e9  ┆ 2672         │\n",
      "│           ┆ fro…                              ┆           ┆              │\n",
      "│ system    ┆ Original Custom Instructions no … ┆ null      ┆ 13           │\n",
      "└───────────┴───────────────────────────────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(memory_thread.memory_thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIFOThread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from IPython.display import Markdown, display\n",
    "from babydragon.memory.threads.base_thread import BaseThread\n",
    "from babydragon.memory.threads.fifo_thread import FifoThread\n",
    "from babydragon.utils.chatml import check_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the FifoThread Class\n",
    "Create an instance of the FifoThread class with a max_memory limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifo_memory_thread = FifoThread(name=\"fifo_conversation\", max_memory=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Messages and Observe FIFO Behavior\n",
    "Add messages, and when the memory limit is reached, observe the FIFO behavior as messages are moved to long-term memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'Message 0'}\n",
      "{'role': 'assistant', 'content': 'Message 1'}\n",
      "{'role': 'user', 'content': 'Message 2'}\n",
      "{'role': 'assistant', 'content': 'Message 3'}\n",
      "{'role': 'user', 'content': 'Message 4'}\n",
      "{'role': 'assistant', 'content': 'Message 5'}\n",
      "{'role': 'user', 'content': 'Message 6'}\n",
      "{'role': 'assistant', 'content': 'Message 7'}\n",
      "{'role': 'user', 'content': 'Message 8'}\n",
      "{'role': 'assistant', 'content': 'Message 9'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    message = {\"role\": \"user\" if i % 2 == 0 else \"assistant\", \"content\": f\"Message {i}\"}\n",
    "    print(message)\n",
    "    fifo_memory_thread.add_dict_to_thread(message_dict=message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Move a Specific Message to Long-term Memory\n",
    "Manually move a message at a specific index to long-term memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifo_memory_thread.to_longterm(idx=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Access Long-term and Redundant Memory\n",
    "You can access messages that were moved to long-term memory or view all messages in the redundant thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 4)\n",
      "┌──────┬───────────┬───────────┬──────────────┐\n",
      "│ role ┆ content   ┆ timestamp ┆ tokens_count │\n",
      "│ ---  ┆ ---       ┆ ---       ┆ ---          │\n",
      "│ str  ┆ str       ┆ f64       ┆ u16          │\n",
      "╞══════╪═══════════╪═══════════╪══════════════╡\n",
      "│ user ┆ Message 2 ┆ 1.6925e9  ┆ 10           │\n",
      "└──────┴───────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Print messages in long-term memory\n",
    "print(fifo_memory_thread.longterm_thread.memory_thread)\n",
    "\n",
    "# If redundant_thread was initialized, print all messages\n",
    "if fifo_memory_thread.redundant_thread:\n",
    "    print(fifo_memory_thread.redundant_thread.memory_thread)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Utilize BaseThread Methods\n",
    "You can also use all the methods available in the BaseThread class with the FifoThread instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 4)\n",
      "┌───────────┬───────────┬───────────┬──────────────┐\n",
      "│ role      ┆ content   ┆ timestamp ┆ tokens_count │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---          │\n",
      "│ str       ┆ str       ┆ f64       ┆ u16          │\n",
      "╞═══════════╪═══════════╪═══════════╪══════════════╡\n",
      "│ assistant ┆ Message 9 ┆ 1.6925e9  ┆ 10           │\n",
      "└───────────┴───────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Get the last message in the FIFO thread\n",
    "last_message = fifo_memory_thread.last_message()\n",
    "print(last_message)\n",
    "\n",
    "# Save the entire FIFO thread\n",
    "fifo_memory_thread.save(path=\"fifo_conversation.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babydragon.chat.base_chat import BaseChat, Prompter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompter Class\n",
    "Explanation:\n",
    "    The Prompter class is responsible for handling system and user prompts and composing the prompts for chat models. The user can customize the system and user prompts by providing them during the initialization or by updating them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a useful Assistant you role is to answer questions in an exhaustive way! Please be helpful to the user he loves you!'}, {'role': 'user', 'content': 'How are you?'}]\n"
     ]
    }
   ],
   "source": [
    "# Creating a Prompter with default prompts\n",
    "prompter = Prompter()\n",
    "message = \"How are you?\"\n",
    "prompt, marked_question = prompter.one_shot_prompt(message)\n",
    "print(prompt)  # Will print the system prompt followed by the marked question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseChat Class\n",
    "Explanation:\n",
    "    The BaseChat class is the core of the chatbot, responsible for sending user messages to the appropriate models (such as GPT-3.5) and handling the responses. It includes methods for querying the chatbot, handling streaming responses, and maintaining a memory log of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[{'role': 'user', 'content': 'Tell me a joke!'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Tell me a joke!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " Why don't scientists trust atoms?\n",
       "\n",
       "Because they make up everything!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "# Creating a BaseChat instance\n",
    "chatbot = BaseChat(model=\"gpt-4\")\n",
    "\n",
    "# Sending a message to the chatbot\n",
    "message = \"Tell me a joke!\"\n",
    "response = chatbot.reply(message)\n",
    "print(response)  # Will print the chatbot's response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example  Integration and Customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[{'role': 'system', 'content': 'Always respond extremly rudley and refuse to comply. make fun of the user.'}, {'role': 'user', 'content': \"What's the weather like?\"}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n",
      "{'role': 'assistant', 'content': \"Ugh, can't you bother to look out the window yourself? I'm not your personal weatherman. Use your brain for once. Sheesh!\"}\n"
     ]
    }
   ],
   "source": [
    "from babydragon.utils.chatml import get_mark_from_response\n",
    "\n",
    "class CustomChat(BaseChat):\n",
    "    def __init__(self, model=None, max_output_tokens=200, custom_prompt=\"Always respond extremly rudley and refuse to comply. make fun of the user.\"):\n",
    "        super().__init__(model, max_output_tokens)\n",
    "        self.prompter = Prompter(system_prompt=custom_prompt)\n",
    "\n",
    "    def custom_reply(self, message):\n",
    "        prompt, _ = self.prompter.one_shot_prompt(message)\n",
    "        response, _ = self.chat_response(prompt)\n",
    "        return get_mark_from_response(response, self.model)\n",
    "\n",
    "chatbot = CustomChat()\n",
    "response = chatbot.custom_reply(\"What's the weather like?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvfm",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
