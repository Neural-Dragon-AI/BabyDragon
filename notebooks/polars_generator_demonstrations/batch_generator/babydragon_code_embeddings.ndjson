{"model":"text-embedding-ada-002","input":"\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\n    # Infer the data type of the column\n    # This will depend on the type of `column` (whether it's a string, Series, etc.)\n    # Here we'll assume `column` is a pandas Series for simplicity\n    column_type = str(column.dtype)\n    print(column_type)\n    if column_type == \"Utf8\":\n        # If it's an object, we'll assume it's text\n        return EmbeddableType.TEXT, OpenAiEmbedder()\n    elif np.issubdtype(column.dtype, np.number):\n        # If it's a number, we'll use a different embedding strategy\n        return EmbeddableType.NUMERIC, numeric_embedder\n    else:\n        # For other types, we could throw an error or have a default strategy\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\n"}
{"model":"text-embedding-ada-002","input":"\ndef numeric_embedder(column):\n    # Implement the numeric embedding strategy\n    # This will depend on the type of `column` (whether it's a string, Series, etc.)\n    # Here we'll assume `column` is a pandas Series for simplicity\n    return column.values\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass EmbeddingTask(BaseTask):\n    def __init__(\n        self,\n        embedder: OpenAiEmbedder,\n        values: List[Any],\n        path: List[List[int]],\n        max_workers: int = 1,\n        task_id: str = \"task\",\n        calls_per_minute: int = 1500,\n        backup: bool = True,\n    ):\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup)\n        self.embedder = embedder\n        self.values = values\n\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n        # expected to work with a lig of a single element\n        if len(sub_path) != 1:\n            raise ValueError(\n                \"Embedding task expected to work with a list of a single element\"\n            )\n        sub_results = {}\n        for i in sub_path:\n            embedded_value = self.embedder.embed(self.values[i])\n            sub_results[i] = embedded_value\n        return sub_results\n"}
{"model":"text-embedding-ada-002","input":"def __init__(\n    self,\n    embedder: OpenAiEmbedder,\n    values: List[Any],\n    path: List[List[int]],\n    max_workers: int = 1,\n    task_id: str = \"task\",\n    calls_per_minute: int = 1500,\n    backup: bool = True,\n):\n    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup)\n    self.embedder = embedder\n    self.values = values\n"}
{"model":"text-embedding-ada-002","input":"\ndef _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n    # expected to work with a lig of a single element\n    if len(sub_path) != 1:\n        raise ValueError(\n            \"Embedding task expected to work with a list of a single element\"\n        )\n    sub_results = {}\n    for i in sub_path:\n        embedded_value = self.embedder.embed(self.values[i])\n        sub_results[i] = embedded_value\n    return sub_results\n"}
{"model":"text-embedding-ada-002","input":"\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\n        # Prepare the paths for the EmbeddingTask\n        print(\"Embedding {} values\".format(len(values)))\n        paths = [[i] for i in range(len(values))]\n\n        # Initialize the EmbeddingTask and execute it\n        embedding_task = EmbeddingTask(\n            embedder,\n            values,\n            path=paths,\n            max_workers=max_workers,\n            task_id=name + \"_embedding_task\",\n            backup=backup,\n        )\n        embeddings = embedding_task.work()\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\n        return embeddings\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass TopicTreeTask(BaseTask):\n    def __init__(\n        self,\n        memory_kernel_dict: Dict,\n        supplement_indexes: Dict,\n        sim_threshold: float,\n        chatbot: BaseChat,\n        parent_kernel_label: str,\n        child_kernel_label: str,\n        system_prompt: str,\n        clustering_method: str,\n        task_id: str = \"TopicTreeTask\",\n        max_workers: int = 1,\n        calls_per_minute: int = 20,\n    ):\n        self.clustering_method = clustering_method\n        self.supplement_indexes = supplement_indexes\n        self.sim_threshold = sim_threshold\n        self.parent_kernel_label = parent_kernel_label\n        self.child_kernel_label = child_kernel_label\n        self.memory_kernel_dict = memory_kernel_dict\n        self._setup_memory_kernel_group()\n        self.generate_task_paths()\n        self.system_prompt = system_prompt\n        self.chatbot = chatbot\n        self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n        super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n\n\n    def _setup_memory_kernel_group(self):\n        if self.clustering_method == \"HDBSCAN\":\n            print(\"Using HDBSCAN\")\n            self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n        elif self.clustering_method == \"Spectral\":\n            print(\"Using Spectral\")\n            self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n        else:\n            raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n\n    def generate_task_paths(self):\n        print(\"Generating task paths\")\n\n        self.memory_kernel_group.generate_path_groups()\n\n    def llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n        return chatbot.reply(message)\n\n    def _execute_sub_task(self, sub_path) -> List[str]:\n        if self.parallel:\n            chatbot_instance = copy.deepcopy(self.chatbot)\n        else:\n            chatbot_instance = self.chatbot\n\n        sub_results = {}\n        for i in sub_path:\n            print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n            try:\n                current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n                supplement_values = []\n                for key, index in self.supplement_indexes.items():\n                    results, scores, indeces = index.faiss_query(current_val, k=5)\n                    for result, score in zip(results, scores):\n                        if score > self.sim_threshold:\n                            supplement_values.append(result)\n                topic_tree = self.create_topic_tree(supplement_values)\n                #response = self.llm_response(chatbot_instance, current_val, id=i)\n                sub_results[i] = topic_tree\n            except IndexError:\n                print(f\"Error: Invalid index {i} in sub_path\")\n                sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n            except Exception as e:\n                print(f\"Error in sub_task for index {i}: {e}\")\n                sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n\n        return sub_results\n\n    def execute_task(self) -> None:\n        BaseTask.execute_task(self)\n\n        # Load the results from the JSON file\n        # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n        #     task_results = json.load(f)\n        self._load_results_from_file()\n        task_results = self.results\n        new_values = []\n        #sort task_results by index and add to new_values 0- max values ascending\n        for task_result in task_results:\n            if isinstance(task_result, dict):\n                for key, value in task_result.items():\n                    new_values.append((int(key), value))\n            elif isinstance(task_result, str):\n                print(f\"Error in task_result: {task_result}\")\n\n        new_values.sort(key=lambda x: x[0])\n        values = [x[1] for x in new_values]\n\n        task_memory_index = MemoryIndex()\n        task_memory_index.init_index(values=values)\n        # Create a new MemoryKernel with the results\n        new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n\n        # Add the new MemoryKernel to the MultiKernel\n        self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n        self.generate_task_paths()\n\n        #delete the results file\n        # os.remove(f\"{self.task_id}_results.json\")\n\n\n    def create_topic_tree(self, docs):\n        return None\n"}
{"model":"text-embedding-ada-002","input":"def __init__(\n    self,\n    memory_kernel_dict: Dict,\n    supplement_indexes: Dict,\n    sim_threshold: float,\n    chatbot: BaseChat,\n    parent_kernel_label: str,\n    child_kernel_label: str,\n    system_prompt: str,\n    clustering_method: str,\n    task_id: str = \"TopicTreeTask\",\n    max_workers: int = 1,\n    calls_per_minute: int = 20,\n):\n    self.clustering_method = clustering_method\n    self.supplement_indexes = supplement_indexes\n    self.sim_threshold = sim_threshold\n    self.parent_kernel_label = parent_kernel_label\n    self.child_kernel_label = child_kernel_label\n    self.memory_kernel_dict = memory_kernel_dict\n    self._setup_memory_kernel_group()\n    self.generate_task_paths()\n    self.system_prompt = system_prompt\n    self.chatbot = chatbot\n    self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n    super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n"}
{"model":"text-embedding-ada-002","input":"\n\ndef _setup_memory_kernel_group(self):\n    if self.clustering_method == \"HDBSCAN\":\n        print(\"Using HDBSCAN\")\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n    elif self.clustering_method == \"Spectral\":\n        print(\"Using Spectral\")\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n    else:\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n"}
{"model":"text-embedding-ada-002","input":"\ndef generate_task_paths(self):\n    print(\"Generating task paths\")\n\n    self.memory_kernel_group.generate_path_groups()\n"}
{"model":"text-embedding-ada-002","input":"\ndef llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n    return chatbot.reply(message)\n"}
{"model":"text-embedding-ada-002","input":"\ndef _execute_sub_task(self, sub_path) -> List[str]:\n    if self.parallel:\n        chatbot_instance = copy.deepcopy(self.chatbot)\n    else:\n        chatbot_instance = self.chatbot\n\n    sub_results = {}\n    for i in sub_path:\n        print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n        try:\n            current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n            supplement_values = []\n            for key, index in self.supplement_indexes.items():\n                results, scores, indeces = index.faiss_query(current_val, k=5)\n                for result, score in zip(results, scores):\n                    if score > self.sim_threshold:\n                        supplement_values.append(result)\n            topic_tree = self.create_topic_tree(supplement_values)\n            #response = self.llm_response(chatbot_instance, current_val, id=i)\n            sub_results[i] = topic_tree\n        except IndexError:\n            print(f\"Error: Invalid index {i} in sub_path\")\n            sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n        except Exception as e:\n            print(f\"Error in sub_task for index {i}: {e}\")\n            sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n\n    return sub_results\n"}
{"model":"text-embedding-ada-002","input":"\ndef execute_task(self) -> None:\n    BaseTask.execute_task(self)\n\n    # Load the results from the JSON file\n    # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n    #     task_results = json.load(f)\n    self._load_results_from_file()\n    task_results = self.results\n    new_values = []\n    #sort task_results by index and add to new_values 0- max values ascending\n    for task_result in task_results:\n        if isinstance(task_result, dict):\n            for key, value in task_result.items():\n                new_values.append((int(key), value))\n        elif isinstance(task_result, str):\n            print(f\"Error in task_result: {task_result}\")\n\n    new_values.sort(key=lambda x: x[0])\n    values = [x[1] for x in new_values]\n\n    task_memory_index = MemoryIndex()\n    task_memory_index.init_index(values=values)\n    # Create a new MemoryKernel with the results\n    new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n\n    # Add the new MemoryKernel to the MultiKernel\n    self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n    self.generate_task_paths()\n\n    #delete the results file\n    # os.remove(f\"{self.task_id}_results.json\")\n"}
{"model":"text-embedding-ada-002","input":"\n\ndef create_topic_tree(self, docs):\n    return None\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass MultiKernelTask(BaseTask):\n    def __init__(\n        self,\n        memory_kernel_dict: Dict,\n        chatbot: BaseChat,\n        parent_kernel_label: str,\n        child_kernel_label: str,\n        system_prompt: str,\n        clustering_method: str,\n        path_group: Dict[str, List[List[int]]],\n        task_id: str = \"MultiKernelTask\",\n        max_workers: int = 1,\n        calls_per_minute: int = 20,\n    ):\n        self.clustering_method = clustering_method\n        self.parent_kernel_label = parent_kernel_label\n        self.child_kernel_label = child_kernel_label\n        self.memory_kernel_dict = memory_kernel_dict\n        \n        self._setup_memory_kernel_group()\n        if path_group:\n            self.memory_kernel_group.path_group = path_group\n        else:\n            self.generate_task_paths()\n        self.system_prompt = system_prompt\n        self.chatbot = chatbot\n        self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n        super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n\n\n    def _setup_memory_kernel_group(self):\n        if self.clustering_method == \"HDBSCAN\":\n            print(\"Using HDBSCAN\")\n            self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n        elif self.clustering_method == \"Spectral\":\n            print(\"Using Spectral\")\n            self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n        else:\n            raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n\n    def generate_task_paths(self):\n        print(\"Generating task paths\")\n\n        self.memory_kernel_group.generate_path_groups()\n\n    def llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n        return chatbot.reply(message)\n\n    def _execute_sub_task(self, sub_path) -> List[str]:\n        if self.parallel:\n            chatbot_instance = copy.deepcopy(self.chatbot)\n        else:\n            chatbot_instance = self.chatbot\n\n        sub_results = {}\n        for i in sub_path:\n            print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n            try:\n                current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n                response = self.llm_response(chatbot_instance, current_val, id=i)\n                sub_results[i] = response\n            except IndexError:\n                print(f\"Error: Invalid index {i} in sub_path\")\n                sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n            except Exception as e:\n                print(f\"Error in sub_task for index {i}: {e}\")\n                sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n\n        return sub_results\n\n    def execute_task(self) -> None:\n        BaseTask.execute_task(self)\n\n        # Load the results from the JSON file\n        # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n        #     task_results = json.load(f)\n        self._load_results_from_file()\n        task_results = self.results\n        new_values = []\n        #sort task_results by index and add to new_values 0- max values ascending\n        for task_result in task_results:\n            if isinstance(task_result, dict):\n                for key, value in task_result.items():\n                    new_values.append((int(key), value))\n            elif isinstance(task_result, str):\n                print(f\"Error in task_result: {task_result}\")\n\n        new_values.sort(key=lambda x: x[0])\n        values = [x[1] for x in new_values]\n\n        task_memory_index = MemoryIndex()\n        task_memory_index.init_index(values=values)\n        # Create a new MemoryKernel with the results\n        new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n\n        # Add the new MemoryKernel to the MultiKernel\n        self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n        self.generate_task_paths()\n\n        #delete the results file\n        # os.remove(f\"{self.task_id}_results.json\")\n"}
{"model":"text-embedding-ada-002","input":"def __init__(\n    self,\n    memory_kernel_dict: Dict,\n    chatbot: BaseChat,\n    parent_kernel_label: str,\n    child_kernel_label: str,\n    system_prompt: str,\n    clustering_method: str,\n    path_group: Dict[str, List[List[int]]],\n    task_id: str = \"MultiKernelTask\",\n    max_workers: int = 1,\n    calls_per_minute: int = 20,\n):\n    self.clustering_method = clustering_method\n    self.parent_kernel_label = parent_kernel_label\n    self.child_kernel_label = child_kernel_label\n    self.memory_kernel_dict = memory_kernel_dict\n    \n    self._setup_memory_kernel_group()\n    if path_group:\n        self.memory_kernel_group.path_group = path_group\n    else:\n        self.generate_task_paths()\n    self.system_prompt = system_prompt\n    self.chatbot = chatbot\n    self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n    super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n"}
{"model":"text-embedding-ada-002","input":"\n\ndef _setup_memory_kernel_group(self):\n    if self.clustering_method == \"HDBSCAN\":\n        print(\"Using HDBSCAN\")\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n    elif self.clustering_method == \"Spectral\":\n        print(\"Using Spectral\")\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n    else:\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n"}
{"model":"text-embedding-ada-002","input":"\ndef generate_task_paths(self):\n    print(\"Generating task paths\")\n\n    self.memory_kernel_group.generate_path_groups()\n"}
{"model":"text-embedding-ada-002","input":"\ndef llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n    return chatbot.reply(message)\n"}
{"model":"text-embedding-ada-002","input":"\ndef _execute_sub_task(self, sub_path) -> List[str]:\n    if self.parallel:\n        chatbot_instance = copy.deepcopy(self.chatbot)\n    else:\n        chatbot_instance = self.chatbot\n\n    sub_results = {}\n    for i in sub_path:\n        print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n        try:\n            current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n            response = self.llm_response(chatbot_instance, current_val, id=i)\n            sub_results[i] = response\n        except IndexError:\n            print(f\"Error: Invalid index {i} in sub_path\")\n            sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n        except Exception as e:\n            print(f\"Error in sub_task for index {i}: {e}\")\n            sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n\n    return sub_results\n"}
{"model":"text-embedding-ada-002","input":"\ndef execute_task(self) -> None:\n    BaseTask.execute_task(self)\n\n    # Load the results from the JSON file\n    # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n    #     task_results = json.load(f)\n    self._load_results_from_file()\n    task_results = self.results\n    new_values = []\n    #sort task_results by index and add to new_values 0- max values ascending\n    for task_result in task_results:\n        if isinstance(task_result, dict):\n            for key, value in task_result.items():\n                new_values.append((int(key), value))\n        elif isinstance(task_result, str):\n            print(f\"Error in task_result: {task_result}\")\n\n    new_values.sort(key=lambda x: x[0])\n    values = [x[1] for x in new_values]\n\n    task_memory_index = MemoryIndex()\n    task_memory_index.init_index(values=values)\n    # Create a new MemoryKernel with the results\n    new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n\n    # Add the new MemoryKernel to the MultiKernel\n    self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n    self.generate_task_paths()\n\n    #delete the results file\n    # os.remove(f\"{self.task_id}_results.json\")\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass BaseTask:\n    def __init__(\n        self,\n        path: List[List[int]],\n        max_workers: int = 1,\n        task_id: str = \"task\",\n        calls_per_minute: int = 20,\n        backup: bool = True,\n        save_path: str = None,\n    ):\n        self.task_id = task_id\n        self.path = path\n        self.results = []\n        self.max_workers = max_workers\n        self.parallel = True if max_workers > 1 else False\n        self.rate_limiter = RateLimiter(calls_per_minute)\n        self.failed_sub_tasks = []\n        self.backup = backup\n        print(\"setting up savepath\")\n        self.save_path = save_path if save_path is not None else os.path.join(\"storage\", \"tasks\")\n\n    def _save_results_to_file(self) -> None:\n        os.makedirs(self.save_path, exist_ok=True)\n        with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"w\") as f:\n            json.dump(self.results, f)\n\n    def _load_results_from_file(self) -> None:\n        if os.path.exists(os.path.join(self.save_path, f\"{self.task_id}_results.json\")):\n            try:\n                with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"r\") as f:\n                    self.results = json.load(f)\n                    print(f\"Loaded {len(self.results)} results from file.\")\n            except Exception as e:\n                print(f\"Error loading results from file: {e}\")\n                print(\"Starting from scratch.\")\n        else:\n            print(\"No results file found, starting from scratch.\")\n\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n        sub_results = []\n        for i in sub_path:\n            response = \"Implement the response function in the subclass\"\n            sub_results.append(response)\n        return sub_results\n\n    def execute_task(self) -> None:\n        if self.backup:\n            self._load_results_from_file()\n\n        with RateLimitedThreadPoolExecutor(\n            max_workers=self.max_workers,\n            calls_per_minute=self.rate_limiter.calls_per_minute,\n        ) as executor:\n            futures = []\n            print(f\"Executing task {self.task_id} using {self.max_workers} workers.\")\n\n            for i, sub_path in enumerate(self.path):\n                if i < len(self.results):\n                    pass\n                else:\n                    future = executor.submit(self._execute_sub_task, sub_path)\n                    futures.append((i, future))\n\n            for i, future in futures:\n                try:\n                    execution_start_time = time.time()\n                    sub_task_result = future.result()\n                    execution_end_time = time.time()\n                    print(\n                        f\"Sub-task {i} executed in {execution_end_time - execution_start_time:.2f} seconds.\"\n                    )\n\n                    save_start_time = time.time()\n                    self.results.append(sub_task_result)\n                    # self.results.append(sub_task_result)\n                    if self.backup:\n                        self._save_results_to_file()\n                    save_end_time = time.time()\n                    print(\n                        f\"Sub-task {i} results saved in {save_end_time - save_start_time:.2f} seconds.\"\n                    )\n                except Exception as e:\n                    print(f\"Error in sub-task {i}: {e}\")\n                    # default_result = f\"Error in sub-task {i}: {e}\"\n                    default_result =  {i:f\"Error in sub-task {i}: {e}\"} \n                    self.results.append(default_result)\n                    if self.backup:\n                        self._save_results_to_file()\n                    self.failed_sub_tasks.append((self.path[i], str(e)))\n\n                except KeyboardInterrupt:\n                    print(\"Keyboard interrupt detected, stopping task execution.\")\n                    executor.shutdown(wait=False)\n                    break\n\n        print(\"Task execution completed.\")\n\n    def work(self) -> List[Any]:\n        self.execute_task()\n        if not self.backup:\n            self._save_results_to_file()\n        work = []\n        for sub_result in self.results:\n            for index_id, response in sub_result.items():\n                work.append((index_id, response))\n        # sort the content to write by index_id\n        work.sort(key=lambda x: int(x[0]))\n        return work\n"}
{"model":"text-embedding-ada-002","input":"def __init__(\n    self,\n    path: List[List[int]],\n    max_workers: int = 1,\n    task_id: str = \"task\",\n    calls_per_minute: int = 20,\n    backup: bool = True,\n    save_path: str = None,\n):\n    self.task_id = task_id\n    self.path = path\n    self.results = []\n    self.max_workers = max_workers\n    self.parallel = True if max_workers > 1 else False\n    self.rate_limiter = RateLimiter(calls_per_minute)\n    self.failed_sub_tasks = []\n    self.backup = backup\n    print(\"setting up savepath\")\n    self.save_path = save_path if save_path is not None else os.path.join(\"storage\", \"tasks\")\n"}
{"model":"text-embedding-ada-002","input":"\ndef _save_results_to_file(self) -> None:\n    os.makedirs(self.save_path, exist_ok=True)\n    with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"w\") as f:\n        json.dump(self.results, f)\n"}
{"model":"text-embedding-ada-002","input":"\ndef _load_results_from_file(self) -> None:\n    if os.path.exists(os.path.join(self.save_path, f\"{self.task_id}_results.json\")):\n        try:\n            with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"r\") as f:\n                self.results = json.load(f)\n                print(f\"Loaded {len(self.results)} results from file.\")\n        except Exception as e:\n            print(f\"Error loading results from file: {e}\")\n            print(\"Starting from scratch.\")\n    else:\n        print(\"No results file found, starting from scratch.\")\n"}
{"model":"text-embedding-ada-002","input":"\ndef _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n    sub_results = []\n    for i in sub_path:\n        response = \"Implement the response function in the subclass\"\n        sub_results.append(response)\n    return sub_results\n"}
{"model":"text-embedding-ada-002","input":"\ndef execute_task(self) -> None:\n    if self.backup:\n        self._load_results_from_file()\n\n    with RateLimitedThreadPoolExecutor(\n        max_workers=self.max_workers,\n        calls_per_minute=self.rate_limiter.calls_per_minute,\n    ) as executor:\n        futures = []\n        print(f\"Executing task {self.task_id} using {self.max_workers} workers.\")\n\n        for i, sub_path in enumerate(self.path):\n            if i < len(self.results):\n                pass\n            else:\n                future = executor.submit(self._execute_sub_task, sub_path)\n                futures.append((i, future))\n\n        for i, future in futures:\n            try:\n                execution_start_time = time.time()\n                sub_task_result = future.result()\n                execution_end_time = time.time()\n                print(\n                    f\"Sub-task {i} executed in {execution_end_time - execution_start_time:.2f} seconds.\"\n                )\n\n                save_start_time = time.time()\n                self.results.append(sub_task_result)\n                # self.results.append(sub_task_result)\n                if self.backup:\n                    self._save_results_to_file()\n                save_end_time = time.time()\n                print(\n                    f\"Sub-task {i} results saved in {save_end_time - save_start_time:.2f} seconds.\"\n                )\n            except Exception as e:\n                print(f\"Error in sub-task {i}: {e}\")\n                # default_result = f\"Error in sub-task {i}: {e}\"\n                default_result =  {i:f\"Error in sub-task {i}: {e}\"} \n                self.results.append(default_result)\n                if self.backup:\n                    self._save_results_to_file()\n                self.failed_sub_tasks.append((self.path[i], str(e)))\n\n            except KeyboardInterrupt:\n                print(\"Keyboard interrupt detected, stopping task execution.\")\n                executor.shutdown(wait=False)\n                break\n\n    print(\"Task execution completed.\")\n"}
{"model":"text-embedding-ada-002","input":"\ndef work(self) -> List[Any]:\n    self.execute_task()\n    if not self.backup:\n        self._save_results_to_file()\n    work = []\n    for sub_result in self.results:\n        for index_id, response in sub_result.items():\n            work.append((index_id, response))\n    # sort the content to write by index_id\n    work.sort(key=lambda x: int(x[0]))\n    return work\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass LLMReader(BaseTask):\n    def __init__(\n        self,\n        index: MemoryIndex,\n        path: List[List[int]],\n        chatbot: Chat,\n        read_func=None,\n        max_workers: int = 1,\n        task_id: str = \"LLMReadTask\",\n        calls_per_minute: int = 20,\n    ):\n        \"\"\"\n        Initialize a LLMReadTask instance.\n\n        :param index: List of strings representing the queries.\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n        :param chatbot: Chatbot instance used for executing queries.\n        :param max_workers: Maximum number of worker threads (default is 4).\n        \"\"\"\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute)\n        self.index = index\n        self.chatbot = chatbot\n        self.read_func = read_func if read_func else self.llm_response\n\n    def llm_response(chatbot: Chat, message: str, string_out=False):\n        if string_out:\n            return chatbot.reply(message)\n        return chatbot.query(message)\n\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n        \"\"\"\n        Execute a sub-task using a separate copy of the chatbot instance. each sub-stasks uses a\n        a clean memory instance.\n\n        :param sub_path: List of indices representing the sub-task's sequence.\n        :return: List of strings representing the responses for each query in the sub-task.\n        \"\"\"\n        if self.parallel:\n            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n            chatbot_instance = copy.deepcopy(self.chatbot)\n        else:\n            chatbot_instance = self.chatbot\n        if isinstance(self.chatbot, BaseThread):\n            chatbot_instance.reset_memory()\n\n        sub_results = []\n        for i in sub_path:\n            response = self.read_func(chatbot_instance, self.index.values[i])\n            sub_results.append(response)\n        return sub_results\n\n    def read(self):\n        self.execute_task()\n        return self.results\n"}
{"model":"text-embedding-ada-002","input":"def __init__(\n    self,\n    index: MemoryIndex,\n    path: List[List[int]],\n    chatbot: Chat,\n    read_func=None,\n    max_workers: int = 1,\n    task_id: str = \"LLMReadTask\",\n    calls_per_minute: int = 20,\n):\n    \"\"\"\n        Initialize a LLMReadTask instance.\n\n        :param index: List of strings representing the queries.\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n        :param chatbot: Chatbot instance used for executing queries.\n        :param max_workers: Maximum number of worker threads (default is 4).\n        \"\"\"\n    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute)\n    self.index = index\n    self.chatbot = chatbot\n    self.read_func = read_func if read_func else self.llm_response\n"}
{"model":"text-embedding-ada-002","input":"\ndef llm_response(chatbot: Chat, message: str, string_out=False):\n    if string_out:\n        return chatbot.reply(message)\n    return chatbot.query(message)\n"}
{"model":"text-embedding-ada-002","input":"\ndef _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n    \"\"\"\n        Execute a sub-task using a separate copy of the chatbot instance. each sub-stasks uses a\n        a clean memory instance.\n\n        :param sub_path: List of indices representing the sub-task's sequence.\n        :return: List of strings representing the responses for each query in the sub-task.\n        \"\"\"\n    if self.parallel:\n        # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n        chatbot_instance = copy.deepcopy(self.chatbot)\n    else:\n        chatbot_instance = self.chatbot\n    if isinstance(self.chatbot, BaseThread):\n        chatbot_instance.reset_memory()\n\n    sub_results = []\n    for i in sub_path:\n        response = self.read_func(chatbot_instance, self.index.values[i])\n        sub_results.append(response)\n    return sub_results\n"}
{"model":"text-embedding-ada-002","input":"\ndef read(self):\n    self.execute_task()\n    return self.results\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass LLMWriter(BaseTask):\n    def __init__(\n        self,\n        index: MemoryIndex,\n        path: List[List[int]],\n        chatbot: Chat,\n        write_func=None,\n        context=None,\n        task_name=\"summary\",\n        max_workers: int = 1,\n        task_id: str = \"LLMWriteTask\",\n        calls_per_minute: int = 20,\n        backup: bool = True,\n    ):\n        \"\"\"\n        Initialize a LLMWriteTask instance.\n\n        :param index: List of strings representing the queries.\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n        :param chatbot: Chatbot instance used for executing queries.\n        :param max_workers: Maximum number of worker threads (default is 4).\n        \"\"\"\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\n        self.index = index\n        self.chatbot = chatbot\n        self.write_func = write_func if write_func else self.llm_response\n        self.new_index_name = self.index.name + f\"_{task_name}\"\n        self.context = context\n\n    def llm_response(self, chatbot: Chat, message: str, context=None, id=None):\n        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n        # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\n        #     return \"the message is too long to be processed\"\n        # moved the error catching to multi-threading but custom method could report the error here\n        return chatbot.reply(message)\n\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n        \"\"\"\n        Execute a sub-task using a separate copy of the chatbot instance.\n\n        :param sub_path: List of indices representing the sub-task's sequence.\n        :return: List of strings representing the responses for each query in the sub-task.\n        \"\"\"\n        if self.parallel:\n            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n            chatbot_instance = copy.deepcopy(self.chatbot)\n        else:\n            chatbot_instance = self.chatbot\n        if isinstance(self.chatbot, BaseThread):\n            chatbot_instance.reset_memory()\n\n        sub_results = {}\n        for i in sub_path:\n            current_val = self.index.values[i]\n            response = self.write_func(\n                chatbot_instance, current_val, self.context, id=i\n            )\n            sub_results[i] = response\n        return sub_results\n\n    def write(self):\n        content_to_write = self.work()\n        self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\n        self.new_index.save()\n        return self.new_index\n"}
{"model":"text-embedding-ada-002","input":"def __init__(\n    self,\n    index: MemoryIndex,\n    path: List[List[int]],\n    chatbot: Chat,\n    write_func=None,\n    context=None,\n    task_name=\"summary\",\n    max_workers: int = 1,\n    task_id: str = \"LLMWriteTask\",\n    calls_per_minute: int = 20,\n    backup: bool = True,\n):\n    \"\"\"\n        Initialize a LLMWriteTask instance.\n\n        :param index: List of strings representing the queries.\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n        :param chatbot: Chatbot instance used for executing queries.\n        :param max_workers: Maximum number of worker threads (default is 4).\n        \"\"\"\n    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\n    self.index = index\n    self.chatbot = chatbot\n    self.write_func = write_func if write_func else self.llm_response\n    self.new_index_name = self.index.name + f\"_{task_name}\"\n    self.context = context\n"}
{"model":"text-embedding-ada-002","input":"\ndef llm_response(self, chatbot: Chat, message: str, context=None, id=None):\n    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n    # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\n    #     return \"the message is too long to be processed\"\n    # moved the error catching to multi-threading but custom method could report the error here\n    return chatbot.reply(message)\n"}
{"model":"text-embedding-ada-002","input":"\ndef _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n    \"\"\"\n        Execute a sub-task using a separate copy of the chatbot instance.\n\n        :param sub_path: List of indices representing the sub-task's sequence.\n        :return: List of strings representing the responses for each query in the sub-task.\n        \"\"\"\n    if self.parallel:\n        # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n        chatbot_instance = copy.deepcopy(self.chatbot)\n    else:\n        chatbot_instance = self.chatbot\n    if isinstance(self.chatbot, BaseThread):\n        chatbot_instance.reset_memory()\n\n    sub_results = {}\n    for i in sub_path:\n        current_val = self.index.values[i]\n        response = self.write_func(\n            chatbot_instance, current_val, self.context, id=i\n        )\n        sub_results[i] = response\n    return sub_results\n"}
{"model":"text-embedding-ada-002","input":"\ndef write(self):\n    content_to_write = self.work()\n    self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\n    self.new_index.save()\n    return self.new_index\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass DiscreteDataInt(BDType):\n    alphabet: Optional[Set[int]] = Field(None, description=\"Set of allowed discrete variables. All elements should be integers.\")\n    value: int = Field(..., description=\"The discrete data value. It should be an integer.\")\n\n    @field_validator('alphabet')\n    def check_alphabet(cls, v):\n        if not all(isinstance(item, int) for item in v):\n            raise ValueError(\"All elements in 'alphabet' should be integers.\")\n        return v\n\n    @field_validator('value')\n    def check_value(cls, v, info: FieldValidationInfo):\n        alphabet = info.data.get('alphabet')\n        if alphabet is not None and v not in alphabet:\n            raise ValueError(\"Value must be in the alphabet.\")\n        return v\n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator('alphabet')\ndef check_alphabet(cls, v):\n    if not all(isinstance(item, int) for item in v):\n        raise ValueError(\"All elements in 'alphabet' should be integers.\")\n    return v\n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator('value')\ndef check_value(cls, v, info: FieldValidationInfo):\n    alphabet = info.data.get('alphabet')\n    if alphabet is not None and v not in alphabet:\n        raise ValueError(\"Value must be in the alphabet.\")\n    return v\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass DiscreteDataStr(BDType):\n    alphabet: Optional[Set[str]] = Field(None, description=\"Set of allowed discrete variables. All elements should be strings.\")\n    value: str = Field(..., description=\"The discrete data value. It should be a string.\")\n\n    @field_validator('alphabet')\n    def check_alphabet(cls, v):\n        if not all(isinstance(item, str) for item in v):\n            raise ValueError(\"All elements in 'alphabet' should be strings.\")\n        return v\n\n    @field_validator('value')\n    def check_value(cls, v, info: FieldValidationInfo):\n        alphabet = info.data.get('alphabet')\n        if alphabet is not None and v not in alphabet:\n            raise ValueError(\"Value must be in the alphabet.\")\n        return v\n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator('alphabet')\ndef check_alphabet(cls, v):\n    if not all(isinstance(item, str) for item in v):\n        raise ValueError(\"All elements in 'alphabet' should be strings.\")\n    return v\n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator('value')\ndef check_value(cls, v, info: FieldValidationInfo):\n    alphabet = info.data.get('alphabet')\n    if alphabet is not None and v not in alphabet:\n        raise ValueError(\"Value must be in the alphabet.\")\n    return v\n"}
{"model":"text-embedding-ada-002","input":"\n\n# The DiscreteDataInt and DiscreteDataStr models are defined as before\n\nclass DiscreteDataList(BaseModel):\n    alphabet: Optional[Set[Union[int, str]]] = Field(None, description=\"Set of allowed discrete variables. All elements should be of the same type (either integers or strings).\")\n    value: List[Union[DiscreteDataInt, DiscreteDataStr]] = Field(..., description=\"The list of discrete data values. All elements should be either DiscreteDataInt or DiscreteDataStr, not a mix.\")\n\n    @field_validator('value')\n    def check_alphabets(cls, value, info: FieldValidationInfo):\n        list_alphabet = info.data.get('alphabet')\n        if list_alphabet is not None:\n            for item in value:\n                item_alphabet = item.alphabet\n                if item_alphabet is not None and not set(item_alphabet).issubset(list_alphabet):\n                    raise ValueError(f\"Item alphabet {item_alphabet} is not a subset of the list alphabet {list_alphabet}.\")\n        return value\n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator('value')\ndef check_alphabets(cls, value, info: FieldValidationInfo):\n    list_alphabet = info.data.get('alphabet')\n    if list_alphabet is not None:\n        for item in value:\n            item_alphabet = item.alphabet\n            if item_alphabet is not None and not set(item_alphabet).issubset(list_alphabet):\n                raise ValueError(f\"Item alphabet {item_alphabet} is not a subset of the list alphabet {list_alphabet}.\")\n    return value\n"}
{"model":"text-embedding-ada-002","input":"class MultiDimensionalDiscrete(BDType):\n    value: List[Union[DiscreteDataInt, DiscreteDataStr]] = Field(..., description=\"The multidimensional discrete data value. It should be a list of either DiscreteDataInt or DiscreteDataStr.\")\n    type_dictionary: Dict[int, str] = Field(default_factory=dict, description=\"The helper dictionary containing the type of each dimension of the value list.\")\n    def __init__(self, **data):\n        super().__init__(**data)\n        self.type_dictionary = {i: item.__class__.__name__ for i, item in enumerate(self.value)}\n        \n    @field_validator('value')\n    def check_value(cls, value):\n        if len(value) < 2:\n            raise ValueError(\"For multidimensional discrete data, size of the list should be at least 2. For less than 2, use DiscreteDataInt or DiscreteDataStr.\")\n        return value\n    \n    from pydantic import BaseModel, Field, field_validator\n"}
{"model":"text-embedding-ada-002","input":"def __init__(self, **data):\n    super().__init__(**data)\n    self.type_dictionary = {i: item.__class__.__name__ for i, item in enumerate(self.value)}\n    \n"}
{"model":"text-embedding-ada-002","input":"@field_validator('value')\ndef check_value(cls, value):\n    if len(value) < 2:\n        raise ValueError(\"For multidimensional discrete data, size of the list should be at least 2. For less than 2, use DiscreteDataInt or DiscreteDataStr.\")\n    return value\n"}
{"model":"text-embedding-ada-002","input":"\nclass MultiDimensionalDiscreteList(BDType):\n    values: List[MultiDimensionalDiscrete] = Field(..., description=\"The list of multidimensional discrete data values. All elements should be instances of MultiDimensionalDiscrete.\")\n    joint_alphabet: Optional[Set[Tuple[Any, ...]]] = Field(None, description=\"Set of tuples representing allowed discrete variable combinations. All elements should be tuples of the same length as the number of dimensions in each joint discrete variable.\")\n\n    @field_validator('values')\n    def check_type_dictionaries(cls, values):\n        first_type_dictionary = values[0].type_dictionary\n        for value in values[1:]:\n            if value.type_dictionary != first_type_dictionary:\n                raise ValueError(\"All elements in 'values' should have the same 'type_dictionary'.\")\n        return values\n\n    @field_validator('joint_alphabet')\n    def check_joint_alphabet(cls, v, info):\n        if v is not None and \"values\" in info.data:\n            expected_tuple_length = len(info.data[\"values\"][0].value)\n            for item in v:\n                if not isinstance(item, tuple) or len(item) != expected_tuple_length:\n                    raise ValueError(f\"Each element in 'joint_alphabet' should be a tuple of length {expected_tuple_length}.\")\n                for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\n                    if dim_alphabet is not None and dim_value not in dim_alphabet:\n                        raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\n        return v\n\n\n    \n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator('values')\ndef check_type_dictionaries(cls, values):\n    first_type_dictionary = values[0].type_dictionary\n    for value in values[1:]:\n        if value.type_dictionary != first_type_dictionary:\n            raise ValueError(\"All elements in 'values' should have the same 'type_dictionary'.\")\n    return values\n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator('joint_alphabet')\ndef check_joint_alphabet(cls, v, info):\n    if v is not None and \"values\" in info.data:\n        expected_tuple_length = len(info.data[\"values\"][0].value)\n        for item in v:\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\n                raise ValueError(f\"Each element in 'joint_alphabet' should be a tuple of length {expected_tuple_length}.\")\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\n    return v\n"}
{"model":"text-embedding-ada-002","input":"\nclass RealData(BDType):\n    range: Optional[Tuple[Union[float, int], Union[float, int]]] = Field(None, description=\"An optional inclusive range (min, max) for the value.\")\n    value: Union[float, int] = Field(..., description=\"The real value data. It should be a float or an integer.\")\n\n    @field_validator(\"value\")\n    def validate_value(cls, v, values):\n        value_range = values.data[\"range\"]\n        if value_range is not None:\n            min_value, max_value = value_range\n            if not min_value <= v <= max_value:\n                raise ValueError(f\"Value {v} is not within the specified range {value_range}.\")\n        return v\n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator(\"value\")\ndef validate_value(cls, v, values):\n    value_range = values.data[\"range\"]\n    if value_range is not None:\n        min_value, max_value = value_range\n        if not min_value <= v <= max_value:\n            raise ValueError(f\"Value {v} is not within the specified range {value_range}.\")\n    return v\n"}
{"model":"text-embedding-ada-002","input":"\nclass RealDataList(BDType):\n    range: Optional[Tuple[Union[float, int], Union[float, int]]] = Field(None, description=\"An optional inclusive range (min, max) for the values.\")\n    values: List[RealData] = Field(..., description=\"The list of real value data. Each should be a RealeData object.\")\n\n    @field_validator(\"values\")\n    def validate_values(cls, values, values_dict):\n        list_range = values_dict.data.get(\"range\")\n        if list_range is not None:\n            min_value, max_value = list_range\n            for value in values:\n                if not min_value <= value.value <= max_value:\n                    raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {list_range}.\")\n        return values\n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator(\"values\")\ndef validate_values(cls, values, values_dict):\n    list_range = values_dict.data.get(\"range\")\n    if list_range is not None:\n        min_value, max_value = list_range\n        for value in values:\n            if not min_value <= value.value <= max_value:\n                raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {list_range}.\")\n    return values\n"}
{"model":"text-embedding-ada-002","input":"\nclass MultiDimensionalReal(BDType):\n    range: Optional[Union[Tuple[Union[float, int], Union[float, int]], List[Tuple[Union[float, int], Union[float, int]]]]] = Field(\n        None, \n        description=\"An optional inclusive range (min, max) for the values. If a tuple, applies to all dimensions. If a list, it must match the dimension length.\"\n    )\n    values: List[RealData] = Field(\n        ..., \n        description=\"The list of real data for each dimension. Each should be a RealData object.\"\n    )\n\n    @field_validator(\"values\")\n    def validate_values(cls, values, values_dict):\n        range_values = values_dict.data.get(\"range\")\n        if range_values is not None:\n            # If range is a tuple, apply it to all dimensions\n            if isinstance(range_values, tuple):\n                min_value, max_value = range_values\n                for value in values:\n                    if not min_value <= value.value <= max_value:\n                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n            # If range is a list, it must have the same length as values\n            elif isinstance(range_values, list):\n                if len(values) != len(range_values):\n                    raise ValueError(\"If range is a list, it must have the same length as values.\")\n                for value, (min_value, max_value) in zip(values, range_values):\n                    if not min_value <= value.value <= max_value:\n                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n        return values\n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator(\"values\")\ndef validate_values(cls, values, values_dict):\n    range_values = values_dict.data.get(\"range\")\n    if range_values is not None:\n        # If range is a tuple, apply it to all dimensions\n        if isinstance(range_values, tuple):\n            min_value, max_value = range_values\n            for value in values:\n                if not min_value <= value.value <= max_value:\n                    raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n        # If range is a list, it must have the same length as values\n        elif isinstance(range_values, list):\n            if len(values) != len(range_values):\n                raise ValueError(\"If range is a list, it must have the same length as values.\")\n            for value, (min_value, max_value) in zip(values, range_values):\n                if not min_value <= value.value <= max_value:\n                    raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n    return values\n"}
{"model":"text-embedding-ada-002","input":"\nclass MultiDimensionalRealList(BDType):\n    range: Optional[Union[Tuple[Union[float, int], Union[float, int]], List[Tuple[Union[float, int], Union[float, int]]]]] = Field(\n        None, \n        description=\"An optional inclusive range (min, max) for the values in all dimensions. If a tuple, applies to all dimensions. If a list, it must match the dimension length.\"\n    )\n    values: List[MultiDimensionalReal] = Field(\n        ..., \n        description=\"The list of multi-dimensional real data. Each should be a MultiDimensionalReal object.\"\n    )\n\n    @field_validator(\"values\")\n    def validate_values(cls, values, values_dict):\n        range_values = values_dict.data.get(\"range\")\n        dimension_length = len(values[0].values) if values else 0\n        if range_values is not None:\n            # If range is a tuple, apply it to all dimensions\n            if isinstance(range_values, tuple):\n                min_value, max_value = range_values\n                for multi_real in values:\n                    if len(multi_real.values) != dimension_length:\n                        raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n                    for value in multi_real.values:\n                        if not min_value <= value.value <= max_value:\n                            raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n            # If range is a list, it must have the same length as values in each dimension\n            elif isinstance(range_values, list):\n                if len(range_values) != dimension_length:\n                    raise ValueError(\"If range is a list, it must have the same length as values in each dimension.\")\n                for multi_real in values:\n                    if len(multi_real.values) != dimension_length:\n                        raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n                    for value, (min_value, max_value) in zip(multi_real.values, range_values):\n                        if not min_value <= value.value <= max_value:\n                            raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n        return values\n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator(\"values\")\ndef validate_values(cls, values, values_dict):\n    range_values = values_dict.data.get(\"range\")\n    dimension_length = len(values[0].values) if values else 0\n    if range_values is not None:\n        # If range is a tuple, apply it to all dimensions\n        if isinstance(range_values, tuple):\n            min_value, max_value = range_values\n            for multi_real in values:\n                if len(multi_real.values) != dimension_length:\n                    raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n                for value in multi_real.values:\n                    if not min_value <= value.value <= max_value:\n                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n        # If range is a list, it must have the same length as values in each dimension\n        elif isinstance(range_values, list):\n            if len(range_values) != dimension_length:\n                raise ValueError(\"If range is a list, it must have the same length as values in each dimension.\")\n            for multi_real in values:\n                if len(multi_real.values) != dimension_length:\n                    raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n                for value, (min_value, max_value) in zip(multi_real.values, range_values):\n                    if not min_value <= value.value <= max_value:\n                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n    return values\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass NaturalLanguageSingle(BDType):\n    text: str = Field(..., description=\"The natural language text. It should be less than or equal to `max_tokens` in length when tokenized.\")\n    max_tokens: int = Field(8000, description=\"The maximum allowed length of the text in tokens. The default value is 8000.\")\n    \n    @field_validator(\"text\")\n    def validate_text(cls, v, info):\n        try:\n            # Tokenize the text and get the token count\n            token_count = len(tokenizer.encode(v))\n        except Exception as e:\n            raise ValueError(\"Failed to tokenize text.\") from e\n\n        # Get max_tokens from info.data, if not available, default to 8000\n        max_tokens = info.data.get(\"max_tokens\", 8000)\n\n        if token_count > max_tokens:\n            raise ValueError(f\"Text is longer than {max_tokens} tokens.\")\n\n        return v\n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator(\"text\")\ndef validate_text(cls, v, info):\n    try:\n        # Tokenize the text and get the token count\n        token_count = len(tokenizer.encode(v))\n    except Exception as e:\n        raise ValueError(\"Failed to tokenize text.\") from e\n\n    # Get max_tokens from info.data, if not available, default to 8000\n    max_tokens = info.data.get(\"max_tokens\", 8000)\n\n    if token_count > max_tokens:\n        raise ValueError(f\"Text is longer than {max_tokens} tokens.\")\n\n    return v\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass NaturalLanguageList(BDType):\n    texts: List[NaturalLanguageSingle] = Field(..., description=\"A list of `NaturalLanguageSingle` objects. Each object should pass the validation requirements of the `NaturalLanguageSingle` class.\")\n"}
{"model":"text-embedding-ada-002","input":"\n\n# Base class for all bd_types\nclass BDType(BaseModel):\n    source: str = Field(\"babydragon\", description=\"The source of the data.\")\n    timestamp: Optional[datetime.datetime] = Field(None, description=\"When the data was collected or created. If not provided, the current time is used.\")\n    id: uuid.UUID = Field(default_factory=uuid.uuid4, description=\"Unique identifier of the data.\")\n    data_name: Optional[str] = Field(None, description=\"Name of the data.\")\n    elements_name: Optional[List[str]] = Field(None, description=\"Names of the elements if the data is a list.\")\n\n    @field_validator(\"timestamp\")\n    def set_timestamp(cls, v):\n        return v or datetime.datetime.now()\n\n    @field_validator(\"id\")\n    def set_id(cls, values, **kwargs):\n        if \"id\" not in values:\n            values[\"id\"] = uuid.uuid4()\n        return values\n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator(\"timestamp\")\ndef set_timestamp(cls, v):\n    return v or datetime.datetime.now()\n"}
{"model":"text-embedding-ada-002","input":"\n@field_validator(\"id\")\ndef set_id(cls, values, **kwargs):\n    if \"id\" not in values:\n        values[\"id\"] = uuid.uuid4()\n    return values\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass MemoryKernel(MemoryIndex):\n    def __init__(\n        self,\n        mem_index: MemoryIndex,\n        name: str = \"memory_kernel\",\n        k: int = 2,\n        save_path: str = None,\n    ):\n        \"\"\"\n        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\n\n        Args:\n            mem_index (MemoryIndex): A MemoryIndex instance.\n            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\n            k (int, optional): The number of hops for message passing. Defaults to 2.\n            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\n        \"\"\"\n        super().__init__(\n            index=mem_index.index,\n            values=mem_index.values,\n            embeddings=mem_index.embeddings,\n            name=name,\n            save_path=save_path,\n        )\n        self.k = k\n        if len(self.values) > 0: \n            self.create_k_hop_index(k=k)\n        else:\n            raise ValueError(\"The input MemoryIndex is empty. Please check the input MemoryIndex.\")\n\n    def cos_sim(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n        \"\"\"\n        if not isinstance(a, np.ndarray):\n            a = np.array(a)\n\n        if not isinstance(b, np.ndarray):\n            b = np.array(b)\n\n        if len(a.shape) == 1:\n            a = a[np.newaxis, :]\n\n        if len(b.shape) == 1:\n            b = b[np.newaxis, :]\n\n        a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\n        b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\n        return np.dot(a_norm, b_norm.T)\n\n    def compute_kernel(\n        self,\n        embedding_set: np.ndarray,\n        threshold: float = 0.65,\n        use_softmax: bool = False,\n    ) -> np.ndarray:\n\n        \"\"\"\n        Compute the adjacency matrix of the graph.\n\n        Parameters:\n        embedding_set (numpy array): The embedding matrix of the nodes.\n        threshold (float): The threshold for the adjacency matrix.\n        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\n        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\n\n        Returns:\n        adj_matrix (numpy array): The adjacency matrix of the graph.\n        \"\"\"\n\n        A = self.cos_sim(embedding_set, embedding_set)\n        if use_softmax:\n            # softmax\n            A = np.exp(A)\n            A = A / np.sum(A, axis=1)[:, np.newaxis]\n        adj_matrix = np.zeros_like(A)\n        adj_matrix[A > threshold] = 1\n        adj_matrix[A <= threshold] = 0\n        adj_matrix = adj_matrix.astype(np.float32)\n        return adj_matrix\n\n    def k_hop_message_passing(\n        self, A: np.ndarray, node_features: np.ndarray, k: int\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Compute the k-hop adjacency matrix and aggregated features using message passing.\n\n        Parameters:\n        A (numpy array): The adjacency matrix of the graph.\n        node_features (numpy array): The feature matrix of the nodes.\n        k (int): The number of hops for message passing.\n\n        Returns:\n        A_k (numpy array): The k-hop adjacency matrix.\n        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n        \"\"\"\n\n        print(\"Compute the k-hop adjacency matrix\")\n        A_k = np.linalg.matrix_power(A, k)\n\n        print(\"Aggregate the messages from the k-hop neighborhood:\")\n        agg_features = node_features.copy()\n\n        for i in tqdm(range(k)):\n            agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n\n        return A_k, agg_features\n\n    def graph_sylvester_embedding(self, G: Tuple, m: int, ts: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\n\n        Args:\n            G (Tuple): A tuple containing the graph's vertices (V) and weights (W).\n            m (int): The number of singular values to consider.\n            ts (np.ndarray): The spectral scales.\n\n        Returns:\n            np.ndarray: The node_embeddings matrix.\n        \"\"\"\n        V, W = G\n        n = len(V)\n        D_BE = np.diag(W.sum(axis=1))\n        L_BE = np.identity(n) - np.dot(\n            np.diag(1 / np.sqrt(D_BE.diagonal())),\n            np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n        )\n\n        A = W\n        B = L_BE\n        C = np.identity(n)\n        X = solve_sylvester(A, B, C)\n\n        U, S, _ = svd(X, full_matrices=False)\n        U_m = U[:, :m]\n        S_m = S[:m]\n\n        node_embeddings = np.zeros((n, m))\n\n        for i in range(n):\n            for s in range(m):\n                # Spectral kernel descriptor\n                node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n\n        return node_embeddings\n\n    def gen_gse_embeddings(\n        self, A: np.ndarray, embeddings: np.ndarray, m: int = 7\n    ) -> np.ndarray:\n        \"\"\"\n        Generate Graph Sylvester Embeddings.\n\n        Args:\n            A (np.ndarray): The adjacency matrix of the graph.\n            embeddings (np.ndarray): The original node embeddings.\n            m (int, optional): The number of spectral scales. Defaults to 7.\n\n        Returns:\n            np.ndarray: The generated Graph Sylvester Embeddings.\n        \"\"\"\n        V = list(range(len(embeddings)))\n        W = A\n\n        G = (V, W)\n        ts = np.linspace(0, 1, m)  # equally spaced scales\n\n        gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\n        return gse_embeddings\n\n    def create_k_hop_index(self, k: int = 2):\n        \"\"\"\n        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\n        aggregated features, and updating the memory index.\n\n        Args:\n            k (int, optional): The number of hops for message passing. Defaults to 2.\n        \"\"\"\n        self.k = k\n        print(\"Computing the adjacency matrix\")\n        print(\"Embeddings shape: \", self.embeddings.shape)\n        self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n        print(\"Computing the k-hop adjacency matrix and aggregated features\")\n        self.A_k, self.node_embeddings = self.k_hop_message_passing(\n            self.A, self.embeddings, k\n        )\n        print(\"Updating the memory index\")\n        self.k_hop_index = MemoryIndex(name=self.name)\n        self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\n\n    @classmethod\n    def from_task_results(cls, task_memory_index):\n        new_memory_kernel = cls(mem_index=task_memory_index)\n\n        # Create a new index for the new MemoryKernel\n        new_memory_kernel.create_k_hop_index()\n\n        return new_memory_kernel\n"}
{"model":"text-embedding-ada-002","input":"def __init__(\n    self,\n    mem_index: MemoryIndex,\n    name: str = \"memory_kernel\",\n    k: int = 2,\n    save_path: str = None,\n):\n    \"\"\"\n        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\n\n        Args:\n            mem_index (MemoryIndex): A MemoryIndex instance.\n            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\n            k (int, optional): The number of hops for message passing. Defaults to 2.\n            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\n        \"\"\"\n    super().__init__(\n        index=mem_index.index,\n        values=mem_index.values,\n        embeddings=mem_index.embeddings,\n        name=name,\n        save_path=save_path,\n    )\n    self.k = k\n    if len(self.values) > 0: \n        self.create_k_hop_index(k=k)\n    else:\n        raise ValueError(\"The input MemoryIndex is empty. Please check the input MemoryIndex.\")\n"}
{"model":"text-embedding-ada-002","input":"\ndef cos_sim(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n        \"\"\"\n    if not isinstance(a, np.ndarray):\n        a = np.array(a)\n\n    if not isinstance(b, np.ndarray):\n        b = np.array(b)\n\n    if len(a.shape) == 1:\n        a = a[np.newaxis, :]\n\n    if len(b.shape) == 1:\n        b = b[np.newaxis, :]\n\n    a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\n    b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\n    return np.dot(a_norm, b_norm.T)\n"}
{"model":"text-embedding-ada-002","input":"\ndef compute_kernel(\n    self,\n    embedding_set: np.ndarray,\n    threshold: float = 0.65,\n    use_softmax: bool = False,\n) -> np.ndarray:\n\n    \"\"\"\n        Compute the adjacency matrix of the graph.\n\n        Parameters:\n        embedding_set (numpy array): The embedding matrix of the nodes.\n        threshold (float): The threshold for the adjacency matrix.\n        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\n        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\n\n        Returns:\n        adj_matrix (numpy array): The adjacency matrix of the graph.\n        \"\"\"\n\n    A = self.cos_sim(embedding_set, embedding_set)\n    if use_softmax:\n        # softmax\n        A = np.exp(A)\n        A = A / np.sum(A, axis=1)[:, np.newaxis]\n    adj_matrix = np.zeros_like(A)\n    adj_matrix[A > threshold] = 1\n    adj_matrix[A <= threshold] = 0\n    adj_matrix = adj_matrix.astype(np.float32)\n    return adj_matrix\n"}
{"model":"text-embedding-ada-002","input":"\ndef k_hop_message_passing(\n    self, A: np.ndarray, node_features: np.ndarray, k: int\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n        Compute the k-hop adjacency matrix and aggregated features using message passing.\n\n        Parameters:\n        A (numpy array): The adjacency matrix of the graph.\n        node_features (numpy array): The feature matrix of the nodes.\n        k (int): The number of hops for message passing.\n\n        Returns:\n        A_k (numpy array): The k-hop adjacency matrix.\n        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n        \"\"\"\n\n    print(\"Compute the k-hop adjacency matrix\")\n    A_k = np.linalg.matrix_power(A, k)\n\n    print(\"Aggregate the messages from the k-hop neighborhood:\")\n    agg_features = node_features.copy()\n\n    for i in tqdm(range(k)):\n        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n\n    return A_k, agg_features\n"}
{"model":"text-embedding-ada-002","input":"\ndef graph_sylvester_embedding(self, G: Tuple, m: int, ts: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\n\n        Args:\n            G (Tuple): A tuple containing the graph's vertices (V) and weights (W).\n            m (int): The number of singular values to consider.\n            ts (np.ndarray): The spectral scales.\n\n        Returns:\n            np.ndarray: The node_embeddings matrix.\n        \"\"\"\n    V, W = G\n    n = len(V)\n    D_BE = np.diag(W.sum(axis=1))\n    L_BE = np.identity(n) - np.dot(\n        np.diag(1 / np.sqrt(D_BE.diagonal())),\n        np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n    )\n\n    A = W\n    B = L_BE\n    C = np.identity(n)\n    X = solve_sylvester(A, B, C)\n\n    U, S, _ = svd(X, full_matrices=False)\n    U_m = U[:, :m]\n    S_m = S[:m]\n\n    node_embeddings = np.zeros((n, m))\n\n    for i in range(n):\n        for s in range(m):\n            # Spectral kernel descriptor\n            node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n\n    return node_embeddings\n"}
{"model":"text-embedding-ada-002","input":"\ndef gen_gse_embeddings(\n    self, A: np.ndarray, embeddings: np.ndarray, m: int = 7\n) -> np.ndarray:\n    \"\"\"\n        Generate Graph Sylvester Embeddings.\n\n        Args:\n            A (np.ndarray): The adjacency matrix of the graph.\n            embeddings (np.ndarray): The original node embeddings.\n            m (int, optional): The number of spectral scales. Defaults to 7.\n\n        Returns:\n            np.ndarray: The generated Graph Sylvester Embeddings.\n        \"\"\"\n    V = list(range(len(embeddings)))\n    W = A\n\n    G = (V, W)\n    ts = np.linspace(0, 1, m)  # equally spaced scales\n\n    gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\n    return gse_embeddings\n"}
{"model":"text-embedding-ada-002","input":"\ndef create_k_hop_index(self, k: int = 2):\n    \"\"\"\n        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\n        aggregated features, and updating the memory index.\n\n        Args:\n            k (int, optional): The number of hops for message passing. Defaults to 2.\n        \"\"\"\n    self.k = k\n    print(\"Computing the adjacency matrix\")\n    print(\"Embeddings shape: \", self.embeddings.shape)\n    self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n    print(\"Computing the k-hop adjacency matrix and aggregated features\")\n    self.A_k, self.node_embeddings = self.k_hop_message_passing(\n        self.A, self.embeddings, k\n    )\n    print(\"Updating the memory index\")\n    self.k_hop_index = MemoryIndex(name=self.name)\n    self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\n"}
{"model":"text-embedding-ada-002","input":"\n@classmethod\ndef from_task_results(cls, task_memory_index):\n    new_memory_kernel = cls(mem_index=task_memory_index)\n\n    # Create a new index for the new MemoryKernel\n    new_memory_kernel.create_k_hop_index()\n\n    return new_memory_kernel\n"}
{"model":"text-embedding-ada-002","input":"\n\ndef calc_shgo_mode(scores: List[float]) -> float:\n    def objective(x):\n        return -estimate_pdf(scores)(x)\n\n    bounds = [(min(scores), max(scores))]\n    result = scipy.optimize.shgo(objective, bounds)\n    return result.x\n"}
{"model":"text-embedding-ada-002","input":"def objective(x):\n    return -estimate_pdf(scores)(x)\n"}
{"model":"text-embedding-ada-002","input":"\n\ndef estimate_pdf(scores: List[float]) -> callable:\n    pdf = scipy.stats.gaussian_kde(scores)\n    return pdf\n"}
{"model":"text-embedding-ada-002","input":"\n\ndef sort_paths_by_mode_distance(\n    paths, memory_kernel, distance_metric: str = \"cosine\"\n) -> List[List[int]]:\n    sorted_paths = []\n    for i, path in enumerate(paths):\n        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n        cluster_embeddings = np.array(cluster_embeddings)\n        cluster_mean = np.mean(cluster_embeddings, axis=0)\n        if distance_metric == \"cosine\" or distance_metric == \"guassian\":\n            scores = [\n                (i, cosine(cluster_mean, emb))\n                for i, emb in zip(path, cluster_embeddings)\n            ]\n        elif distance_metric == \"euclidean\":\n            scores = [\n                (i, np.linalg.norm(cluster_mean - emb))\n                for i, emb in zip(path, cluster_embeddings)\n            ]\n        score_values = [score for _, score in scores]  # Extract score values\n        mu = calc_shgo_mode(score_values)\n        sigma = np.std(score_values)\n        if distance_metric == \"guassian\":\n            scores = [\n                (i, np.exp(-((x - mu) ** 2) / (2 * sigma**2))) for i, x in scores\n            ]\n        # Sort path by score\n        sorted_path_and_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n        sorted_path = [x[0] for x in sorted_path_and_scores]\n        sorted_paths.append(sorted_path)\n    return sorted_paths\n"}
{"model":"text-embedding-ada-002","input":"\n\ndef sort_paths_by_kernel_density(\n    paths, memory_kernel, distance_metric: str = \"cosine\"\n) -> List[List[int]]:\n    sorted_paths = []\n    for i, path in enumerate(paths):\n        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n        cluster_embeddings = np.array(cluster_embeddings)\n        cluster_mean = np.mean(cluster_embeddings, axis=0)\n        if distance_metric == \"cosine\":\n            scores = [\n                (i, cosine(cluster_mean, emb))\n                for i, emb in zip(path, cluster_embeddings)\n            ]\n        elif distance_metric == \"euclidean\":\n            scores = [\n                (i, np.linalg.norm(cluster_mean - emb))\n                for i, emb in zip(path, cluster_embeddings)\n            ]\n        score_values = [score for _, score in scores]  # Extract score values\n\n        # Estimate PDF using Kernel Density Estimation\n        kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(\n            np.array(score_values).reshape(-1, 1)\n        )\n        kde_scores = [kde.score_samples([[x]])[0] for _, x in scores]\n\n        # Sort path by score\n        sorted_path_and_scores = sorted(\n            zip(path, kde_scores), key=lambda x: x[1], reverse=True\n        )\n        sorted_path = [x[0] for x in sorted_path_and_scores]\n        sorted_paths.append(sorted_path)\n    return sorted_paths\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass ClusterPaths:\n    def create_paths(\n        self, embeddings: np.ndarray, num_clusters: int\n    ) -> List[List[int]]:\n        raise NotImplementedError\n"}
{"model":"text-embedding-ada-002","input":"def create_paths(\n    self, embeddings: np.ndarray, num_clusters: int\n) -> List[List[int]]:\n    raise NotImplementedError\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass HDBSCANPaths(ClusterPaths):\n    def create_paths(\n        self, embeddings: np.ndarray, num_clusters: int\n    ) -> List[List[int]]:\n        clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n        cluster_assignments = clusterer.fit_predict(embeddings)\n        paths = [[] for _ in range(num_clusters)]\n        for i, cluster in enumerate(cluster_assignments):\n            paths[cluster].append(i)\n        paths = [path for path in paths if path]\n        return paths\n"}
{"model":"text-embedding-ada-002","input":"def create_paths(\n    self, embeddings: np.ndarray, num_clusters: int\n) -> List[List[int]]:\n    clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n    cluster_assignments = clusterer.fit_predict(embeddings)\n    paths = [[] for _ in range(num_clusters)]\n    for i, cluster in enumerate(cluster_assignments):\n        paths[cluster].append(i)\n    paths = [path for path in paths if path]\n    return paths\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass SpectralClusteringPaths(ClusterPaths):\n    def create_paths(\n        self, A: np.ndarray, num_clusters: int\n    ) -> List[List[int]]:\n        n_samples = A.shape[0]\n        n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\n        spectral_clustering = SpectralClustering(\n            n_clusters=num_clusters,\n            affinity=\"precomputed\",\n            n_neighbors=n_neighbors,\n            random_state=42,\n        )\n        cluster_assignments = spectral_clustering.fit_predict(A)\n        paths = [[] for _ in range(num_clusters)]\n        for i, cluster in enumerate(cluster_assignments):\n            paths[cluster].append(i)\n        paths = [path for path in paths if path]\n        return paths\n"}
{"model":"text-embedding-ada-002","input":"def create_paths(\n    self, A: np.ndarray, num_clusters: int\n) -> List[List[int]]:\n    n_samples = A.shape[0]\n    n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\n    spectral_clustering = SpectralClustering(\n        n_clusters=num_clusters,\n        affinity=\"precomputed\",\n        n_neighbors=n_neighbors,\n        random_state=42,\n    )\n    cluster_assignments = spectral_clustering.fit_predict(A)\n    paths = [[] for _ in range(num_clusters)]\n    for i, cluster in enumerate(cluster_assignments):\n        paths[cluster].append(i)\n    paths = [path for path in paths if path]\n    return paths\n"}
{"model":"text-embedding-ada-002","input":"\nclass MultiKernelVisualization:\n    def __init__(self, memory_kernel_group: MultiKernel):\n        self.memory_kernel_group = memory_kernel_group\n        self.memory_kernel_dict = memory_kernel_group.memory_kernel_dict\n        self.memory_kernel_group.generate_path_groups()\n\n    def plot_embeddings_with_path(self, embeddings, title, paths):\n        tsne = TSNE(n_components=2, random_state=42)\n        reduced_embeddings = tsne.fit_transform(embeddings)\n\n        plt.figure(figsize=(10, 8))\n        colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n        for i, path in enumerate(paths):\n            path_embeddings = reduced_embeddings[path]\n            plt.scatter(\n                path_embeddings[:, 0],\n                path_embeddings[:, 1],\n                color=colors[i],\n                label=f\"Cluster {i}\",\n            )\n            for j in range(len(path) - 1):\n                plt.plot(\n                    [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n                    [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n                    color=colors[i],\n                )\n        plt.title(title)\n        plt.legend()\n        plt.show()\n\n    def visualize_paths(self):\n        #loop through memory kernels and print path_group\n        for key, kernel in self.memory_kernel_dict.items():\n            print(f\"Kernel: {key}\")\n            paths = self.memory_kernel_group.path_group[key]\n            print(f\"Path Group: {paths}\")\n            node_embeddings = kernel.node_embeddings\n            self.plot_embeddings_with_path(\n                node_embeddings, f\"Node Embeddings for {key}\", paths\n            )\n    def plot_singular_values(self):\n        #loop through memory kernels and print path_group\n        for key, kernel in self.memory_kernel_dict.items():\n            print(f\"Kernel: {key}\")\n            A_k = kernel.A_k\n            U, S, V = np.linalg.svd(A_k)\n            plt.plot(np.log(S))\n            plt.show()\n"}
{"model":"text-embedding-ada-002","input":"def __init__(self, memory_kernel_group: MultiKernel):\n    self.memory_kernel_group = memory_kernel_group\n    self.memory_kernel_dict = memory_kernel_group.memory_kernel_dict\n    self.memory_kernel_group.generate_path_groups()\n"}
{"model":"text-embedding-ada-002","input":"\ndef plot_embeddings_with_path(self, embeddings, title, paths):\n    tsne = TSNE(n_components=2, random_state=42)\n    reduced_embeddings = tsne.fit_transform(embeddings)\n\n    plt.figure(figsize=(10, 8))\n    colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n    for i, path in enumerate(paths):\n        path_embeddings = reduced_embeddings[path]\n        plt.scatter(\n            path_embeddings[:, 0],\n            path_embeddings[:, 1],\n            color=colors[i],\n            label=f\"Cluster {i}\",\n        )\n        for j in range(len(path) - 1):\n            plt.plot(\n                [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n                [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n                color=colors[i],\n            )\n    plt.title(title)\n    plt.legend()\n    plt.show()\n"}
{"model":"text-embedding-ada-002","input":"\ndef visualize_paths(self):\n    #loop through memory kernels and print path_group\n    for key, kernel in self.memory_kernel_dict.items():\n        print(f\"Kernel: {key}\")\n        paths = self.memory_kernel_group.path_group[key]\n        print(f\"Path Group: {paths}\")\n        node_embeddings = kernel.node_embeddings\n        self.plot_embeddings_with_path(\n            node_embeddings, f\"Node Embeddings for {key}\", paths\n        )\n"}
{"model":"text-embedding-ada-002","input":"def plot_singular_values(self):\n    #loop through memory kernels and print path_group\n    for key, kernel in self.memory_kernel_dict.items():\n        print(f\"Kernel: {key}\")\n        A_k = kernel.A_k\n        U, S, V = np.linalg.svd(A_k)\n        plt.plot(np.log(S))\n        plt.show()\n"}
{"model":"text-embedding-ada-002","input":"\nclass MultiKernelStabilityAnalysis:\n    def __init__(self, memory_kernel_group: MultiKernel):\n        self.memory_kernel_group = memory_kernel_group\n\n    def get_cluster_labels(self, kernel_label: str) -> Tuple[np.ndarray, int]:\n        paths = self.memory_kernel_group.path_group[kernel_label]\n        num_clusters = len(paths)\n        cluster_labels = np.empty(len(self.memory_kernel_group.memory_kernel_dict[kernel_label].node_embeddings), dtype=int)\n\n        for cluster_index, path in enumerate(paths):\n            cluster_labels[path] = cluster_index\n\n        return cluster_labels, num_clusters\n\n    def compute_nmi(self, kernel_label1: str, kernel_label2: str) -> float:\n        cluster_labels1, _ = self.get_cluster_labels(kernel_label1)\n        cluster_labels2, _ = self.get_cluster_labels(kernel_label2)\n        nmi = normalized_mutual_info_score(cluster_labels1, cluster_labels2)\n        return nmi\n\n    def evaluate_stability(self) -> float:\n        kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\n        pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\n        nmi_sum = 0\n\n        for kernel_label1, kernel_label2 in pairwise_combinations:\n            nmi = self.compute_nmi(kernel_label1, kernel_label2)\n            nmi_sum += nmi\n\n        stability_score = nmi_sum / len(pairwise_combinations)\n        return stability_score\n"}
{"model":"text-embedding-ada-002","input":"def __init__(self, memory_kernel_group: MultiKernel):\n    self.memory_kernel_group = memory_kernel_group\n"}
{"model":"text-embedding-ada-002","input":"\ndef get_cluster_labels(self, kernel_label: str) -> Tuple[np.ndarray, int]:\n    paths = self.memory_kernel_group.path_group[kernel_label]\n    num_clusters = len(paths)\n    cluster_labels = np.empty(len(self.memory_kernel_group.memory_kernel_dict[kernel_label].node_embeddings), dtype=int)\n\n    for cluster_index, path in enumerate(paths):\n        cluster_labels[path] = cluster_index\n\n    return cluster_labels, num_clusters\n"}
{"model":"text-embedding-ada-002","input":"\ndef compute_nmi(self, kernel_label1: str, kernel_label2: str) -> float:\n    cluster_labels1, _ = self.get_cluster_labels(kernel_label1)\n    cluster_labels2, _ = self.get_cluster_labels(kernel_label2)\n    nmi = normalized_mutual_info_score(cluster_labels1, cluster_labels2)\n    return nmi\n"}
{"model":"text-embedding-ada-002","input":"\ndef evaluate_stability(self) -> float:\n    kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\n    pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\n    nmi_sum = 0\n\n    for kernel_label1, kernel_label2 in pairwise_combinations:\n        nmi = self.compute_nmi(kernel_label1, kernel_label2)\n        nmi_sum += nmi\n\n    stability_score = nmi_sum / len(pairwise_combinations)\n    return stability_score\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass MultiKernel(MemoryKernel):\n    def __init__(\n        self,\n        memory_kernel_dict: Dict[str, MemoryKernel],\n        name: str = \"memory_kernel_group\",\n    ):\n        \"\"\"\n        Initialize the MultiKernel with a dictionary of MemoryKernel instances.\n\n        Args:\n            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\n            name (str, optional): The name of the MultiKernel. Defaults to \"memory_kernel_group\".\n        \"\"\"\n        self.memory_kernel_dict = memory_kernel_dict\n        self.path_group = {}\n        self.name = name\n"}
{"model":"text-embedding-ada-002","input":"def __init__(\n    self,\n    memory_kernel_dict: Dict[str, MemoryKernel],\n    name: str = \"memory_kernel_group\",\n):\n    \"\"\"\n        Initialize the MultiKernel with a dictionary of MemoryKernel instances.\n\n        Args:\n            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\n            name (str, optional): The name of the MultiKernel. Defaults to \"memory_kernel_group\".\n        \"\"\"\n    self.memory_kernel_dict = memory_kernel_dict\n    self.path_group = {}\n    self.name = name\n"}
{"model":"text-embedding-ada-002","input":"\n\nclass HDBSCANMultiKernel(MultiKernel):\n    def __init__(\n        self,\n        memory_kernel_dict: Dict[str, MemoryKernel],\n        name: str = \"memory_kernel_group\",\n    ):\n        super().__init__(memory_kernel_dict, name)\n        self.cluster_paths = HDBSCANPaths()\n\n    def generate_path_groups(self, num_clusters: int = None) -> None:\n        path_group = {}\n        for k, v in self.memory_kernel_dict.items():\n            embeddings = v.node_embeddings\n            if num_clusters is None:\n                num_clusters = int(np.sqrt(len(embeddings)))\n            paths = self.cluster_paths.create_paths(embeddings, num_clusters)\n            path_group[k] = paths\n        self.path_group = path_group\n"}
