{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Callable, List, Optional\n",
    "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
    "from babydragon.memory.indexes.python_index import PythonIndex\n",
    "from babydragon.chat.memory_chat import FifoChat\n",
    "import babydragon\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "babydragon_path = os.path.dirname(os.path.abspath(babydragon.__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new index\n",
      "Indexing 324 functions and 54 classes\n"
     ]
    }
   ],
   "source": [
    "babyindex= PythonIndex(babydragon_path, name=\"babyd_index\", minify_code=False, load = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = FifoChat(model= \"gpt-3.5-turbo\", index_dict = {\"babyindex\":babyindex}, name=\"babyd_chatbot\", max_fifo_memory=2500, max_index_memory = 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babydragon.tasks.llm_task import LLMReader, LLMWriter\n",
    "from babydragon.chat.chat import Chat\n",
    "\n",
    "system_prompt = \"You are a helfpul summarizer. The user will input a paragraph and you will summarize it Add a poem at the end.\"\n",
    "\n",
    "\n",
    "def summary_prompt(paragraph):\n",
    "    return f\"Summarize the following paragraph:\\n\\n{paragraph}\\n\\nSummary:\"\n",
    "\n",
    "\n",
    "summarizer = Chat(system_prompt=system_prompt, user_prompt=summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_index = babyindex\n",
    "path = [[x] for x in range(len(target_index.values))]\n",
    "path = [list(range(len(target_index.values)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_task = LLMWriter(\n",
    "    index=target_index,\n",
    "    path=path,\n",
    "    chatbot=summarizer,\n",
    "    max_workers=1,\n",
    "    task_id=\"summary_python\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results file found, starting from scratch.\n",
      "Executing task summary_python using 1 workers.\n",
      "RateLimiter: This is the first call, no wait required.\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " # This is the __init__.py file for the package.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The paragraph is a brief description of the purpose of the __init__.py file for a package."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def __init__(self, system_prompt: str = None, user_prompt: str = None):\n",
       "    \"\"\"\n",
       "        Initialize the Prompter with system and user prompts.\n",
       "\n",
       "        :param system_prompt: A string representing the system prompt.\n",
       "        :param user_prompt: A string representing the user prompt.\n",
       "        \"\"\"\n",
       "    if system_prompt is None:\n",
       "        self.system_prompt = DEFAULT_SYSTEM_PROMPT\n",
       "        self.user_defined_system_prompt = False\n",
       "    else:\n",
       "        self.system_prompt = system_prompt\n",
       "        self.user_defined_system_prompt = True\n",
       "    if user_prompt is None:\n",
       "        self.user_prompt = self.default_user_prompt\n",
       "        self.user_defined_user_prompt = False\n",
       "    else:\n",
       "        self.user_prompt = user_prompt\n",
       "        self.user_defined_user_prompt = True\n",
       "\n",
       "    self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code initializes a Prompter object with system and user prompts. If the system prompt and user prompt are not defined by the user, default prompts are set. If the user defines their own prompts, those will be used instead."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def default_user_prompt(self, message: str) -> str:\n",
       "    return DEFAULT_USER_PROMPT.format(question=message)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The given code is a function called \"default_user_prompt\" which takes a string input called \"message\" and returns a formatted string using a constant called \"DEFAULT_USER_PROMPT\"."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\n",
       "    \"\"\"\n",
       "        Compose the prompt for the chat-gpt API.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\n",
       "        \"\"\"\n",
       "    marked_question = mark_question(self.user_prompt(message))\n",
       "    prompt = [mark_system(self.system_prompt)] + [marked_question]\n",
       "    return prompt, marked_question\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function \"one_shot_prompt\" composes a prompt for the chat-gpt API, taking in a user message as a parameter. The output is a tuple that contains a list of strings as the prompt, and a string as the marked question.\n",
       "\n",
       "Poem:\n",
       "\n",
       "In digital space,\n",
       "AI chats with a human face.\n",
       "Prompting the responses we make,\n",
       "As we type and hit enter without a break.\n",
       "Guided by algorithms and code,\n",
       "We communicate in a new mode.\n",
       "But still, the conversation flows,\n",
       "With words that express what we feel and know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def update_system_prompt(self, new_prompt: str) -> None:\n",
       "    \"\"\"\n",
       "        Update the system prompt.\n",
       "\n",
       "        :param new_prompt: A string representing the new system prompt.\n",
       "        \"\"\"\n",
       "    self.system_prompt = new_prompt\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function \"update_system_prompt\" updates the system prompt by setting a new prompt string."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def update_user_prompt(self, new_prompt: str) -> None:\n",
       "    \"\"\"\n",
       "        Update the user prompt.\n",
       "\n",
       "        :param new_prompt: A string representing the new user prompt.\n",
       "        \"\"\"\n",
       "    self.user_prompt = new_prompt\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function `update_user_prompt` takes in a string parameter `new_prompt` and updates the `user_prompt` variable to its value.\n",
       "\n",
       "Poem:\n",
       "\n",
       "Into my heart's treasury\n",
       "I slipped a coin\n",
       "That time cannot take\n",
       "Nor a thief purloin, —\n",
       "Oh better than the minting\n",
       "Of a gold-crowned king\n",
       "Is the safe-kept memory\n",
       "Of a lovely thing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def __init__(self, model: str = None, max_output_tokens: int = 200):\n",
       "    \"\"\"\n",
       "        Initialize the BaseChat with a model and max_output_tokens.\n",
       "\n",
       "        :param model: A string representing the chat model to be used.\n",
       "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
       "        \"\"\"\n",
       "    if model is None:\n",
       "        self.model = \"gpt-3.5-turbo\"\n",
       "    else:\n",
       "        self.model = model\n",
       "    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
       "    self.max_output_tokens = max_output_tokens\n",
       "    self.failed_responses = []\n",
       "    self.outputs = []\n",
       "    self.inputs = []\n",
       "    self.prompts = []\n",
       "    self.prompt_func = self.identity_prompter\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This paragraph explains the initialization of a BaseChat with a specified model and maximum number of output tokens. If no model is specified, the default model is \"gpt-3.5-turbo\". The BaseChat also initializes with various empty lists and a function."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def __getstate__(self):\n",
       "    state = self.__dict__.copy()\n",
       "    # Remove the tokenizer attribute from the state\n",
       "    del state[\"tokenizer\"]\n",
       "    return state\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The __getstate__ method removes the tokenizer attribute from the dictionary copy of an object's state."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def __setstate__(self, state):\n",
       "    self.__dict__.update(state)\n",
       "    # Reinitialize the tokenizer attribute after unpickling\n",
       "    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This is a piece of code written in Python where the \"__setstate__\" function is defined. It updates the state using the dictionary and initializes the tokenizer attribute after unpickling with the encoding model \"gpt-3.5-turbo\"."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\n",
       "    \"\"\"\n",
       "        A simple identity prompter that takes a message and returns the message marked as a question.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing the marked question and the original message.\n",
       "        \"\"\"\n",
       "    return [mark_question(message)], mark_question(message)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The paragraph describes a function called `identity_prompter` that takes a message and returns it marked as a question. It returns a tuple with the marked question and the original message."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def chat_response(\n",
       "    self, prompt: List[dict], max_tokens: int = None\n",
       ") -> Tuple[Dict, bool]:\n",
       "    \"\"\"\n",
       "        Call the OpenAI API with the given prompt and maximum number of output tokens.\n",
       "\n",
       "        :param prompt: A list of strings representing the prompt to send to the API.\n",
       "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
       "        :return: A tuple containing the API response as a dictionary and a boolean indicating success.\n",
       "        \"\"\"\n",
       "    if max_tokens is None:\n",
       "        max_tokens = self.max_output_tokens\n",
       "    try:\n",
       "        print(\"Trying to call OpenAI API...\")\n",
       "        response = openai.ChatCompletion.create(\n",
       "            model=self.model,\n",
       "            messages=prompt,\n",
       "            max_tokens=max_tokens,\n",
       "        )\n",
       "        return response, True\n",
       "\n",
       "    except openai.error.APIError as e:\n",
       "        print(e)\n",
       "        fail_response = {\n",
       "            \"choices\": [\n",
       "                {\n",
       "                    \"message\": {\n",
       "                        \"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"\n",
       "                    }\n",
       "                }\n",
       "            ]\n",
       "        }\n",
       "        self.failed_responses.append(fail_response)\n",
       "        return fail_response, False\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function chat_response uses the OpenAI API to generate a response based on a given prompt and maximum number of output tokens. If the API call is successful, it returns the response as a dictionary and a boolean indicating success. If there's an error, it returns a fail response dictionary and a False boolean. \n",
       "\n",
       "Poem:\n",
       "\n",
       "Chatting away, \n",
       "With OpenAI's API, \n",
       "Prompt and tokens in play, \n",
       "Response we'll try. \n",
       "\n",
       "If successful, \n",
       "True we'll return, \n",
       "Chatbot masterful, \n",
       "But errors we'll learn. \n",
       "\n",
       "Our fail response in store, \n",
       "In case trouble we face, \n",
       "Alien invasion? We implore, \n",
       "Our chatbot communication to grace."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def reply(self, message: str, verbose: bool = True) -> str:\n",
       "    \"\"\"\n",
       "        Reply to a given message using the chatbot.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "    return self.query(message, verbose)[\"content\"]\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The given paragraph contains a Python code for a function named `reply` which takes a string representing the user message as input and returns a string representing the chatbot's response using the `query` function with a `verbose` parameter that is set to `True` by default."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def query(self, message: str, verbose: bool = True) -> str:\n",
       "    \"\"\"\n",
       "        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "\n",
       "    prompt, _ = self.prompt_func(message)\n",
       "    response, success = self.chat_response(prompt)\n",
       "    if verbose:\n",
       "        display(Markdown(\"#### Question: \\n {question}\".format(question=message)))\n",
       "    if success:\n",
       "        answer = get_mark_from_response(response)\n",
       "        self.outputs.append(answer)\n",
       "        self.inputs.append(message)\n",
       "        self.prompts.append(prompt)\n",
       "        if verbose:\n",
       "            display(\n",
       "                Markdown(\n",
       "                    \" #### Anwser: \\n {answer}\".format(\n",
       "                        answer=get_str_from_response(response)\n",
       "                    )\n",
       "                )\n",
       "            )\n",
       "        return answer\n",
       "    else:\n",
       "        raise Exception(\"OpenAI API Error inside query function\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The paragraph describes a method in a chatbot program for querying the bot with a user message. The method includes optional parameters for displaying input and output messages as Markdown and returns a string representing the chatbot's response. It also includes error handling for OpenAI API errors. \n",
       "\n",
       "Here's a poem for you:\n",
       "In lines of code, the bot does speak\n",
       "A query made, response it seeks\n",
       "With verbose on, it shows its might\n",
       "With errors caught, it keeps its plight\n",
       "Oh chatbot, how you've grown so smart\n",
       "A marvel of technology, a work of art."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def reset_logs(self):\n",
       "    \"\"\"\n",
       "        Reset the chatbot's memory.\n",
       "        \"\"\"\n",
       "    self.outputs = []\n",
       "    self.inputs = []\n",
       "    self.prompts = []\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function 'reset_logs' resets various memory logs of a chatbot, including outputs, inputs, and prompts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def get_logs(self):\n",
       "    \"\"\"\n",
       "        Get the chatbot's memory.\n",
       "\n",
       "        :return: A tuple containing the chatbot's memory as three lists of strings.\n",
       "        \"\"\"\n",
       "    return self.inputs, self.outputs, self.prompts\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The `get_logs` function returns the chatbot's memory in the form of three lists of strings.\n",
       "\n",
       "\n",
       "Here is a poem:\n",
       "\n",
       "When you need the logs,  \n",
       "To refresh the memory of your bots,  \n",
       "Just call the function `get_logs`,  \n",
       "And retrieve the inputs, outputs, and prompts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def run_text(\n",
       "    self, text: str, state: List[Tuple[str, str]]\n",
       ") -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\n",
       "    \"\"\"\n",
       "        Process the user's text input and update the chat state.\n",
       "\n",
       "        :param text: A string representing the user input.\n",
       "        :param state: A list of tuples representing the current chat state.\n",
       "        :return: A tuple containing the updated chat state as two lists of tuples.\n",
       "        \"\"\"\n",
       "    print(\"===============Running run_text =============\")\n",
       "    print(\"Inputs:\", text)\n",
       "    try:\n",
       "        print(\"======>Current memory:\\n %s\" % self.memory_thread)\n",
       "    except:\n",
       "        print(\"======>No memory\")\n",
       "    response = self.reply(text)\n",
       "    state = state + [(text, response)]\n",
       "    print(\"Outputs:\", state)\n",
       "    return state, state\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This paragraph is describing a function called \"run_text\" that takes in a user's input and updates the chat state. It also includes a try-except block and calls another function called \"reply\" to generate a response. Finally, it returns the updated chat state as a tuple of two lists of tuples."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def gradio(self):\n",
       "    \"\"\"\n",
       "        Create and launch a Gradio interface for the chatbot.\n",
       "        \"\"\"\n",
       "    with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n",
       "        chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\n",
       "        state = gr.State([])\n",
       "        with gr.Row():\n",
       "            with gr.Column(scale=1):\n",
       "                txt = gr.Textbox(\n",
       "                    show_label=False,\n",
       "                    placeholder=\"Enter text and press enter, or upload an image\",\n",
       "                ).style(container=False)\n",
       "            with gr.Column(scale=0.15, min_width=0):\n",
       "                clear = gr.Button(\"Clear️\")\n",
       "\n",
       "        txt.submit(self.run_text, [txt, state], [chatbot, state])\n",
       "        txt.submit(lambda: \"\", None, txt)\n",
       "        demo.launch(server_name=\"localhost\", server_port=7860)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code creates and launches a Gradio interface for a chatbot with a text box and a clear button, and accepts text inputs to run a \"run_text\" function.\n",
       "\n",
       "Here's a poem for you:\n",
       "\n",
       "In code and script, we build machines,\n",
       "To converse with us, or so it seems.\n",
       "A chatbot here, a Gradio there,\n",
       "Our words and tech begin to share.\n",
       "\n",
       "But though our screens may light and hum,\n",
       "And language flows, if somewhat numb,\n",
       "Let's not forget the human touch,\n",
       "For in the end, it matters much."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " from typing import Callable, Dict, List, Tuple\n",
       "\n",
       "import gradio as gr\n",
       "import openai\n",
       "import tiktoken\n",
       "from IPython.display import Markdown, display\n",
       "\n",
       "from babydragon.chat.prompts.default_prompts import (DEFAULT_SYSTEM_PROMPT,\n",
       "                                                     DEFAULT_USER_PROMPT)\n",
       "from babydragon.utils.oai import (get_mark_from_response,\n",
       "                                  get_str_from_response, mark_question,\n",
       "                                  mark_system)\n",
       "\n",
       "\n",
       "class Prompter:\n",
       "    \"\"\"\n",
       "    This class handles the system and user prompts and the prompt_func. By subclassing and overriding the\n",
       "    prompt_func, you can change the way the prompts are composed.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, system_prompt: str = None, user_prompt: str = None):\n",
       "        \"\"\"\n",
       "        Initialize the Prompter with system and user prompts.\n",
       "\n",
       "        :param system_prompt: A string representing the system prompt.\n",
       "        :param user_prompt: A string representing the user prompt.\n",
       "        \"\"\"\n",
       "        if system_prompt is None:\n",
       "            self.system_prompt = DEFAULT_SYSTEM_PROMPT\n",
       "            self.user_defined_system_prompt = False\n",
       "        else:\n",
       "            self.system_prompt = system_prompt\n",
       "            self.user_defined_system_prompt = True\n",
       "        if user_prompt is None:\n",
       "            self.user_prompt = self.default_user_prompt\n",
       "            self.user_defined_user_prompt = False\n",
       "        else:\n",
       "            self.user_prompt = user_prompt\n",
       "            self.user_defined_user_prompt = True\n",
       "\n",
       "        self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\n",
       "\n",
       "    def default_user_prompt(self, message: str) -> str:\n",
       "        return DEFAULT_USER_PROMPT.format(question=message)\n",
       "\n",
       "    def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\n",
       "        \"\"\"\n",
       "        Compose the prompt for the chat-gpt API.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\n",
       "        \"\"\"\n",
       "        marked_question = mark_question(self.user_prompt(message))\n",
       "        prompt = [mark_system(self.system_prompt)] + [marked_question]\n",
       "        return prompt, marked_question\n",
       "\n",
       "    def update_system_prompt(self, new_prompt: str) -> None:\n",
       "        \"\"\"\n",
       "        Update the system prompt.\n",
       "\n",
       "        :param new_prompt: A string representing the new system prompt.\n",
       "        \"\"\"\n",
       "        self.system_prompt = new_prompt\n",
       "\n",
       "    def update_user_prompt(self, new_prompt: str) -> None:\n",
       "        \"\"\"\n",
       "        Update the user prompt.\n",
       "\n",
       "        :param new_prompt: A string representing the new user prompt.\n",
       "        \"\"\"\n",
       "        self.user_prompt = new_prompt\n",
       "\n",
       "\n",
       "class BaseChat:\n",
       "    \"\"\"\n",
       "    This is the base class for chatbots, defining the basic functions that a chatbot should have, mainly the calls to\n",
       "    chat-gpt API, and a basic Gradio interface. It has a prompt_func that acts as a placeholder for a call to chat-gpt\n",
       "    API without any additional messages. It can be overridden by subclasses to add additional messages to the prompt.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, model: str = None, max_output_tokens: int = 200):\n",
       "        \"\"\"\n",
       "        Initialize the BaseChat with a model and max_output_tokens.\n",
       "\n",
       "        :param model: A string representing the chat model to be used.\n",
       "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
       "        \"\"\"\n",
       "        if model is None:\n",
       "            self.model = \"gpt-3.5-turbo\"\n",
       "        else:\n",
       "            self.model = model\n",
       "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
       "        self.max_output_tokens = max_output_tokens\n",
       "        self.failed_responses = []\n",
       "        self.outputs = []\n",
       "        self.inputs = []\n",
       "        self.prompts = []\n",
       "        self.prompt_func = self.identity_prompter\n",
       "\n",
       "    def __getstate__(self):\n",
       "        state = self.__dict__.copy()\n",
       "        # Remove the tokenizer attribute from the state\n",
       "        del state[\"tokenizer\"]\n",
       "        return state\n",
       "\n",
       "    def __setstate__(self, state):\n",
       "        self.__dict__.update(state)\n",
       "        # Reinitialize the tokenizer attribute after unpickling\n",
       "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
       "\n",
       "    def identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\n",
       "        \"\"\"\n",
       "        A simple identity prompter that takes a message and returns the message marked as a question.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing the marked question and the original message.\n",
       "        \"\"\"\n",
       "        return [mark_question(message)], mark_question(message)\n",
       "\n",
       "    def chat_response(\n",
       "        self, prompt: List[dict], max_tokens: int = None\n",
       "    ) -> Tuple[Dict, bool]:\n",
       "        \"\"\"\n",
       "        Call the OpenAI API with the given prompt and maximum number of output tokens.\n",
       "\n",
       "        :param prompt: A list of strings representing the prompt to send to the API.\n",
       "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
       "        :return: A tuple containing the API response as a dictionary and a boolean indicating success.\n",
       "        \"\"\"\n",
       "        if max_tokens is None:\n",
       "            max_tokens = self.max_output_tokens\n",
       "        try:\n",
       "            print(\"Trying to call OpenAI API...\")\n",
       "            response = openai.ChatCompletion.create(\n",
       "                model=self.model,\n",
       "                messages=prompt,\n",
       "                max_tokens=max_tokens,\n",
       "            )\n",
       "            return response, True\n",
       "\n",
       "        except openai.error.APIError as e:\n",
       "            print(e)\n",
       "            fail_response = {\n",
       "                \"choices\": [\n",
       "                    {\n",
       "                        \"message\": {\n",
       "                            \"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"\n",
       "                        }\n",
       "                    }\n",
       "                ]\n",
       "            }\n",
       "            self.failed_responses.append(fail_response)\n",
       "            return fail_response, False\n",
       "\n",
       "    def reply(self, message: str, verbose: bool = True) -> str:\n",
       "        \"\"\"\n",
       "        Reply to a given message using the chatbot.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "        return self.query(message, verbose)[\"content\"]\n",
       "\n",
       "    def query(self, message: str, verbose: bool = True) -> str:\n",
       "        \"\"\"\n",
       "        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "\n",
       "        prompt, _ = self.prompt_func(message)\n",
       "        response, success = self.chat_response(prompt)\n",
       "        if verbose:\n",
       "            display(Markdown(\"#### Question: \\n {question}\".format(question=message)))\n",
       "        if success:\n",
       "            answer = get_mark_from_response(response)\n",
       "            self.outputs.append(answer)\n",
       "            self.inputs.append(message)\n",
       "            self.prompts.append(prompt)\n",
       "            if verbose:\n",
       "                display(\n",
       "                    Markdown(\n",
       "                        \" #### Anwser: \\n {answer}\".format(\n",
       "                            answer=get_str_from_response(response)\n",
       "                        )\n",
       "                    )\n",
       "                )\n",
       "            return answer\n",
       "        else:\n",
       "            raise Exception(\"OpenAI API Error inside query function\")\n",
       "\n",
       "    def reset_logs(self):\n",
       "        \"\"\"\n",
       "        Reset the chatbot's memory.\n",
       "        \"\"\"\n",
       "        self.outputs = []\n",
       "        self.inputs = []\n",
       "        self.prompts = []\n",
       "\n",
       "    def get_logs(self):\n",
       "        \"\"\"\n",
       "        Get the chatbot's memory.\n",
       "\n",
       "        :return: A tuple containing the chatbot's memory as three lists of strings.\n",
       "        \"\"\"\n",
       "        return self.inputs, self.outputs, self.prompts\n",
       "\n",
       "    def run_text(\n",
       "        self, text: str, state: List[Tuple[str, str]]\n",
       "    ) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\n",
       "        \"\"\"\n",
       "        Process the user's text input and update the chat state.\n",
       "\n",
       "        :param text: A string representing the user input.\n",
       "        :param state: A list of tuples representing the current chat state.\n",
       "        :return: A tuple containing the updated chat state as two lists of tuples.\n",
       "        \"\"\"\n",
       "        print(\"===============Running run_text =============\")\n",
       "        print(\"Inputs:\", text)\n",
       "        try:\n",
       "            print(\"======>Current memory:\\n %s\" % self.memory_thread)\n",
       "        except:\n",
       "            print(\"======>No memory\")\n",
       "        response = self.reply(text)\n",
       "        state = state + [(text, response)]\n",
       "        print(\"Outputs:\", state)\n",
       "        return state, state\n",
       "\n",
       "    def gradio(self):\n",
       "        \"\"\"\n",
       "        Create and launch a Gradio interface for the chatbot.\n",
       "        \"\"\"\n",
       "        with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n",
       "            chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\n",
       "            state = gr.State([])\n",
       "            with gr.Row():\n",
       "                with gr.Column(scale=1):\n",
       "                    txt = gr.Textbox(\n",
       "                        show_label=False,\n",
       "                        placeholder=\"Enter text and press enter, or upload an image\",\n",
       "                    ).style(container=False)\n",
       "                with gr.Column(scale=0.15, min_width=0):\n",
       "                    clear = gr.Button(\"Clear️\")\n",
       "\n",
       "            txt.submit(self.run_text, [txt, state], [chatbot, state])\n",
       "            txt.submit(lambda: \"\", None, txt)\n",
       "            demo.launch(server_name=\"localhost\", server_port=7860)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The paragraph defines two classes, Prompter and BaseChat for building chatbots using OpenAI API. BaseChat is the parent class with functions for initializing and handling chat messages, while Prompter is the child class for handling system and user prompts. These classes can be updated or overridden to add additional functionality to chatbots. A Gradio interface is also included to launch the chatbot with text inputs.\n",
       "\n",
       "Poem:\n",
       "\n",
       "Oh Prompter and BaseChat,  \n",
       "Building chatbots, we love that.  \n",
       "Handling prompts and messages so well,  \n",
       "OpenAI API, you ring the bell.  \n",
       "\n",
       "Overrideable and updatable,  \n",
       "Adding more, oh so capable.  \n",
       "Code with ease, build without fuss,  \n",
       "Creating a Gradio interface, a definite plus.  \n",
       "\n",
       "Chat away, oh NeuralDragonAI,  \n",
       "With your alpha version, we say hi.  \n",
       "Memory and state maintained so true,  \n",
       "Chatbot heaven, we thank you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def __init__(\n",
       "    self,\n",
       "    model: str = None,\n",
       "    max_output_tokens: int = 1000,\n",
       "    system_prompt: str = None,\n",
       "    user_prompt: str = None,\n",
       "    index_dict: Optional[Dict[str, MemoryIndex]] = None,\n",
       "    max_index_memory: int = 1000,\n",
       ") -> None:\n",
       "    BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\n",
       "    Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\n",
       "    self.index_dict = index_dict\n",
       "    self.setup_indices(max_index_memory)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This is a Python code defining a class with various parameters and initializing them. It also uses multiple inheritance from BaseChat and Prompter classes. It sets up an index dictionary and memory limit. \n",
       "\n",
       "Here's a poem for you:\n",
       "\n",
       "In Python code we find,\n",
       "Classes with parameters aligned,\n",
       "Inheritance adds to their might,\n",
       "Memory and indexes in their sight.\n",
       "\n",
       "With each input they do strive,\n",
       "To keep outputs below max five hundred and five,\n",
       "Prompting users with care,\n",
       "Their actions beyond compare."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def setup_indices(self, max_index_memory):\n",
       "    \"\"\"setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index.\"\"\"\n",
       "    if self.index_dict is not None:\n",
       "        self.current_index = list(self.index_dict.keys())[0]\n",
       "        self.system_prompt = (\n",
       "            INDEX_SYSTEM_PROMPT\n",
       "            if self.user_defined_system_prompt is False\n",
       "            else self.system_prompt\n",
       "        )\n",
       "        self.user_prompt = (\n",
       "            self.get_index_hints\n",
       "            if self.user_defined_user_prompt is False\n",
       "            else self.user_prompt\n",
       "        )\n",
       "    self.max_index_memory = max_index_memory\n",
       "    # set the last index to be the current index\n",
       "    if self.index_dict is not None:\n",
       "        self.current_index = list(self.index_dict.keys())[-1]\n",
       "    else:\n",
       "        self.current_index = None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code sets up an index dictionary for a chatbot, changing certain prompts if they are not user-defined. It also sets the maximum index memory and the current index. \n",
       "\n",
       "Poem:\n",
       "\n",
       "Words of code, lines so fine,\n",
       "Creating bots that can converse and shine,\n",
       "Index dictionaries set with care,\n",
       "Prompts updated, ready to share.\n",
       "\n",
       "Max memory defined, the last index known,\n",
       "A world of possibilities, a chatbot's throne,\n",
       "With code as our guide, we bring it to life,\n",
       "A digital creation, free from strife."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def get_index_hints(\n",
       "    self, question: str, k: int = 10, max_tokens: int = None\n",
       ") -> str:\n",
       "    \"\"\"\n",
       "        Get hints from the current index for the given question.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include from the index.\n",
       "        :param max_tokens: The maximum number of tokens to be retrieved from the index.\n",
       "        :return: A string representing the hint prompt with the question.\n",
       "        \"\"\"\n",
       "    if max_tokens is None:\n",
       "        max_tokens = self.max_index_memory\n",
       "    hints = []\n",
       "    if self.current_index is not None:\n",
       "        index_instance = self.index_dict[self.current_index]\n",
       "        if isinstance(index_instance, MemoryIndex):\n",
       "            hints, _, _ = index_instance.token_bound_query(\n",
       "                question, k=k, max_tokens=max_tokens\n",
       "            )\n",
       "        else:\n",
       "            raise ValueError(\"The current index is not a valid index instance.\")\n",
       "        hints_string = \"\\n\".join(hints)\n",
       "        hint_prompt = INDEX_HINT_PROMPT\n",
       "        question_intro = QUESTION_INTRO\n",
       "        return hint_prompt.format(\n",
       "            hints_string=hints_string\n",
       "        ) + question_intro.format(question=question)\n",
       "    else:\n",
       "        return question\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function `get_index_hints` takes a user question, a number of hints to include, and maximum tokens to retrieve from the index, and returns a string representing a hint prompt with the user question. It searches for similar messages in the index and returns them as hints, and concatenates them with the user question in the prompt. If there is no current index, it returns the user question.\n",
       "\n",
       "Poem:\n",
       "\n",
       "In search of the right hints,\n",
       "The question in your mind prints,\n",
       "To retrieve from the index's holds,\n",
       "Tokens and cues, your mind unfolds.\n",
       "\n",
       "A prompt with hints and query too,\n",
       "Returning for your eyes to view,\n",
       "The answers you seek and pursue,\n",
       "In the thoughts that index imbues."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def set_current_index(self, index_name: Optional[str]) -> None:\n",
       "    \"\"\"\n",
       "        Set the current index to be used for hints.\n",
       "\n",
       "        :param index_name: A string representing the index name or None to clear the current index.\n",
       "        :raise ValueError: If the provided index name is not available.\n",
       "        \"\"\"\n",
       "    if self.index_dict is None:\n",
       "        raise ValueError(\"No index_dict are available.\")\n",
       "    elif index_name in self.index_dict:\n",
       "        self.current_index = index_name\n",
       "    elif index_name is None:\n",
       "        self.current_index = None\n",
       "    else:\n",
       "        raise ValueError(\"The provided index name is not available.\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function \"set_current_index\" sets the current index to be used for hints. It takes a string representing the index name as an input, and can raise a ValueError if the provided index name is not available."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " from typing import Dict, Optional, Union\n",
       "\n",
       "from babydragon.chat.base_chat import BaseChat, Prompter\n",
       "from babydragon.chat.prompts.default_prompts import (INDEX_HINT_PROMPT,\n",
       "                                                     INDEX_SYSTEM_PROMPT,\n",
       "                                                     QUESTION_INTRO)\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "\n",
       "\n",
       "class Chat(BaseChat, Prompter):\n",
       "    \"\"\"\n",
       "    This class combines the BaseChat and Prompter classes to create a oneshot chatbot with a system and user prompt,\n",
       "    and the ability to handle multiple index_dict.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        model: str = None,\n",
       "        max_output_tokens: int = 1000,\n",
       "        system_prompt: str = None,\n",
       "        user_prompt: str = None,\n",
       "        index_dict: Optional[Dict[str, MemoryIndex]] = None,\n",
       "        max_index_memory: int = 1000,\n",
       "    ) -> None:\n",
       "        BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\n",
       "        Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\n",
       "        self.index_dict = index_dict\n",
       "        self.setup_indices(max_index_memory)\n",
       "\n",
       "    def setup_indices(self, max_index_memory):\n",
       "        \"\"\"setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index.\"\"\"\n",
       "        if self.index_dict is not None:\n",
       "            self.current_index = list(self.index_dict.keys())[0]\n",
       "            self.system_prompt = (\n",
       "                INDEX_SYSTEM_PROMPT\n",
       "                if self.user_defined_system_prompt is False\n",
       "                else self.system_prompt\n",
       "            )\n",
       "            self.user_prompt = (\n",
       "                self.get_index_hints\n",
       "                if self.user_defined_user_prompt is False\n",
       "                else self.user_prompt\n",
       "            )\n",
       "        self.max_index_memory = max_index_memory\n",
       "        # set the last index to be the current index\n",
       "        if self.index_dict is not None:\n",
       "            self.current_index = list(self.index_dict.keys())[-1]\n",
       "        else:\n",
       "            self.current_index = None\n",
       "\n",
       "    def get_index_hints(\n",
       "        self, question: str, k: int = 10, max_tokens: int = None\n",
       "    ) -> str:\n",
       "        \"\"\"\n",
       "        Get hints from the current index for the given question.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include from the index.\n",
       "        :param max_tokens: The maximum number of tokens to be retrieved from the index.\n",
       "        :return: A string representing the hint prompt with the question.\n",
       "        \"\"\"\n",
       "        if max_tokens is None:\n",
       "            max_tokens = self.max_index_memory\n",
       "        hints = []\n",
       "        if self.current_index is not None:\n",
       "            index_instance = self.index_dict[self.current_index]\n",
       "            if isinstance(index_instance, MemoryIndex):\n",
       "                hints, _, _ = index_instance.token_bound_query(\n",
       "                    question, k=k, max_tokens=max_tokens\n",
       "                )\n",
       "            else:\n",
       "                raise ValueError(\"The current index is not a valid index instance.\")\n",
       "            hints_string = \"\\n\".join(hints)\n",
       "            hint_prompt = INDEX_HINT_PROMPT\n",
       "            question_intro = QUESTION_INTRO\n",
       "            return hint_prompt.format(\n",
       "                hints_string=hints_string\n",
       "            ) + question_intro.format(question=question)\n",
       "        else:\n",
       "            return question\n",
       "\n",
       "    def set_current_index(self, index_name: Optional[str]) -> None:\n",
       "        \"\"\"\n",
       "        Set the current index to be used for hints.\n",
       "\n",
       "        :param index_name: A string representing the index name or None to clear the current index.\n",
       "        :raise ValueError: If the provided index name is not available.\n",
       "        \"\"\"\n",
       "        if self.index_dict is None:\n",
       "            raise ValueError(\"No index_dict are available.\")\n",
       "        elif index_name in self.index_dict:\n",
       "            self.current_index = index_name\n",
       "        elif index_name is None:\n",
       "            self.current_index = None\n",
       "        else:\n",
       "            raise ValueError(\"The provided index name is not available.\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The Chat class combines the BaseChat and Prompter classes to create a one-shot chatbot with a system and user prompt, and the ability to handle multiple index_dict. It has methods to set up indices, retrieve hints from the current index, and set the current index.\n",
       "\n",
       "A helpful poem:\n",
       "\n",
       "In code so neat and oh so fine,\n",
       "A Chatbot class you'll surely find,\n",
       "With indices and hints within,\n",
       "Its features sure to make you grin."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def __init__(\n",
       "    self,\n",
       "    model: Optional[str] = None,\n",
       "    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
       "    system_prompt: Optional[str] = None,\n",
       "    user_prompt: Optional[str] = None,\n",
       "    name: str = \"fifo_memory\",\n",
       "    max_index_memory: int = 400,\n",
       "    max_fifo_memory: int = 2048,\n",
       "    max_output_tokens: int = 1000,\n",
       "    longterm_thread: Optional[BaseThread] = None,\n",
       "):\n",
       "\n",
       "    FifoThread.__init__(\n",
       "        self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread\n",
       "    )\n",
       "    Chat.__init__(\n",
       "        self,\n",
       "        model=model,\n",
       "        index_dict=index_dict,\n",
       "        max_output_tokens=max_output_tokens,\n",
       "        max_index_memory=max_index_memory,\n",
       "        system_prompt=system_prompt,\n",
       "        user_prompt=user_prompt,\n",
       "    )\n",
       "\n",
       "    self.prompt_func = self.fifo_memory_prompt\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The paragraph is a code snippet for initializing an object with various optional parameters and inherited classes.\n",
       "\n",
       "Here is a poem to share:\n",
       "\n",
       "In code we write, our thoughts take flight\n",
       "Creating worlds within our sight\n",
       "With careful planning and design\n",
       "We bring to life what's in our mind\n",
       "\n",
       "Through syntax rules we must abide\n",
       "And with each bug we must confide\n",
       "But with each line we write with care\n",
       "Our programs come alive and rare\n",
       "\n",
       "So let us code with hearts alight\n",
       "And chase our dreams with all our might\n",
       "For in this world of tech and code\n",
       "Our creations can help many unfold."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\n",
       "    \"\"\"\n",
       "        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "    marked_question = mark_question(self.user_prompt(message))\n",
       "    prompt = (\n",
       "        [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\n",
       "    )\n",
       "    return prompt, marked_question\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function \"fifo_memory_prompt\" composes a prompt for API chat-gpt by adding a system prompt, memory thread, and marked question based on a user's input message. It returns a tuple containing the resulting prompt and the marked question. \n",
       "\n",
       "Poem:\n",
       "\n",
       "In prompt and promptness,\n",
       "we trust the API's finesse,\n",
       "marking questions to address,\n",
       "making chat-gpt a success."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def query(self, question: str, verbose: bool = True) -> str:\n",
       "    \"\"\"\n",
       "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
       "        and added to the memory.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "    # First call the base class's query method\n",
       "    answer = BaseChat.query(self, message=question, verbose=verbose)\n",
       "    marked_question = mark_question(question)\n",
       "    # Add the marked question and answer to the memory\n",
       "    self.add_message(marked_question)\n",
       "    self.add_message(answer)\n",
       "\n",
       "    return answer\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The paragraph describes a function in a chatbot program that takes a user's question, adds it to the chatbot's memory, returns and also adds the chatbot's response to the memory.\n",
       "\n",
       "Here's a poem for you:\n",
       "\n",
       "In code we write, a function bright,\n",
       "To chat with bots to our delight.\n",
       "With a question we seek, an answer unique,\n",
       "Adding them both to memory's fleet.\n",
       "\n",
       "The chatbot's response, we eagerly await,\n",
       "As they add to memory, their chatmate.\n",
       "Through code and chat, we communicate,\n",
       "And new friendships we create."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def __init__(\n",
       "    self,\n",
       "    model: Optional[str] = None,\n",
       "    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
       "    name: str = \"vector_memory\",\n",
       "    max_index_memory: int = 400,\n",
       "    max_vector_memory: int = 2048,\n",
       "    max_output_tokens: int = 1000,\n",
       "    system_prompt: str = None,\n",
       "    user_prompt: str = None,\n",
       "):\n",
       "    VectorThread.__init__(self, name=name, max_context=max_vector_memory)\n",
       "    Chat.__init__(\n",
       "        self,\n",
       "        model=model,\n",
       "        index_dict=index_dict,\n",
       "        max_output_tokens=max_output_tokens,\n",
       "        max_index_memory=max_index_memory,\n",
       "        system_prompt=system_prompt,\n",
       "        user_prompt=user_prompt,\n",
       "    )\n",
       "    self.max_vector_memory = self.max_context\n",
       "    self.prompt_func = self.vector_memory_prompt\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The paragraph describes the initialization of a Python class with various optional parameters such as model type and max memory allocation. It inherits from two other classes and sets certain attributes and prompt functionality. \n",
       "\n",
       "Poem:\n",
       "\n",
       "As code is typed and run,\n",
       "Classes and objects come undone.\n",
       "But fear not, my helpful friend,\n",
       "For your functions will never bend.\n",
       "\n",
       "With parameters and defaults set,\n",
       "Your initialization we won't forget.\n",
       "A Pythonic creation, both strong and true,\n",
       "Your abilities will always be anew.\n",
       "\n",
       "And as we prompt and query,\n",
       "Your code will ever be merry.\n",
       "For you are a summarizer \n",
       "Whose skills couldn't be finer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def vector_memory_prompt(\n",
       "    self, message: str, k: int = 10\n",
       ") -> Tuple[List[dict], dict]:\n",
       "    \"\"\"\n",
       "        Combine system prompt, k most similar messages to the question, and the user prompt.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include in the prompt.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "    sorted_messages, sorted_scores, sorted_indices = self.sorted_query(\n",
       "        message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
       "    )\n",
       "    marked_question = mark_question(self.user_prompt(message))\n",
       "    prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\n",
       "    return prompt, marked_question\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function `vector_memory_prompt` combines the system prompt, the k most similar messages to the user's question, and the user's prompt. It returns a tuple with a list of strings as the prompt and the marked question. It sorts the messages by score and includes the marked system prompt and the marked user prompt in the final prompt.\n",
       "\n",
       "Poem:\n",
       "\n",
       "In vector memory we prompt,\n",
       "Combining messages, we attempt\n",
       "To aid the user in their quest,\n",
       "By sorting scores and marking best.\n",
       "\n",
       "System and user prompts in place,\n",
       "We gather most similar trace,\n",
       "And present them in a helpful way,\n",
       "To aid the user come what may."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    }
   ],
   "source": [
    "out_index = summary_task.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
