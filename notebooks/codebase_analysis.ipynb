{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielhug/neuraldragon/frames_arc/BabyDragon/notebooks/venv/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/danielhug/neuraldragon/frames_arc/BabyDragon/notebooks/venv/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/danielhug/neuraldragon/frames_arc/BabyDragon/notebooks/venv/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/danielhug/neuraldragon/frames_arc/BabyDragon/notebooks/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/danielhug/neuraldragon/frames_arc/BabyDragon/notebooks/venv/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "from babydragon.memory.frames.code_frame import CodeFrame\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import Union, List, Optional\n",
    "import tiktoken\n",
    "import datetime\n",
    "import uuid\n",
    "import matplotlib.pyplot as plt\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "babydragon.utils.main_logger - INFO - Found 1184 values in the directory /Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon\n"
     ]
    }
   ],
   "source": [
    "code_repo = \"/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon\"\n",
    "mfp = CodeFrame.from_python(directory_path=code_repo, value_column=\"code\", embeddable_columns=[\"code\"], context_columns=[\"libcst_tree\", \"filename\"], name=\"babydragon_frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfp.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfp = CodeFrame.load(frame_path='./storage/babydragon_frame', name='babydragon_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfp = mfp.tokenize_column(\"code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012073, -0.004771, … -0.043605]</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.030298, 0.011617, … -0.039327]</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012848, 0.010885, … -0.027489]</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025782, -0.008832, … -0.04199]</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 198]</td><td>102</td><td>[-0.018791, -0.018855, … -0.04652]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌────────────────┬────────────────┬────────────────┬───────────────┬───────────────┬───────────────┐\n",
       "│ code           ┆ libcst tree    ┆ filename       ┆ tokens|code   ┆ tokens_len|co ┆ embedding|cod │\n",
       "│ ---            ┆ ---            ┆ ---            ┆ ---           ┆ de            ┆ e             │\n",
       "│ str            ┆ str            ┆ str            ┆ list[i64]     ┆ ---           ┆ ---           │\n",
       "│                ┆                ┆                ┆               ┆ i64           ┆ list[f64]     │\n",
       "╞════════════════╪════════════════╪════════════════╪═══════════════╪═══════════════╪═══════════════╡\n",
       "│                ┆ ClassDef(      ┆ /Users/danielh ┆ [198, 1058, … ┆ 40            ┆ [-0.012073,   │\n",
       "│ class Embeddab ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ -0.004771, …  │\n",
       "│ leType(Enum):  ┆        …       ┆ n/gi…          ┆               ┆               ┆ -0.0436…      │\n",
       "│    …           ┆                ┆                ┆               ┆               ┆               │\n",
       "│                ┆ FunctionDef(   ┆ /Users/danielh ┆ [198, 755, …  ┆ 194           ┆ [0.030298,    │\n",
       "│ def infer_embe ┆     name=Name( ┆ ug/neuraldrago ┆ 14790]        ┆               ┆ 0.011617, …   │\n",
       "│ ddable_type(co ┆     …          ┆ n/gi…          ┆               ┆               ┆ -0.039327…    │\n",
       "│ lum…           ┆                ┆                ┆               ┆               ┆               │\n",
       "│                ┆ FunctionDef(   ┆ /Users/danielh ┆ [198, 755, …  ┆ 59            ┆ [0.012848,    │\n",
       "│ def numeric_em ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ 0.010885, …   │\n",
       "│ bedder(column) ┆     …          ┆ n/gi…          ┆               ┆               ┆ -0.027489…    │\n",
       "│ :              ┆                ┆                ┆               ┆               ┆               │\n",
       "│  …             ┆                ┆                ┆               ┆               ┆               │\n",
       "│                ┆ ClassDef(      ┆ /Users/danielh ┆ [271, 1058, … ┆ 213           ┆ [-0.025782,   │\n",
       "│                ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ -0.008832, …  │\n",
       "│ class Embeddin ┆        …       ┆ n/gi…          ┆               ┆               ┆ -0.0419…      │\n",
       "│ gTask(BaseTask ┆                ┆                ┆               ┆               ┆               │\n",
       "│ ):…            ┆                ┆                ┆               ┆               ┆               │\n",
       "│ def __init__(  ┆ FunctionDef(   ┆ /Users/danielh ┆ [755, 1328, … ┆ 102           ┆ [-0.018791,   │\n",
       "│     self,      ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ -0.018855, …  │\n",
       "│     embe…      ┆     …          ┆ n/gi…          ┆               ┆               ┆ -0.0465…      │\n",
       "└────────────────┴────────────────┴────────────────┴───────────────┴───────────────┴───────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>value</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>1179.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>130.87447</td></tr><tr><td>&quot;std&quot;</td><td>276.200543</td></tr><tr><td>&quot;min&quot;</td><td>9.0</td></tr><tr><td>&quot;max&quot;</td><td>3568.0</td></tr><tr><td>&quot;median&quot;</td><td>53.0</td></tr><tr><td>&quot;25%&quot;</td><td>27.0</td></tr><tr><td>&quot;75%&quot;</td><td>110.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 2)\n",
       "┌────────────┬────────────┐\n",
       "│ statistic  ┆ value      │\n",
       "│ ---        ┆ ---        │\n",
       "│ str        ┆ f64        │\n",
       "╞════════════╪════════════╡\n",
       "│ count      ┆ 1179.0     │\n",
       "│ null_count ┆ 0.0        │\n",
       "│ mean       ┆ 130.87447  │\n",
       "│ std        ┆ 276.200543 │\n",
       "│ min        ┆ 9.0        │\n",
       "│ max        ┆ 3568.0     │\n",
       "│ median     ┆ 53.0       │\n",
       "│ 25%        ┆ 27.0       │\n",
       "│ 75%        ┆ 110.0      │\n",
       "└────────────┴────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print stats on tokens_len|code\n",
    "mfp.df['tokens_len|code'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from babydragon.types.text import NaturalLanguageSingle\n",
    "#mfp.apply_validator_to_column(\"code\", NaturalLanguageSingle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "babydragon.utils.main_logger - INFO - Batch embedding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utf8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "babydragon.utils.main_logger - INFO - Batch 1 of 2\n",
      "babydragon.utils.main_logger - INFO - Embedding batch 1 took 4.05147910118103 seconds\n",
      "babydragon.utils.main_logger - INFO - Batch 2 of 2\n",
      "babydragon.utils.main_logger - INFO - Embedding batch 2 took 9.383416891098022 seconds\n",
      "babydragon.utils.main_logger - INFO - Total number of embeddings 1184\n"
     ]
    }
   ],
   "source": [
    "mfp.embed_columns([\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfp.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfp = CodeFrame.load(frame_path='./storage/babydragon_frame', name='babydragon_frame')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th><th>code_main_entity|MainEntityVisitor</th><th>code_function_calls|FunctionCallCollector</th><th>code_parent_classes|ClassInheritanceVisitor</th><th>code_with_related_prompt</th><th>tokens|code_with_related_prompt</th><th>tokens_len|code_with_related_prompt</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td><td>str</td><td>list[str]</td><td>list[list[str]]</td><td>str</td><td>list[i64]</td><td>i64</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012073, -0.004771, … -0.043605]</td><td>&quot;EmbeddableType…</td><td>[]</td><td>[[&quot;Enum&quot;]]</td><td>&quot;\n",
       "class Embedda…</td><td>[198, 1058, … 198]</td><td>40</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.03026, 0.011673, … -0.039312]</td><td>&quot;infer_embeddab…</td><td>[&quot;str&quot;, &quot;print&quot;, … &quot;ValueError&quot;]</td><td>[]</td><td>&quot;\n",
       "def infer_emb…</td><td>[198, 755, … 663]</td><td>953</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012823, 0.010932, … -0.027359]</td><td>&quot;numeric_embedd…</td><td>[]</td><td>[]</td><td>&quot;\n",
       "def numeric_e…</td><td>[198, 755, … 198]</td><td>59</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025804, -0.008756, … -0.042009]</td><td>&quot;EmbeddingTask&quot;</td><td>[&quot;len&quot;, &quot;ValueError&quot;]</td><td>[[&quot;BaseTask&quot;]]</td><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>[271, 1058, … 663]</td><td>236</td></tr><tr><td>&quot;\n",
       "def _execute_…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>101</td><td>[-0.037655, -0.008194, … -0.016726]</td><td>&quot;_execute_sub_t…</td><td>[&quot;len&quot;, &quot;ValueError&quot;]</td><td>[]</td><td>&quot;\n",
       "def _execute_…</td><td>[198, 755, … 663]</td><td>124</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 12)\n",
       "┌─────────┬───────────┬────────┬───────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ code    ┆ libcst    ┆ filena ┆ tokens|co ┆ … ┆ code_paren ┆ code_with_ ┆ tokens|cod ┆ tokens_len │\n",
       "│ ---     ┆ tree      ┆ me     ┆ de        ┆   ┆ t_classes| ┆ related_pr ┆ e_with_rel ┆ |code_with │\n",
       "│ str     ┆ ---       ┆ ---    ┆ ---       ┆   ┆ ClassInher ┆ ompt       ┆ ated_promp ┆ _related_p │\n",
       "│         ┆ str       ┆ str    ┆ list[i64] ┆   ┆ it…        ┆ ---        ┆ t          ┆ ro…        │\n",
       "│         ┆           ┆        ┆           ┆   ┆ ---        ┆ str        ┆ ---        ┆ ---        │\n",
       "│         ┆           ┆        ┆           ┆   ┆ list[list[ ┆            ┆ list[i64]  ┆ i64        │\n",
       "│         ┆           ┆        ┆           ┆   ┆ str]]      ┆            ┆            ┆            │\n",
       "╞═════════╪═══════════╪════════╪═══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [198,     ┆ … ┆ [[\"Enum\"]] ┆            ┆ [198,      ┆ 40         │\n",
       "│ class   ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆            ┆ class Embe ┆ 1058, …    ┆            │\n",
       "│ Embedda ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆            ┆ ddableType ┆ 198]       ┆            │\n",
       "│ bleType ┆        …  ┆ eurald ┆           ┆   ┆            ┆ (Enum):    ┆            ┆            │\n",
       "│ (Enum): ┆           ┆ ragon/ ┆           ┆   ┆            ┆    …       ┆            ┆            │\n",
       "│    …    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ []         ┆            ┆ [198, 755, ┆ 953        │\n",
       "│ def inf ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆            ┆ def infer_ ┆ … 663]     ┆            │\n",
       "│ er_embe ┆ name=Name ┆ lhug/n ┆ 14790]    ┆   ┆            ┆ embeddable ┆            ┆            │\n",
       "│ ddable_ ┆ (         ┆ eurald ┆           ┆   ┆            ┆ _type(colu ┆            ┆            │\n",
       "│ type(co ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆ m…         ┆            ┆            │\n",
       "│ lum…    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ []         ┆            ┆ [198, 755, ┆ 59         │\n",
       "│ def num ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆            ┆ def numeri ┆ … 198]     ┆            │\n",
       "│ eric_em ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆            ┆ c_embedder ┆            ┆            │\n",
       "│ bedder( ┆ (         ┆ eurald ┆           ┆   ┆            ┆ (column):  ┆            ┆            │\n",
       "│ column) ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆  …         ┆            ┆            │\n",
       "│ :       ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│  …      ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [271,     ┆ … ┆ [[\"BaseTas ┆            ┆ [271,      ┆ 236        │\n",
       "│         ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆ k\"]]       ┆            ┆ 1058, …    ┆            │\n",
       "│ class   ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆            ┆ class Embe ┆ 663]       ┆            │\n",
       "│ Embeddi ┆        …  ┆ eurald ┆           ┆   ┆            ┆ ddingTask( ┆            ┆            │\n",
       "│ ngTask( ┆           ┆ ragon/ ┆           ┆   ┆            ┆ BaseTask): ┆            ┆            │\n",
       "│ BaseTas ┆           ┆ gi…    ┆           ┆   ┆            ┆ …          ┆            ┆            │\n",
       "│ k):…    ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ []         ┆            ┆ [198, 755, ┆ 124        │\n",
       "│ def _ex ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆            ┆ def _execu ┆ … 663]     ┆            │\n",
       "│ ecute_s ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆            ┆ te_sub_tas ┆            ┆            │\n",
       "│ ub_task ┆ (         ┆ eurald ┆           ┆   ┆            ┆ k(self,    ┆            ┆            │\n",
       "│ (self,  ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆ sub…       ┆            ┆            │\n",
       "│ sub…    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "└─────────┴───────────┴────────┴───────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run node and operator count analysis on code base and plot histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfp = mfp.count_node_types('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 29)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th><th>code_node_count|FunctionCallCounter</th><th>code_node_count|ArgumentTypeCounter</th><th>code_node_count|ImportCounter</th><th>code_node_count|IfStatementCounter</th><th>code_node_count|BaseCompoundStatementCounter</th><th>code_node_count|ForLoopCounter</th><th>code_node_count|WhileLoopCounter</th><th>code_node_count|TryExceptCounter</th><th>code_node_count|WithStatementCounter</th><th>code_node_count|LambdaFunctionCounter</th><th>code_node_count|GlobalStatementCounter</th><th>code_node_count|NonlocalStatementCounter</th><th>code_node_count|ListComprehensionCounter</th><th>code_node_count|DictComprehensionCounter</th><th>code_node_count|SetComprehensionCounter</th><th>code_node_count|GeneratorExpressionCounter</th><th>code_node_count|AwaitCounter</th><th>code_node_count|ReturnCounter</th><th>code_node_count|BreakCounter</th><th>code_node_count|ContinueCounter</th><th>code_node_count|RaiseCounter</th><th>code_node_count|AssertCounter</th><th>code_node_count|PassCounter</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012073, -0.004771, … -0.043605]</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.030298, 0.011617, … -0.039327]</td><td>5</td><td>1</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012823, 0.010932, … -0.027359]</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025782, -0.008832, … -0.04199]</td><td>4</td><td>20</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 198]</td><td>102</td><td>[-0.018791, -0.018855, … -0.04652]</td><td>1</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 29)\n",
       "┌─────────┬───────────┬────────┬───────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ code    ┆ libcst    ┆ filena ┆ tokens|co ┆ … ┆ code_node_ ┆ code_node_ ┆ code_node_ ┆ code_node_ │\n",
       "│ ---     ┆ tree      ┆ me     ┆ de        ┆   ┆ count|Cont ┆ count|Rais ┆ count|Asse ┆ count|Pass │\n",
       "│ str     ┆ ---       ┆ ---    ┆ ---       ┆   ┆ inueCounte ┆ eCounter   ┆ rtCounter  ┆ Counter    │\n",
       "│         ┆ str       ┆ str    ┆ list[i64] ┆   ┆ r          ┆ ---        ┆ ---        ┆ ---        │\n",
       "│         ┆           ┆        ┆           ┆   ┆ ---        ┆ i64        ┆ i64        ┆ i64        │\n",
       "│         ┆           ┆        ┆           ┆   ┆ i64        ┆            ┆            ┆            │\n",
       "╞═════════╪═══════════╪════════╪═══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [198,     ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0          │\n",
       "│ class   ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆            ┆            ┆            ┆            │\n",
       "│ Embedda ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆            ┆            ┆            ┆            │\n",
       "│ bleType ┆        …  ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ (Enum): ┆           ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│    …    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ 0          ┆ 1          ┆ 0          ┆ 0          │\n",
       "│ def inf ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆            ┆            ┆            ┆            │\n",
       "│ er_embe ┆ name=Name ┆ lhug/n ┆ 14790]    ┆   ┆            ┆            ┆            ┆            │\n",
       "│ ddable_ ┆ (         ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ type(co ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ lum…    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0          │\n",
       "│ def num ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆            ┆            ┆            ┆            │\n",
       "│ eric_em ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆            ┆            ┆            ┆            │\n",
       "│ bedder( ┆ (         ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ column) ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ :       ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│  …      ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [271,     ┆ … ┆ 0          ┆ 1          ┆ 0          ┆ 0          │\n",
       "│         ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆            ┆            ┆            ┆            │\n",
       "│ class   ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆            ┆            ┆            ┆            │\n",
       "│ Embeddi ┆        …  ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ ngTask( ┆           ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ BaseTas ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ k):…    ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ def __i ┆ FunctionD ┆ /Users ┆ [755,     ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0          │\n",
       "│ nit__(  ┆ ef(       ┆ /danie ┆ 1328, …   ┆   ┆            ┆            ┆            ┆            │\n",
       "│ self,   ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆            ┆            ┆            ┆            │\n",
       "│ embe…   ┆ (         ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "└─────────┴───────────┴────────┴───────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot a histogram for all columns of pattern code_count|\n",
    "\n",
    "prefix = 'code_node_count|'\n",
    "\n",
    "# Select columns that starts with prefix 'code_count|'\n",
    "selected_columns = [col for col in mfp.df.columns if col.startswith(prefix)]\n",
    "counts = []\n",
    "\n",
    "#describe columns\n",
    "\n",
    "# Iterate over selected columns and calculate the counts\n",
    "for column in selected_columns:\n",
    "    counts.append(mfp.df[column].sum())\n",
    "\n",
    "# Now plot the counts using a bar chart\n",
    "plt.figure(figsize=(6,10))  # You can adjust the figure size as you wish\n",
    "plt.barh(selected_columns, counts)\n",
    "plt.xlabel('Counts')\n",
    "plt.ylabel('Columns')\n",
    "plt.title(f'Counts of columns with prefix {prefix}')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility\n",
    "plt.show()\n",
    "# Now plot the counts using a bar chart\n",
    "plt.figure(figsize=(10,10))  # You can adjust the figure size as you wish\n",
    "plt.barh(selected_columns, counts)\n",
    "plt.xlabel('Counts')\n",
    "plt.ylabel('Columns')\n",
    "plt.title(f'Counts of columns with prefix {prefix}')\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 24)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>describe</th><th>code_node_count|FunctionCallCounter</th><th>code_node_count|ArgumentTypeCounter</th><th>code_node_count|ImportCounter</th><th>code_node_count|IfStatementCounter</th><th>code_node_count|BaseCompoundStatementCounter</th><th>code_node_count|ForLoopCounter</th><th>code_node_count|WhileLoopCounter</th><th>code_node_count|TryExceptCounter</th><th>code_node_count|WithStatementCounter</th><th>code_node_count|LambdaFunctionCounter</th><th>code_node_count|GlobalStatementCounter</th><th>code_node_count|NonlocalStatementCounter</th><th>code_node_count|ListComprehensionCounter</th><th>code_node_count|DictComprehensionCounter</th><th>code_node_count|SetComprehensionCounter</th><th>code_node_count|GeneratorExpressionCounter</th><th>code_node_count|AwaitCounter</th><th>code_node_count|ReturnCounter</th><th>code_node_count|BreakCounter</th><th>code_node_count|ContinueCounter</th><th>code_node_count|RaiseCounter</th><th>code_node_count|AssertCounter</th><th>code_node_count|PassCounter</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td><td>1171.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>4.719898</td><td>5.210931</td><td>0.000854</td><td>0.761742</td><td>0.0</td><td>0.239966</td><td>0.009394</td><td>0.043553</td><td>0.063194</td><td>0.025619</td><td>0.0</td><td>0.0</td><td>0.104184</td><td>0.023057</td><td>0.0</td><td>0.015371</td><td>0.018787</td><td>1.030743</td><td>0.014518</td><td>0.006832</td><td>0.171648</td><td>0.0</td><td>0.052946</td></tr><tr><td>&quot;std&quot;</td><td>11.210196</td><td>11.629351</td><td>0.029223</td><td>2.620844</td><td>0.0</td><td>0.734713</td><td>0.096506</td><td>0.242457</td><td>0.357249</td><td>0.20513</td><td>0.0</td><td>0.0</td><td>0.581252</td><td>0.171413</td><td>0.0</td><td>0.142395</td><td>0.217968</td><td>1.417501</td><td>0.133183</td><td>0.082407</td><td>0.835381</td><td>0.0</td><td>0.531662</td></tr><tr><td>&quot;min&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;max&quot;</td><td>169.0</td><td>174.0</td><td>1.0</td><td>46.0</td><td>0.0</td><td>8.0</td><td>1.0</td><td>3.0</td><td>4.0</td><td>3.0</td><td>0.0</td><td>0.0</td><td>10.0</td><td>2.0</td><td>0.0</td><td>3.0</td><td>5.0</td><td>25.0</td><td>2.0</td><td>1.0</td><td>19.0</td><td>0.0</td><td>11.0</td></tr><tr><td>&quot;median&quot;</td><td>2.0</td><td>2.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;25%&quot;</td><td>1.0</td><td>2.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;75%&quot;</td><td>4.0</td><td>5.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 24)\n",
       "┌───┬────────────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ d ┆ code_node_ ┆ code_node_ ┆ code_node_ ┆ … ┆ code_node_ ┆ code_node_ ┆ code_node_ ┆ code_node_ │\n",
       "│ e ┆ count|Func ┆ count|Argu ┆ count|Impo ┆   ┆ count|Cont ┆ count|Rais ┆ count|Asse ┆ count|Pass │\n",
       "│ s ┆ tionCallCo ┆ mentTypeCo ┆ rtCounter  ┆   ┆ inueCounte ┆ eCounter   ┆ rtCounter  ┆ Counter    │\n",
       "│ c ┆ un…        ┆ un…        ┆ ---        ┆   ┆ r          ┆ ---        ┆ ---        ┆ ---        │\n",
       "│ r ┆ ---        ┆ ---        ┆ f64        ┆   ┆ ---        ┆ f64        ┆ f64        ┆ f64        │\n",
       "│ i ┆ f64        ┆ f64        ┆            ┆   ┆ f64        ┆            ┆            ┆            │\n",
       "│ b ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ e ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ - ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ - ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ - ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ s ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ r ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "╞═══╪════════════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ c ┆ 1171.0     ┆ 1171.0     ┆ 1171.0     ┆ … ┆ 1171.0     ┆ 1171.0     ┆ 1171.0     ┆ 1171.0     │\n",
       "│ o ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ u ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 0.0        │\n",
       "│ u ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ l ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ l ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ _ ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ c ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ o ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ u ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ m ┆ 4.719898   ┆ 5.210931   ┆ 0.000854   ┆ … ┆ 0.006832   ┆ 0.171648   ┆ 0.0        ┆ 0.052946   │\n",
       "│ e ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ a ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ s ┆ 11.210196  ┆ 11.629351  ┆ 0.029223   ┆ … ┆ 0.082407   ┆ 0.835381   ┆ 0.0        ┆ 0.531662   │\n",
       "│ t ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ d ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ m ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 0.0        │\n",
       "│ i ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ m ┆ 169.0      ┆ 174.0      ┆ 1.0        ┆ … ┆ 1.0        ┆ 19.0       ┆ 0.0        ┆ 11.0       │\n",
       "│ a ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ x ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ m ┆ 2.0        ┆ 2.0        ┆ 0.0        ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 0.0        │\n",
       "│ e ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ d ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ i ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ a ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 2 ┆ 1.0        ┆ 2.0        ┆ 0.0        ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 0.0        │\n",
       "│ 5 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ % ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 7 ┆ 4.0        ┆ 5.0        ┆ 0.0        ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 0.0        │\n",
       "│ 5 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ % ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "└───┴────────────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df[selected_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfp = mfp.count_operators('code')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 52)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst_tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th><th>code_operator_count|BitInvertOperatorCounter</th><th>code_operator_count|MinusOperatorCounter</th><th>code_operator_count|NotOperatorCounter</th><th>code_operator_count|PlusOperatorCounter</th><th>code_operator_count|AndOperatorCounter</th><th>code_operator_count|OrOperatorCounter</th><th>code_operator_count|AddOperatorCounter</th><th>code_operator_count|BitAndOperatorCounter</th><th>code_operator_count|BitOrOperatorCounter</th><th>code_operator_count|BitXorOperatorCounter</th><th>code_operator_count|DivideOperatorCounter</th><th>code_operator_count|FloorDivideOperatorCounter</th><th>code_operator_count|LeftShiftOperatorCounter</th><th>code_operator_count|MatrixMultiplyOperatorCounter</th><th>code_operator_count|ModuloOperatorCounter</th><th>code_operator_count|MultiplyOperatorCounter</th><th>code_operator_count|PowerOperatorCounter</th><th>code_operator_count|RightShiftOperatorCounter</th><th>code_operator_count|SubtractOperatorCounter</th><th>code_operator_count|EqualOperatorCounter</th><th>code_operator_count|GreaterThanOperatorCounter</th><th>code_operator_count|GreaterThanEqualOperatorCounter</th><th>code_operator_count|InOperatorCounter</th><th>code_operator_count|IsOperatorCounter</th><th>code_operator_count|LessThanOperatorCounter</th><th>code_operator_count|LessThanEqualOperatorCounter</th><th>code_operator_count|NotEqualOperatorCounter</th><th>code_operator_count|IsNotOperatorCounter</th><th>code_operator_count|NotInOperatorCounter</th><th>code_operator_count|AddAssignOperatorCounter</th><th>code_operator_count|BitAndAssignOperatorCounter</th><th>code_operator_count|BitOrAssignOperatorCounter</th><th>code_operator_count|BitXorAssignOperatorCounter</th><th>code_operator_count|DivideAssignOperatorCounter</th><th>code_operator_count|FloorDivideAssignOperatorCounter</th><th>code_operator_count|LeftShiftAssignOperatorCounter</th><th>code_operator_count|MatrixMultiplyAssignOperatorCounter</th><th>code_operator_count|ModuloAssignOperatorCounter</th><th>code_operator_count|MultiplyAssignOperatorCounter</th><th>code_operator_count|PowerAssignOperatorCounter</th><th>code_operator_count|RightShiftAssignOperatorCounter</th><th>code_operator_count|SubtractAssignOperatorCounter</th><th>code_operator_count|AssignEqualOperatorCounter</th><th>code_operator_count|ColonOperatorCounter</th><th>code_operator_count|CommaOperatorCounter</th><th>code_operator_count|DotOperatorCounter</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012065, -0.004709, … -0.043599]</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.030298, 0.011617, … -0.039327]</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4</td><td>7</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012823, 0.010932, … -0.027359]</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025782, -0.008832, … -0.04199]</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4</td><td>0</td><td>14</td><td>6</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 198]</td><td>102</td><td>[-0.018791, -0.018855, … -0.04652]</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4</td><td>0</td><td>13</td><td>3</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 52)\n",
       "┌─────────┬───────────┬────────┬───────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ code    ┆ libcst_tr ┆ filena ┆ tokens|co ┆ … ┆ code_opera ┆ code_opera ┆ code_opera ┆ code_opera │\n",
       "│ ---     ┆ ee        ┆ me     ┆ de        ┆   ┆ tor_count| ┆ tor_count| ┆ tor_count| ┆ tor_count| │\n",
       "│ str     ┆ ---       ┆ ---    ┆ ---       ┆   ┆ AssignEqua ┆ ColonOpera ┆ CommaOpera ┆ DotOperato │\n",
       "│         ┆ str       ┆ str    ┆ list[i64] ┆   ┆ lO…        ┆ to…        ┆ to…        ┆ rC…        │\n",
       "│         ┆           ┆        ┆           ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│         ┆           ┆        ┆           ┆   ┆ i64        ┆ i64        ┆ i64        ┆ i64        │\n",
       "╞═════════╪═══════════╪════════╪═══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [198,     ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0          │\n",
       "│ class   ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆            ┆            ┆            ┆            │\n",
       "│ Embedda ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆            ┆            ┆            ┆            │\n",
       "│ bleType ┆        …  ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ (Enum): ┆           ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│    …    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ 0          ┆ 0          ┆ 4          ┆ 7          │\n",
       "│ def inf ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆            ┆            ┆            ┆            │\n",
       "│ er_embe ┆ name=Name ┆ lhug/n ┆ 14790]    ┆   ┆            ┆            ┆            ┆            │\n",
       "│ ddable_ ┆ (         ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ type(co ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ lum…    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 1          │\n",
       "│ def num ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆            ┆            ┆            ┆            │\n",
       "│ eric_em ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆            ┆            ┆            ┆            │\n",
       "│ bedder( ┆ (         ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ column) ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ :       ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│  …      ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [271,     ┆ … ┆ 4          ┆ 0          ┆ 14         ┆ 6          │\n",
       "│         ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆            ┆            ┆            ┆            │\n",
       "│ class   ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆            ┆            ┆            ┆            │\n",
       "│ Embeddi ┆        …  ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ ngTask( ┆           ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ BaseTas ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ k):…    ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ def __i ┆ FunctionD ┆ /Users ┆ [755,     ┆ … ┆ 4          ┆ 0          ┆ 13         ┆ 3          │\n",
       "│ nit__(  ┆ ef(       ┆ /danie ┆ 1328, …   ┆   ┆            ┆            ┆            ┆            │\n",
       "│ self,   ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆            ┆            ┆            ┆            │\n",
       "│ embe…   ┆ (         ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "└─────────┴───────────┴────────┴───────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefix = 'code_operator_count|'\n",
    "\n",
    "# Select columns that starts with prefix 'code_count|'\n",
    "selected_columns = [col for col in mfp.df.columns if col.startswith(prefix)]\n",
    "counts = []\n",
    "\n",
    "# Iterate over selected columns and calculate the counts\n",
    "for column in selected_columns:\n",
    "    counts.append(mfp.df[column].sum())\n",
    "\n",
    "# Now plot the counts using a bar chart\n",
    "plt.figure(figsize=(6,20))  # You can adjust the figure size as you wish\n",
    "plt.barh(selected_columns, counts)\n",
    "plt.xlabel('Counts')\n",
    "plt.ylabel('Columns')\n",
    "plt.title('Counts of columns with prefix \"code_operator_count|\"')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility\n",
    "plt.show()\n",
    "# Now plot the counts using a bar chart\n",
    "plt.figure(figsize=(10,20))  # You can adjust the figure size as you wish\n",
    "plt.barh(selected_columns, counts)\n",
    "plt.xlabel('Counts')\n",
    "plt.ylabel('Columns')\n",
    "plt.title('Counts of columns with prefix \"code_operator_count|\"')\n",
    "plt.xscale('log')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 47)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>describe</th><th>code_operator_count|BitInvertOperatorCounter</th><th>code_operator_count|MinusOperatorCounter</th><th>code_operator_count|NotOperatorCounter</th><th>code_operator_count|PlusOperatorCounter</th><th>code_operator_count|AndOperatorCounter</th><th>code_operator_count|OrOperatorCounter</th><th>code_operator_count|AddOperatorCounter</th><th>code_operator_count|BitAndOperatorCounter</th><th>code_operator_count|BitOrOperatorCounter</th><th>code_operator_count|BitXorOperatorCounter</th><th>code_operator_count|DivideOperatorCounter</th><th>code_operator_count|FloorDivideOperatorCounter</th><th>code_operator_count|LeftShiftOperatorCounter</th><th>code_operator_count|MatrixMultiplyOperatorCounter</th><th>code_operator_count|ModuloOperatorCounter</th><th>code_operator_count|MultiplyOperatorCounter</th><th>code_operator_count|PowerOperatorCounter</th><th>code_operator_count|RightShiftOperatorCounter</th><th>code_operator_count|SubtractOperatorCounter</th><th>code_operator_count|EqualOperatorCounter</th><th>code_operator_count|GreaterThanOperatorCounter</th><th>code_operator_count|GreaterThanEqualOperatorCounter</th><th>code_operator_count|InOperatorCounter</th><th>code_operator_count|IsOperatorCounter</th><th>code_operator_count|LessThanOperatorCounter</th><th>code_operator_count|LessThanEqualOperatorCounter</th><th>code_operator_count|NotEqualOperatorCounter</th><th>code_operator_count|IsNotOperatorCounter</th><th>code_operator_count|NotInOperatorCounter</th><th>code_operator_count|AddAssignOperatorCounter</th><th>code_operator_count|BitAndAssignOperatorCounter</th><th>code_operator_count|BitOrAssignOperatorCounter</th><th>code_operator_count|BitXorAssignOperatorCounter</th><th>code_operator_count|DivideAssignOperatorCounter</th><th>code_operator_count|FloorDivideAssignOperatorCounter</th><th>code_operator_count|LeftShiftAssignOperatorCounter</th><th>code_operator_count|MatrixMultiplyAssignOperatorCounter</th><th>code_operator_count|ModuloAssignOperatorCounter</th><th>code_operator_count|MultiplyAssignOperatorCounter</th><th>code_operator_count|PowerAssignOperatorCounter</th><th>code_operator_count|RightShiftAssignOperatorCounter</th><th>code_operator_count|SubtractAssignOperatorCounter</th><th>code_operator_count|AssignEqualOperatorCounter</th><th>code_operator_count|ColonOperatorCounter</th><th>code_operator_count|CommaOperatorCounter</th><th>code_operator_count|DotOperatorCounter</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td><td>1084.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>0.00369</td><td>0.039668</td><td>0.080258</td><td>0.0</td><td>0.128229</td><td>0.112546</td><td>0.120849</td><td>0.00369</td><td>0.001845</td><td>0.0</td><td>0.032288</td><td>0.005535</td><td>0.0</td><td>0.005535</td><td>0.00369</td><td>0.024908</td><td>0.005535</td><td>0.0</td><td>0.083948</td><td>0.121771</td><td>0.086716</td><td>0.013838</td><td>0.042435</td><td>0.161439</td><td>0.032288</td><td>0.020295</td><td>0.027675</td><td>0.102399</td><td>0.02583</td><td>0.177122</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.017528</td><td>1.663284</td><td>0.098708</td><td>4.744465</td><td>8.032288</td></tr><tr><td>&quot;std&quot;</td><td>0.074341</td><td>0.260148</td><td>0.510298</td><td>0.0</td><td>0.885492</td><td>0.919808</td><td>0.635346</td><td>0.060661</td><td>0.042934</td><td>0.0</td><td>0.278284</td><td>0.095932</td><td>0.0</td><td>0.113562</td><td>0.060661</td><td>0.219812</td><td>0.095932</td><td>0.0</td><td>0.625232</td><td>0.734285</td><td>0.60914</td><td>0.151302</td><td>0.27858</td><td>1.01273</td><td>0.23512</td><td>0.153607</td><td>0.252722</td><td>0.685985</td><td>0.221796</td><td>0.493721</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.207582</td><td>5.530082</td><td>0.765386</td><td>12.229651</td><td>13.714402</td></tr><tr><td>&quot;min&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;max&quot;</td><td>2.0</td><td>3.0</td><td>11.0</td><td>0.0</td><td>16.0</td><td>12.0</td><td>11.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>5.0</td><td>2.0</td><td>0.0</td><td>3.0</td><td>1.0</td><td>3.0</td><td>2.0</td><td>0.0</td><td>15.0</td><td>18.0</td><td>16.0</td><td>3.0</td><td>4.0</td><td>20.0</td><td>5.0</td><td>2.0</td><td>6.0</td><td>14.0</td><td>4.0</td><td>5.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>3.0</td><td>82.0</td><td>18.0</td><td>160.0</td><td>208.0</td></tr><tr><td>&quot;median&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>4.0</td></tr><tr><td>&quot;25%&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>3.0</td></tr><tr><td>&quot;75%&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>4.0</td><td>9.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 47)\n",
       "┌───┬────────────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ d ┆ code_opera ┆ code_opera ┆ code_opera ┆ … ┆ code_opera ┆ code_opera ┆ code_opera ┆ code_opera │\n",
       "│ e ┆ tor_count| ┆ tor_count| ┆ tor_count| ┆   ┆ tor_count| ┆ tor_count| ┆ tor_count| ┆ tor_count| │\n",
       "│ s ┆ BitInvertO ┆ MinusOpera ┆ NotOperato ┆   ┆ AssignEqua ┆ ColonOpera ┆ CommaOpera ┆ DotOperato │\n",
       "│ c ┆ pe…        ┆ to…        ┆ rC…        ┆   ┆ lO…        ┆ to…        ┆ to…        ┆ rC…        │\n",
       "│ r ┆ ---        ┆ ---        ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│ i ┆ f64        ┆ f64        ┆ f64        ┆   ┆ f64        ┆ f64        ┆ f64        ┆ f64        │\n",
       "│ b ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ e ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ - ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ - ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ - ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ s ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ r ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "╞═══╪════════════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ c ┆ 1084.0     ┆ 1084.0     ┆ 1084.0     ┆ … ┆ 1084.0     ┆ 1084.0     ┆ 1084.0     ┆ 1084.0     │\n",
       "│ o ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ u ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 0.0        │\n",
       "│ u ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ l ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ l ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ _ ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ c ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ o ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ u ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ m ┆ 0.00369    ┆ 0.039668   ┆ 0.080258   ┆ … ┆ 1.663284   ┆ 0.098708   ┆ 4.744465   ┆ 8.032288   │\n",
       "│ e ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ a ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ s ┆ 0.074341   ┆ 0.260148   ┆ 0.510298   ┆ … ┆ 5.530082   ┆ 0.765386   ┆ 12.229651  ┆ 13.714402  │\n",
       "│ t ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ d ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ m ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 0.0        │\n",
       "│ i ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ m ┆ 2.0        ┆ 3.0        ┆ 11.0       ┆ … ┆ 82.0       ┆ 18.0       ┆ 160.0      ┆ 208.0      │\n",
       "│ a ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ x ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ m ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ … ┆ 0.0        ┆ 0.0        ┆ 1.0        ┆ 4.0        │\n",
       "│ e ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ d ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ i ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ a ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 2 ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ … ┆ 0.0        ┆ 0.0        ┆ 1.0        ┆ 3.0        │\n",
       "│ 5 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ % ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 7 ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ … ┆ 1.0        ┆ 0.0        ┆ 4.0        ┆ 9.0        │\n",
       "│ 5 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ % ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "└───┴────────────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df[selected_columns].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate column for Class Inheritance list, Function calls used in class and what modules use class\n",
    "- Who is the classes parent\n",
    "- What does the class use to do its job\n",
    "- Where is the class used in the repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfp = CodeFrame.load(frame_path='./storage/babydragon_frame', name='babydragon_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012073, -0.004771, … -0.043605]</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.030298, 0.011617, … -0.039327]</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012823, 0.010932, … -0.027359]</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025782, -0.008832, … -0.04199]</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 198]</td><td>102</td><td>[-0.018791, -0.018855, … -0.04652]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌────────────────┬────────────────┬────────────────┬───────────────┬───────────────┬───────────────┐\n",
       "│ code           ┆ libcst tree    ┆ filename       ┆ tokens|code   ┆ tokens_len|co ┆ embedding|cod │\n",
       "│ ---            ┆ ---            ┆ ---            ┆ ---           ┆ de            ┆ e             │\n",
       "│ str            ┆ str            ┆ str            ┆ list[i64]     ┆ ---           ┆ ---           │\n",
       "│                ┆                ┆                ┆               ┆ i64           ┆ list[f64]     │\n",
       "╞════════════════╪════════════════╪════════════════╪═══════════════╪═══════════════╪═══════════════╡\n",
       "│                ┆ ClassDef(      ┆ /Users/danielh ┆ [198, 1058, … ┆ 40            ┆ [-0.012073,   │\n",
       "│ class Embeddab ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ -0.004771, …  │\n",
       "│ leType(Enum):  ┆        …       ┆ n/gi…          ┆               ┆               ┆ -0.0436…      │\n",
       "│    …           ┆                ┆                ┆               ┆               ┆               │\n",
       "│                ┆ FunctionDef(   ┆ /Users/danielh ┆ [198, 755, …  ┆ 194           ┆ [0.030298,    │\n",
       "│ def infer_embe ┆     name=Name( ┆ ug/neuraldrago ┆ 14790]        ┆               ┆ 0.011617, …   │\n",
       "│ ddable_type(co ┆     …          ┆ n/gi…          ┆               ┆               ┆ -0.039327…    │\n",
       "│ lum…           ┆                ┆                ┆               ┆               ┆               │\n",
       "│                ┆ FunctionDef(   ┆ /Users/danielh ┆ [198, 755, …  ┆ 59            ┆ [0.012823,    │\n",
       "│ def numeric_em ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ 0.010932, …   │\n",
       "│ bedder(column) ┆     …          ┆ n/gi…          ┆               ┆               ┆ -0.027359…    │\n",
       "│ :              ┆                ┆                ┆               ┆               ┆               │\n",
       "│  …             ┆                ┆                ┆               ┆               ┆               │\n",
       "│                ┆ ClassDef(      ┆ /Users/danielh ┆ [271, 1058, … ┆ 213           ┆ [-0.025782,   │\n",
       "│                ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ -0.008832, …  │\n",
       "│ class Embeddin ┆        …       ┆ n/gi…          ┆               ┆               ┆ -0.0419…      │\n",
       "│ gTask(BaseTask ┆                ┆                ┆               ┆               ┆               │\n",
       "│ ):…            ┆                ┆                ┆               ┆               ┆               │\n",
       "│ def __init__(  ┆ FunctionDef(   ┆ /Users/danielh ┆ [755, 1328, … ┆ 102           ┆ [-0.018791,   │\n",
       "│     self,      ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ -0.018855, …  │\n",
       "│     embe…      ┆     …          ┆ n/gi…          ┆               ┆               ┆ -0.0465…      │\n",
       "└────────────────┴────────────────┴────────────────┴───────────────┴───────────────┴───────────────┘"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libcst as cst\n",
    "\n",
    "class MainEntityVisitor(cst.CSTVisitor):\n",
    "    def __init__(self, code: str):\n",
    "        self.module = cst.parse_module(code)\n",
    "        self.main_entity = None\n",
    "\n",
    "    def visit_ClassDef(self, node: cst.ClassDef):\n",
    "        if self.main_entity is None:\n",
    "            self.main_entity = node.name.value\n",
    "        return False  # Prevent visiting inner scopes\n",
    "\n",
    "    def visit_FunctionDef(self, node: cst.FunctionDef):\n",
    "        if self.main_entity is None:\n",
    "            self.main_entity = node.name.value\n",
    "        return False  # Prevent visiting inner scopes\n",
    "\n",
    "    def collect(self):\n",
    "        self.module.visit(self)\n",
    "        return self.main_entity\n",
    "\n",
    "mfp = mfp.apply_visitor_to_column(\"code\", MainEntityVisitor, \"main_entity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th><th>code_main_entity|MainEntityVisitor</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td><td>str</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012073, -0.004771, … -0.043605]</td><td>&quot;EmbeddableType…</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.030298, 0.011617, … -0.039327]</td><td>&quot;infer_embeddab…</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012823, 0.010932, … -0.027359]</td><td>&quot;numeric_embedd…</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025782, -0.008832, … -0.04199]</td><td>&quot;EmbeddingTask&quot;</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 198]</td><td>102</td><td>[-0.018791, -0.018855, … -0.04652]</td><td>&quot;__init__&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌──────────────┬──────────────┬──────────────┬──────────────┬────────────┬────────────┬────────────┐\n",
       "│ code         ┆ libcst tree  ┆ filename     ┆ tokens|code  ┆ tokens_len ┆ embedding| ┆ code_main_ │\n",
       "│ ---          ┆ ---          ┆ ---          ┆ ---          ┆ |code      ┆ code       ┆ entity|Mai │\n",
       "│ str          ┆ str          ┆ str          ┆ list[i64]    ┆ ---        ┆ ---        ┆ nEntityVis │\n",
       "│              ┆              ┆              ┆              ┆ i64        ┆ list[f64]  ┆ it…        │\n",
       "│              ┆              ┆              ┆              ┆            ┆            ┆ ---        │\n",
       "│              ┆              ┆              ┆              ┆            ┆            ┆ str        │\n",
       "╞══════════════╪══════════════╪══════════════╪══════════════╪════════════╪════════════╪════════════╡\n",
       "│              ┆ ClassDef(    ┆ /Users/danie ┆ [198, 1058,  ┆ 40         ┆ [-0.012073 ┆ Embeddable │\n",
       "│ class Embedd ┆ name=Name(   ┆ lhug/neurald ┆ … 198]       ┆            ┆ ,          ┆ Type       │\n",
       "│ ableType(Enu ┆        …     ┆ ragon/gi…    ┆              ┆            ┆ -0.004771, ┆            │\n",
       "│ m):          ┆              ┆              ┆              ┆            ┆ … -0.0436… ┆            │\n",
       "│    …         ┆              ┆              ┆              ┆            ┆            ┆            │\n",
       "│              ┆ FunctionDef( ┆ /Users/danie ┆ [198, 755, … ┆ 194        ┆ [0.030298, ┆ infer_embe │\n",
       "│ def infer_em ┆ name=Name(   ┆ lhug/neurald ┆ 14790]       ┆            ┆ 0.011617,  ┆ ddable_typ │\n",
       "│ beddable_typ ┆     …        ┆ ragon/gi…    ┆              ┆            ┆ …          ┆ e          │\n",
       "│ e(colum…     ┆              ┆              ┆              ┆            ┆ -0.039327… ┆            │\n",
       "│              ┆ FunctionDef( ┆ /Users/danie ┆ [198, 755, … ┆ 59         ┆ [0.012823, ┆ numeric_em │\n",
       "│ def numeric_ ┆ name=Name(   ┆ lhug/neurald ┆ 198]         ┆            ┆ 0.010932,  ┆ bedder     │\n",
       "│ embedder(col ┆     …        ┆ ragon/gi…    ┆              ┆            ┆ …          ┆            │\n",
       "│ umn):        ┆              ┆              ┆              ┆            ┆ -0.027359… ┆            │\n",
       "│  …           ┆              ┆              ┆              ┆            ┆            ┆            │\n",
       "│              ┆ ClassDef(    ┆ /Users/danie ┆ [271, 1058,  ┆ 213        ┆ [-0.025782 ┆ EmbeddingT │\n",
       "│              ┆ name=Name(   ┆ lhug/neurald ┆ … 198]       ┆            ┆ ,          ┆ ask        │\n",
       "│ class Embedd ┆        …     ┆ ragon/gi…    ┆              ┆            ┆ -0.008832, ┆            │\n",
       "│ ingTask(Base ┆              ┆              ┆              ┆            ┆ … -0.0419… ┆            │\n",
       "│ Task):…      ┆              ┆              ┆              ┆            ┆            ┆            │\n",
       "│ def          ┆ FunctionDef( ┆ /Users/danie ┆ [755, 1328,  ┆ 102        ┆ [-0.018791 ┆ __init__   │\n",
       "│ __init__(    ┆ name=Name(   ┆ lhug/neurald ┆ … 198]       ┆            ┆ ,          ┆            │\n",
       "│     self,    ┆     …        ┆ ragon/gi…    ┆              ┆            ┆ -0.018855, ┆            │\n",
       "│     embe…    ┆              ┆              ┆              ┆            ┆ … -0.0465… ┆            │\n",
       "└──────────────┴──────────────┴──────────────┴──────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babydragon.memory.frames.visitors.node_type_collectors import FunctionCallCollector\n",
    "mfp = mfp.apply_visitor_to_column(\"code\", FunctionCallCollector, \"function_calls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th><th>code_main_entity|MainEntityVisitor</th><th>code_function_calls|FunctionCallCollector</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td><td>str</td><td>list[str]</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012073, -0.004771, … -0.043605]</td><td>&quot;EmbeddableType…</td><td>[]</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.03026, 0.011673, … -0.039312]</td><td>&quot;infer_embeddab…</td><td>[&quot;str&quot;, &quot;print&quot;, … &quot;ValueError&quot;]</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012823, 0.010932, … -0.027359]</td><td>&quot;numeric_embedd…</td><td>[]</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025804, -0.008756, … -0.042009]</td><td>&quot;EmbeddingTask&quot;</td><td>[&quot;len&quot;, &quot;ValueError&quot;]</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 198]</td><td>102</td><td>[-0.018768, -0.018833, … -0.046555]</td><td>&quot;__init__&quot;</td><td>[]</td></tr><tr><td>&quot;\n",
       "def _execute_…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>101</td><td>[-0.037655, -0.008194, … -0.016726]</td><td>&quot;_execute_sub_t…</td><td>[&quot;len&quot;, &quot;ValueError&quot;]</td></tr><tr><td>&quot;\n",
       "def parallel_…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>140</td><td>[-0.041914, -0.0109, … -0.027008]</td><td>&quot;parallel_embed…</td><td>[&quot;print&quot;, &quot;len&quot;, … &quot;sorted&quot;]</td></tr><tr><td>&quot;\n",
       "\n",
       "class TopicT…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>992</td><td>[-0.023198, -0.002724, … -0.053914]</td><td>&quot;TopicTreeTask&quot;</td><td>[&quot;super&quot;, &quot;print&quot;, … &quot;MemoryIndex&quot;]</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 340]</td><td>220</td><td>[-0.004693, 0.001029, … -0.048643]</td><td>&quot;__init__&quot;</td><td>[&quot;super&quot;]</td></tr><tr><td>&quot;\n",
       "\n",
       "def _setup_m…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[271, 755, … 14790]</td><td>104</td><td>[-0.01272, 0.016058, … -0.001598]</td><td>&quot;_setup_memory_…</td><td>[&quot;print&quot;, &quot;HDBSCANMultiKernel&quot;, … &quot;ValueError&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 8)\n",
       "┌───────────┬───────────┬──────────┬───────────┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ code      ┆ libcst    ┆ filename ┆ tokens|co ┆ tokens_len ┆ embedding| ┆ code_main_ ┆ code_funct │\n",
       "│ ---       ┆ tree      ┆ ---      ┆ de        ┆ |code      ┆ code       ┆ entity|Mai ┆ ion_calls| │\n",
       "│ str       ┆ ---       ┆ str      ┆ ---       ┆ ---        ┆ ---        ┆ nEntityVis ┆ FunctionCa │\n",
       "│           ┆ str       ┆          ┆ list[i64] ┆ i64        ┆ list[f64]  ┆ it…        ┆ ll…        │\n",
       "│           ┆           ┆          ┆           ┆            ┆            ┆ ---        ┆ ---        │\n",
       "│           ┆           ┆          ┆           ┆            ┆            ┆ str        ┆ list[str]  │\n",
       "╞═══════════╪═══════════╪══════════╪═══════════╪════════════╪════════════╪════════════╪════════════╡\n",
       "│           ┆ ClassDef( ┆ /Users/d ┆ [198,     ┆ 40         ┆ [-0.012073 ┆ Embeddable ┆ []         │\n",
       "│ class Emb ┆ name=Name ┆ anielhug ┆ 1058, …   ┆            ┆ ,          ┆ Type       ┆            │\n",
       "│ eddableTy ┆ (         ┆ /neurald ┆ 198]      ┆            ┆ -0.004771, ┆            ┆            │\n",
       "│ pe(Enum): ┆        …  ┆ ragon/gi ┆           ┆            ┆ … -0.0436… ┆            ┆            │\n",
       "│    …      ┆           ┆ …        ┆           ┆            ┆            ┆            ┆            │\n",
       "│           ┆ FunctionD ┆ /Users/d ┆ [198,     ┆ 194        ┆ [0.03026,  ┆ infer_embe ┆ [\"str\",    │\n",
       "│ def infer ┆ ef(       ┆ anielhug ┆ 755, …    ┆            ┆ 0.011673,  ┆ ddable_typ ┆ \"print\", … │\n",
       "│ _embeddab ┆ name=Name ┆ /neurald ┆ 14790]    ┆            ┆ …          ┆ e          ┆ \"ValueErro │\n",
       "│ le_type(c ┆ (         ┆ ragon/gi ┆           ┆            ┆ -0.039312] ┆            ┆ r\"]        │\n",
       "│ olum…     ┆     …     ┆ …        ┆           ┆            ┆            ┆            ┆            │\n",
       "│           ┆ FunctionD ┆ /Users/d ┆ [198,     ┆ 59         ┆ [0.012823, ┆ numeric_em ┆ []         │\n",
       "│ def numer ┆ ef(       ┆ anielhug ┆ 755, …    ┆            ┆ 0.010932,  ┆ bedder     ┆            │\n",
       "│ ic_embedd ┆ name=Name ┆ /neurald ┆ 198]      ┆            ┆ …          ┆            ┆            │\n",
       "│ er(column ┆ (         ┆ ragon/gi ┆           ┆            ┆ -0.027359… ┆            ┆            │\n",
       "│ ):        ┆     …     ┆ …        ┆           ┆            ┆            ┆            ┆            │\n",
       "│  …        ┆           ┆          ┆           ┆            ┆            ┆            ┆            │\n",
       "│           ┆ ClassDef( ┆ /Users/d ┆ [271,     ┆ 213        ┆ [-0.025804 ┆ EmbeddingT ┆ [\"len\",    │\n",
       "│           ┆ name=Name ┆ anielhug ┆ 1058, …   ┆            ┆ ,          ┆ ask        ┆ \"ValueErro │\n",
       "│ class Emb ┆ (         ┆ /neurald ┆ 198]      ┆            ┆ -0.008756, ┆            ┆ r\"]        │\n",
       "│ eddingTas ┆        …  ┆ ragon/gi ┆           ┆            ┆ … -0.0420… ┆            ┆            │\n",
       "│ k(BaseTas ┆           ┆ …        ┆           ┆            ┆            ┆            ┆            │\n",
       "│ k):…      ┆           ┆          ┆           ┆            ┆            ┆            ┆            │\n",
       "│ …         ┆ …         ┆ …        ┆ …         ┆ …          ┆ …          ┆ …          ┆ …          │\n",
       "│           ┆ FunctionD ┆ /Users/d ┆ [198,     ┆ 140        ┆ [-0.041914 ┆ parallel_e ┆ [\"print\",  │\n",
       "│ def paral ┆ ef(       ┆ anielhug ┆ 755, …    ┆            ┆ , -0.0109, ┆ mbeddings  ┆ \"len\", …   │\n",
       "│ lel_embed ┆ name=Name ┆ /neurald ┆ 198]      ┆            ┆ …          ┆            ┆ \"sorted\"]  │\n",
       "│ dings(emb ┆ (         ┆ ragon/gi ┆           ┆            ┆ -0.027008… ┆            ┆            │\n",
       "│ edde…     ┆     …     ┆ …        ┆           ┆            ┆            ┆            ┆            │\n",
       "│           ┆ ClassDef( ┆ /Users/d ┆ [271,     ┆ 992        ┆ [-0.023198 ┆ TopicTreeT ┆ [\"super\",  │\n",
       "│           ┆ name=Name ┆ anielhug ┆ 1058, …   ┆            ┆ ,          ┆ ask        ┆ \"print\", … │\n",
       "│ class Top ┆ (         ┆ /neurald ┆ 198]      ┆            ┆ -0.002724, ┆            ┆ \"MemoryInd │\n",
       "│ icTreeTas ┆        …  ┆ ragon/gi ┆           ┆            ┆ … -0.0539… ┆            ┆ e…         │\n",
       "│ k(BaseTas ┆           ┆ …        ┆           ┆            ┆            ┆            ┆            │\n",
       "│ k):…      ┆           ┆          ┆           ┆            ┆            ┆            ┆            │\n",
       "│ def       ┆ FunctionD ┆ /Users/d ┆ [755,     ┆ 220        ┆ [-0.004693 ┆ __init__   ┆ [\"super\"]  │\n",
       "│ __init__( ┆ ef(       ┆ anielhug ┆ 1328, …   ┆            ┆ ,          ┆            ┆            │\n",
       "│     self, ┆ name=Name ┆ /neurald ┆ 340]      ┆            ┆ 0.001029,  ┆            ┆            │\n",
       "│     memo… ┆ (         ┆ ragon/gi ┆           ┆            ┆ …          ┆            ┆            │\n",
       "│           ┆     …     ┆ …        ┆           ┆            ┆ -0.04864…  ┆            ┆            │\n",
       "│           ┆ FunctionD ┆ /Users/d ┆ [271,     ┆ 104        ┆ [-0.01272, ┆ _setup_mem ┆ [\"print\",  │\n",
       "│           ┆ ef(       ┆ anielhug ┆ 755, …    ┆            ┆ 0.016058,  ┆ ory_kernel ┆ \"HDBSCANMu │\n",
       "│ def _setu ┆ name=Name ┆ /neurald ┆ 14790]    ┆            ┆ …          ┆ _group     ┆ ltiKernel\" │\n",
       "│ p_memory_ ┆ (         ┆ ragon/gi ┆           ┆            ┆ -0.001598… ┆            ┆ , …        │\n",
       "│ kernel_gr ┆     …     ┆ …        ┆           ┆            ┆            ┆            ┆            │\n",
       "│ oup…      ┆           ┆          ┆           ┆            ┆            ┆            ┆            │\n",
       "└───────────┴───────────┴──────────┴───────────┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libcst as cst\n",
    "\n",
    "class ClassInheritanceVisitor(cst.CSTVisitor):\n",
    "    def __init__(self, code: str):\n",
    "        self.module = cst.parse_module(code)\n",
    "        self.parent_classes = []\n",
    "\n",
    "    def visit_ClassDef(self, node: cst.ClassDef):\n",
    "        self.parent_classes.append([cst.Module([base]).code for base in node.bases])\n",
    "        return False  # Prevent visiting inner scopes\n",
    "\n",
    "    def collect(self):\n",
    "        self.module.visit(self)\n",
    "        return self.parent_classes\n",
    "\n",
    "mfp = mfp.apply_visitor_to_column(\"code\", ClassInheritanceVisitor, \"parent_classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th><th>code_main_entity|MainEntityVisitor</th><th>code_function_calls|FunctionCallCollector</th><th>code_parent_classes|ClassInheritanceVisitor</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td><td>str</td><td>list[str]</td><td>list[list[str]]</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012073, -0.004771, … -0.043605]</td><td>&quot;EmbeddableType…</td><td>[]</td><td>[[&quot;Enum&quot;]]</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.03026, 0.011673, … -0.039312]</td><td>&quot;infer_embeddab…</td><td>[&quot;str&quot;, &quot;print&quot;, … &quot;ValueError&quot;]</td><td>[]</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012823, 0.010932, … -0.027359]</td><td>&quot;numeric_embedd…</td><td>[]</td><td>[]</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025804, -0.008756, … -0.042009]</td><td>&quot;EmbeddingTask&quot;</td><td>[&quot;len&quot;, &quot;ValueError&quot;]</td><td>[[&quot;BaseTask&quot;]]</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 198]</td><td>102</td><td>[-0.018768, -0.018833, … -0.046555]</td><td>&quot;__init__&quot;</td><td>[]</td><td>[]</td></tr><tr><td>&quot;\n",
       "def _execute_…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>101</td><td>[-0.037655, -0.008194, … -0.016726]</td><td>&quot;_execute_sub_t…</td><td>[&quot;len&quot;, &quot;ValueError&quot;]</td><td>[]</td></tr><tr><td>&quot;\n",
       "def parallel_…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>140</td><td>[-0.041914, -0.0109, … -0.027008]</td><td>&quot;parallel_embed…</td><td>[&quot;print&quot;, &quot;len&quot;, … &quot;sorted&quot;]</td><td>[]</td></tr><tr><td>&quot;\n",
       "\n",
       "class TopicT…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>992</td><td>[-0.023198, -0.002724, … -0.053914]</td><td>&quot;TopicTreeTask&quot;</td><td>[&quot;super&quot;, &quot;print&quot;, … &quot;MemoryIndex&quot;]</td><td>[[&quot;BaseTask&quot;]]</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 340]</td><td>220</td><td>[-0.004693, 0.001029, … -0.048643]</td><td>&quot;__init__&quot;</td><td>[&quot;super&quot;]</td><td>[]</td></tr><tr><td>&quot;\n",
       "\n",
       "def _setup_m…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[271, 755, … 14790]</td><td>104</td><td>[-0.01272, 0.016058, … -0.001598]</td><td>&quot;_setup_memory_…</td><td>[&quot;print&quot;, &quot;HDBSCANMultiKernel&quot;, … &quot;ValueError&quot;]</td><td>[]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 9)\n",
       "┌─────────┬───────────┬────────┬───────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ code    ┆ libcst    ┆ filena ┆ tokens|co ┆ … ┆ embedding| ┆ code_main_ ┆ code_funct ┆ code_paren │\n",
       "│ ---     ┆ tree      ┆ me     ┆ de        ┆   ┆ code       ┆ entity|Mai ┆ ion_calls| ┆ t_classes| │\n",
       "│ str     ┆ ---       ┆ ---    ┆ ---       ┆   ┆ ---        ┆ nEntityVis ┆ FunctionCa ┆ ClassInher │\n",
       "│         ┆ str       ┆ str    ┆ list[i64] ┆   ┆ list[f64]  ┆ it…        ┆ ll…        ┆ it…        │\n",
       "│         ┆           ┆        ┆           ┆   ┆            ┆ ---        ┆ ---        ┆ ---        │\n",
       "│         ┆           ┆        ┆           ┆   ┆            ┆ str        ┆ list[str]  ┆ list[list[ │\n",
       "│         ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆ str]]      │\n",
       "╞═════════╪═══════════╪════════╪═══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [198,     ┆ … ┆ [-0.012073 ┆ Embeddable ┆ []         ┆ [[\"Enum\"]] │\n",
       "│ class   ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆ ,          ┆ Type       ┆            ┆            │\n",
       "│ Embedda ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆ -0.004771, ┆            ┆            ┆            │\n",
       "│ bleType ┆        …  ┆ eurald ┆           ┆   ┆ … -0.0436… ┆            ┆            ┆            │\n",
       "│ (Enum): ┆           ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│    …    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ [0.03026,  ┆ infer_embe ┆ [\"str\",    ┆ []         │\n",
       "│ def inf ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆ 0.011673,  ┆ ddable_typ ┆ \"print\", … ┆            │\n",
       "│ er_embe ┆ name=Name ┆ lhug/n ┆ 14790]    ┆   ┆ …          ┆ e          ┆ \"ValueErro ┆            │\n",
       "│ ddable_ ┆ (         ┆ eurald ┆           ┆   ┆ -0.039312] ┆            ┆ r\"]        ┆            │\n",
       "│ type(co ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ lum…    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ [0.012823, ┆ numeric_em ┆ []         ┆ []         │\n",
       "│ def num ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆ 0.010932,  ┆ bedder     ┆            ┆            │\n",
       "│ eric_em ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆ …          ┆            ┆            ┆            │\n",
       "│ bedder( ┆ (         ┆ eurald ┆           ┆   ┆ -0.027359… ┆            ┆            ┆            │\n",
       "│ column) ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ :       ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│  …      ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [271,     ┆ … ┆ [-0.025804 ┆ EmbeddingT ┆ [\"len\",    ┆ [[\"BaseTas │\n",
       "│         ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆ ,          ┆ ask        ┆ \"ValueErro ┆ k\"]]       │\n",
       "│ class   ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆ -0.008756, ┆            ┆ r\"]        ┆            │\n",
       "│ Embeddi ┆        …  ┆ eurald ┆           ┆   ┆ … -0.0420… ┆            ┆            ┆            │\n",
       "│ ngTask( ┆           ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ BaseTas ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ k):…    ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ …       ┆ …         ┆ …      ┆ …         ┆ … ┆ …          ┆ …          ┆ …          ┆ …          │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ [-0.041914 ┆ parallel_e ┆ [\"print\",  ┆ []         │\n",
       "│ def par ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆ , -0.0109, ┆ mbeddings  ┆ \"len\", …   ┆            │\n",
       "│ allel_e ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆ …          ┆            ┆ \"sorted\"]  ┆            │\n",
       "│ mbeddin ┆ (         ┆ eurald ┆           ┆   ┆ -0.027008… ┆            ┆            ┆            │\n",
       "│ gs(embe ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ dde…    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [271,     ┆ … ┆ [-0.023198 ┆ TopicTreeT ┆ [\"super\",  ┆ [[\"BaseTas │\n",
       "│         ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆ ,          ┆ ask        ┆ \"print\", … ┆ k\"]]       │\n",
       "│ class   ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆ -0.002724, ┆            ┆ \"MemoryInd ┆            │\n",
       "│ TopicTr ┆        …  ┆ eurald ┆           ┆   ┆ … -0.0539… ┆            ┆ e…         ┆            │\n",
       "│ eeTask( ┆           ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ BaseTas ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ k):…    ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ def __i ┆ FunctionD ┆ /Users ┆ [755,     ┆ … ┆ [-0.004693 ┆ __init__   ┆ [\"super\"]  ┆ []         │\n",
       "│ nit__(  ┆ ef(       ┆ /danie ┆ 1328, …   ┆   ┆ ,          ┆            ┆            ┆            │\n",
       "│ self,   ┆ name=Name ┆ lhug/n ┆ 340]      ┆   ┆ 0.001029,  ┆            ┆            ┆            │\n",
       "│ memo…   ┆ (         ┆ eurald ┆           ┆   ┆ …          ┆            ┆            ┆            │\n",
       "│         ┆     …     ┆ ragon/ ┆           ┆   ┆ -0.04864…  ┆            ┆            ┆            │\n",
       "│         ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [271,     ┆ … ┆ [-0.01272, ┆ _setup_mem ┆ [\"print\",  ┆ []         │\n",
       "│         ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆ 0.016058,  ┆ ory_kernel ┆ \"HDBSCANMu ┆            │\n",
       "│ def _se ┆ name=Name ┆ lhug/n ┆ 14790]    ┆   ┆ …          ┆ _group     ┆ ltiKernel\" ┆            │\n",
       "│ tup_mem ┆ (         ┆ eurald ┆           ┆   ┆ -0.001598… ┆            ┆ , …        ┆            │\n",
       "│ ory_ker ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ nel_gro ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ up…     ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "└─────────┴───────────┴────────┴───────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mfp.df.filter(pl.col('code_parent_classes|ClassInheritanceVisitor').str.contains(\"__init__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst_tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th><th>code_main_entity|MainEntityVisitor</th><th>code_function_calls|FunctionCallCollector</th><th>code_parent_classes|ClassInheritanceVisitor</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td><td>str</td><td>list[str]</td><td>list[list[str]]</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012065, -0.004709, … -0.043599]</td><td>&quot;EmbeddableType…</td><td>[]</td><td>[[&quot;Enum&quot;]]</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.030298, 0.011617, … -0.039327]</td><td>&quot;infer_embeddab…</td><td>[&quot;str&quot;, &quot;print&quot;, … &quot;ValueError&quot;]</td><td>[]</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012823, 0.010932, … -0.027359]</td><td>&quot;numeric_embedd…</td><td>[]</td><td>[]</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025782, -0.008832, … -0.04199]</td><td>&quot;EmbeddingTask&quot;</td><td>[&quot;len&quot;, &quot;ValueError&quot;]</td><td>[[&quot;BaseTask&quot;]]</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 198]</td><td>102</td><td>[-0.018791, -0.018855, … -0.04652]</td><td>&quot;__init__&quot;</td><td>[]</td><td>[]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌─────────┬───────────┬────────┬───────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ code    ┆ libcst_tr ┆ filena ┆ tokens|co ┆ … ┆ embedding| ┆ code_main_ ┆ code_funct ┆ code_paren │\n",
       "│ ---     ┆ ee        ┆ me     ┆ de        ┆   ┆ code       ┆ entity|Mai ┆ ion_calls| ┆ t_classes| │\n",
       "│ str     ┆ ---       ┆ ---    ┆ ---       ┆   ┆ ---        ┆ nEntityVis ┆ FunctionCa ┆ ClassInher │\n",
       "│         ┆ str       ┆ str    ┆ list[i64] ┆   ┆ list[f64]  ┆ it…        ┆ ll…        ┆ it…        │\n",
       "│         ┆           ┆        ┆           ┆   ┆            ┆ ---        ┆ ---        ┆ ---        │\n",
       "│         ┆           ┆        ┆           ┆   ┆            ┆ str        ┆ list[str]  ┆ list[list[ │\n",
       "│         ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆ str]]      │\n",
       "╞═════════╪═══════════╪════════╪═══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [198,     ┆ … ┆ [-0.012065 ┆ Embeddable ┆ []         ┆ [[\"Enum\"]] │\n",
       "│ class   ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆ ,          ┆ Type       ┆            ┆            │\n",
       "│ Embedda ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆ -0.004709, ┆            ┆            ┆            │\n",
       "│ bleType ┆        …  ┆ eurald ┆           ┆   ┆ … -0.0435… ┆            ┆            ┆            │\n",
       "│ (Enum): ┆           ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│    …    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ [0.030298, ┆ infer_embe ┆ [\"str\",    ┆ []         │\n",
       "│ def inf ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆ 0.011617,  ┆ ddable_typ ┆ \"print\", … ┆            │\n",
       "│ er_embe ┆ name=Name ┆ lhug/n ┆ 14790]    ┆   ┆ …          ┆ e          ┆ \"ValueErro ┆            │\n",
       "│ ddable_ ┆ (         ┆ eurald ┆           ┆   ┆ -0.039327… ┆            ┆ r\"]        ┆            │\n",
       "│ type(co ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ lum…    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ [0.012823, ┆ numeric_em ┆ []         ┆ []         │\n",
       "│ def num ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆ 0.010932,  ┆ bedder     ┆            ┆            │\n",
       "│ eric_em ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆ …          ┆            ┆            ┆            │\n",
       "│ bedder( ┆ (         ┆ eurald ┆           ┆   ┆ -0.027359… ┆            ┆            ┆            │\n",
       "│ column) ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ :       ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│  …      ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [271,     ┆ … ┆ [-0.025782 ┆ EmbeddingT ┆ [\"len\",    ┆ [[\"BaseTas │\n",
       "│         ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆ ,          ┆ ask        ┆ \"ValueErro ┆ k\"]]       │\n",
       "│ class   ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆ -0.008832, ┆            ┆ r\"]        ┆            │\n",
       "│ Embeddi ┆        …  ┆ eurald ┆           ┆   ┆ … -0.0419… ┆            ┆            ┆            │\n",
       "│ ngTask( ┆           ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ BaseTas ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ k):…    ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ def __i ┆ FunctionD ┆ /Users ┆ [755,     ┆ … ┆ [-0.018791 ┆ __init__   ┆ []         ┆ []         │\n",
       "│ nit__(  ┆ ef(       ┆ /danie ┆ 1328, …   ┆   ┆ ,          ┆            ┆            ┆            │\n",
       "│ self,   ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆ -0.018855, ┆            ┆            ┆            │\n",
       "│ embe…   ┆ (         ┆ eurald ┆           ┆   ┆ … -0.0465… ┆            ┆            ┆            │\n",
       "│         ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "└─────────┴───────────┴────────┴───────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libcst as cst\n",
    "from collections import defaultdict\n",
    "\n",
    "class ClassInheritanceVisitor(cst.CSTVisitor):\n",
    "    def __init__(self, code: str):\n",
    "        self.module = cst.parse_module(code)\n",
    "        self.classes = defaultdict(list)\n",
    "\n",
    "    def visit_ClassDef(self, node: cst.ClassDef):\n",
    "        for base in node.bases:\n",
    "            base_class_name = cst.Module([base]).code.strip()\n",
    "            self.classes[node.name.value].append(base_class_name)\n",
    "            self.classes[base_class_name]  # Initialize key in dict if not already present\n",
    "        return False  # Prevent visiting inner scopes\n",
    "\n",
    "    def collect(self):\n",
    "        self.module.visit(self)\n",
    "        return self._build_hierarchy()\n",
    "\n",
    "    def _build_hierarchy(self):\n",
    "        hierarchy_list = []\n",
    "        for class_name, direct_parents in self.classes.items():\n",
    "            all_parents = set(direct_parents)\n",
    "            to_visit = list(direct_parents)\n",
    "            while to_visit:\n",
    "                parent_class_name = to_visit.pop()\n",
    "                for grand_parent in self.classes[parent_class_name]:\n",
    "                    if grand_parent not in all_parents:\n",
    "                        all_parents.add(grand_parent)\n",
    "                        to_visit.append(grand_parent)\n",
    "            hierarchy_list.append((class_name, list(all_parents)))\n",
    "        return hierarchy_list\n",
    "\n",
    "# Your usage of this class might look something like this\n",
    "mfp = mfp.apply_visitor_to_column(\"code\", ClassInheritanceVisitor, \"parent_classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through frame and print code and parent_classes row items\n",
    "for code, parent_classes in zip(mfp.df['code'], mfp.df['code_parent_classes|ClassInheritanceVisitor']):\n",
    "    print(code)\n",
    "    print(f\"Parent classes: {parent_classes}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babydragon.memory.frames.visitors.node_type_collectors import FunctionCallCollector\n",
    "mfp = mfp.apply_visitor_to_column(\"code\", FunctionCallCollector, \"function_calls\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "class EmbeddableType(Enum):\n",
      "    TEXT = \"text\"\n",
      "    NUMERIC = \"numeric\"\n",
      "    CATEGORICAL = \"categorical\"\n",
      "    # Add more data types as required\n",
      "\n",
      "\n",
      "\n",
      "def infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\n",
      "    # Infer the data type of the column\n",
      "    # This will depend on the type of `column` (whether it's a string, Series, etc.)\n",
      "    # Here we'll assume `column` is a pandas Series for simplicity\n",
      "    column_type = str(column.dtype)\n",
      "    print(column_type)\n",
      "    if column_type == \"Utf8\":\n",
      "        # If it's an object, we'll assume it's text\n",
      "        return EmbeddableType.TEXT, OpenAiEmbedder()\n",
      "    elif np.issubdtype(column.dtype, np.number):\n",
      "        # If it's a number, we'll use a different embedding strategy\n",
      "        return EmbeddableType.NUMERIC, numeric_embedder\n",
      "    else:\n",
      "        # For other types, we could throw an error or have a default strategy\n",
      "        raise ValueError(f\"Cannot infer type for column {column.name}\")\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"str\"\n",
      "\t\"print\"\n",
      "\t\"OpenAiEmbedder…\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def numeric_embedder(column):\n",
      "    # Implement the numeric embedding strategy\n",
      "    # This will depend on the type of `column` (whether it's a string, Series, etc.)\n",
      "    # Here we'll assume `column` is a pandas Series for simplicity\n",
      "    return column.values\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class EmbeddingTask(BaseTask):\n",
      "    def __init__(\n",
      "        self,\n",
      "        embedder: OpenAiEmbedder,\n",
      "        values: List[Any],\n",
      "        path: List[List[int]],\n",
      "        max_workers: int = 1,\n",
      "        task_id: str = \"task\",\n",
      "        calls_per_minute: int = 1500,\n",
      "        backup: bool = True,\n",
      "    ):\n",
      "        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup)\n",
      "        self.embedder = embedder\n",
      "        self.values = values\n",
      "\n",
      "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "        # expected to work with a lig of a single element\n",
      "        if len(sub_path) != 1:\n",
      "            raise ValueError(\n",
      "                \"Embedding task expected to work with a list of a single element\"\n",
      "            )\n",
      "        sub_results = {}\n",
      "        for i in sub_path:\n",
      "            embedded_value = self.embedder.embed(self.values[i])\n",
      "            sub_results[i] = embedded_value\n",
      "        return sub_results\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    embedder: OpenAiEmbedder,\n",
      "    values: List[Any],\n",
      "    path: List[List[int]],\n",
      "    max_workers: int = 1,\n",
      "    task_id: str = \"task\",\n",
      "    calls_per_minute: int = 1500,\n",
      "    backup: bool = True,\n",
      "):\n",
      "    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup)\n",
      "    self.embedder = embedder\n",
      "    self.values = values\n",
      "\n",
      "\n",
      "\n",
      "def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "    # expected to work with a lig of a single element\n",
      "    if len(sub_path) != 1:\n",
      "        raise ValueError(\n",
      "            \"Embedding task expected to work with a list of a single element\"\n",
      "        )\n",
      "    sub_results = {}\n",
      "    for i in sub_path:\n",
      "        embedded_value = self.embedder.embed(self.values[i])\n",
      "        sub_results[i] = embedded_value\n",
      "    return sub_results\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def parallel_embeddings(embedder, values, max_workers, backup, name):\n",
      "        # Prepare the paths for the EmbeddingTask\n",
      "        print(\"Embedding {} values\".format(len(values)))\n",
      "        paths = [[i] for i in range(len(values))]\n",
      "\n",
      "        # Initialize the EmbeddingTask and execute it\n",
      "        embedding_task = EmbeddingTask(\n",
      "            embedder,\n",
      "            values,\n",
      "            path=paths,\n",
      "            max_workers=max_workers,\n",
      "            task_id=name + \"_embedding_task\",\n",
      "            backup=backup,\n",
      "        )\n",
      "        embeddings = embedding_task.work()\n",
      "        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\n",
      "        return embeddings\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"EmbeddingTask\"\n",
      "\t\"sorted\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class TopicTreeTask(BaseTask):\n",
      "    def __init__(\n",
      "        self,\n",
      "        memory_kernel_dict: Dict,\n",
      "        supplement_indexes: Dict,\n",
      "        sim_threshold: float,\n",
      "        chatbot: BaseChat,\n",
      "        parent_kernel_label: str,\n",
      "        child_kernel_label: str,\n",
      "        system_prompt: str,\n",
      "        clustering_method: str,\n",
      "        task_id: str = \"TopicTreeTask\",\n",
      "        max_workers: int = 1,\n",
      "        calls_per_minute: int = 20,\n",
      "    ):\n",
      "        self.clustering_method = clustering_method\n",
      "        self.supplement_indexes = supplement_indexes\n",
      "        self.sim_threshold = sim_threshold\n",
      "        self.parent_kernel_label = parent_kernel_label\n",
      "        self.child_kernel_label = child_kernel_label\n",
      "        self.memory_kernel_dict = memory_kernel_dict\n",
      "        self._setup_memory_kernel_group()\n",
      "        self.generate_task_paths()\n",
      "        self.system_prompt = system_prompt\n",
      "        self.chatbot = chatbot\n",
      "        self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n",
      "        super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n",
      "\n",
      "\n",
      "    def _setup_memory_kernel_group(self):\n",
      "        if self.clustering_method == \"HDBSCAN\":\n",
      "            print(\"Using HDBSCAN\")\n",
      "            self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "        elif self.clustering_method == \"Spectral\":\n",
      "            print(\"Using Spectral\")\n",
      "            self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "        else:\n",
      "            raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n",
      "\n",
      "    def generate_task_paths(self):\n",
      "        print(\"Generating task paths\")\n",
      "\n",
      "        self.memory_kernel_group.generate_path_groups()\n",
      "\n",
      "    def llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n",
      "        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n",
      "        return chatbot.reply(message)\n",
      "\n",
      "    def _execute_sub_task(self, sub_path) -> List[str]:\n",
      "        if self.parallel:\n",
      "            chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "        else:\n",
      "            chatbot_instance = self.chatbot\n",
      "\n",
      "        sub_results = {}\n",
      "        for i in sub_path:\n",
      "            print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n",
      "            try:\n",
      "                current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n",
      "                supplement_values = []\n",
      "                for key, index in self.supplement_indexes.items():\n",
      "                    results, scores, indeces = index.faiss_query(current_val, k=5)\n",
      "                    for result, score in zip(results, scores):\n",
      "                        if score > self.sim_threshold:\n",
      "                            supplement_values.append(result)\n",
      "                topic_tree = self.create_topic_tree(supplement_values)\n",
      "                #response = self.llm_response(chatbot_instance, current_val, id=i)\n",
      "                sub_results[i] = topic_tree\n",
      "            except IndexError:\n",
      "                print(f\"Error: Invalid index {i} in sub_path\")\n",
      "                sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n",
      "            except Exception as e:\n",
      "                print(f\"Error in sub_task for index {i}: {e}\")\n",
      "                sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n",
      "\n",
      "        return sub_results\n",
      "\n",
      "    def execute_task(self) -> None:\n",
      "        BaseTask.execute_task(self)\n",
      "\n",
      "        # Load the results from the JSON file\n",
      "        # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n",
      "        #     task_results = json.load(f)\n",
      "        self._load_results_from_file()\n",
      "        task_results = self.results\n",
      "        new_values = []\n",
      "        #sort task_results by index and add to new_values 0- max values ascending\n",
      "        for task_result in task_results:\n",
      "            if isinstance(task_result, dict):\n",
      "                for key, value in task_result.items():\n",
      "                    new_values.append((int(key), value))\n",
      "            elif isinstance(task_result, str):\n",
      "                print(f\"Error in task_result: {task_result}\")\n",
      "\n",
      "        new_values.sort(key=lambda x: x[0])\n",
      "        values = [x[1] for x in new_values]\n",
      "\n",
      "        task_memory_index = MemoryIndex()\n",
      "        task_memory_index.init_index(values=values)\n",
      "        # Create a new MemoryKernel with the results\n",
      "        new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n",
      "\n",
      "        # Add the new MemoryKernel to the MultiKernel\n",
      "        self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n",
      "        self.generate_task_paths()\n",
      "\n",
      "        #delete the results file\n",
      "        # os.remove(f\"{self.task_id}_results.json\")\n",
      "\n",
      "\n",
      "    def create_topic_tree(self, docs):\n",
      "        return None\n",
      "\n",
      "Function calls: shape: (17,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"print\"\n",
      "\t\"HDBSCANMultiKe…\n",
      "\t\"print\"\n",
      "\t\"SpectralCluste…\n",
      "\t\"ValueError\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"zip\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"isinstance\"\n",
      "\t\"int\"\n",
      "\t\"isinstance\"\n",
      "\t\"print\"\n",
      "\t\"MemoryIndex\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    memory_kernel_dict: Dict,\n",
      "    supplement_indexes: Dict,\n",
      "    sim_threshold: float,\n",
      "    chatbot: BaseChat,\n",
      "    parent_kernel_label: str,\n",
      "    child_kernel_label: str,\n",
      "    system_prompt: str,\n",
      "    clustering_method: str,\n",
      "    task_id: str = \"TopicTreeTask\",\n",
      "    max_workers: int = 1,\n",
      "    calls_per_minute: int = 20,\n",
      "):\n",
      "    self.clustering_method = clustering_method\n",
      "    self.supplement_indexes = supplement_indexes\n",
      "    self.sim_threshold = sim_threshold\n",
      "    self.parent_kernel_label = parent_kernel_label\n",
      "    self.child_kernel_label = child_kernel_label\n",
      "    self.memory_kernel_dict = memory_kernel_dict\n",
      "    self._setup_memory_kernel_group()\n",
      "    self.generate_task_paths()\n",
      "    self.system_prompt = system_prompt\n",
      "    self.chatbot = chatbot\n",
      "    self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n",
      "    super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def _setup_memory_kernel_group(self):\n",
      "    if self.clustering_method == \"HDBSCAN\":\n",
      "        print(\"Using HDBSCAN\")\n",
      "        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "    elif self.clustering_method == \"Spectral\":\n",
      "        print(\"Using Spectral\")\n",
      "        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "    else:\n",
      "        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"HDBSCANMultiKe…\n",
      "\t\"print\"\n",
      "\t\"SpectralCluste…\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def generate_task_paths(self):\n",
      "    print(\"Generating task paths\")\n",
      "\n",
      "    self.memory_kernel_group.generate_path_groups()\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n",
      "    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n",
      "    return chatbot.reply(message)\n",
      "\n",
      "\n",
      "\n",
      "def _execute_sub_task(self, sub_path) -> List[str]:\n",
      "    if self.parallel:\n",
      "        chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "    else:\n",
      "        chatbot_instance = self.chatbot\n",
      "\n",
      "    sub_results = {}\n",
      "    for i in sub_path:\n",
      "        print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n",
      "        try:\n",
      "            current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n",
      "            supplement_values = []\n",
      "            for key, index in self.supplement_indexes.items():\n",
      "                results, scores, indeces = index.faiss_query(current_val, k=5)\n",
      "                for result, score in zip(results, scores):\n",
      "                    if score > self.sim_threshold:\n",
      "                        supplement_values.append(result)\n",
      "            topic_tree = self.create_topic_tree(supplement_values)\n",
      "            #response = self.llm_response(chatbot_instance, current_val, id=i)\n",
      "            sub_results[i] = topic_tree\n",
      "        except IndexError:\n",
      "            print(f\"Error: Invalid index {i} in sub_path\")\n",
      "            sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n",
      "        except Exception as e:\n",
      "            print(f\"Error in sub_task for index {i}: {e}\")\n",
      "            sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n",
      "\n",
      "    return sub_results\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"zip\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def execute_task(self) -> None:\n",
      "    BaseTask.execute_task(self)\n",
      "\n",
      "    # Load the results from the JSON file\n",
      "    # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n",
      "    #     task_results = json.load(f)\n",
      "    self._load_results_from_file()\n",
      "    task_results = self.results\n",
      "    new_values = []\n",
      "    #sort task_results by index and add to new_values 0- max values ascending\n",
      "    for task_result in task_results:\n",
      "        if isinstance(task_result, dict):\n",
      "            for key, value in task_result.items():\n",
      "                new_values.append((int(key), value))\n",
      "        elif isinstance(task_result, str):\n",
      "            print(f\"Error in task_result: {task_result}\")\n",
      "\n",
      "    new_values.sort(key=lambda x: x[0])\n",
      "    values = [x[1] for x in new_values]\n",
      "\n",
      "    task_memory_index = MemoryIndex()\n",
      "    task_memory_index.init_index(values=values)\n",
      "    # Create a new MemoryKernel with the results\n",
      "    new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n",
      "\n",
      "    # Add the new MemoryKernel to the MultiKernel\n",
      "    self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n",
      "    self.generate_task_paths()\n",
      "\n",
      "    #delete the results file\n",
      "    # os.remove(f\"{self.task_id}_results.json\")\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"int\"\n",
      "\t\"isinstance\"\n",
      "\t\"print\"\n",
      "\t\"MemoryIndex\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def create_topic_tree(self, docs):\n",
      "    return None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class MultiKernelTask(BaseTask):\n",
      "    def __init__(\n",
      "        self,\n",
      "        memory_kernel_dict: Dict,\n",
      "        chatbot: BaseChat,\n",
      "        parent_kernel_label: str,\n",
      "        child_kernel_label: str,\n",
      "        system_prompt: str,\n",
      "        clustering_method: str,\n",
      "        path_group: Dict[str, List[List[int]]],\n",
      "        task_id: str = \"MultiKernelTask\",\n",
      "        max_workers: int = 1,\n",
      "        calls_per_minute: int = 20,\n",
      "    ):\n",
      "        self.clustering_method = clustering_method\n",
      "        self.parent_kernel_label = parent_kernel_label\n",
      "        self.child_kernel_label = child_kernel_label\n",
      "        self.memory_kernel_dict = memory_kernel_dict\n",
      "        \n",
      "        self._setup_memory_kernel_group()\n",
      "        if path_group:\n",
      "            self.memory_kernel_group.path_group = path_group\n",
      "        else:\n",
      "            self.generate_task_paths()\n",
      "        self.system_prompt = system_prompt\n",
      "        self.chatbot = chatbot\n",
      "        self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n",
      "        super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n",
      "\n",
      "\n",
      "    def _setup_memory_kernel_group(self):\n",
      "        if self.clustering_method == \"HDBSCAN\":\n",
      "            print(\"Using HDBSCAN\")\n",
      "            self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "        elif self.clustering_method == \"Spectral\":\n",
      "            print(\"Using Spectral\")\n",
      "            self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "        else:\n",
      "            raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n",
      "\n",
      "    def generate_task_paths(self):\n",
      "        print(\"Generating task paths\")\n",
      "\n",
      "        self.memory_kernel_group.generate_path_groups()\n",
      "\n",
      "    def llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n",
      "        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n",
      "        return chatbot.reply(message)\n",
      "\n",
      "    def _execute_sub_task(self, sub_path) -> List[str]:\n",
      "        if self.parallel:\n",
      "            chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "        else:\n",
      "            chatbot_instance = self.chatbot\n",
      "\n",
      "        sub_results = {}\n",
      "        for i in sub_path:\n",
      "            print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n",
      "            try:\n",
      "                current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n",
      "                response = self.llm_response(chatbot_instance, current_val, id=i)\n",
      "                sub_results[i] = response\n",
      "            except IndexError:\n",
      "                print(f\"Error: Invalid index {i} in sub_path\")\n",
      "                sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n",
      "            except Exception as e:\n",
      "                print(f\"Error in sub_task for index {i}: {e}\")\n",
      "                sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n",
      "\n",
      "        return sub_results\n",
      "\n",
      "    def execute_task(self) -> None:\n",
      "        BaseTask.execute_task(self)\n",
      "\n",
      "        # Load the results from the JSON file\n",
      "        # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n",
      "        #     task_results = json.load(f)\n",
      "        self._load_results_from_file()\n",
      "        task_results = self.results\n",
      "        new_values = []\n",
      "        #sort task_results by index and add to new_values 0- max values ascending\n",
      "        for task_result in task_results:\n",
      "            if isinstance(task_result, dict):\n",
      "                for key, value in task_result.items():\n",
      "                    new_values.append((int(key), value))\n",
      "            elif isinstance(task_result, str):\n",
      "                print(f\"Error in task_result: {task_result}\")\n",
      "\n",
      "        new_values.sort(key=lambda x: x[0])\n",
      "        values = [x[1] for x in new_values]\n",
      "\n",
      "        task_memory_index = MemoryIndex()\n",
      "        task_memory_index.init_index(values=values)\n",
      "        # Create a new MemoryKernel with the results\n",
      "        new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n",
      "\n",
      "        # Add the new MemoryKernel to the MultiKernel\n",
      "        self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n",
      "        self.generate_task_paths()\n",
      "\n",
      "        #delete the results file\n",
      "        # os.remove(f\"{self.task_id}_results.json\")\n",
      "\n",
      "Function calls: shape: (16,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"print\"\n",
      "\t\"HDBSCANMultiKe…\n",
      "\t\"print\"\n",
      "\t\"SpectralCluste…\n",
      "\t\"ValueError\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"isinstance\"\n",
      "\t\"int\"\n",
      "\t\"isinstance\"\n",
      "\t\"print\"\n",
      "\t\"MemoryIndex\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    memory_kernel_dict: Dict,\n",
      "    chatbot: BaseChat,\n",
      "    parent_kernel_label: str,\n",
      "    child_kernel_label: str,\n",
      "    system_prompt: str,\n",
      "    clustering_method: str,\n",
      "    path_group: Dict[str, List[List[int]]],\n",
      "    task_id: str = \"MultiKernelTask\",\n",
      "    max_workers: int = 1,\n",
      "    calls_per_minute: int = 20,\n",
      "):\n",
      "    self.clustering_method = clustering_method\n",
      "    self.parent_kernel_label = parent_kernel_label\n",
      "    self.child_kernel_label = child_kernel_label\n",
      "    self.memory_kernel_dict = memory_kernel_dict\n",
      "    \n",
      "    self._setup_memory_kernel_group()\n",
      "    if path_group:\n",
      "        self.memory_kernel_group.path_group = path_group\n",
      "    else:\n",
      "        self.generate_task_paths()\n",
      "    self.system_prompt = system_prompt\n",
      "    self.chatbot = chatbot\n",
      "    self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n",
      "    super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def _setup_memory_kernel_group(self):\n",
      "    if self.clustering_method == \"HDBSCAN\":\n",
      "        print(\"Using HDBSCAN\")\n",
      "        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "    elif self.clustering_method == \"Spectral\":\n",
      "        print(\"Using Spectral\")\n",
      "        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "    else:\n",
      "        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"HDBSCANMultiKe…\n",
      "\t\"print\"\n",
      "\t\"SpectralCluste…\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def generate_task_paths(self):\n",
      "    print(\"Generating task paths\")\n",
      "\n",
      "    self.memory_kernel_group.generate_path_groups()\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n",
      "    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n",
      "    return chatbot.reply(message)\n",
      "\n",
      "\n",
      "\n",
      "def _execute_sub_task(self, sub_path) -> List[str]:\n",
      "    if self.parallel:\n",
      "        chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "    else:\n",
      "        chatbot_instance = self.chatbot\n",
      "\n",
      "    sub_results = {}\n",
      "    for i in sub_path:\n",
      "        print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n",
      "        try:\n",
      "            current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n",
      "            response = self.llm_response(chatbot_instance, current_val, id=i)\n",
      "            sub_results[i] = response\n",
      "        except IndexError:\n",
      "            print(f\"Error: Invalid index {i} in sub_path\")\n",
      "            sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n",
      "        except Exception as e:\n",
      "            print(f\"Error in sub_task for index {i}: {e}\")\n",
      "            sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n",
      "\n",
      "    return sub_results\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def execute_task(self) -> None:\n",
      "    BaseTask.execute_task(self)\n",
      "\n",
      "    # Load the results from the JSON file\n",
      "    # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n",
      "    #     task_results = json.load(f)\n",
      "    self._load_results_from_file()\n",
      "    task_results = self.results\n",
      "    new_values = []\n",
      "    #sort task_results by index and add to new_values 0- max values ascending\n",
      "    for task_result in task_results:\n",
      "        if isinstance(task_result, dict):\n",
      "            for key, value in task_result.items():\n",
      "                new_values.append((int(key), value))\n",
      "        elif isinstance(task_result, str):\n",
      "            print(f\"Error in task_result: {task_result}\")\n",
      "\n",
      "    new_values.sort(key=lambda x: x[0])\n",
      "    values = [x[1] for x in new_values]\n",
      "\n",
      "    task_memory_index = MemoryIndex()\n",
      "    task_memory_index.init_index(values=values)\n",
      "    # Create a new MemoryKernel with the results\n",
      "    new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n",
      "\n",
      "    # Add the new MemoryKernel to the MultiKernel\n",
      "    self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n",
      "    self.generate_task_paths()\n",
      "\n",
      "    #delete the results file\n",
      "    # os.remove(f\"{self.task_id}_results.json\")\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"int\"\n",
      "\t\"isinstance\"\n",
      "\t\"print\"\n",
      "\t\"MemoryIndex\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class BaseTask:\n",
      "    def __init__(\n",
      "        self,\n",
      "        path: List[List[int]],\n",
      "        max_workers: int = 1,\n",
      "        task_id: str = \"task\",\n",
      "        calls_per_minute: int = 20,\n",
      "        backup: bool = True,\n",
      "        save_path: str = None,\n",
      "    ):\n",
      "        self.task_id = task_id\n",
      "        self.path = path\n",
      "        self.results = []\n",
      "        self.max_workers = max_workers\n",
      "        self.parallel = True if max_workers > 1 else False\n",
      "        self.rate_limiter = RateLimiter(calls_per_minute)\n",
      "        self.failed_sub_tasks = []\n",
      "        self.backup = backup\n",
      "        print(\"setting up savepath\")\n",
      "        self.save_path = save_path if save_path is not None else os.path.join(\"storage\", \"tasks\")\n",
      "\n",
      "    def _save_results_to_file(self) -> None:\n",
      "        os.makedirs(self.save_path, exist_ok=True)\n",
      "        with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"w\") as f:\n",
      "            json.dump(self.results, f)\n",
      "\n",
      "    def _load_results_from_file(self) -> None:\n",
      "        if os.path.exists(os.path.join(self.save_path, f\"{self.task_id}_results.json\")):\n",
      "            try:\n",
      "                with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"r\") as f:\n",
      "                    self.results = json.load(f)\n",
      "                    print(f\"Loaded {len(self.results)} results from file.\")\n",
      "            except Exception as e:\n",
      "                print(f\"Error loading results from file: {e}\")\n",
      "                print(\"Starting from scratch.\")\n",
      "        else:\n",
      "            print(\"No results file found, starting from scratch.\")\n",
      "\n",
      "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "        sub_results = []\n",
      "        for i in sub_path:\n",
      "            response = \"Implement the response function in the subclass\"\n",
      "            sub_results.append(response)\n",
      "        return sub_results\n",
      "\n",
      "    def execute_task(self) -> None:\n",
      "        if self.backup:\n",
      "            self._load_results_from_file()\n",
      "\n",
      "        with RateLimitedThreadPoolExecutor(\n",
      "            max_workers=self.max_workers,\n",
      "            calls_per_minute=self.rate_limiter.calls_per_minute,\n",
      "        ) as executor:\n",
      "            futures = []\n",
      "            print(f\"Executing task {self.task_id} using {self.max_workers} workers.\")\n",
      "\n",
      "            for i, sub_path in enumerate(self.path):\n",
      "                if i < len(self.results):\n",
      "                    pass\n",
      "                else:\n",
      "                    future = executor.submit(self._execute_sub_task, sub_path)\n",
      "                    futures.append((i, future))\n",
      "\n",
      "            for i, future in futures:\n",
      "                try:\n",
      "                    execution_start_time = time.time()\n",
      "                    sub_task_result = future.result()\n",
      "                    execution_end_time = time.time()\n",
      "                    print(\n",
      "                        f\"Sub-task {i} executed in {execution_end_time - execution_start_time:.2f} seconds.\"\n",
      "                    )\n",
      "\n",
      "                    save_start_time = time.time()\n",
      "                    self.results.append(sub_task_result)\n",
      "                    # self.results.append(sub_task_result)\n",
      "                    if self.backup:\n",
      "                        self._save_results_to_file()\n",
      "                    save_end_time = time.time()\n",
      "                    print(\n",
      "                        f\"Sub-task {i} results saved in {save_end_time - save_start_time:.2f} seconds.\"\n",
      "                    )\n",
      "                except Exception as e:\n",
      "                    print(f\"Error in sub-task {i}: {e}\")\n",
      "                    # default_result = f\"Error in sub-task {i}: {e}\"\n",
      "                    default_result =  {i:f\"Error in sub-task {i}: {e}\"} \n",
      "                    self.results.append(default_result)\n",
      "                    if self.backup:\n",
      "                        self._save_results_to_file()\n",
      "                    self.failed_sub_tasks.append((self.path[i], str(e)))\n",
      "\n",
      "                except KeyboardInterrupt:\n",
      "                    print(\"Keyboard interrupt detected, stopping task execution.\")\n",
      "                    executor.shutdown(wait=False)\n",
      "                    break\n",
      "\n",
      "        print(\"Task execution completed.\")\n",
      "\n",
      "    def work(self) -> List[Any]:\n",
      "        self.execute_task()\n",
      "        if not self.backup:\n",
      "            self._save_results_to_file()\n",
      "        work = []\n",
      "        for sub_result in self.results:\n",
      "            for index_id, response in sub_result.items():\n",
      "                work.append((index_id, response))\n",
      "        # sort the content to write by index_id\n",
      "        work.sort(key=lambda x: int(x[0]))\n",
      "        return work\n",
      "\n",
      "Function calls: shape: (20,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"RateLimiter\"\n",
      "\t\"print\"\n",
      "\t\"open\"\n",
      "\t\"open\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"RateLimitedThr…\n",
      "\t\"print\"\n",
      "\t\"enumerate\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"str\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"int\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    path: List[List[int]],\n",
      "    max_workers: int = 1,\n",
      "    task_id: str = \"task\",\n",
      "    calls_per_minute: int = 20,\n",
      "    backup: bool = True,\n",
      "    save_path: str = None,\n",
      "):\n",
      "    self.task_id = task_id\n",
      "    self.path = path\n",
      "    self.results = []\n",
      "    self.max_workers = max_workers\n",
      "    self.parallel = True if max_workers > 1 else False\n",
      "    self.rate_limiter = RateLimiter(calls_per_minute)\n",
      "    self.failed_sub_tasks = []\n",
      "    self.backup = backup\n",
      "    print(\"setting up savepath\")\n",
      "    self.save_path = save_path if save_path is not None else os.path.join(\"storage\", \"tasks\")\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"RateLimiter\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def _save_results_to_file(self) -> None:\n",
      "    os.makedirs(self.save_path, exist_ok=True)\n",
      "    with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"w\") as f:\n",
      "        json.dump(self.results, f)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "]\n",
      "\n",
      "\n",
      "def _load_results_from_file(self) -> None:\n",
      "    if os.path.exists(os.path.join(self.save_path, f\"{self.task_id}_results.json\")):\n",
      "        try:\n",
      "            with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"r\") as f:\n",
      "                self.results = json.load(f)\n",
      "                print(f\"Loaded {len(self.results)} results from file.\")\n",
      "        except Exception as e:\n",
      "            print(f\"Error loading results from file: {e}\")\n",
      "            print(\"Starting from scratch.\")\n",
      "    else:\n",
      "        print(\"No results file found, starting from scratch.\")\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "    sub_results = []\n",
      "    for i in sub_path:\n",
      "        response = \"Implement the response function in the subclass\"\n",
      "        sub_results.append(response)\n",
      "    return sub_results\n",
      "\n",
      "\n",
      "\n",
      "def execute_task(self) -> None:\n",
      "    if self.backup:\n",
      "        self._load_results_from_file()\n",
      "\n",
      "    with RateLimitedThreadPoolExecutor(\n",
      "        max_workers=self.max_workers,\n",
      "        calls_per_minute=self.rate_limiter.calls_per_minute,\n",
      "    ) as executor:\n",
      "        futures = []\n",
      "        print(f\"Executing task {self.task_id} using {self.max_workers} workers.\")\n",
      "\n",
      "        for i, sub_path in enumerate(self.path):\n",
      "            if i < len(self.results):\n",
      "                pass\n",
      "            else:\n",
      "                future = executor.submit(self._execute_sub_task, sub_path)\n",
      "                futures.append((i, future))\n",
      "\n",
      "        for i, future in futures:\n",
      "            try:\n",
      "                execution_start_time = time.time()\n",
      "                sub_task_result = future.result()\n",
      "                execution_end_time = time.time()\n",
      "                print(\n",
      "                    f\"Sub-task {i} executed in {execution_end_time - execution_start_time:.2f} seconds.\"\n",
      "                )\n",
      "\n",
      "                save_start_time = time.time()\n",
      "                self.results.append(sub_task_result)\n",
      "                # self.results.append(sub_task_result)\n",
      "                if self.backup:\n",
      "                    self._save_results_to_file()\n",
      "                save_end_time = time.time()\n",
      "                print(\n",
      "                    f\"Sub-task {i} results saved in {save_end_time - save_start_time:.2f} seconds.\"\n",
      "                )\n",
      "            except Exception as e:\n",
      "                print(f\"Error in sub-task {i}: {e}\")\n",
      "                # default_result = f\"Error in sub-task {i}: {e}\"\n",
      "                default_result =  {i:f\"Error in sub-task {i}: {e}\"} \n",
      "                self.results.append(default_result)\n",
      "                if self.backup:\n",
      "                    self._save_results_to_file()\n",
      "                self.failed_sub_tasks.append((self.path[i], str(e)))\n",
      "\n",
      "            except KeyboardInterrupt:\n",
      "                print(\"Keyboard interrupt detected, stopping task execution.\")\n",
      "                executor.shutdown(wait=False)\n",
      "                break\n",
      "\n",
      "    print(\"Task execution completed.\")\n",
      "\n",
      "Function calls: shape: (10,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"RateLimitedThr…\n",
      "\t\"print\"\n",
      "\t\"enumerate\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"str\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def work(self) -> List[Any]:\n",
      "    self.execute_task()\n",
      "    if not self.backup:\n",
      "        self._save_results_to_file()\n",
      "    work = []\n",
      "    for sub_result in self.results:\n",
      "        for index_id, response in sub_result.items():\n",
      "            work.append((index_id, response))\n",
      "    # sort the content to write by index_id\n",
      "    work.sort(key=lambda x: int(x[0]))\n",
      "    return work\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"int\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class LLMReader(BaseTask):\n",
      "    def __init__(\n",
      "        self,\n",
      "        index: MemoryIndex,\n",
      "        path: List[List[int]],\n",
      "        chatbot: Chat,\n",
      "        read_func=None,\n",
      "        max_workers: int = 1,\n",
      "        task_id: str = \"LLMReadTask\",\n",
      "        calls_per_minute: int = 20,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize a LLMReadTask instance.\n",
      "\n",
      "        :param index: List of strings representing the queries.\n",
      "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
      "        :param chatbot: Chatbot instance used for executing queries.\n",
      "        :param max_workers: Maximum number of worker threads (default is 4).\n",
      "        \"\"\"\n",
      "        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute)\n",
      "        self.index = index\n",
      "        self.chatbot = chatbot\n",
      "        self.read_func = read_func if read_func else self.llm_response\n",
      "\n",
      "    def llm_response(chatbot: Chat, message: str, string_out=False):\n",
      "        if string_out:\n",
      "            return chatbot.reply(message)\n",
      "        return chatbot.query(message)\n",
      "\n",
      "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "        \"\"\"\n",
      "        Execute a sub-task using a separate copy of the chatbot instance. each sub-stasks uses a\n",
      "        a clean memory instance.\n",
      "\n",
      "        :param sub_path: List of indices representing the sub-task's sequence.\n",
      "        :return: List of strings representing the responses for each query in the sub-task.\n",
      "        \"\"\"\n",
      "        if self.parallel:\n",
      "            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
      "            chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "        else:\n",
      "            chatbot_instance = self.chatbot\n",
      "        if isinstance(self.chatbot, BaseThread):\n",
      "            chatbot_instance.reset_memory()\n",
      "\n",
      "        sub_results = []\n",
      "        for i in sub_path:\n",
      "            response = self.read_func(chatbot_instance, self.index.values[i])\n",
      "            sub_results.append(response)\n",
      "        return sub_results\n",
      "\n",
      "    def read(self):\n",
      "        self.execute_task()\n",
      "        return self.results\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    index: MemoryIndex,\n",
      "    path: List[List[int]],\n",
      "    chatbot: Chat,\n",
      "    read_func=None,\n",
      "    max_workers: int = 1,\n",
      "    task_id: str = \"LLMReadTask\",\n",
      "    calls_per_minute: int = 20,\n",
      "):\n",
      "    \"\"\"\n",
      "        Initialize a LLMReadTask instance.\n",
      "\n",
      "        :param index: List of strings representing the queries.\n",
      "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
      "        :param chatbot: Chatbot instance used for executing queries.\n",
      "        :param max_workers: Maximum number of worker threads (default is 4).\n",
      "        \"\"\"\n",
      "    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute)\n",
      "    self.index = index\n",
      "    self.chatbot = chatbot\n",
      "    self.read_func = read_func if read_func else self.llm_response\n",
      "\n",
      "\n",
      "\n",
      "def llm_response(chatbot: Chat, message: str, string_out=False):\n",
      "    if string_out:\n",
      "        return chatbot.reply(message)\n",
      "    return chatbot.query(message)\n",
      "\n",
      "\n",
      "\n",
      "def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "    \"\"\"\n",
      "        Execute a sub-task using a separate copy of the chatbot instance. each sub-stasks uses a\n",
      "        a clean memory instance.\n",
      "\n",
      "        :param sub_path: List of indices representing the sub-task's sequence.\n",
      "        :return: List of strings representing the responses for each query in the sub-task.\n",
      "        \"\"\"\n",
      "    if self.parallel:\n",
      "        # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
      "        chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "    else:\n",
      "        chatbot_instance = self.chatbot\n",
      "    if isinstance(self.chatbot, BaseThread):\n",
      "        chatbot_instance.reset_memory()\n",
      "\n",
      "    sub_results = []\n",
      "    for i in sub_path:\n",
      "        response = self.read_func(chatbot_instance, self.index.values[i])\n",
      "        sub_results.append(response)\n",
      "    return sub_results\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def read(self):\n",
      "    self.execute_task()\n",
      "    return self.results\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class LLMWriter(BaseTask):\n",
      "    def __init__(\n",
      "        self,\n",
      "        index: MemoryIndex,\n",
      "        path: List[List[int]],\n",
      "        chatbot: Chat,\n",
      "        write_func=None,\n",
      "        context=None,\n",
      "        task_name=\"summary\",\n",
      "        max_workers: int = 1,\n",
      "        task_id: str = \"LLMWriteTask\",\n",
      "        calls_per_minute: int = 20,\n",
      "        backup: bool = True,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize a LLMWriteTask instance.\n",
      "\n",
      "        :param index: List of strings representing the queries.\n",
      "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
      "        :param chatbot: Chatbot instance used for executing queries.\n",
      "        :param max_workers: Maximum number of worker threads (default is 4).\n",
      "        \"\"\"\n",
      "        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\n",
      "        self.index = index\n",
      "        self.chatbot = chatbot\n",
      "        self.write_func = write_func if write_func else self.llm_response\n",
      "        self.new_index_name = self.index.name + f\"_{task_name}\"\n",
      "        self.context = context\n",
      "\n",
      "    def llm_response(self, chatbot: Chat, message: str, context=None, id=None):\n",
      "        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n",
      "        # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\n",
      "        #     return \"the message is too long to be processed\"\n",
      "        # moved the error catching to multi-threading but custom method could report the error here\n",
      "        return chatbot.reply(message)\n",
      "\n",
      "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "        \"\"\"\n",
      "        Execute a sub-task using a separate copy of the chatbot instance.\n",
      "\n",
      "        :param sub_path: List of indices representing the sub-task's sequence.\n",
      "        :return: List of strings representing the responses for each query in the sub-task.\n",
      "        \"\"\"\n",
      "        if self.parallel:\n",
      "            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
      "            chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "        else:\n",
      "            chatbot_instance = self.chatbot\n",
      "        if isinstance(self.chatbot, BaseThread):\n",
      "            chatbot_instance.reset_memory()\n",
      "\n",
      "        sub_results = {}\n",
      "        for i in sub_path:\n",
      "            current_val = self.index.values[i]\n",
      "            response = self.write_func(\n",
      "                chatbot_instance, current_val, self.context, id=i\n",
      "            )\n",
      "            sub_results[i] = response\n",
      "        return sub_results\n",
      "\n",
      "    def write(self):\n",
      "        content_to_write = self.work()\n",
      "        self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\n",
      "        self.new_index.save()\n",
      "        return self.new_index\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"MemoryIndex\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    index: MemoryIndex,\n",
      "    path: List[List[int]],\n",
      "    chatbot: Chat,\n",
      "    write_func=None,\n",
      "    context=None,\n",
      "    task_name=\"summary\",\n",
      "    max_workers: int = 1,\n",
      "    task_id: str = \"LLMWriteTask\",\n",
      "    calls_per_minute: int = 20,\n",
      "    backup: bool = True,\n",
      "):\n",
      "    \"\"\"\n",
      "        Initialize a LLMWriteTask instance.\n",
      "\n",
      "        :param index: List of strings representing the queries.\n",
      "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
      "        :param chatbot: Chatbot instance used for executing queries.\n",
      "        :param max_workers: Maximum number of worker threads (default is 4).\n",
      "        \"\"\"\n",
      "    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\n",
      "    self.index = index\n",
      "    self.chatbot = chatbot\n",
      "    self.write_func = write_func if write_func else self.llm_response\n",
      "    self.new_index_name = self.index.name + f\"_{task_name}\"\n",
      "    self.context = context\n",
      "\n",
      "\n",
      "\n",
      "def llm_response(self, chatbot: Chat, message: str, context=None, id=None):\n",
      "    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n",
      "    # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\n",
      "    #     return \"the message is too long to be processed\"\n",
      "    # moved the error catching to multi-threading but custom method could report the error here\n",
      "    return chatbot.reply(message)\n",
      "\n",
      "\n",
      "\n",
      "def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "    \"\"\"\n",
      "        Execute a sub-task using a separate copy of the chatbot instance.\n",
      "\n",
      "        :param sub_path: List of indices representing the sub-task's sequence.\n",
      "        :return: List of strings representing the responses for each query in the sub-task.\n",
      "        \"\"\"\n",
      "    if self.parallel:\n",
      "        # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
      "        chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "    else:\n",
      "        chatbot_instance = self.chatbot\n",
      "    if isinstance(self.chatbot, BaseThread):\n",
      "        chatbot_instance.reset_memory()\n",
      "\n",
      "    sub_results = {}\n",
      "    for i in sub_path:\n",
      "        current_val = self.index.values[i]\n",
      "        response = self.write_func(\n",
      "            chatbot_instance, current_val, self.context, id=i\n",
      "        )\n",
      "        sub_results[i] = response\n",
      "    return sub_results\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def write(self):\n",
      "    content_to_write = self.work()\n",
      "    self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\n",
      "    self.new_index.save()\n",
      "    return self.new_index\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"MemoryIndex\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class DiscreteDataInt(BDType):\n",
      "    alphabet: Optional[Set[int]] = Field(None, description=\"Set of allowed discrete variables. All elements should be integers.\")\n",
      "    value: int = Field(..., description=\"The discrete data value. It should be an integer.\")\n",
      "\n",
      "    @field_validator('alphabet')\n",
      "    def check_alphabet(cls, v):\n",
      "        if not all(isinstance(item, int) for item in v):\n",
      "            raise ValueError(\"All elements in 'alphabet' should be integers.\")\n",
      "        return v\n",
      "\n",
      "    @field_validator('value')\n",
      "    def check_value(cls, v, info: FieldValidationInfo):\n",
      "        alphabet = info.data.get('alphabet')\n",
      "        if alphabet is not None and v not in alphabet:\n",
      "            raise ValueError(\"Value must be in the alphabet.\")\n",
      "        return v\n",
      "\n",
      "Function calls: shape: (8,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"field_validato…\n",
      "\t\"all\"\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"field_validato…\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator('alphabet')\n",
      "def check_alphabet(cls, v):\n",
      "    if not all(isinstance(item, int) for item in v):\n",
      "        raise ValueError(\"All elements in 'alphabet' should be integers.\")\n",
      "    return v\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "\t\"all\"\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator('value')\n",
      "def check_value(cls, v, info: FieldValidationInfo):\n",
      "    alphabet = info.data.get('alphabet')\n",
      "    if alphabet is not None and v not in alphabet:\n",
      "        raise ValueError(\"Value must be in the alphabet.\")\n",
      "    return v\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class DiscreteDataStr(BDType):\n",
      "    alphabet: Optional[Set[str]] = Field(None, description=\"Set of allowed discrete variables. All elements should be strings.\")\n",
      "    value: str = Field(..., description=\"The discrete data value. It should be a string.\")\n",
      "\n",
      "    @field_validator('alphabet')\n",
      "    def check_alphabet(cls, v):\n",
      "        if not all(isinstance(item, str) for item in v):\n",
      "            raise ValueError(\"All elements in 'alphabet' should be strings.\")\n",
      "        return v\n",
      "\n",
      "    @field_validator('value')\n",
      "    def check_value(cls, v, info: FieldValidationInfo):\n",
      "        alphabet = info.data.get('alphabet')\n",
      "        if alphabet is not None and v not in alphabet:\n",
      "            raise ValueError(\"Value must be in the alphabet.\")\n",
      "        return v\n",
      "\n",
      "Function calls: shape: (8,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"field_validato…\n",
      "\t\"all\"\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"field_validato…\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator('alphabet')\n",
      "def check_alphabet(cls, v):\n",
      "    if not all(isinstance(item, str) for item in v):\n",
      "        raise ValueError(\"All elements in 'alphabet' should be strings.\")\n",
      "    return v\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "\t\"all\"\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator('value')\n",
      "def check_value(cls, v, info: FieldValidationInfo):\n",
      "    alphabet = info.data.get('alphabet')\n",
      "    if alphabet is not None and v not in alphabet:\n",
      "        raise ValueError(\"Value must be in the alphabet.\")\n",
      "    return v\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "# The DiscreteDataInt and DiscreteDataStr models are defined as before\n",
      "\n",
      "class DiscreteDataList(BaseModel):\n",
      "    alphabet: Optional[Set[Union[int, str]]] = Field(None, description=\"Set of allowed discrete variables. All elements should be of the same type (either integers or strings).\")\n",
      "    value: List[Union[DiscreteDataInt, DiscreteDataStr]] = Field(..., description=\"The list of discrete data values. All elements should be either DiscreteDataInt or DiscreteDataStr, not a mix.\")\n",
      "\n",
      "    @field_validator('value')\n",
      "    def check_alphabets(cls, value, info: FieldValidationInfo):\n",
      "        list_alphabet = info.data.get('alphabet')\n",
      "        if list_alphabet is not None:\n",
      "            for item in value:\n",
      "                item_alphabet = item.alphabet\n",
      "                if item_alphabet is not None and not set(item_alphabet).issubset(list_alphabet):\n",
      "                    raise ValueError(f\"Item alphabet {item_alphabet} is not a subset of the list alphabet {list_alphabet}.\")\n",
      "        return value\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"field_validato…\n",
      "\t\"set\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator('value')\n",
      "def check_alphabets(cls, value, info: FieldValidationInfo):\n",
      "    list_alphabet = info.data.get('alphabet')\n",
      "    if list_alphabet is not None:\n",
      "        for item in value:\n",
      "            item_alphabet = item.alphabet\n",
      "            if item_alphabet is not None and not set(item_alphabet).issubset(list_alphabet):\n",
      "                raise ValueError(f\"Item alphabet {item_alphabet} is not a subset of the list alphabet {list_alphabet}.\")\n",
      "    return value\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "\t\"set\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "class MultiDimensionalDiscrete(BDType):\n",
      "    value: List[Union[DiscreteDataInt, DiscreteDataStr]] = Field(..., description=\"The multidimensional discrete data value. It should be a list of either DiscreteDataInt or DiscreteDataStr.\")\n",
      "    type_dictionary: Dict[int, str] = Field(default_factory=dict, description=\"The helper dictionary containing the type of each dimension of the value list.\")\n",
      "    def __init__(self, **data):\n",
      "        super().__init__(**data)\n",
      "        self.type_dictionary = {i: item.__class__.__name__ for i, item in enumerate(self.value)}\n",
      "        \n",
      "    @field_validator('value')\n",
      "    def check_value(cls, value):\n",
      "        if len(value) < 2:\n",
      "            raise ValueError(\"For multidimensional discrete data, size of the list should be at least 2. For less than 2, use DiscreteDataInt or DiscreteDataStr.\")\n",
      "        return value\n",
      "    \n",
      "    from pydantic import BaseModel, Field, field_validator\n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"super\"\n",
      "\t\"enumerate\"\n",
      "\t\"field_validato…\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "def __init__(self, **data):\n",
      "    super().__init__(**data)\n",
      "    self.type_dictionary = {i: item.__class__.__name__ for i, item in enumerate(self.value)}\n",
      "    \n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"enumerate\"\n",
      "]\n",
      "\n",
      "@field_validator('value')\n",
      "def check_value(cls, value):\n",
      "    if len(value) < 2:\n",
      "        raise ValueError(\"For multidimensional discrete data, size of the list should be at least 2. For less than 2, use DiscreteDataInt or DiscreteDataStr.\")\n",
      "    return value\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "class MultiDimensionalDiscreteList(BDType):\n",
      "    values: List[MultiDimensionalDiscrete] = Field(..., description=\"The list of multidimensional discrete data values. All elements should be instances of MultiDimensionalDiscrete.\")\n",
      "    joint_alphabet: Optional[Set[Tuple[Any, ...]]] = Field(None, description=\"Set of tuples representing allowed discrete variable combinations. All elements should be tuples of the same length as the number of dimensions in each joint discrete variable.\")\n",
      "\n",
      "    @field_validator('values')\n",
      "    def check_type_dictionaries(cls, values):\n",
      "        first_type_dictionary = values[0].type_dictionary\n",
      "        for value in values[1:]:\n",
      "            if value.type_dictionary != first_type_dictionary:\n",
      "                raise ValueError(\"All elements in 'values' should have the same 'type_dictionary'.\")\n",
      "        return values\n",
      "\n",
      "    @field_validator('joint_alphabet')\n",
      "    def check_joint_alphabet(cls, v, info):\n",
      "        if v is not None and \"values\" in info.data:\n",
      "            expected_tuple_length = len(info.data[\"values\"][0].value)\n",
      "            for item in v:\n",
      "                if not isinstance(item, tuple) or len(item) != expected_tuple_length:\n",
      "                    raise ValueError(f\"Each element in 'joint_alphabet' should be a tuple of length {expected_tuple_length}.\")\n",
      "                for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\n",
      "                    if dim_alphabet is not None and dim_value not in dim_alphabet:\n",
      "                        raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\n",
      "        return v\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "Function calls: shape: (11,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"field_validato…\n",
      "\t\"ValueError\"\n",
      "\t\"field_validato…\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"zip\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator('values')\n",
      "def check_type_dictionaries(cls, values):\n",
      "    first_type_dictionary = values[0].type_dictionary\n",
      "    for value in values[1:]:\n",
      "        if value.type_dictionary != first_type_dictionary:\n",
      "            raise ValueError(\"All elements in 'values' should have the same 'type_dictionary'.\")\n",
      "    return values\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator('joint_alphabet')\n",
      "def check_joint_alphabet(cls, v, info):\n",
      "    if v is not None and \"values\" in info.data:\n",
      "        expected_tuple_length = len(info.data[\"values\"][0].value)\n",
      "        for item in v:\n",
      "            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\n",
      "                raise ValueError(f\"Each element in 'joint_alphabet' should be a tuple of length {expected_tuple_length}.\")\n",
      "            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\n",
      "                if dim_alphabet is not None and dim_value not in dim_alphabet:\n",
      "                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\n",
      "    return v\n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"zip\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "class RealData(BDType):\n",
      "    range: Optional[Tuple[Union[float, int], Union[float, int]]] = Field(None, description=\"An optional inclusive range (min, max) for the value.\")\n",
      "    value: Union[float, int] = Field(..., description=\"The real value data. It should be a float or an integer.\")\n",
      "\n",
      "    @field_validator(\"value\")\n",
      "    def validate_value(cls, v, values):\n",
      "        value_range = values.data[\"range\"]\n",
      "        if value_range is not None:\n",
      "            min_value, max_value = value_range\n",
      "            if not min_value <= v <= max_value:\n",
      "                raise ValueError(f\"Value {v} is not within the specified range {value_range}.\")\n",
      "        return v\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"field_validato…\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator(\"value\")\n",
      "def validate_value(cls, v, values):\n",
      "    value_range = values.data[\"range\"]\n",
      "    if value_range is not None:\n",
      "        min_value, max_value = value_range\n",
      "        if not min_value <= v <= max_value:\n",
      "            raise ValueError(f\"Value {v} is not within the specified range {value_range}.\")\n",
      "    return v\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "class RealDataList(BDType):\n",
      "    range: Optional[Tuple[Union[float, int], Union[float, int]]] = Field(None, description=\"An optional inclusive range (min, max) for the values.\")\n",
      "    values: List[RealData] = Field(..., description=\"The list of real value data. Each should be a RealeData object.\")\n",
      "\n",
      "    @field_validator(\"values\")\n",
      "    def validate_values(cls, values, values_dict):\n",
      "        list_range = values_dict.data.get(\"range\")\n",
      "        if list_range is not None:\n",
      "            min_value, max_value = list_range\n",
      "            for value in values:\n",
      "                if not min_value <= value.value <= max_value:\n",
      "                    raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {list_range}.\")\n",
      "        return values\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"field_validato…\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator(\"values\")\n",
      "def validate_values(cls, values, values_dict):\n",
      "    list_range = values_dict.data.get(\"range\")\n",
      "    if list_range is not None:\n",
      "        min_value, max_value = list_range\n",
      "        for value in values:\n",
      "            if not min_value <= value.value <= max_value:\n",
      "                raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {list_range}.\")\n",
      "    return values\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "class MultiDimensionalReal(BDType):\n",
      "    range: Optional[Union[Tuple[Union[float, int], Union[float, int]], List[Tuple[Union[float, int], Union[float, int]]]]] = Field(\n",
      "        None, \n",
      "        description=\"An optional inclusive range (min, max) for the values. If a tuple, applies to all dimensions. If a list, it must match the dimension length.\"\n",
      "    )\n",
      "    values: List[RealData] = Field(\n",
      "        ..., \n",
      "        description=\"The list of real data for each dimension. Each should be a RealData object.\"\n",
      "    )\n",
      "\n",
      "    @field_validator(\"values\")\n",
      "    def validate_values(cls, values, values_dict):\n",
      "        range_values = values_dict.data.get(\"range\")\n",
      "        if range_values is not None:\n",
      "            # If range is a tuple, apply it to all dimensions\n",
      "            if isinstance(range_values, tuple):\n",
      "                min_value, max_value = range_values\n",
      "                for value in values:\n",
      "                    if not min_value <= value.value <= max_value:\n",
      "                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n",
      "            # If range is a list, it must have the same length as values\n",
      "            elif isinstance(range_values, list):\n",
      "                if len(values) != len(range_values):\n",
      "                    raise ValueError(\"If range is a list, it must have the same length as values.\")\n",
      "                for value, (min_value, max_value) in zip(values, range_values):\n",
      "                    if not min_value <= value.value <= max_value:\n",
      "                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n",
      "        return values\n",
      "\n",
      "Function calls: shape: (11,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"field_validato…\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"zip\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator(\"values\")\n",
      "def validate_values(cls, values, values_dict):\n",
      "    range_values = values_dict.data.get(\"range\")\n",
      "    if range_values is not None:\n",
      "        # If range is a tuple, apply it to all dimensions\n",
      "        if isinstance(range_values, tuple):\n",
      "            min_value, max_value = range_values\n",
      "            for value in values:\n",
      "                if not min_value <= value.value <= max_value:\n",
      "                    raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n",
      "        # If range is a list, it must have the same length as values\n",
      "        elif isinstance(range_values, list):\n",
      "            if len(values) != len(range_values):\n",
      "                raise ValueError(\"If range is a list, it must have the same length as values.\")\n",
      "            for value, (min_value, max_value) in zip(values, range_values):\n",
      "                if not min_value <= value.value <= max_value:\n",
      "                    raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n",
      "    return values\n",
      "\n",
      "Function calls: shape: (9,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"zip\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "class MultiDimensionalRealList(BDType):\n",
      "    range: Optional[Union[Tuple[Union[float, int], Union[float, int]], List[Tuple[Union[float, int], Union[float, int]]]]] = Field(\n",
      "        None, \n",
      "        description=\"An optional inclusive range (min, max) for the values in all dimensions. If a tuple, applies to all dimensions. If a list, it must match the dimension length.\"\n",
      "    )\n",
      "    values: List[MultiDimensionalReal] = Field(\n",
      "        ..., \n",
      "        description=\"The list of multi-dimensional real data. Each should be a MultiDimensionalReal object.\"\n",
      "    )\n",
      "\n",
      "    @field_validator(\"values\")\n",
      "    def validate_values(cls, values, values_dict):\n",
      "        range_values = values_dict.data.get(\"range\")\n",
      "        dimension_length = len(values[0].values) if values else 0\n",
      "        if range_values is not None:\n",
      "            # If range is a tuple, apply it to all dimensions\n",
      "            if isinstance(range_values, tuple):\n",
      "                min_value, max_value = range_values\n",
      "                for multi_real in values:\n",
      "                    if len(multi_real.values) != dimension_length:\n",
      "                        raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n",
      "                    for value in multi_real.values:\n",
      "                        if not min_value <= value.value <= max_value:\n",
      "                            raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n",
      "            # If range is a list, it must have the same length as values in each dimension\n",
      "            elif isinstance(range_values, list):\n",
      "                if len(range_values) != dimension_length:\n",
      "                    raise ValueError(\"If range is a list, it must have the same length as values in each dimension.\")\n",
      "                for multi_real in values:\n",
      "                    if len(multi_real.values) != dimension_length:\n",
      "                        raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n",
      "                    for value, (min_value, max_value) in zip(multi_real.values, range_values):\n",
      "                        if not min_value <= value.value <= max_value:\n",
      "                            raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n",
      "        return values\n",
      "\n",
      "Function calls: shape: (15,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"field_validato…\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"zip\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator(\"values\")\n",
      "def validate_values(cls, values, values_dict):\n",
      "    range_values = values_dict.data.get(\"range\")\n",
      "    dimension_length = len(values[0].values) if values else 0\n",
      "    if range_values is not None:\n",
      "        # If range is a tuple, apply it to all dimensions\n",
      "        if isinstance(range_values, tuple):\n",
      "            min_value, max_value = range_values\n",
      "            for multi_real in values:\n",
      "                if len(multi_real.values) != dimension_length:\n",
      "                    raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n",
      "                for value in multi_real.values:\n",
      "                    if not min_value <= value.value <= max_value:\n",
      "                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n",
      "        # If range is a list, it must have the same length as values in each dimension\n",
      "        elif isinstance(range_values, list):\n",
      "            if len(range_values) != dimension_length:\n",
      "                raise ValueError(\"If range is a list, it must have the same length as values in each dimension.\")\n",
      "            for multi_real in values:\n",
      "                if len(multi_real.values) != dimension_length:\n",
      "                    raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n",
      "                for value, (min_value, max_value) in zip(multi_real.values, range_values):\n",
      "                    if not min_value <= value.value <= max_value:\n",
      "                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n",
      "    return values\n",
      "\n",
      "Function calls: shape: (13,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"zip\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class NaturalLanguageSingle(BDType):\n",
      "    text: str = Field(..., description=\"The natural language text. It should be less than or equal to `max_tokens` in length when tokenized.\")\n",
      "    max_tokens: int = Field(8000, description=\"The maximum allowed length of the text in tokens. The default value is 8000.\")\n",
      "    \n",
      "    @field_validator(\"text\")\n",
      "    def validate_text(cls, v, info):\n",
      "        try:\n",
      "            # Tokenize the text and get the token count\n",
      "            token_count = len(tokenizer.encode(v))\n",
      "        except Exception as e:\n",
      "            raise ValueError(\"Failed to tokenize text.\") from e\n",
      "\n",
      "        # Get max_tokens from info.data, if not available, default to 8000\n",
      "        max_tokens = info.data.get(\"max_tokens\", 8000)\n",
      "\n",
      "        if token_count > max_tokens:\n",
      "            raise ValueError(f\"Text is longer than {max_tokens} tokens.\")\n",
      "\n",
      "        return v\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"field_validato…\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator(\"text\")\n",
      "def validate_text(cls, v, info):\n",
      "    try:\n",
      "        # Tokenize the text and get the token count\n",
      "        token_count = len(tokenizer.encode(v))\n",
      "    except Exception as e:\n",
      "        raise ValueError(\"Failed to tokenize text.\") from e\n",
      "\n",
      "    # Get max_tokens from info.data, if not available, default to 8000\n",
      "    max_tokens = info.data.get(\"max_tokens\", 8000)\n",
      "\n",
      "    if token_count > max_tokens:\n",
      "        raise ValueError(f\"Text is longer than {max_tokens} tokens.\")\n",
      "\n",
      "    return v\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class NaturalLanguageList(BDType):\n",
      "    texts: List[NaturalLanguageSingle] = Field(..., description=\"A list of `NaturalLanguageSingle` objects. Each object should pass the validation requirements of the `NaturalLanguageSingle` class.\")\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Field\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "# Base class for all bd_types\n",
      "class BDType(BaseModel):\n",
      "    source: str = Field(\"babydragon\", description=\"The source of the data.\")\n",
      "    timestamp: Optional[datetime.datetime] = Field(None, description=\"When the data was collected or created. If not provided, the current time is used.\")\n",
      "    id: uuid.UUID = Field(default_factory=uuid.uuid4, description=\"Unique identifier of the data.\")\n",
      "    data_name: Optional[str] = Field(None, description=\"Name of the data.\")\n",
      "    elements_name: Optional[List[str]] = Field(None, description=\"Names of the elements if the data is a list.\")\n",
      "\n",
      "    @field_validator(\"timestamp\")\n",
      "    def set_timestamp(cls, v):\n",
      "        return v or datetime.datetime.now()\n",
      "\n",
      "    @field_validator(\"id\")\n",
      "    def set_id(cls, values, **kwargs):\n",
      "        if \"id\" not in values:\n",
      "            values[\"id\"] = uuid.uuid4()\n",
      "        return values\n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"Field\"\n",
      "\t\"field_validato…\n",
      "\t\"field_validato…\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator(\"timestamp\")\n",
      "def set_timestamp(cls, v):\n",
      "    return v or datetime.datetime.now()\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "]\n",
      "\n",
      "\n",
      "@field_validator(\"id\")\n",
      "def set_id(cls, values, **kwargs):\n",
      "    if \"id\" not in values:\n",
      "        values[\"id\"] = uuid.uuid4()\n",
      "    return values\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field_validato…\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class MemoryKernel(MemoryIndex):\n",
      "    def __init__(\n",
      "        self,\n",
      "        mem_index: MemoryIndex,\n",
      "        name: str = \"memory_kernel\",\n",
      "        k: int = 2,\n",
      "        save_path: str = None,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\n",
      "\n",
      "        Args:\n",
      "            mem_index (MemoryIndex): A MemoryIndex instance.\n",
      "            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\n",
      "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
      "            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\n",
      "        \"\"\"\n",
      "        super().__init__(\n",
      "            index=mem_index.index,\n",
      "            values=mem_index.values,\n",
      "            embeddings=mem_index.embeddings,\n",
      "            name=name,\n",
      "            save_path=save_path,\n",
      "        )\n",
      "        self.k = k\n",
      "        if len(self.values) > 0: \n",
      "            self.create_k_hop_index(k=k)\n",
      "        else:\n",
      "            raise ValueError(\"The input MemoryIndex is empty. Please check the input MemoryIndex.\")\n",
      "\n",
      "    def cos_sim(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
      "        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
      "        \"\"\"\n",
      "        if not isinstance(a, np.ndarray):\n",
      "            a = np.array(a)\n",
      "\n",
      "        if not isinstance(b, np.ndarray):\n",
      "            b = np.array(b)\n",
      "\n",
      "        if len(a.shape) == 1:\n",
      "            a = a[np.newaxis, :]\n",
      "\n",
      "        if len(b.shape) == 1:\n",
      "            b = b[np.newaxis, :]\n",
      "\n",
      "        a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\n",
      "        b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\n",
      "        return np.dot(a_norm, b_norm.T)\n",
      "\n",
      "    def compute_kernel(\n",
      "        self,\n",
      "        embedding_set: np.ndarray,\n",
      "        threshold: float = 0.65,\n",
      "        use_softmax: bool = False,\n",
      "    ) -> np.ndarray:\n",
      "\n",
      "        \"\"\"\n",
      "        Compute the adjacency matrix of the graph.\n",
      "\n",
      "        Parameters:\n",
      "        embedding_set (numpy array): The embedding matrix of the nodes.\n",
      "        threshold (float): The threshold for the adjacency matrix.\n",
      "        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\n",
      "        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\n",
      "\n",
      "        Returns:\n",
      "        adj_matrix (numpy array): The adjacency matrix of the graph.\n",
      "        \"\"\"\n",
      "\n",
      "        A = self.cos_sim(embedding_set, embedding_set)\n",
      "        if use_softmax:\n",
      "            # softmax\n",
      "            A = np.exp(A)\n",
      "            A = A / np.sum(A, axis=1)[:, np.newaxis]\n",
      "        adj_matrix = np.zeros_like(A)\n",
      "        adj_matrix[A > threshold] = 1\n",
      "        adj_matrix[A <= threshold] = 0\n",
      "        adj_matrix = adj_matrix.astype(np.float32)\n",
      "        return adj_matrix\n",
      "\n",
      "    def k_hop_message_passing(\n",
      "        self, A: np.ndarray, node_features: np.ndarray, k: int\n",
      "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
      "        \"\"\"\n",
      "        Compute the k-hop adjacency matrix and aggregated features using message passing.\n",
      "\n",
      "        Parameters:\n",
      "        A (numpy array): The adjacency matrix of the graph.\n",
      "        node_features (numpy array): The feature matrix of the nodes.\n",
      "        k (int): The number of hops for message passing.\n",
      "\n",
      "        Returns:\n",
      "        A_k (numpy array): The k-hop adjacency matrix.\n",
      "        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n",
      "        \"\"\"\n",
      "\n",
      "        print(\"Compute the k-hop adjacency matrix\")\n",
      "        A_k = np.linalg.matrix_power(A, k)\n",
      "\n",
      "        print(\"Aggregate the messages from the k-hop neighborhood:\")\n",
      "        agg_features = node_features.copy()\n",
      "\n",
      "        for i in tqdm(range(k)):\n",
      "            agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n",
      "\n",
      "        return A_k, agg_features\n",
      "\n",
      "    def graph_sylvester_embedding(self, G: Tuple, m: int, ts: np.ndarray) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\n",
      "\n",
      "        Args:\n",
      "            G (Tuple): A tuple containing the graph's vertices (V) and weights (W).\n",
      "            m (int): The number of singular values to consider.\n",
      "            ts (np.ndarray): The spectral scales.\n",
      "\n",
      "        Returns:\n",
      "            np.ndarray: The node_embeddings matrix.\n",
      "        \"\"\"\n",
      "        V, W = G\n",
      "        n = len(V)\n",
      "        D_BE = np.diag(W.sum(axis=1))\n",
      "        L_BE = np.identity(n) - np.dot(\n",
      "            np.diag(1 / np.sqrt(D_BE.diagonal())),\n",
      "            np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n",
      "        )\n",
      "\n",
      "        A = W\n",
      "        B = L_BE\n",
      "        C = np.identity(n)\n",
      "        X = solve_sylvester(A, B, C)\n",
      "\n",
      "        U, S, _ = svd(X, full_matrices=False)\n",
      "        U_m = U[:, :m]\n",
      "        S_m = S[:m]\n",
      "\n",
      "        node_embeddings = np.zeros((n, m))\n",
      "\n",
      "        for i in range(n):\n",
      "            for s in range(m):\n",
      "                # Spectral kernel descriptor\n",
      "                node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n",
      "\n",
      "        return node_embeddings\n",
      "\n",
      "    def gen_gse_embeddings(\n",
      "        self, A: np.ndarray, embeddings: np.ndarray, m: int = 7\n",
      "    ) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Generate Graph Sylvester Embeddings.\n",
      "\n",
      "        Args:\n",
      "            A (np.ndarray): The adjacency matrix of the graph.\n",
      "            embeddings (np.ndarray): The original node embeddings.\n",
      "            m (int, optional): The number of spectral scales. Defaults to 7.\n",
      "\n",
      "        Returns:\n",
      "            np.ndarray: The generated Graph Sylvester Embeddings.\n",
      "        \"\"\"\n",
      "        V = list(range(len(embeddings)))\n",
      "        W = A\n",
      "\n",
      "        G = (V, W)\n",
      "        ts = np.linspace(0, 1, m)  # equally spaced scales\n",
      "\n",
      "        gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\n",
      "        return gse_embeddings\n",
      "\n",
      "    def create_k_hop_index(self, k: int = 2):\n",
      "        \"\"\"\n",
      "        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\n",
      "        aggregated features, and updating the memory index.\n",
      "\n",
      "        Args:\n",
      "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
      "        \"\"\"\n",
      "        self.k = k\n",
      "        print(\"Computing the adjacency matrix\")\n",
      "        print(\"Embeddings shape: \", self.embeddings.shape)\n",
      "        self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n",
      "        print(\"Computing the k-hop adjacency matrix and aggregated features\")\n",
      "        self.A_k, self.node_embeddings = self.k_hop_message_passing(\n",
      "            self.A, self.embeddings, k\n",
      "        )\n",
      "        print(\"Updating the memory index\")\n",
      "        self.k_hop_index = MemoryIndex(name=self.name)\n",
      "        self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\n",
      "\n",
      "    @classmethod\n",
      "    def from_task_results(cls, task_memory_index):\n",
      "        new_memory_kernel = cls(mem_index=task_memory_index)\n",
      "\n",
      "        # Create a new index for the new MemoryKernel\n",
      "        new_memory_kernel.create_k_hop_index()\n",
      "\n",
      "        return new_memory_kernel\n",
      "\n",
      "Function calls: shape: (25,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"tqdm\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"solve_sylveste…\n",
      "\t\"svd\"\n",
      "\t\"range\"\n",
      "\t\"range\"\n",
      "\t\"list\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"MemoryIndex\"\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    mem_index: MemoryIndex,\n",
      "    name: str = \"memory_kernel\",\n",
      "    k: int = 2,\n",
      "    save_path: str = None,\n",
      "):\n",
      "    \"\"\"\n",
      "        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\n",
      "\n",
      "        Args:\n",
      "            mem_index (MemoryIndex): A MemoryIndex instance.\n",
      "            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\n",
      "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
      "            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\n",
      "        \"\"\"\n",
      "    super().__init__(\n",
      "        index=mem_index.index,\n",
      "        values=mem_index.values,\n",
      "        embeddings=mem_index.embeddings,\n",
      "        name=name,\n",
      "        save_path=save_path,\n",
      "    )\n",
      "    self.k = k\n",
      "    if len(self.values) > 0: \n",
      "        self.create_k_hop_index(k=k)\n",
      "    else:\n",
      "        raise ValueError(\"The input MemoryIndex is empty. Please check the input MemoryIndex.\")\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def cos_sim(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
      "    \"\"\"\n",
      "        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
      "        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
      "        \"\"\"\n",
      "    if not isinstance(a, np.ndarray):\n",
      "        a = np.array(a)\n",
      "\n",
      "    if not isinstance(b, np.ndarray):\n",
      "        b = np.array(b)\n",
      "\n",
      "    if len(a.shape) == 1:\n",
      "        a = a[np.newaxis, :]\n",
      "\n",
      "    if len(b.shape) == 1:\n",
      "        b = b[np.newaxis, :]\n",
      "\n",
      "    a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\n",
      "    b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\n",
      "    return np.dot(a_norm, b_norm.T)\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def compute_kernel(\n",
      "    self,\n",
      "    embedding_set: np.ndarray,\n",
      "    threshold: float = 0.65,\n",
      "    use_softmax: bool = False,\n",
      ") -> np.ndarray:\n",
      "\n",
      "    \"\"\"\n",
      "        Compute the adjacency matrix of the graph.\n",
      "\n",
      "        Parameters:\n",
      "        embedding_set (numpy array): The embedding matrix of the nodes.\n",
      "        threshold (float): The threshold for the adjacency matrix.\n",
      "        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\n",
      "        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\n",
      "\n",
      "        Returns:\n",
      "        adj_matrix (numpy array): The adjacency matrix of the graph.\n",
      "        \"\"\"\n",
      "\n",
      "    A = self.cos_sim(embedding_set, embedding_set)\n",
      "    if use_softmax:\n",
      "        # softmax\n",
      "        A = np.exp(A)\n",
      "        A = A / np.sum(A, axis=1)[:, np.newaxis]\n",
      "    adj_matrix = np.zeros_like(A)\n",
      "    adj_matrix[A > threshold] = 1\n",
      "    adj_matrix[A <= threshold] = 0\n",
      "    adj_matrix = adj_matrix.astype(np.float32)\n",
      "    return adj_matrix\n",
      "\n",
      "\n",
      "\n",
      "def k_hop_message_passing(\n",
      "    self, A: np.ndarray, node_features: np.ndarray, k: int\n",
      ") -> Tuple[np.ndarray, np.ndarray]:\n",
      "    \"\"\"\n",
      "        Compute the k-hop adjacency matrix and aggregated features using message passing.\n",
      "\n",
      "        Parameters:\n",
      "        A (numpy array): The adjacency matrix of the graph.\n",
      "        node_features (numpy array): The feature matrix of the nodes.\n",
      "        k (int): The number of hops for message passing.\n",
      "\n",
      "        Returns:\n",
      "        A_k (numpy array): The k-hop adjacency matrix.\n",
      "        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n",
      "        \"\"\"\n",
      "\n",
      "    print(\"Compute the k-hop adjacency matrix\")\n",
      "    A_k = np.linalg.matrix_power(A, k)\n",
      "\n",
      "    print(\"Aggregate the messages from the k-hop neighborhood:\")\n",
      "    agg_features = node_features.copy()\n",
      "\n",
      "    for i in tqdm(range(k)):\n",
      "        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n",
      "\n",
      "    return A_k, agg_features\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"tqdm\"\n",
      "\t\"range\"\n",
      "]\n",
      "\n",
      "\n",
      "def graph_sylvester_embedding(self, G: Tuple, m: int, ts: np.ndarray) -> np.ndarray:\n",
      "    \"\"\"\n",
      "        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\n",
      "\n",
      "        Args:\n",
      "            G (Tuple): A tuple containing the graph's vertices (V) and weights (W).\n",
      "            m (int): The number of singular values to consider.\n",
      "            ts (np.ndarray): The spectral scales.\n",
      "\n",
      "        Returns:\n",
      "            np.ndarray: The node_embeddings matrix.\n",
      "        \"\"\"\n",
      "    V, W = G\n",
      "    n = len(V)\n",
      "    D_BE = np.diag(W.sum(axis=1))\n",
      "    L_BE = np.identity(n) - np.dot(\n",
      "        np.diag(1 / np.sqrt(D_BE.diagonal())),\n",
      "        np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n",
      "    )\n",
      "\n",
      "    A = W\n",
      "    B = L_BE\n",
      "    C = np.identity(n)\n",
      "    X = solve_sylvester(A, B, C)\n",
      "\n",
      "    U, S, _ = svd(X, full_matrices=False)\n",
      "    U_m = U[:, :m]\n",
      "    S_m = S[:m]\n",
      "\n",
      "    node_embeddings = np.zeros((n, m))\n",
      "\n",
      "    for i in range(n):\n",
      "        for s in range(m):\n",
      "            # Spectral kernel descriptor\n",
      "            node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n",
      "\n",
      "    return node_embeddings\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"solve_sylveste…\n",
      "\t\"svd\"\n",
      "\t\"range\"\n",
      "\t\"range\"\n",
      "]\n",
      "\n",
      "\n",
      "def gen_gse_embeddings(\n",
      "    self, A: np.ndarray, embeddings: np.ndarray, m: int = 7\n",
      ") -> np.ndarray:\n",
      "    \"\"\"\n",
      "        Generate Graph Sylvester Embeddings.\n",
      "\n",
      "        Args:\n",
      "            A (np.ndarray): The adjacency matrix of the graph.\n",
      "            embeddings (np.ndarray): The original node embeddings.\n",
      "            m (int, optional): The number of spectral scales. Defaults to 7.\n",
      "\n",
      "        Returns:\n",
      "            np.ndarray: The generated Graph Sylvester Embeddings.\n",
      "        \"\"\"\n",
      "    V = list(range(len(embeddings)))\n",
      "    W = A\n",
      "\n",
      "    G = (V, W)\n",
      "    ts = np.linspace(0, 1, m)  # equally spaced scales\n",
      "\n",
      "    gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\n",
      "    return gse_embeddings\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"list\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def create_k_hop_index(self, k: int = 2):\n",
      "    \"\"\"\n",
      "        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\n",
      "        aggregated features, and updating the memory index.\n",
      "\n",
      "        Args:\n",
      "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
      "        \"\"\"\n",
      "    self.k = k\n",
      "    print(\"Computing the adjacency matrix\")\n",
      "    print(\"Embeddings shape: \", self.embeddings.shape)\n",
      "    self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n",
      "    print(\"Computing the k-hop adjacency matrix and aggregated features\")\n",
      "    self.A_k, self.node_embeddings = self.k_hop_message_passing(\n",
      "        self.A, self.embeddings, k\n",
      "    )\n",
      "    print(\"Updating the memory index\")\n",
      "    self.k_hop_index = MemoryIndex(name=self.name)\n",
      "    self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"MemoryIndex\"\n",
      "]\n",
      "\n",
      "\n",
      "@classmethod\n",
      "def from_task_results(cls, task_memory_index):\n",
      "    new_memory_kernel = cls(mem_index=task_memory_index)\n",
      "\n",
      "    # Create a new index for the new MemoryKernel\n",
      "    new_memory_kernel.create_k_hop_index()\n",
      "\n",
      "    return new_memory_kernel\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def calc_shgo_mode(scores: List[float]) -> float:\n",
      "    def objective(x):\n",
      "        return -estimate_pdf(scores)(x)\n",
      "\n",
      "    bounds = [(min(scores), max(scores))]\n",
      "    result = scipy.optimize.shgo(objective, bounds)\n",
      "    return result.x\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"estimate_pdf\"\n",
      "\t\"min\"\n",
      "\t\"max\"\n",
      "]\n",
      "\n",
      "def objective(x):\n",
      "    return -estimate_pdf(scores)(x)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"estimate_pdf\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def estimate_pdf(scores: List[float]) -> callable:\n",
      "    pdf = scipy.stats.gaussian_kde(scores)\n",
      "    return pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def sort_paths_by_mode_distance(\n",
      "    paths, memory_kernel, distance_metric: str = \"cosine\"\n",
      ") -> List[List[int]]:\n",
      "    sorted_paths = []\n",
      "    for i, path in enumerate(paths):\n",
      "        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
      "        cluster_embeddings = np.array(cluster_embeddings)\n",
      "        cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
      "        if distance_metric == \"cosine\" or distance_metric == \"guassian\":\n",
      "            scores = [\n",
      "                (i, cosine(cluster_mean, emb))\n",
      "                for i, emb in zip(path, cluster_embeddings)\n",
      "            ]\n",
      "        elif distance_metric == \"euclidean\":\n",
      "            scores = [\n",
      "                (i, np.linalg.norm(cluster_mean - emb))\n",
      "                for i, emb in zip(path, cluster_embeddings)\n",
      "            ]\n",
      "        score_values = [score for _, score in scores]  # Extract score values\n",
      "        mu = calc_shgo_mode(score_values)\n",
      "        sigma = np.std(score_values)\n",
      "        if distance_metric == \"guassian\":\n",
      "            scores = [\n",
      "                (i, np.exp(-((x - mu) ** 2) / (2 * sigma**2))) for i, x in scores\n",
      "            ]\n",
      "        # Sort path by score\n",
      "        sorted_path_and_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
      "        sorted_path = [x[0] for x in sorted_path_and_scores]\n",
      "        sorted_paths.append(sorted_path)\n",
      "    return sorted_paths\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"enumerate\"\n",
      "\t\"cosine\"\n",
      "\t\"zip\"\n",
      "\t\"zip\"\n",
      "\t\"calc_shgo_mode…\n",
      "\t\"sorted\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def sort_paths_by_kernel_density(\n",
      "    paths, memory_kernel, distance_metric: str = \"cosine\"\n",
      ") -> List[List[int]]:\n",
      "    sorted_paths = []\n",
      "    for i, path in enumerate(paths):\n",
      "        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
      "        cluster_embeddings = np.array(cluster_embeddings)\n",
      "        cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
      "        if distance_metric == \"cosine\":\n",
      "            scores = [\n",
      "                (i, cosine(cluster_mean, emb))\n",
      "                for i, emb in zip(path, cluster_embeddings)\n",
      "            ]\n",
      "        elif distance_metric == \"euclidean\":\n",
      "            scores = [\n",
      "                (i, np.linalg.norm(cluster_mean - emb))\n",
      "                for i, emb in zip(path, cluster_embeddings)\n",
      "            ]\n",
      "        score_values = [score for _, score in scores]  # Extract score values\n",
      "\n",
      "        # Estimate PDF using Kernel Density Estimation\n",
      "        kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(\n",
      "            np.array(score_values).reshape(-1, 1)\n",
      "        )\n",
      "        kde_scores = [kde.score_samples([[x]])[0] for _, x in scores]\n",
      "\n",
      "        # Sort path by score\n",
      "        sorted_path_and_scores = sorted(\n",
      "            zip(path, kde_scores), key=lambda x: x[1], reverse=True\n",
      "        )\n",
      "        sorted_path = [x[0] for x in sorted_path_and_scores]\n",
      "        sorted_paths.append(sorted_path)\n",
      "    return sorted_paths\n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"enumerate\"\n",
      "\t\"cosine\"\n",
      "\t\"zip\"\n",
      "\t\"zip\"\n",
      "\t\"KernelDensity\"\n",
      "\t\"sorted\"\n",
      "\t\"zip\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class ClusterPaths:\n",
      "    def create_paths(\n",
      "        self, embeddings: np.ndarray, num_clusters: int\n",
      "    ) -> List[List[int]]:\n",
      "        raise NotImplementedError\n",
      "\n",
      "\n",
      "def create_paths(\n",
      "    self, embeddings: np.ndarray, num_clusters: int\n",
      ") -> List[List[int]]:\n",
      "    raise NotImplementedError\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class HDBSCANPaths(ClusterPaths):\n",
      "    def create_paths(\n",
      "        self, embeddings: np.ndarray, num_clusters: int\n",
      "    ) -> List[List[int]]:\n",
      "        clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n",
      "        cluster_assignments = clusterer.fit_predict(embeddings)\n",
      "        paths = [[] for _ in range(num_clusters)]\n",
      "        for i, cluster in enumerate(cluster_assignments):\n",
      "            paths[cluster].append(i)\n",
      "        paths = [path for path in paths if path]\n",
      "        return paths\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"range\"\n",
      "\t\"enumerate\"\n",
      "]\n",
      "\n",
      "def create_paths(\n",
      "    self, embeddings: np.ndarray, num_clusters: int\n",
      ") -> List[List[int]]:\n",
      "    clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n",
      "    cluster_assignments = clusterer.fit_predict(embeddings)\n",
      "    paths = [[] for _ in range(num_clusters)]\n",
      "    for i, cluster in enumerate(cluster_assignments):\n",
      "        paths[cluster].append(i)\n",
      "    paths = [path for path in paths if path]\n",
      "    return paths\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"range\"\n",
      "\t\"enumerate\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class SpectralClusteringPaths(ClusterPaths):\n",
      "    def create_paths(\n",
      "        self, A: np.ndarray, num_clusters: int\n",
      "    ) -> List[List[int]]:\n",
      "        n_samples = A.shape[0]\n",
      "        n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\n",
      "        spectral_clustering = SpectralClustering(\n",
      "            n_clusters=num_clusters,\n",
      "            affinity=\"precomputed\",\n",
      "            n_neighbors=n_neighbors,\n",
      "            random_state=42,\n",
      "        )\n",
      "        cluster_assignments = spectral_clustering.fit_predict(A)\n",
      "        paths = [[] for _ in range(num_clusters)]\n",
      "        for i, cluster in enumerate(cluster_assignments):\n",
      "            paths[cluster].append(i)\n",
      "        paths = [path for path in paths if path]\n",
      "        return paths\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"min\"\n",
      "\t\"SpectralCluste…\n",
      "\t\"range\"\n",
      "\t\"enumerate\"\n",
      "]\n",
      "\n",
      "def create_paths(\n",
      "    self, A: np.ndarray, num_clusters: int\n",
      ") -> List[List[int]]:\n",
      "    n_samples = A.shape[0]\n",
      "    n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\n",
      "    spectral_clustering = SpectralClustering(\n",
      "        n_clusters=num_clusters,\n",
      "        affinity=\"precomputed\",\n",
      "        n_neighbors=n_neighbors,\n",
      "        random_state=42,\n",
      "    )\n",
      "    cluster_assignments = spectral_clustering.fit_predict(A)\n",
      "    paths = [[] for _ in range(num_clusters)]\n",
      "    for i, cluster in enumerate(cluster_assignments):\n",
      "        paths[cluster].append(i)\n",
      "    paths = [path for path in paths if path]\n",
      "    return paths\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"min\"\n",
      "\t\"SpectralCluste…\n",
      "\t\"range\"\n",
      "\t\"enumerate\"\n",
      "]\n",
      "\n",
      "\n",
      "class MultiKernelVisualization:\n",
      "    def __init__(self, memory_kernel_group: MultiKernel):\n",
      "        self.memory_kernel_group = memory_kernel_group\n",
      "        self.memory_kernel_dict = memory_kernel_group.memory_kernel_dict\n",
      "        self.memory_kernel_group.generate_path_groups()\n",
      "\n",
      "    def plot_embeddings_with_path(self, embeddings, title, paths):\n",
      "        tsne = TSNE(n_components=2, random_state=42)\n",
      "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
      "\n",
      "        plt.figure(figsize=(10, 8))\n",
      "        colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n",
      "        for i, path in enumerate(paths):\n",
      "            path_embeddings = reduced_embeddings[path]\n",
      "            plt.scatter(\n",
      "                path_embeddings[:, 0],\n",
      "                path_embeddings[:, 1],\n",
      "                color=colors[i],\n",
      "                label=f\"Cluster {i}\",\n",
      "            )\n",
      "            for j in range(len(path) - 1):\n",
      "                plt.plot(\n",
      "                    [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n",
      "                    [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n",
      "                    color=colors[i],\n",
      "                )\n",
      "        plt.title(title)\n",
      "        plt.legend()\n",
      "        plt.show()\n",
      "\n",
      "    def visualize_paths(self):\n",
      "        #loop through memory kernels and print path_group\n",
      "        for key, kernel in self.memory_kernel_dict.items():\n",
      "            print(f\"Kernel: {key}\")\n",
      "            paths = self.memory_kernel_group.path_group[key]\n",
      "            print(f\"Path Group: {paths}\")\n",
      "            node_embeddings = kernel.node_embeddings\n",
      "            self.plot_embeddings_with_path(\n",
      "                node_embeddings, f\"Node Embeddings for {key}\", paths\n",
      "            )\n",
      "    def plot_singular_values(self):\n",
      "        #loop through memory kernels and print path_group\n",
      "        for key, kernel in self.memory_kernel_dict.items():\n",
      "            print(f\"Kernel: {key}\")\n",
      "            A_k = kernel.A_k\n",
      "            U, S, V = np.linalg.svd(A_k)\n",
      "            plt.plot(np.log(S))\n",
      "            plt.show()\n",
      "\n",
      "Function calls: shape: (8,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"TSNE\"\n",
      "\t\"len\"\n",
      "\t\"enumerate\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "def __init__(self, memory_kernel_group: MultiKernel):\n",
      "    self.memory_kernel_group = memory_kernel_group\n",
      "    self.memory_kernel_dict = memory_kernel_group.memory_kernel_dict\n",
      "    self.memory_kernel_group.generate_path_groups()\n",
      "\n",
      "\n",
      "\n",
      "def plot_embeddings_with_path(self, embeddings, title, paths):\n",
      "    tsne = TSNE(n_components=2, random_state=42)\n",
      "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
      "\n",
      "    plt.figure(figsize=(10, 8))\n",
      "    colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n",
      "    for i, path in enumerate(paths):\n",
      "        path_embeddings = reduced_embeddings[path]\n",
      "        plt.scatter(\n",
      "            path_embeddings[:, 0],\n",
      "            path_embeddings[:, 1],\n",
      "            color=colors[i],\n",
      "            label=f\"Cluster {i}\",\n",
      "        )\n",
      "        for j in range(len(path) - 1):\n",
      "            plt.plot(\n",
      "                [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n",
      "                [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n",
      "                color=colors[i],\n",
      "            )\n",
      "    plt.title(title)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"TSNE\"\n",
      "\t\"len\"\n",
      "\t\"enumerate\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def visualize_paths(self):\n",
      "    #loop through memory kernels and print path_group\n",
      "    for key, kernel in self.memory_kernel_dict.items():\n",
      "        print(f\"Kernel: {key}\")\n",
      "        paths = self.memory_kernel_group.path_group[key]\n",
      "        print(f\"Path Group: {paths}\")\n",
      "        node_embeddings = kernel.node_embeddings\n",
      "        self.plot_embeddings_with_path(\n",
      "            node_embeddings, f\"Node Embeddings for {key}\", paths\n",
      "        )\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "def plot_singular_values(self):\n",
      "    #loop through memory kernels and print path_group\n",
      "    for key, kernel in self.memory_kernel_dict.items():\n",
      "        print(f\"Kernel: {key}\")\n",
      "        A_k = kernel.A_k\n",
      "        U, S, V = np.linalg.svd(A_k)\n",
      "        plt.plot(np.log(S))\n",
      "        plt.show()\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "class MultiKernelStabilityAnalysis:\n",
      "    def __init__(self, memory_kernel_group: MultiKernel):\n",
      "        self.memory_kernel_group = memory_kernel_group\n",
      "\n",
      "    def get_cluster_labels(self, kernel_label: str) -> Tuple[np.ndarray, int]:\n",
      "        paths = self.memory_kernel_group.path_group[kernel_label]\n",
      "        num_clusters = len(paths)\n",
      "        cluster_labels = np.empty(len(self.memory_kernel_group.memory_kernel_dict[kernel_label].node_embeddings), dtype=int)\n",
      "\n",
      "        for cluster_index, path in enumerate(paths):\n",
      "            cluster_labels[path] = cluster_index\n",
      "\n",
      "        return cluster_labels, num_clusters\n",
      "\n",
      "    def compute_nmi(self, kernel_label1: str, kernel_label2: str) -> float:\n",
      "        cluster_labels1, _ = self.get_cluster_labels(kernel_label1)\n",
      "        cluster_labels2, _ = self.get_cluster_labels(kernel_label2)\n",
      "        nmi = normalized_mutual_info_score(cluster_labels1, cluster_labels2)\n",
      "        return nmi\n",
      "\n",
      "    def evaluate_stability(self) -> float:\n",
      "        kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\n",
      "        pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\n",
      "        nmi_sum = 0\n",
      "\n",
      "        for kernel_label1, kernel_label2 in pairwise_combinations:\n",
      "            nmi = self.compute_nmi(kernel_label1, kernel_label2)\n",
      "            nmi_sum += nmi\n",
      "\n",
      "        stability_score = nmi_sum / len(pairwise_combinations)\n",
      "        return stability_score\n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"enumerate\"\n",
      "\t\"normalized_mut…\n",
      "\t\"list\"\n",
      "\t\"list\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "def __init__(self, memory_kernel_group: MultiKernel):\n",
      "    self.memory_kernel_group = memory_kernel_group\n",
      "\n",
      "\n",
      "\n",
      "def get_cluster_labels(self, kernel_label: str) -> Tuple[np.ndarray, int]:\n",
      "    paths = self.memory_kernel_group.path_group[kernel_label]\n",
      "    num_clusters = len(paths)\n",
      "    cluster_labels = np.empty(len(self.memory_kernel_group.memory_kernel_dict[kernel_label].node_embeddings), dtype=int)\n",
      "\n",
      "    for cluster_index, path in enumerate(paths):\n",
      "        cluster_labels[path] = cluster_index\n",
      "\n",
      "    return cluster_labels, num_clusters\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"enumerate\"\n",
      "]\n",
      "\n",
      "\n",
      "def compute_nmi(self, kernel_label1: str, kernel_label2: str) -> float:\n",
      "    cluster_labels1, _ = self.get_cluster_labels(kernel_label1)\n",
      "    cluster_labels2, _ = self.get_cluster_labels(kernel_label2)\n",
      "    nmi = normalized_mutual_info_score(cluster_labels1, cluster_labels2)\n",
      "    return nmi\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"normalized_mut…\n",
      "]\n",
      "\n",
      "\n",
      "def evaluate_stability(self) -> float:\n",
      "    kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\n",
      "    pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\n",
      "    nmi_sum = 0\n",
      "\n",
      "    for kernel_label1, kernel_label2 in pairwise_combinations:\n",
      "        nmi = self.compute_nmi(kernel_label1, kernel_label2)\n",
      "        nmi_sum += nmi\n",
      "\n",
      "    stability_score = nmi_sum / len(pairwise_combinations)\n",
      "    return stability_score\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"list\"\n",
      "\t\"list\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class MultiKernel(MemoryKernel):\n",
      "    def __init__(\n",
      "        self,\n",
      "        memory_kernel_dict: Dict[str, MemoryKernel],\n",
      "        name: str = \"memory_kernel_group\",\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize the MultiKernel with a dictionary of MemoryKernel instances.\n",
      "\n",
      "        Args:\n",
      "            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\n",
      "            name (str, optional): The name of the MultiKernel. Defaults to \"memory_kernel_group\".\n",
      "        \"\"\"\n",
      "        self.memory_kernel_dict = memory_kernel_dict\n",
      "        self.path_group = {}\n",
      "        self.name = name\n",
      "\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    memory_kernel_dict: Dict[str, MemoryKernel],\n",
      "    name: str = \"memory_kernel_group\",\n",
      "):\n",
      "    \"\"\"\n",
      "        Initialize the MultiKernel with a dictionary of MemoryKernel instances.\n",
      "\n",
      "        Args:\n",
      "            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\n",
      "            name (str, optional): The name of the MultiKernel. Defaults to \"memory_kernel_group\".\n",
      "        \"\"\"\n",
      "    self.memory_kernel_dict = memory_kernel_dict\n",
      "    self.path_group = {}\n",
      "    self.name = name\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class HDBSCANMultiKernel(MultiKernel):\n",
      "    def __init__(\n",
      "        self,\n",
      "        memory_kernel_dict: Dict[str, MemoryKernel],\n",
      "        name: str = \"memory_kernel_group\",\n",
      "    ):\n",
      "        super().__init__(memory_kernel_dict, name)\n",
      "        self.cluster_paths = HDBSCANPaths()\n",
      "\n",
      "    def generate_path_groups(self, num_clusters: int = None) -> None:\n",
      "        path_group = {}\n",
      "        for k, v in self.memory_kernel_dict.items():\n",
      "            embeddings = v.node_embeddings\n",
      "            if num_clusters is None:\n",
      "                num_clusters = int(np.sqrt(len(embeddings)))\n",
      "            paths = self.cluster_paths.create_paths(embeddings, num_clusters)\n",
      "            path_group[k] = paths\n",
      "        self.path_group = path_group\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"HDBSCANPaths\"\n",
      "\t\"int\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    memory_kernel_dict: Dict[str, MemoryKernel],\n",
      "    name: str = \"memory_kernel_group\",\n",
      "):\n",
      "    super().__init__(memory_kernel_dict, name)\n",
      "    self.cluster_paths = HDBSCANPaths()\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"HDBSCANPaths\"\n",
      "]\n",
      "\n",
      "\n",
      "def generate_path_groups(self, num_clusters: int = None) -> None:\n",
      "    path_group = {}\n",
      "    for k, v in self.memory_kernel_dict.items():\n",
      "        embeddings = v.node_embeddings\n",
      "        if num_clusters is None:\n",
      "            num_clusters = int(np.sqrt(len(embeddings)))\n",
      "        paths = self.cluster_paths.create_paths(embeddings, num_clusters)\n",
      "        path_group[k] = paths\n",
      "    self.path_group = path_group\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"int\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class SpectralClusteringMultiKernel(MultiKernel):\n",
      "    def __init__(\n",
      "        self,\n",
      "        memory_kernel_dict: Dict[str, MemoryKernel],\n",
      "        name: str = \"memory_kernel_group\",\n",
      "    ):\n",
      "        super().__init__(memory_kernel_dict, name)\n",
      "        self.cluster_paths = SpectralClusteringPaths()\n",
      "\n",
      "    def generate_path_groups(self, num_clusters: int = None) -> None:\n",
      "        path_group = {}\n",
      "        for k, v in self.memory_kernel_dict.items():\n",
      "            A_k = v.A_k\n",
      "            if num_clusters is None:\n",
      "                num_clusters = int(np.sqrt(len(A_k)))\n",
      "            paths = self.cluster_paths.create_paths(A_k, num_clusters)\n",
      "            path_group[k] = paths\n",
      "        self.path_group = path_group\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"SpectralCluste…\n",
      "\t\"int\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    memory_kernel_dict: Dict[str, MemoryKernel],\n",
      "    name: str = \"memory_kernel_group\",\n",
      "):\n",
      "    super().__init__(memory_kernel_dict, name)\n",
      "    self.cluster_paths = SpectralClusteringPaths()\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"SpectralCluste…\n",
      "]\n",
      "\n",
      "\n",
      "def generate_path_groups(self, num_clusters: int = None) -> None:\n",
      "    path_group = {}\n",
      "    for k, v in self.memory_kernel_dict.items():\n",
      "        A_k = v.A_k\n",
      "        if num_clusters is None:\n",
      "            num_clusters = int(np.sqrt(len(A_k)))\n",
      "        paths = self.cluster_paths.create_paths(A_k, num_clusters)\n",
      "        path_group[k] = paths\n",
      "    self.path_group = path_group\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"int\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "class BaseFrame(ABC):\n",
      "    def __init__(self,\n",
      "                context_columns: List = [],\n",
      "                embeddable_columns: List = [],\n",
      "                embedding_columns: List = [],\n",
      "                name: str = \"base_frame\",\n",
      "                save_path: Optional[str] = \"/storage\",\n",
      "                text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "                markdown: str = \"text/markdown\",):\n",
      "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "        self.meta_columns = ['ID', 'Name', 'Source', 'Author', 'Created At', 'Last Modified At']\n",
      "        self.context_columns = context_columns\n",
      "        self.embeddable_columns = embeddable_columns\n",
      "        self.embedding_columns = embedding_columns\n",
      "        self.name = name\n",
      "        self.save_path = save_path\n",
      "        self.save_dir = f'{self.save_path}/{self.name}'\n",
      "        self.text_embedder = text_embedder\n",
      "        self.markdown = markdown\n",
      "\n",
      "    @abstractmethod\n",
      "    def __getattr__(self, name: str):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def get_overwritten_attr(self):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def embed_columns(self, embeddable_columns: List):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def _embed_column(self, column, embedder):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def search_column_polar(self, query, embeddable_column_name, top_k):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def save(self):\n",
      "        pass\n",
      "\n",
      "    @classmethod\n",
      "    @abstractmethod\n",
      "    def load(cls, frame_path, name):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def generate_column(self, row_generator, new_column_name):\n",
      "        pass\n",
      "\n",
      "\n",
      "def __init__(self,\n",
      "            context_columns: List = [],\n",
      "            embeddable_columns: List = [],\n",
      "            embedding_columns: List = [],\n",
      "            name: str = \"base_frame\",\n",
      "            save_path: Optional[str] = \"/storage\",\n",
      "            text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "            markdown: str = \"text/markdown\",):\n",
      "    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "    self.meta_columns = ['ID', 'Name', 'Source', 'Author', 'Created At', 'Last Modified At']\n",
      "    self.context_columns = context_columns\n",
      "    self.embeddable_columns = embeddable_columns\n",
      "    self.embedding_columns = embedding_columns\n",
      "    self.name = name\n",
      "    self.save_path = save_path\n",
      "    self.save_dir = f'{self.save_path}/{self.name}'\n",
      "    self.text_embedder = text_embedder\n",
      "    self.markdown = markdown\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def __getattr__(self, name: str):\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def get_overwritten_attr(self):\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def embed_columns(self, embeddable_columns: List):\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def _embed_column(self, column, embedder):\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def search_column_polar(self, query, embeddable_column_name, top_k):\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def save(self):\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@classmethod\n",
      "@abstractmethod\n",
      "def load(cls, frame_path, name):\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def generate_column(self, row_generator, new_column_name):\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class CodeFramePydantic(BaseModel):\n",
      "    df_path: str\n",
      "    context_columns: List\n",
      "    embeddable_columns: List\n",
      "    embedding_columns: List\n",
      "    name: str\n",
      "    save_path: Optional[str]\n",
      "    save_dir: str\n",
      "    text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder\n",
      "    markdown: str\n",
      "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"ConfigDict\"\n",
      "]\n",
      "\n",
      "\n",
      "class CodeFrame(BaseFrame):\n",
      "    def __init__(self, df: pl.DataFrame, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.df = df\n",
      "        self.frame_template = CodeFramePydantic(df_path=f'{self.save_dir}/{self.name}.parquet', context_columns=self.context_columns, embeddable_columns=self.embeddable_columns, embedding_columns=self.embedding_columns, name=self.name, save_path=self.save_path, save_dir=self.save_dir, load=True, text_embedder=self.text_embedder, markdown=self.markdown)\n",
      "\n",
      "    def __getattr__(self, name: str):\n",
      "        if \"df\" in self.__dict__:\n",
      "            return getattr(self.df.lazy(), name)\n",
      "        raise AttributeError(f\"{self.__class__.__name__} object has no attribute {name}\")\n",
      "\n",
      "    def get_overwritten_attr(self):\n",
      "        df_methods = [method for method in dir(self.df) if callable(getattr(self.df, method))]\n",
      "        memory_frame_methods = [method for method in dir(CodeFrame) if callable(getattr(CodeFrame, method))]\n",
      "        common_methods = list(set(df_methods) & set(memory_frame_methods))\n",
      "        return common_methods\n",
      "\n",
      "    def tokenize_column(self, column_name: str):\n",
      "        new_values = self.tokenizer.encode_batch(self.df[column_name].to_list())\n",
      "        new_series = pl.Series(f'tokens|{column_name}', new_values)\n",
      "        len_values = [len(x) for x in new_values]\n",
      "        new_series_len = pl.Series(f'tokens_len|{column_name}', len_values)\n",
      "        self.df = self.df.with_columns(new_series)\n",
      "        self.df = self.df.with_columns(new_series_len)\n",
      "        return self\n",
      "\n",
      "    def embed_columns(self, embeddable_columns: List):\n",
      "        for column_name in embeddable_columns:\n",
      "            column = self.df[column_name]\n",
      "            _, embedder = infer_embeddable_type(column)\n",
      "            self._embed_column(column, embedder)\n",
      "\n",
      "    def _embed_column(self, column, embedder):\n",
      "        # Add the embeddings as a new column\n",
      "        # Generate new values\n",
      "        new_values = embedder.embed(self.df[column.name].to_list())\n",
      "        # Add new column to DataFrame\n",
      "        new_column_name = f'embedding|{column.name}'\n",
      "        new_series = pl.Series(new_column_name, new_values)\n",
      "        self.df = self.df.with_columns(new_series)\n",
      "        self.embedding_columns.append(new_column_name)\n",
      "\n",
      "    def apply_validator_to_column(self, column_name: str, validator: type):\n",
      "        # Ensure the validator is a subclass of BaseModel from Pydantic\n",
      "        if not issubclass(validator, BaseModel):\n",
      "            raise TypeError('validator must be a subclass of BaseModel from Pydantic')\n",
      "        if column_name not in self.df.columns:\n",
      "            raise ValueError(f\"Column '{column_name}' does not exist.\")\n",
      "        if column_name not in self.embeddable_columns:\n",
      "            raise ValueError(f\"Column '{column_name}' is not set to embeddable.\")\n",
      "        # Iterate over the specified column\n",
      "        for text in self.df[column_name]:\n",
      "            # Create a validator instance and validate the text\n",
      "            try:\n",
      "                _ = validator(text=text).text\n",
      "            except Exception as e:\n",
      "                raise ValueError(f\"Failed to validate text in column '{column_name}'.\") from e\n",
      "\n",
      "        return self\n",
      "\n",
      "    def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n",
      "        df = self.df.filter(sql_query)\n",
      "        embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "        query_as_series = pl.Series(query)\n",
      "        dot_product_frame = df.with_columns(df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "        # Sort by dot product and select top_k rows\n",
      "        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "        return result\n",
      "\n",
      "    def search_column_polar(self, query, embeddable_column_name, top_k):\n",
      "        embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "        query_as_series = pl.Series(query)\n",
      "        dot_product_frame = self.df.with_columns(self.df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "        # Sort by dot product and select top_k rows\n",
      "        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "        return result\n",
      "\n",
      "    def save(self):\n",
      "        #create dir in storage if not exists\n",
      "        if not os.path.exists(self.save_dir):\n",
      "            os.makedirs(self.save_dir)\n",
      "        self.full_save_path = f'{self.save_path}/{self.name}/{self.name}.parquet'\n",
      "        self.df.write_parquet(self.full_save_path)\n",
      "        frame_template_json = self.frame_template.json()\n",
      "        with open(f'{self.save_dir}/{self.name}.json', 'w') as f:\n",
      "            f.write(frame_template_json)\n",
      "\n",
      "    @classmethod\n",
      "    def load(cls, frame_path, name):\n",
      "        df = pl.read_parquet(f'{frame_path}/{name}.parquet')\n",
      "        with open(f'{frame_path}/{name}.json', 'r') as f:\n",
      "            frame_template = CodeFramePydantic.parse_raw(f.read())\n",
      "        return cls(df=df, context_columns=frame_template.context_columns, embeddable_columns=frame_template.embeddable_columns, embedding_columns=frame_template.embedding_columns, name=frame_template.name, save_path=frame_template.save_path, text_embedder=frame_template.text_embedder, markdown=frame_template.markdown)\n",
      "\n",
      "    def generate_column(self, row_generator, new_column_name):\n",
      "        # Generate new values\n",
      "        new_values = row_generator.generate(self.df)\n",
      "        # Add new column to DataFrame\n",
      "        new_df = pl.DataFrame({ new_column_name: new_values })\n",
      "        # Concatenate horizontally\n",
      "        self.df = self.df.hstack([new_df])\n",
      "\n",
      "    def apply_visitor_to_column(self, column_name: str, visitor_class: type, new_column_prefix: Optional[str] = None):\n",
      "        # Ensure the visitor_class is a subclass of PythonCodeVisitor\n",
      "        if not issubclass(visitor_class, cst.CSTVisitor):\n",
      "            raise TypeError('visitor_class must be a subclass of PythonCodeVisitor')\n",
      "\n",
      "        # Iterate over the specified column\n",
      "        new_values = []\n",
      "        for code in self.df[column_name]:\n",
      "            # Create a visitor and apply it to the code\n",
      "            visitor = visitor_class(code)\n",
      "            new_value = visitor.collect()\n",
      "            new_values.append(new_value)\n",
      "        # Generate new column\n",
      "        new_column_name = f'{column_name}_{new_column_prefix}|{visitor_class.__name__}'\n",
      "        new_series = pl.Series(new_column_name, new_values)\n",
      "        self.df = self.df.with_columns(new_series)\n",
      "\n",
      "        return self\n",
      "\n",
      "    def count_node_types(self, column_name: str, new_column_prefix: str = 'node_count'):\n",
      "        for node_type_counter in NODETYPE_COUNTERS:\n",
      "            self.apply_visitor_to_column(column_name, globals()[node_type_counter], new_column_prefix)\n",
      "        return self\n",
      "\n",
      "    def count_operators(self, column_name: str, new_column_prefix: str = 'operator_count'):\n",
      "        for operator_counter in OPERATOR_COUNTERS:\n",
      "            self.apply_visitor_to_column(column_name, globals()[operator_counter], new_column_prefix)\n",
      "        return self\n",
      "\n",
      "\n",
      "    def replace_code_in_files(self, filename_column: str, original_code_column: str, replacing_code_column: str):\n",
      "        visitor = CodeReplacerVisitor(filename_column, original_code_column, replacing_code_column)\n",
      "        for row in self.df.rows():\n",
      "            filename = row[filename_column]\n",
      "            original_code = row[original_code_column]\n",
      "            replacing_code = row[replacing_code_column]\n",
      "\n",
      "            if filename and original_code and replacing_code and os.path.isfile(filename):\n",
      "                node = cst.parse_module(original_code)\n",
      "                node.metadata[original_code_column] = original_code\n",
      "                node.metadata[replacing_code_column] = replacing_code\n",
      "                node.metadata[filename_column] = filename\n",
      "\n",
      "                modified_node = node.visit(visitor)\n",
      "                modified_code = cst.Module(body=modified_node.body).code\n",
      "                row[original_code_column] = modified_code\n",
      "\n",
      "        return self\n",
      "\n",
      "    @classmethod\n",
      "    def from_python(\n",
      "        cls,\n",
      "        directory_path: str,\n",
      "        value_column: str,\n",
      "        minify_code: bool = False,\n",
      "        remove_docstrings: bool = False,\n",
      "        resolution: str = \"both\",\n",
      "        embeddings_column: List = [],\n",
      "        embeddable_columns: List = [],\n",
      "        context_columns: Optional[List[str]] = None,\n",
      "        name: str = \"code_frame\",\n",
      "        save_path: Optional[str] = \"./storage\",\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "        markdown: str = \"text/markdown\",\n",
      "    ) -> \"CodeFrame\":\n",
      "        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n",
      "        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n",
      "        #convert retrieved data to polars dataframe\n",
      "        df = pl.DataFrame({value_column: values})\n",
      "        context_df = pl.DataFrame(context)\n",
      "        #merge context columns with dataframe\n",
      "        df = pl.concat([df, context_df], how='horizontal')\n",
      "        if value_column not in embeddable_columns:\n",
      "            embeddable_columns.append(value_column)\n",
      "\n",
      "        kwargs = {\n",
      "            \"context_columns\": context_columns,\n",
      "            \"embeddable_columns\": embeddable_columns,\n",
      "            \"embedding_columns\": embeddings_column,\n",
      "            \"name\": name,\n",
      "            \"save_path\": save_path,\n",
      "            \"text_embedder\": embedder,\n",
      "            \"markdown\": markdown\n",
      "        }\n",
      "        return cls(df, **kwargs)\n",
      "\n",
      "Function calls: shape: (33,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"CodeFramePydan…\n",
      "\t\"getattr\"\n",
      "\t\"AttributeError…\n",
      "\t\"dir\"\n",
      "\t\"callable\"\n",
      "\t\"getattr\"\n",
      "\t\"dir\"\n",
      "\t\"callable\"\n",
      "\t\"getattr\"\n",
      "\t\"list\"\n",
      "\t\"set\"\n",
      "\t…\n",
      "\t\"ValueError\"\n",
      "\t\"open\"\n",
      "\t\"open\"\n",
      "\t\"cls\"\n",
      "\t\"issubclass\"\n",
      "\t\"TypeError\"\n",
      "\t\"visitor_class\"\n",
      "\t\"globals\"\n",
      "\t\"globals\"\n",
      "\t\"CodeReplacerVi…\n",
      "\t\"extract_values…\n",
      "\t\"len\"\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "def __init__(self, df: pl.DataFrame, **kwargs):\n",
      "    super().__init__(**kwargs)\n",
      "    self.df = df\n",
      "    self.frame_template = CodeFramePydantic(df_path=f'{self.save_dir}/{self.name}.parquet', context_columns=self.context_columns, embeddable_columns=self.embeddable_columns, embedding_columns=self.embedding_columns, name=self.name, save_path=self.save_path, save_dir=self.save_dir, load=True, text_embedder=self.text_embedder, markdown=self.markdown)\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"CodeFramePydan…\n",
      "]\n",
      "\n",
      "\n",
      "def __getattr__(self, name: str):\n",
      "    if \"df\" in self.__dict__:\n",
      "        return getattr(self.df.lazy(), name)\n",
      "    raise AttributeError(f\"{self.__class__.__name__} object has no attribute {name}\")\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"getattr\"\n",
      "\t\"AttributeError…\n",
      "]\n",
      "\n",
      "\n",
      "def get_overwritten_attr(self):\n",
      "    df_methods = [method for method in dir(self.df) if callable(getattr(self.df, method))]\n",
      "    memory_frame_methods = [method for method in dir(CodeFrame) if callable(getattr(CodeFrame, method))]\n",
      "    common_methods = list(set(df_methods) & set(memory_frame_methods))\n",
      "    return common_methods\n",
      "\n",
      "Function calls: shape: (9,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"dir\"\n",
      "\t\"callable\"\n",
      "\t\"getattr\"\n",
      "\t\"dir\"\n",
      "\t\"callable\"\n",
      "\t\"getattr\"\n",
      "\t\"list\"\n",
      "\t\"set\"\n",
      "\t\"set\"\n",
      "]\n",
      "\n",
      "\n",
      "def tokenize_column(self, column_name: str):\n",
      "    new_values = self.tokenizer.encode_batch(self.df[column_name].to_list())\n",
      "    new_series = pl.Series(f'tokens|{column_name}', new_values)\n",
      "    len_values = [len(x) for x in new_values]\n",
      "    new_series_len = pl.Series(f'tokens_len|{column_name}', len_values)\n",
      "    self.df = self.df.with_columns(new_series)\n",
      "    self.df = self.df.with_columns(new_series_len)\n",
      "    return self\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def embed_columns(self, embeddable_columns: List):\n",
      "    for column_name in embeddable_columns:\n",
      "        column = self.df[column_name]\n",
      "        _, embedder = infer_embeddable_type(column)\n",
      "        self._embed_column(column, embedder)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"infer_embeddab…\n",
      "]\n",
      "\n",
      "\n",
      "def _embed_column(self, column, embedder):\n",
      "    # Add the embeddings as a new column\n",
      "    # Generate new values\n",
      "    new_values = embedder.embed(self.df[column.name].to_list())\n",
      "    # Add new column to DataFrame\n",
      "    new_column_name = f'embedding|{column.name}'\n",
      "    new_series = pl.Series(new_column_name, new_values)\n",
      "    self.df = self.df.with_columns(new_series)\n",
      "    self.embedding_columns.append(new_column_name)\n",
      "\n",
      "\n",
      "\n",
      "def apply_validator_to_column(self, column_name: str, validator: type):\n",
      "    # Ensure the validator is a subclass of BaseModel from Pydantic\n",
      "    if not issubclass(validator, BaseModel):\n",
      "        raise TypeError('validator must be a subclass of BaseModel from Pydantic')\n",
      "    if column_name not in self.df.columns:\n",
      "        raise ValueError(f\"Column '{column_name}' does not exist.\")\n",
      "    if column_name not in self.embeddable_columns:\n",
      "        raise ValueError(f\"Column '{column_name}' is not set to embeddable.\")\n",
      "    # Iterate over the specified column\n",
      "    for text in self.df[column_name]:\n",
      "        # Create a validator instance and validate the text\n",
      "        try:\n",
      "            _ = validator(text=text).text\n",
      "        except Exception as e:\n",
      "            raise ValueError(f\"Failed to validate text in column '{column_name}'.\") from e\n",
      "\n",
      "    return self\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"issubclass\"\n",
      "\t\"TypeError\"\n",
      "\t\"ValueError\"\n",
      "\t\"ValueError\"\n",
      "\t\"validator\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n",
      "    df = self.df.filter(sql_query)\n",
      "    embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "    query_as_series = pl.Series(query)\n",
      "    dot_product_frame = df.with_columns(df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "    # Sort by dot product and select top_k rows\n",
      "    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "    return result\n",
      "\n",
      "\n",
      "\n",
      "def search_column_polar(self, query, embeddable_column_name, top_k):\n",
      "    embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "    query_as_series = pl.Series(query)\n",
      "    dot_product_frame = self.df.with_columns(self.df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "    # Sort by dot product and select top_k rows\n",
      "    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "    return result\n",
      "\n",
      "\n",
      "\n",
      "def save(self):\n",
      "    #create dir in storage if not exists\n",
      "    if not os.path.exists(self.save_dir):\n",
      "        os.makedirs(self.save_dir)\n",
      "    self.full_save_path = f'{self.save_path}/{self.name}/{self.name}.parquet'\n",
      "    self.df.write_parquet(self.full_save_path)\n",
      "    frame_template_json = self.frame_template.json()\n",
      "    with open(f'{self.save_dir}/{self.name}.json', 'w') as f:\n",
      "        f.write(frame_template_json)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "]\n",
      "\n",
      "\n",
      "@classmethod\n",
      "def load(cls, frame_path, name):\n",
      "    df = pl.read_parquet(f'{frame_path}/{name}.parquet')\n",
      "    with open(f'{frame_path}/{name}.json', 'r') as f:\n",
      "        frame_template = CodeFramePydantic.parse_raw(f.read())\n",
      "    return cls(df=df, context_columns=frame_template.context_columns, embeddable_columns=frame_template.embeddable_columns, embedding_columns=frame_template.embedding_columns, name=frame_template.name, save_path=frame_template.save_path, text_embedder=frame_template.text_embedder, markdown=frame_template.markdown)\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "\n",
      "def generate_column(self, row_generator, new_column_name):\n",
      "    # Generate new values\n",
      "    new_values = row_generator.generate(self.df)\n",
      "    # Add new column to DataFrame\n",
      "    new_df = pl.DataFrame({ new_column_name: new_values })\n",
      "    # Concatenate horizontally\n",
      "    self.df = self.df.hstack([new_df])\n",
      "\n",
      "\n",
      "\n",
      "def apply_visitor_to_column(self, column_name: str, visitor_class: type, new_column_prefix: Optional[str] = None):\n",
      "    # Ensure the visitor_class is a subclass of PythonCodeVisitor\n",
      "    if not issubclass(visitor_class, cst.CSTVisitor):\n",
      "        raise TypeError('visitor_class must be a subclass of PythonCodeVisitor')\n",
      "\n",
      "    # Iterate over the specified column\n",
      "    new_values = []\n",
      "    for code in self.df[column_name]:\n",
      "        # Create a visitor and apply it to the code\n",
      "        visitor = visitor_class(code)\n",
      "        new_value = visitor.collect()\n",
      "        new_values.append(new_value)\n",
      "    # Generate new column\n",
      "    new_column_name = f'{column_name}_{new_column_prefix}|{visitor_class.__name__}'\n",
      "    new_series = pl.Series(new_column_name, new_values)\n",
      "    self.df = self.df.with_columns(new_series)\n",
      "\n",
      "    return self\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"issubclass\"\n",
      "\t\"TypeError\"\n",
      "\t\"visitor_class\"\n",
      "]\n",
      "\n",
      "\n",
      "def count_node_types(self, column_name: str, new_column_prefix: str = 'node_count'):\n",
      "    for node_type_counter in NODETYPE_COUNTERS:\n",
      "        self.apply_visitor_to_column(column_name, globals()[node_type_counter], new_column_prefix)\n",
      "    return self\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"globals\"\n",
      "]\n",
      "\n",
      "\n",
      "def count_operators(self, column_name: str, new_column_prefix: str = 'operator_count'):\n",
      "    for operator_counter in OPERATOR_COUNTERS:\n",
      "        self.apply_visitor_to_column(column_name, globals()[operator_counter], new_column_prefix)\n",
      "    return self\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"globals\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def replace_code_in_files(self, filename_column: str, original_code_column: str, replacing_code_column: str):\n",
      "    visitor = CodeReplacerVisitor(filename_column, original_code_column, replacing_code_column)\n",
      "    for row in self.df.rows():\n",
      "        filename = row[filename_column]\n",
      "        original_code = row[original_code_column]\n",
      "        replacing_code = row[replacing_code_column]\n",
      "\n",
      "        if filename and original_code and replacing_code and os.path.isfile(filename):\n",
      "            node = cst.parse_module(original_code)\n",
      "            node.metadata[original_code_column] = original_code\n",
      "            node.metadata[replacing_code_column] = replacing_code\n",
      "            node.metadata[filename_column] = filename\n",
      "\n",
      "            modified_node = node.visit(visitor)\n",
      "            modified_code = cst.Module(body=modified_node.body).code\n",
      "            row[original_code_column] = modified_code\n",
      "\n",
      "    return self\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"CodeReplacerVi…\n",
      "]\n",
      "\n",
      "\n",
      "@classmethod\n",
      "def from_python(\n",
      "    cls,\n",
      "    directory_path: str,\n",
      "    value_column: str,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "    resolution: str = \"both\",\n",
      "    embeddings_column: List = [],\n",
      "    embeddable_columns: List = [],\n",
      "    context_columns: Optional[List[str]] = None,\n",
      "    name: str = \"code_frame\",\n",
      "    save_path: Optional[str] = \"./storage\",\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "    markdown: str = \"text/markdown\",\n",
      ") -> \"CodeFrame\":\n",
      "    values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n",
      "    logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n",
      "    #convert retrieved data to polars dataframe\n",
      "    df = pl.DataFrame({value_column: values})\n",
      "    context_df = pl.DataFrame(context)\n",
      "    #merge context columns with dataframe\n",
      "    df = pl.concat([df, context_df], how='horizontal')\n",
      "    if value_column not in embeddable_columns:\n",
      "        embeddable_columns.append(value_column)\n",
      "\n",
      "    kwargs = {\n",
      "        \"context_columns\": context_columns,\n",
      "        \"embeddable_columns\": embeddable_columns,\n",
      "        \"embedding_columns\": embeddings_column,\n",
      "        \"name\": name,\n",
      "        \"save_path\": save_path,\n",
      "        \"text_embedder\": embedder,\n",
      "        \"markdown\": markdown\n",
      "    }\n",
      "    return cls(df, **kwargs)\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"extract_values…\n",
      "\t\"len\"\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "class MemoryFrame:\n",
      "    def __init__(self, df: pl.DataFrame,\n",
      "                context_columns: List = [],\n",
      "                embeddable_columns: List = [],\n",
      "                time_series_columns: List = [],\n",
      "                name: str = \"memory_frame\",\n",
      "                save_path: Optional[str] = None,\n",
      "                load: bool = False,\n",
      "                text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "                markdown: str = \"text/markdown\",):\n",
      "        self.df = df\n",
      "        self.context_columns = context_columns\n",
      "        self.time_series_columns = time_series_columns\n",
      "        self.embeddable_columns = embeddable_columns\n",
      "        self.meta_columns = ['ID', 'Name', 'Source', 'Author', 'Created At', 'Last Modified At']\n",
      "        self.embedding_columns = []\n",
      "        self.name = name\n",
      "        self.save_path = save_path\n",
      "        self.load = load\n",
      "        self.text_embedder = text_embedder\n",
      "        self.markdown = markdown\n",
      "\n",
      "\n",
      "    def __getattr__(self, name: str):\n",
      "        # delegate to the self.df object\n",
      "        return getattr(self.df, name)\n",
      "\n",
      "    def get_overwritten_attr(self):\n",
      "        df_methods = [method for method in dir(self.df) if callable(getattr(self.df, method))]\n",
      "        memory_frame_methods = [method for method in dir(MemoryFrame) if callable(getattr(MemoryFrame, method))]\n",
      "        common_methods = list(set(df_methods) & set(memory_frame_methods))\n",
      "        return common_methods\n",
      "\n",
      "    def embed_columns(self, embeddable_columns: List):\n",
      "        for column_name in embeddable_columns:\n",
      "            column = self.df[column_name]\n",
      "            _, embedder = infer_embeddable_type(column)\n",
      "            self._embed_column(column, embedder)\n",
      "\n",
      "    def _embed_column(self, column, embedder):\n",
      "        # Add the embeddings as a new column\n",
      "        # Generate new values\n",
      "        new_values = embedder.embed(self.df[column.name].to_list())\n",
      "        # Add new column to DataFrame\n",
      "        new_column_name = f'embedding|{column.name}'\n",
      "        new_series = pl.Series(new_column_name, new_values)\n",
      "        self.df = self.df.with_columns(new_series)\n",
      "        self.embedding_columns.append(new_column_name)\n",
      "\n",
      "\n",
      "    def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n",
      "        df = self.df.filter(sql_query)\n",
      "        embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "        query_as_series = pl.Series(query)\n",
      "        dot_product_frame = df.with_columns(df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "        # Sort by dot product and select top_k rows\n",
      "        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "        return result\n",
      "\n",
      "\n",
      "    def search_column_polar(self, query, embeddable_column_name, top_k):\n",
      "        embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "        query_as_series = pl.Series(query)\n",
      "        dot_product_frame = self.df.with_columns(self.df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "        # Sort by dot product and select top_k rows\n",
      "        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "        return result\n",
      "\n",
      "    def search_column_numpy(self, query, embeddable_column_name, top_k):\n",
      "        embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "        #convert query and column to numpy arrays\n",
      "        column_np = self.df[embedding_column_name].to_numpy()\n",
      "        #calculate dot product\n",
      "        dot_product = np.dot(column_np, query)\n",
      "        #add dot products as column to dataframe\n",
      "        dot_product_frame = self.df.with_columns(dot_product)\n",
      "        # Sort by dot product and select top_k rows\n",
      "        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "        return result\n",
      "\n",
      "    def save_parquet(self):\n",
      "        #save to arrow\n",
      "        self.full_save_path = self.save_path + self.name + '.parquet'\n",
      "        self.df.write_parquet(self.full_save_path)\n",
      "\n",
      "    def load_parquet(self):\n",
      "        self.full_save_path = self.save_path + self.name + '.parquet'\n",
      "        self.df = pl.read_parquet(self.full_save_path)\n",
      "\n",
      "\n",
      "    def search_time_series_column(self, query, embeddable_column_name, top_k):\n",
      "        ## uses dtw to match any sub-sequence of the query to the time series in the column\n",
      "        ## time series column have a date o time or delta time column associated with them \n",
      "        ## each row-value is a list of across rows variable length for both the time series and the date or time column\n",
      "        pass\n",
      "\n",
      "    def generate_column(self, row_generator, new_column_name):\n",
      "        # Generate new values\n",
      "        new_values = row_generator.generate(self.df)\n",
      "        # Add new column to DataFrame\n",
      "        new_df = pl.DataFrame({ new_column_name: new_values })\n",
      "\n",
      "        # Concatenate horizontally\n",
      "        self.df = self.df.hstack([new_df])\n",
      "    \n",
      "    def create_stratas(self):\n",
      "        \"\"\"\n",
      "        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def _create_strata(self, column_name: str):\n",
      "        \"\"\"\n",
      "        Determine the correct strata creation function to call based on the column's data type,\n",
      "        and then calls the corresponding function.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def _create_strata_from_categorical(self, column_name: str):\n",
      "        \"\"\"\n",
      "        Create strata for a categorical column.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def _create_strata_from_real(self, column_name: str):\n",
      "        \"\"\"\n",
      "        Create strata for a real valued column.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def _create_strata_from_embeddings(self, column_name: str):\n",
      "        \"\"\"\n",
      "        Create strata for a column with embeddings.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def _create_strata_from_episodic_time_series(self, column_name: str):\n",
      "        \"\"\"\n",
      "        Create strata for a column with episodic time series.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def create_joint_strata(self, column_names: list):\n",
      "        \"\"\"\n",
      "        Create strata based on the unique combinations of values across given columns.\n",
      "        \n",
      "        Args:\n",
      "            column_names (list): The names of the columns.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def stratified_sampling(self, strata_columns: list, n_samples: int):\n",
      "        \"\"\"\n",
      "        Perform stratified sampling on given stratum columns.\n",
      "        \n",
      "        Args:\n",
      "            strata_columns (list): The names of the stratum columns.\n",
      "            n_samples (int): The number of samples to draw.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def stratified_cross_validation(self, strata_columns: list, n_folds: int):\n",
      "        \"\"\"\n",
      "        Perform stratified cross-validation on given stratum columns.\n",
      "        \n",
      "        Args:\n",
      "            strata_columns (list): The names of the stratum columns.\n",
      "            n_folds (int): The number of folds for the cross-validation.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    @classmethod\n",
      "    def from_hf_dataset(\n",
      "        cls,\n",
      "        dataset_url: str,\n",
      "        value_column: str,\n",
      "        data_split: str = \"train\",\n",
      "        embeddings_column: Optional[str] = None,\n",
      "        embeddable_columns: List = [],\n",
      "        context_columns: Optional[List[str]] = None,\n",
      "        time_series_columns: List = [],\n",
      "        name: str = \"memory_frame\",\n",
      "        save_path: Optional[str] = None,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "        markdown: str = \"text/markdown\",\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ) -> \"MemoryFrame\":\n",
      "        dataset = load_dataset(dataset_url)[data_split]\n",
      "        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\n",
      "        if context_columns is not None:\n",
      "            context = get_context_from_hf(dataset, context_columns)\n",
      "        else:\n",
      "            context = None\n",
      "        #convert retrieved data to polars dataframe\n",
      "        if embeddings is not None:\n",
      "            df = pl.DataFrame({value_column: values, embeddings_column: embeddings})\n",
      "        else:\n",
      "            df = pl.DataFrame({value_column: values})\n",
      "        context_df = pl.DataFrame(context)\n",
      "        #merge context columns with dataframe\n",
      "        df = pl.concat([df, context_df], how='horizontal')\n",
      "        if value_column not in embeddable_columns:\n",
      "            embeddable_columns.append(value_column)\n",
      "        return cls(df, context_columns, embeddable_columns, time_series_columns, name, save_path, embedder, markdown, token_overflow_strategy)\n",
      "\n",
      "    @classmethod\n",
      "    def from_python(\n",
      "        cls,\n",
      "        directory_path: str,\n",
      "        value_column: str,\n",
      "        minify_code: bool = False,\n",
      "        remove_docstrings: bool = False,\n",
      "        resolution: str = \"both\",\n",
      "        embeddings_column: Optional[str] = None,\n",
      "        embeddable_columns: List = [],\n",
      "        context_columns: Optional[List[str]] = None,\n",
      "        time_series_columns: List = [],\n",
      "        name: str = \"memory_frame\",\n",
      "        save_path: Optional[str] = None,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "        markdown: str = \"text/markdown\",\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ) -> \"MemoryFrame\":\n",
      "        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n",
      "        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n",
      "        #convert retrieved data to polars dataframe\n",
      "        df = pl.DataFrame({value_column: values})\n",
      "        context_df = pl.DataFrame(context)\n",
      "        #merge context columns with dataframe\n",
      "        df = pl.concat([df, context_df], how='horizontal')\n",
      "        if value_column not in embeddable_columns:\n",
      "            embeddable_columns.append(value_column)\n",
      "        return cls(df, context_columns, embeddable_columns, time_series_columns, name, save_path, embedder, markdown, token_overflow_strategy)\n",
      "\n",
      "Function calls: shape: (18,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"getattr\"\n",
      "\t\"dir\"\n",
      "\t\"callable\"\n",
      "\t\"getattr\"\n",
      "\t\"dir\"\n",
      "\t\"callable\"\n",
      "\t\"getattr\"\n",
      "\t\"list\"\n",
      "\t\"set\"\n",
      "\t\"set\"\n",
      "\t\"infer_embeddab…\n",
      "\t\"load_dataset\"\n",
      "\t\"extract_values…\n",
      "\t\"get_context_fr…\n",
      "\t\"cls\"\n",
      "\t\"extract_values…\n",
      "\t\"len\"\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "def __init__(self, df: pl.DataFrame,\n",
      "            context_columns: List = [],\n",
      "            embeddable_columns: List = [],\n",
      "            time_series_columns: List = [],\n",
      "            name: str = \"memory_frame\",\n",
      "            save_path: Optional[str] = None,\n",
      "            load: bool = False,\n",
      "            text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "            markdown: str = \"text/markdown\",):\n",
      "    self.df = df\n",
      "    self.context_columns = context_columns\n",
      "    self.time_series_columns = time_series_columns\n",
      "    self.embeddable_columns = embeddable_columns\n",
      "    self.meta_columns = ['ID', 'Name', 'Source', 'Author', 'Created At', 'Last Modified At']\n",
      "    self.embedding_columns = []\n",
      "    self.name = name\n",
      "    self.save_path = save_path\n",
      "    self.load = load\n",
      "    self.text_embedder = text_embedder\n",
      "    self.markdown = markdown\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def __getattr__(self, name: str):\n",
      "    # delegate to the self.df object\n",
      "    return getattr(self.df, name)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"getattr\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_overwritten_attr(self):\n",
      "    df_methods = [method for method in dir(self.df) if callable(getattr(self.df, method))]\n",
      "    memory_frame_methods = [method for method in dir(MemoryFrame) if callable(getattr(MemoryFrame, method))]\n",
      "    common_methods = list(set(df_methods) & set(memory_frame_methods))\n",
      "    return common_methods\n",
      "\n",
      "Function calls: shape: (9,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"dir\"\n",
      "\t\"callable\"\n",
      "\t\"getattr\"\n",
      "\t\"dir\"\n",
      "\t\"callable\"\n",
      "\t\"getattr\"\n",
      "\t\"list\"\n",
      "\t\"set\"\n",
      "\t\"set\"\n",
      "]\n",
      "\n",
      "\n",
      "def embed_columns(self, embeddable_columns: List):\n",
      "    for column_name in embeddable_columns:\n",
      "        column = self.df[column_name]\n",
      "        _, embedder = infer_embeddable_type(column)\n",
      "        self._embed_column(column, embedder)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"infer_embeddab…\n",
      "]\n",
      "\n",
      "\n",
      "def _embed_column(self, column, embedder):\n",
      "    # Add the embeddings as a new column\n",
      "    # Generate new values\n",
      "    new_values = embedder.embed(self.df[column.name].to_list())\n",
      "    # Add new column to DataFrame\n",
      "    new_column_name = f'embedding|{column.name}'\n",
      "    new_series = pl.Series(new_column_name, new_values)\n",
      "    self.df = self.df.with_columns(new_series)\n",
      "    self.embedding_columns.append(new_column_name)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n",
      "    df = self.df.filter(sql_query)\n",
      "    embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "    query_as_series = pl.Series(query)\n",
      "    dot_product_frame = df.with_columns(df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "    # Sort by dot product and select top_k rows\n",
      "    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "    return result\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def search_column_polar(self, query, embeddable_column_name, top_k):\n",
      "    embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "    query_as_series = pl.Series(query)\n",
      "    dot_product_frame = self.df.with_columns(self.df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "    # Sort by dot product and select top_k rows\n",
      "    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "    return result\n",
      "\n",
      "\n",
      "\n",
      "def search_column_numpy(self, query, embeddable_column_name, top_k):\n",
      "    embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "    #convert query and column to numpy arrays\n",
      "    column_np = self.df[embedding_column_name].to_numpy()\n",
      "    #calculate dot product\n",
      "    dot_product = np.dot(column_np, query)\n",
      "    #add dot products as column to dataframe\n",
      "    dot_product_frame = self.df.with_columns(dot_product)\n",
      "    # Sort by dot product and select top_k rows\n",
      "    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "    return result\n",
      "\n",
      "\n",
      "\n",
      "def save_parquet(self):\n",
      "    #save to arrow\n",
      "    self.full_save_path = self.save_path + self.name + '.parquet'\n",
      "    self.df.write_parquet(self.full_save_path)\n",
      "\n",
      "\n",
      "\n",
      "def load_parquet(self):\n",
      "    self.full_save_path = self.save_path + self.name + '.parquet'\n",
      "    self.df = pl.read_parquet(self.full_save_path)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def search_time_series_column(self, query, embeddable_column_name, top_k):\n",
      "    ## uses dtw to match any sub-sequence of the query to the time series in the column\n",
      "    ## time series column have a date o time or delta time column associated with them \n",
      "    ## each row-value is a list of across rows variable length for both the time series and the date or time column\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "def generate_column(self, row_generator, new_column_name):\n",
      "    # Generate new values\n",
      "    new_values = row_generator.generate(self.df)\n",
      "    # Add new column to DataFrame\n",
      "    new_df = pl.DataFrame({ new_column_name: new_values })\n",
      "\n",
      "    # Concatenate horizontally\n",
      "    self.df = self.df.hstack([new_df])\n",
      "\n",
      "\n",
      "\n",
      "def create_stratas(self):\n",
      "    \"\"\"\n",
      "        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "def _create_strata(self, column_name: str):\n",
      "    \"\"\"\n",
      "        Determine the correct strata creation function to call based on the column's data type,\n",
      "        and then calls the corresponding function.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "def _create_strata_from_categorical(self, column_name: str):\n",
      "    \"\"\"\n",
      "        Create strata for a categorical column.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "def _create_strata_from_real(self, column_name: str):\n",
      "    \"\"\"\n",
      "        Create strata for a real valued column.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "def _create_strata_from_embeddings(self, column_name: str):\n",
      "    \"\"\"\n",
      "        Create strata for a column with embeddings.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "def _create_strata_from_episodic_time_series(self, column_name: str):\n",
      "    \"\"\"\n",
      "        Create strata for a column with episodic time series.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "def create_joint_strata(self, column_names: list):\n",
      "    \"\"\"\n",
      "        Create strata based on the unique combinations of values across given columns.\n",
      "        \n",
      "        Args:\n",
      "            column_names (list): The names of the columns.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "def stratified_sampling(self, strata_columns: list, n_samples: int):\n",
      "    \"\"\"\n",
      "        Perform stratified sampling on given stratum columns.\n",
      "        \n",
      "        Args:\n",
      "            strata_columns (list): The names of the stratum columns.\n",
      "            n_samples (int): The number of samples to draw.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "def stratified_cross_validation(self, strata_columns: list, n_folds: int):\n",
      "    \"\"\"\n",
      "        Perform stratified cross-validation on given stratum columns.\n",
      "        \n",
      "        Args:\n",
      "            strata_columns (list): The names of the stratum columns.\n",
      "            n_folds (int): The number of folds for the cross-validation.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@classmethod\n",
      "def from_hf_dataset(\n",
      "    cls,\n",
      "    dataset_url: str,\n",
      "    value_column: str,\n",
      "    data_split: str = \"train\",\n",
      "    embeddings_column: Optional[str] = None,\n",
      "    embeddable_columns: List = [],\n",
      "    context_columns: Optional[List[str]] = None,\n",
      "    time_series_columns: List = [],\n",
      "    name: str = \"memory_frame\",\n",
      "    save_path: Optional[str] = None,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "    markdown: str = \"text/markdown\",\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      ") -> \"MemoryFrame\":\n",
      "    dataset = load_dataset(dataset_url)[data_split]\n",
      "    values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\n",
      "    if context_columns is not None:\n",
      "        context = get_context_from_hf(dataset, context_columns)\n",
      "    else:\n",
      "        context = None\n",
      "    #convert retrieved data to polars dataframe\n",
      "    if embeddings is not None:\n",
      "        df = pl.DataFrame({value_column: values, embeddings_column: embeddings})\n",
      "    else:\n",
      "        df = pl.DataFrame({value_column: values})\n",
      "    context_df = pl.DataFrame(context)\n",
      "    #merge context columns with dataframe\n",
      "    df = pl.concat([df, context_df], how='horizontal')\n",
      "    if value_column not in embeddable_columns:\n",
      "        embeddable_columns.append(value_column)\n",
      "    return cls(df, context_columns, embeddable_columns, time_series_columns, name, save_path, embedder, markdown, token_overflow_strategy)\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"load_dataset\"\n",
      "\t\"extract_values…\n",
      "\t\"get_context_fr…\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "\n",
      "@classmethod\n",
      "def from_python(\n",
      "    cls,\n",
      "    directory_path: str,\n",
      "    value_column: str,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "    resolution: str = \"both\",\n",
      "    embeddings_column: Optional[str] = None,\n",
      "    embeddable_columns: List = [],\n",
      "    context_columns: Optional[List[str]] = None,\n",
      "    time_series_columns: List = [],\n",
      "    name: str = \"memory_frame\",\n",
      "    save_path: Optional[str] = None,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "    markdown: str = \"text/markdown\",\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      ") -> \"MemoryFrame\":\n",
      "    values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n",
      "    logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n",
      "    #convert retrieved data to polars dataframe\n",
      "    df = pl.DataFrame({value_column: values})\n",
      "    context_df = pl.DataFrame(context)\n",
      "    #merge context columns with dataframe\n",
      "    df = pl.concat([df, context_df], how='horizontal')\n",
      "    if value_column not in embeddable_columns:\n",
      "        embeddable_columns.append(value_column)\n",
      "    return cls(df, context_columns, embeddable_columns, time_series_columns, name, save_path, embedder, markdown, token_overflow_strategy)\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"extract_values…\n",
      "\t\"len\"\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "\n",
      "class FunctionCallCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.function_call_count = 0\n",
      "\n",
      "    def visit_Call(self, node: cst.Call) -> bool:\n",
      "        self.function_call_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.function_call_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.function_call_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Call(self, node: cst.Call) -> bool:\n",
      "    self.function_call_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.function_call_count\n",
      "\n",
      "\n",
      "\n",
      "# Argument Type Counter\n",
      "class ArgumentTypeCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.argument_type_count = 0\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n",
      "        self.argument_type_count += len(node.params.params)\n",
      "        return True\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n",
      "        for stmt in node.body.body:\n",
      "            if isinstance(stmt, cst.FunctionDef):\n",
      "                self.argument_type_count += len(stmt.params.params)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.argument_type_count\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.argument_type_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n",
      "    self.argument_type_count += len(node.params.params)\n",
      "    return True\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n",
      "    for stmt in node.body.body:\n",
      "        if isinstance(stmt, cst.FunctionDef):\n",
      "            self.argument_type_count += len(stmt.params.params)\n",
      "    return True\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.argument_type_count\n",
      "\n",
      "\n",
      "\n",
      "# Import Counter\n",
      "class ImportCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.import_count = 0\n",
      "\n",
      "    def visit_Import(self, node: cst.Import) -> bool:\n",
      "        self.import_count += 1\n",
      "        return True\n",
      "\n",
      "    def visit_ImportFrom(self, node: cst.ImportFrom) -> bool:\n",
      "        self.import_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.import_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.import_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Import(self, node: cst.Import) -> bool:\n",
      "    self.import_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def visit_ImportFrom(self, node: cst.ImportFrom) -> bool:\n",
      "    self.import_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.import_count\n",
      "\n",
      "\n",
      "\n",
      "# If Statement Counter\n",
      "class IfStatementCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.if_statement_count = 0\n",
      "\n",
      "    def visit_If(self, node: cst.If) -> bool:\n",
      "        self.if_statement_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.if_statement_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.if_statement_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_If(self, node: cst.If) -> bool:\n",
      "    self.if_statement_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.if_statement_count\n",
      "\n",
      "\n",
      "\n",
      "# Base Compound Statement Counter\n",
      "class BaseCompoundStatementCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.compound_statement_count = 0\n",
      "\n",
      "    def visit_BaseCompoundStatement(self, node: cst.BaseCompoundStatement) -> bool:\n",
      "        self.compound_statement_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.compound_statement_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.compound_statement_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_BaseCompoundStatement(self, node: cst.BaseCompoundStatement) -> bool:\n",
      "    self.compound_statement_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.compound_statement_count\n",
      "\n",
      "\n",
      "\n",
      "# For Loop Counter\n",
      "class ForLoopCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.for_loop_count = 0\n",
      "\n",
      "    def visit_For(self, node: cst.For) -> bool:\n",
      "        self.for_loop_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.for_loop_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.for_loop_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_For(self, node: cst.For) -> bool:\n",
      "    self.for_loop_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.for_loop_count\n",
      "\n",
      "\n",
      "\n",
      "# While Loop Counter\n",
      "class WhileLoopCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.while_loop_count = 0\n",
      "\n",
      "    def visit_While(self, node: cst.While) -> bool:\n",
      "        self.while_loop_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.while_loop_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.while_loop_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_While(self, node: cst.While) -> bool:\n",
      "    self.while_loop_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.while_loop_count\n",
      "\n",
      "\n",
      "\n",
      "# Try Except Counter\n",
      "class TryExceptCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.try_except_count = 0\n",
      "\n",
      "    def visit_Try(self, node: cst.Try) -> bool:\n",
      "        self.try_except_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.try_except_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.try_except_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Try(self, node: cst.Try) -> bool:\n",
      "    self.try_except_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.try_except_count\n",
      "\n",
      "\n",
      "\n",
      "# With Statement Counter\n",
      "class WithStatementCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.with_statement_count = 0\n",
      "\n",
      "    def visit_With(self, node: cst.With) -> bool:\n",
      "        self.with_statement_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.with_statement_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.with_statement_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_With(self, node: cst.With) -> bool:\n",
      "    self.with_statement_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.with_statement_count\n",
      "\n",
      "\n",
      "\n",
      "# Lambda Function Counter\n",
      "class LambdaFunctionCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.lambda_function_count = 0\n",
      "\n",
      "    def visit_Lambda(self, node: cst.Lambda) -> bool:\n",
      "        self.lambda_function_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.lambda_function_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.lambda_function_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Lambda(self, node: cst.Lambda) -> bool:\n",
      "    self.lambda_function_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.lambda_function_count\n",
      "\n",
      "\n",
      "\n",
      "# Global Statement Counter\n",
      "class GlobalStatementCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.global_statement_count = 0\n",
      "\n",
      "    def visit_Global(self, node: cst.Global) -> bool:\n",
      "        self.global_statement_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.global_statement_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.global_statement_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Global(self, node: cst.Global) -> bool:\n",
      "    self.global_statement_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.global_statement_count\n",
      "\n",
      "\n",
      "\n",
      "# Nonlocal Statement Counter\n",
      "class NonlocalStatementCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.nonlocal_statement_count = 0\n",
      "\n",
      "    def visit_Nonlocal(self, node: cst.Nonlocal) -> bool:\n",
      "        self.nonlocal_statement_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.nonlocal_statement_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.nonlocal_statement_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Nonlocal(self, node: cst.Nonlocal) -> bool:\n",
      "    self.nonlocal_statement_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.nonlocal_statement_count\n",
      "\n",
      "\n",
      "\n",
      "class ListComprehensionCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.list_comprehension_count = 0\n",
      "\n",
      "    def visit_ListComp(self, node: cst.ListComp) -> bool:\n",
      "        self.list_comprehension_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.list_comprehension_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.list_comprehension_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_ListComp(self, node: cst.ListComp) -> bool:\n",
      "    self.list_comprehension_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.list_comprehension_count\n",
      "\n",
      "\n",
      "\n",
      "class DictComprehensionCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.dict_comprehension_count = 0\n",
      "\n",
      "    def visit_DictComp(self, node: cst.DictComp) -> bool:\n",
      "        self.dict_comprehension_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.dict_comprehension_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.dict_comprehension_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_DictComp(self, node: cst.DictComp) -> bool:\n",
      "    self.dict_comprehension_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.dict_comprehension_count\n",
      "\n",
      "\n",
      "\n",
      "class SetComprehensionCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.set_comprehension_count = 0\n",
      "\n",
      "    def visit_SetComp(self, node: cst.SetComp) -> bool:\n",
      "        self.set_comprehension_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.set_comprehension_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.set_comprehension_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_SetComp(self, node: cst.SetComp) -> bool:\n",
      "    self.set_comprehension_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.set_comprehension_count\n",
      "\n",
      "\n",
      "\n",
      "class GeneratorExpressionCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.generator_expression_count = 0\n",
      "\n",
      "    def visit_GeneratorExp(self, node: cst.GeneratorExp) -> bool:\n",
      "        self.generator_expression_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.generator_expression_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.generator_expression_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_GeneratorExp(self, node: cst.GeneratorExp) -> bool:\n",
      "    self.generator_expression_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.generator_expression_count\n",
      "\n",
      "\n",
      "\n",
      "class YieldCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.yield_count = 0\n",
      "\n",
      "    def visit_Yield(self, node: cst.Yield) -> bool:\n",
      "        self.yield_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.yield_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.yield_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Yield(self, node: cst.Yield) -> bool:\n",
      "    self.yield_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.yield_count\n",
      "\n",
      "\n",
      "\n",
      "class AwaitCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.await_count = 0\n",
      "\n",
      "    def visit_Await(self, node: cst.Await) -> bool:\n",
      "        self.await_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.await_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.await_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Await(self, node: cst.Await) -> bool:\n",
      "    self.await_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.await_count\n",
      "\n",
      "\n",
      "\n",
      "class ReturnCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.return_count = 0\n",
      "\n",
      "    def visit_Return(self, node: cst.Return) -> bool:\n",
      "        self.return_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.return_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.return_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Return(self, node: cst.Return) -> bool:\n",
      "    self.return_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.return_count\n",
      "\n",
      "\n",
      "\n",
      "class BreakCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.break_count = 0\n",
      "\n",
      "    def visit_Break(self, node: cst.Break) -> bool:\n",
      "        self.break_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.break_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.break_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Break(self, node: cst.Break) -> bool:\n",
      "    self.break_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.break_count\n",
      "\n",
      "\n",
      "\n",
      "class ContinueCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.continue_count = 0\n",
      "\n",
      "    def visit_Continue(self, node: cst.Continue) -> bool:\n",
      "        self.continue_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.continue_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.continue_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Continue(self, node: cst.Continue) -> bool:\n",
      "    self.continue_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.continue_count\n",
      "\n",
      "\n",
      "\n",
      "class RaiseCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.raise_count = 0\n",
      "\n",
      "    def visit_Raise(self, node: cst.Raise) -> bool:\n",
      "        self.raise_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.raise_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.raise_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Raise(self, node: cst.Raise) -> bool:\n",
      "    self.raise_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.raise_count\n",
      "\n",
      "\n",
      "\n",
      "class AssertCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.assert_count = 0\n",
      "\n",
      "    def visit_Assert(self, node: cst.Assert) -> bool:\n",
      "        self.assert_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.assert_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.assert_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Assert(self, node: cst.Assert) -> bool:\n",
      "    self.assert_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.assert_count\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class PassCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.pass_count = 0\n",
      "\n",
      "    def visit_Pass(self, node: cst.Pass) -> bool:\n",
      "        self.pass_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.pass_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.pass_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Pass(self, node: cst.Pass) -> bool:\n",
      "    self.pass_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.pass_count\n",
      "\n",
      "\n",
      "\n",
      "# Unary Operators\n",
      "class UnaryOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.unary_operators = []\n",
      "\n",
      "    def visit_UnaryOperation(self, node: cst.UnaryOperation) -> bool:\n",
      "        if isinstance(node.operator, cst.BitInvert) or isinstance(node.operator, cst.Minus) or \\\n",
      "                isinstance(node.operator, cst.Not) or isinstance(node.operator, cst.Plus):\n",
      "            self.unary_operators.append(node.operator.__class__.__name__)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.unary_operators\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.unary_operators = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_UnaryOperation(self, node: cst.UnaryOperation) -> bool:\n",
      "    if isinstance(node.operator, cst.BitInvert) or isinstance(node.operator, cst.Minus) or \\\n",
      "                isinstance(node.operator, cst.Not) or isinstance(node.operator, cst.Plus):\n",
      "        self.unary_operators.append(node.operator.__class__.__name__)\n",
      "    return True\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.unary_operators\n",
      "\n",
      "\n",
      "\n",
      "# Boolean Operators\n",
      "class BooleanOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.boolean_operators = []\n",
      "\n",
      "    def visit_BooleanOperation(self, node: cst.BooleanOperation) -> bool:\n",
      "        if isinstance(node.operator, cst.And) or isinstance(node.operator, cst.Or):\n",
      "            self.boolean_operators.append(node.operator.__class__.__name__)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.boolean_operators\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.boolean_operators = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_BooleanOperation(self, node: cst.BooleanOperation) -> bool:\n",
      "    if isinstance(node.operator, cst.And) or isinstance(node.operator, cst.Or):\n",
      "        self.boolean_operators.append(node.operator.__class__.__name__)\n",
      "    return True\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.boolean_operators\n",
      "\n",
      "\n",
      "\n",
      "# Binary Operators\n",
      "class BinaryOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.binary_operators = []\n",
      "\n",
      "    def visit_BinaryOperation(self, node: cst.BinaryOperation) -> bool:\n",
      "        if isinstance(node.operator, cst.Add) or isinstance(node.operator, cst.BitAnd) or \\\n",
      "                isinstance(node.operator, cst.BitOr) or isinstance(node.operator, cst.BitXor) or \\\n",
      "                isinstance(node.operator, cst.Divide) or isinstance(node.operator, cst.FloorDivide) or \\\n",
      "                isinstance(node.operator, cst.LeftShift) or isinstance(node.operator, cst.MatrixMultiply) or \\\n",
      "                isinstance(node.operator, cst.Modulo) or isinstance(node.operator, cst.Multiply) or \\\n",
      "                isinstance(node.operator, cst.Power) or isinstance(node.operator, cst.RightShift) or \\\n",
      "                isinstance(node.operator, cst.Subtract):\n",
      "            self.binary_operators.append(node.operator.__class__.__name__)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.binary_operators\n",
      "\n",
      "Function calls: shape: (13,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.binary_operators = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_BinaryOperation(self, node: cst.BinaryOperation) -> bool:\n",
      "    if isinstance(node.operator, cst.Add) or isinstance(node.operator, cst.BitAnd) or \\\n",
      "                isinstance(node.operator, cst.BitOr) or isinstance(node.operator, cst.BitXor) or \\\n",
      "                isinstance(node.operator, cst.Divide) or isinstance(node.operator, cst.FloorDivide) or \\\n",
      "                isinstance(node.operator, cst.LeftShift) or isinstance(node.operator, cst.MatrixMultiply) or \\\n",
      "                isinstance(node.operator, cst.Modulo) or isinstance(node.operator, cst.Multiply) or \\\n",
      "                isinstance(node.operator, cst.Power) or isinstance(node.operator, cst.RightShift) or \\\n",
      "                isinstance(node.operator, cst.Subtract):\n",
      "        self.binary_operators.append(node.operator.__class__.__name__)\n",
      "    return True\n",
      "\n",
      "Function calls: shape: (13,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.binary_operators\n",
      "\n",
      "\n",
      "\n",
      "# Comparison Operators\n",
      "class ComparisonOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.comparison_operators = []\n",
      "\n",
      "    def visit_Comparison(self, node: cst.Comparison) -> bool:\n",
      "        for operator in node.operators:\n",
      "            if isinstance(operator, cst.Equal) or isinstance(operator, cst.GreaterThan) or \\\n",
      "                    isinstance(operator, cst.GreaterThanEqual) or isinstance(operator, cst.In) or \\\n",
      "                    isinstance(operator, cst.Is) or isinstance(operator, cst.LessThan) or \\\n",
      "                    isinstance(operator, cst.LessThanEqual) or isinstance(operator, cst.NotEqual) or \\\n",
      "                    isinstance(operator, cst.IsNot) or isinstance(operator, cst.NotIn):\n",
      "                self.comparison_operators.append(operator.__class__.__name__)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.comparison_operators\n",
      "\n",
      "Function calls: shape: (10,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.comparison_operators = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Comparison(self, node: cst.Comparison) -> bool:\n",
      "    for operator in node.operators:\n",
      "        if isinstance(operator, cst.Equal) or isinstance(operator, cst.GreaterThan) or \\\n",
      "                    isinstance(operator, cst.GreaterThanEqual) or isinstance(operator, cst.In) or \\\n",
      "                    isinstance(operator, cst.Is) or isinstance(operator, cst.LessThan) or \\\n",
      "                    isinstance(operator, cst.LessThanEqual) or isinstance(operator, cst.NotEqual) or \\\n",
      "                    isinstance(operator, cst.IsNot) or isinstance(operator, cst.NotIn):\n",
      "            self.comparison_operators.append(operator.__class__.__name__)\n",
      "    return True\n",
      "\n",
      "Function calls: shape: (10,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.comparison_operators\n",
      "\n",
      "\n",
      "\n",
      "# Augmented Assignment Operators\n",
      "class AugmentedAssignmentOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.augmented_assignment_operators = []\n",
      "\n",
      "    def visit_AugAssign(self, node: cst.AugAssign) -> bool:\n",
      "        if isinstance(node.operator, cst.AddAssign) or isinstance(node.operator, cst.BitAndAssign) or \\\n",
      "                isinstance(node.operator, cst.BitOrAssign) or isinstance(node.operator, cst.BitXorAssign) or \\\n",
      "                isinstance(node.operator, cst.DivideAssign) or isinstance(node.operator, cst.FloorDivideAssign) or \\\n",
      "                isinstance(node.operator, cst.LeftShiftAssign) or isinstance(node.operator, cst.MatrixMultiplyAssign) or \\\n",
      "                isinstance(node.operator, cst.ModuloAssign) or isinstance(node.operator, cst.MultiplyAssign) or \\\n",
      "                isinstance(node.operator, cst.PowerAssign) or isinstance(node.operator, cst.RightShiftAssign) or \\\n",
      "                isinstance(node.operator, cst.SubtractAssign):\n",
      "            self.augmented_assignment_operators.append(node.operator.__class__.__name__)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.augmented_assignment_operators\n",
      "\n",
      "Function calls: shape: (13,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.augmented_assignment_operators = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_AugAssign(self, node: cst.AugAssign) -> bool:\n",
      "    if isinstance(node.operator, cst.AddAssign) or isinstance(node.operator, cst.BitAndAssign) or \\\n",
      "                isinstance(node.operator, cst.BitOrAssign) or isinstance(node.operator, cst.BitXorAssign) or \\\n",
      "                isinstance(node.operator, cst.DivideAssign) or isinstance(node.operator, cst.FloorDivideAssign) or \\\n",
      "                isinstance(node.operator, cst.LeftShiftAssign) or isinstance(node.operator, cst.MatrixMultiplyAssign) or \\\n",
      "                isinstance(node.operator, cst.ModuloAssign) or isinstance(node.operator, cst.MultiplyAssign) or \\\n",
      "                isinstance(node.operator, cst.PowerAssign) or isinstance(node.operator, cst.RightShiftAssign) or \\\n",
      "                isinstance(node.operator, cst.SubtractAssign):\n",
      "        self.augmented_assignment_operators.append(node.operator.__class__.__name__)\n",
      "    return True\n",
      "\n",
      "Function calls: shape: (13,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.augmented_assignment_operators\n",
      "\n",
      "\n",
      "\n",
      "# Miscellaneous Operators\n",
      "class MiscellaneousOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.miscellaneous_operators = []\n",
      "\n",
      "    def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "        self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def visit_Colon(self, node: cst.Colon) -> bool:\n",
      "        self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def visit_Comma(self, node: cst.Comma) -> bool:\n",
      "        self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def visit_Dot(self, node: cst.Dot) -> bool:\n",
      "        self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n",
      "        self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n",
      "        self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.miscellaneous_operators\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.miscellaneous_operators = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "    self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def visit_Colon(self, node: cst.Colon) -> bool:\n",
      "    self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def visit_Comma(self, node: cst.Comma) -> bool:\n",
      "    self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def visit_Dot(self, node: cst.Dot) -> bool:\n",
      "    self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n",
      "    self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n",
      "    self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.miscellaneous_operators\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Unary Operators\n",
      "class BitInvertOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_invert_count = []\n",
      "\n",
      "    def visit_BitInvert(self, node: cst.BitInvert) -> bool:\n",
      "        self.bit_invert_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_invert_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_invert_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitInvert(self, node: cst.BitInvert) -> bool:\n",
      "    self.bit_invert_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_invert_count\n",
      "\n",
      "\n",
      "\n",
      "class MinusOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.minus_count = []\n",
      "\n",
      "    def visit_Minus(self, node: cst.Minus) -> bool:\n",
      "        self.minus_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.minus_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.minus_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Minus(self, node: cst.Minus) -> bool:\n",
      "    self.minus_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.minus_count\n",
      "\n",
      "\n",
      "\n",
      "class NotOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.not_count = []\n",
      "\n",
      "    def visit_Not(self, node: cst.Not) -> bool:\n",
      "        self.not_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.not_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.not_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Not(self, node: cst.Not) -> bool:\n",
      "    self.not_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.not_count\n",
      "\n",
      "\n",
      "\n",
      "class PlusOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.plus_count = []\n",
      "\n",
      "    def visit_Plus(self, node: cst.Plus) -> bool:\n",
      "        self.plus_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.plus_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.plus_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Plus(self, node: cst.Plus) -> bool:\n",
      "    self.plus_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.plus_count\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Boolean Operators\n",
      "class AndOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.and_count = []\n",
      "\n",
      "    def visit_And(self, node: cst.And) -> bool:\n",
      "        self.and_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.and_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.and_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_And(self, node: cst.And) -> bool:\n",
      "    self.and_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.and_count\n",
      "\n",
      "\n",
      "\n",
      "class OrOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.or_count = []\n",
      "\n",
      "    def visit_Or(self, node: cst.Or) -> bool:\n",
      "        self.or_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.or_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.or_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Or(self, node: cst.Or) -> bool:\n",
      "    self.or_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.or_count\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Binary Operators\n",
      "class AddOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.add_count = []\n",
      "\n",
      "    def visit_Add(self, node: cst.Add) -> bool:\n",
      "        self.add_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.add_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.add_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Add(self, node: cst.Add) -> bool:\n",
      "    self.add_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.add_count\n",
      "\n",
      "\n",
      "\n",
      "class BitAndOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_and_count = []\n",
      "\n",
      "    def visit_BitAnd(self, node: cst.BitAnd) -> bool:\n",
      "        self.bit_and_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_and_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_and_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitAnd(self, node: cst.BitAnd) -> bool:\n",
      "    self.bit_and_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_and_count\n",
      "\n",
      "\n",
      "\n",
      "class BitOrOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_or_count = []\n",
      "\n",
      "    def visit_BitOr(self, node: cst.BitOr) -> bool:\n",
      "        self.bit_or_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_or_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_or_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitOr(self, node: cst.BitOr) -> bool:\n",
      "    self.bit_or_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_or_count\n",
      "\n",
      "\n",
      "\n",
      "class BitXorOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_xor_count = []\n",
      "\n",
      "    def visit_BitXor(self, node: cst.BitXor) -> bool:\n",
      "        self.bit_xor_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_xor_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_xor_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitXor(self, node: cst.BitXor) -> bool:\n",
      "    self.bit_xor_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_xor_count\n",
      "\n",
      "\n",
      "\n",
      "class DivideOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.divide_count = []\n",
      "\n",
      "    def visit_Divide(self, node: cst.Divide) -> bool:\n",
      "        self.divide_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.divide_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.divide_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Divide(self, node: cst.Divide) -> bool:\n",
      "    self.divide_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.divide_count\n",
      "\n",
      "\n",
      "\n",
      "class FloorDivideOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.floor_divide_count = []\n",
      "\n",
      "    def visit_FloorDivide(self, node: cst.FloorDivide) -> bool:\n",
      "        self.floor_divide_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.floor_divide_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.floor_divide_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_FloorDivide(self, node: cst.FloorDivide) -> bool:\n",
      "    self.floor_divide_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.floor_divide_count\n",
      "\n",
      "\n",
      "\n",
      "class LeftShiftOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.left_shift_count = []\n",
      "\n",
      "    def visit_LeftShift(self, node: cst.LeftShift) -> bool:\n",
      "        self.left_shift_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.left_shift_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.left_shift_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_LeftShift(self, node: cst.LeftShift) -> bool:\n",
      "    self.left_shift_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.left_shift_count\n",
      "\n",
      "\n",
      "\n",
      "class MatrixMultiplyOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.matrix_multiply_count = []\n",
      "\n",
      "    def visit_MatrixMultiply(self, node: cst.MatrixMultiply) -> bool:\n",
      "        self.matrix_multiply_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.matrix_multiply_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.matrix_multiply_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_MatrixMultiply(self, node: cst.MatrixMultiply) -> bool:\n",
      "    self.matrix_multiply_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.matrix_multiply_count\n",
      "\n",
      "\n",
      "\n",
      "class ModuloOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.modulo_count = []\n",
      "\n",
      "    def visit_Modulo(self, node: cst.Modulo) -> bool:\n",
      "        self.modulo_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.modulo_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.modulo_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Modulo(self, node: cst.Modulo) -> bool:\n",
      "    self.modulo_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.modulo_count\n",
      "\n",
      "\n",
      "\n",
      "class MultiplyOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.multiply_count = []\n",
      "\n",
      "    def visit_Multiply(self, node: cst.Multiply) -> bool:\n",
      "        self.multiply_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.multiply_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.multiply_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Multiply(self, node: cst.Multiply) -> bool:\n",
      "    self.multiply_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.multiply_count\n",
      "\n",
      "\n",
      "\n",
      "class PowerOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.power_count = []\n",
      "\n",
      "    def visit_Power(self, node: cst.Power) -> bool:\n",
      "        self.power_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.power_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.power_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Power(self, node: cst.Power) -> bool:\n",
      "    self.power_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.power_count\n",
      "\n",
      "\n",
      "\n",
      "class RightShiftOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.right_shift_count = []\n",
      "\n",
      "    def visit_RightShift(self, node: cst.RightShift) -> bool:\n",
      "        self.right_shift_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.right_shift_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.right_shift_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_RightShift(self, node: cst.RightShift) -> bool:\n",
      "    self.right_shift_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.right_shift_count\n",
      "\n",
      "\n",
      "\n",
      "class SubtractOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.subtract_count = []\n",
      "\n",
      "    def visit_Subtract(self, node: cst.Subtract) -> bool:\n",
      "        self.subtract_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.subtract_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.subtract_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Subtract(self, node: cst.Subtract) -> bool:\n",
      "    self.subtract_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.subtract_count\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Comparison Operators\n",
      "class EqualOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.equal_count = []\n",
      "\n",
      "    def visit_Equal(self, node: cst.Equal) -> bool:\n",
      "        self.equal_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.equal_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.equal_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Equal(self, node: cst.Equal) -> bool:\n",
      "    self.equal_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.equal_count\n",
      "\n",
      "\n",
      "\n",
      "class GreaterThanOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.greater_than_count = []\n",
      "\n",
      "    def visit_GreaterThan(self, node: cst.GreaterThan) -> bool:\n",
      "        self.greater_than_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.greater_than_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.greater_than_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_GreaterThan(self, node: cst.GreaterThan) -> bool:\n",
      "    self.greater_than_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.greater_than_count\n",
      "\n",
      "\n",
      "\n",
      "# ... continue with other comparison operators\n",
      "\n",
      "\n",
      "# Augmented Assignment Operators\n",
      "class AddAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.add_assign_count = []\n",
      "\n",
      "    def visit_AddAssign(self, node: cst.AddAssign) -> bool:\n",
      "        self.add_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.add_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.add_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_AddAssign(self, node: cst.AddAssign) -> bool:\n",
      "    self.add_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.add_assign_count\n",
      "\n",
      "\n",
      "\n",
      "class BitAndAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_and_assign_count = []\n",
      "\n",
      "    def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n",
      "        self.bit_and_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_and_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_and_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n",
      "    self.bit_and_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_and_assign_count\n",
      "\n",
      "\n",
      "class BitAndAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_and_assign_count = []\n",
      "\n",
      "    def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n",
      "        self.bit_and_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_and_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_and_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n",
      "    self.bit_and_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_and_assign_count\n",
      "\n",
      "\n",
      "\n",
      "class BitOrAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_or_assign_count = []\n",
      "\n",
      "    def visit_BitOrAssign(self, node: cst.BitOrAssign) -> bool:\n",
      "        self.bit_or_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_or_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_or_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitOrAssign(self, node: cst.BitOrAssign) -> bool:\n",
      "    self.bit_or_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_or_assign_count\n",
      "\n",
      "\n",
      "\n",
      "class BitXorAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_xor_assign_count = []\n",
      "\n",
      "    def visit_BitXorAssign(self, node: cst.BitXorAssign) -> bool:\n",
      "        self.bit_xor_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_xor_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_xor_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitXorAssign(self, node: cst.BitXorAssign) -> bool:\n",
      "    self.bit_xor_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_xor_assign_count\n",
      "\n",
      "\n",
      "\n",
      "class DivideAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.divide_assign_count = []\n",
      "\n",
      "    def visit_DivideAssign(self, node: cst.DivideAssign) -> bool:\n",
      "        self.divide_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.divide_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.divide_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_DivideAssign(self, node: cst.DivideAssign) -> bool:\n",
      "    self.divide_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.divide_assign_count\n",
      "\n",
      "\n",
      "\n",
      "class FloorDivideAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.floor_divide_assign_count = []\n",
      "\n",
      "    def visit_FloorDivideAssign(self, node: cst.FloorDivideAssign) -> bool:\n",
      "        self.floor_divide_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.floor_divide_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.floor_divide_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_FloorDivideAssign(self, node: cst.FloorDivideAssign) -> bool:\n",
      "    self.floor_divide_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.floor_divide_assign_count\n",
      "\n",
      "\n",
      "\n",
      "class LeftShiftAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.left_shift_assign_count = []\n",
      "\n",
      "    def visit_LeftShiftAssign(self, node: cst.LeftShiftAssign) -> bool:\n",
      "        self.left_shift_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.left_shift_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.left_shift_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_LeftShiftAssign(self, node: cst.LeftShiftAssign) -> bool:\n",
      "    self.left_shift_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.left_shift_assign_count\n",
      "\n",
      "\n",
      "\n",
      "class MatrixMultiplyAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.matrix_multiply_assign_count = []\n",
      "\n",
      "    def visit_MatrixMultiplyAssign(self, node: cst.MatrixMultiplyAssign) -> bool:\n",
      "        self.matrix_multiply_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.matrix_multiply_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.matrix_multiply_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_MatrixMultiplyAssign(self, node: cst.MatrixMultiplyAssign) -> bool:\n",
      "    self.matrix_multiply_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.matrix_multiply_assign_count\n",
      "\n",
      "\n",
      "\n",
      "class ModuloAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.modulo_assign_count = []\n",
      "\n",
      "    def visit_ModuloAssign(self, node: cst.ModuloAssign) -> bool:\n",
      "        self.modulo_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.modulo_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.modulo_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_ModuloAssign(self, node: cst.ModuloAssign) -> bool:\n",
      "    self.modulo_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.modulo_assign_count\n",
      "\n",
      "\n",
      "\n",
      "class MultiplyAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.multiply_assign_count = []\n",
      "\n",
      "    def visit_MultiplyAssign(self, node: cst.MultiplyAssign) -> bool:\n",
      "        self.multiply_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.multiply_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.multiply_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_MultiplyAssign(self, node: cst.MultiplyAssign) -> bool:\n",
      "    self.multiply_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.multiply_assign_count\n",
      "\n",
      "\n",
      "\n",
      "class PowerAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.power_assign_count = []\n",
      "\n",
      "    def visit_PowerAssign(self, node: cst.PowerAssign) -> bool:\n",
      "        self.power_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.power_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.power_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_PowerAssign(self, node: cst.PowerAssign) -> bool:\n",
      "    self.power_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.power_assign_count\n",
      "\n",
      "\n",
      "\n",
      "class RightShiftAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.right_shift_assign_count = []\n",
      "\n",
      "    def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n",
      "        self.right_shift_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.right_shift_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.right_shift_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n",
      "    self.right_shift_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.right_shift_assign_count\n",
      "\n",
      "\n",
      "\n",
      "class RightShiftAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.right_shift_assign_count = []\n",
      "\n",
      "    def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n",
      "        self.right_shift_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.right_shift_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.right_shift_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n",
      "    self.right_shift_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.right_shift_assign_count\n",
      "\n",
      "\n",
      "\n",
      "class SubtractAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.subtract_assign_count = []\n",
      "\n",
      "    def visit_SubtractAssign(self, node: cst.SubtractAssign) -> bool:\n",
      "        self.subtract_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.subtract_assign_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.subtract_assign_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_SubtractAssign(self, node: cst.SubtractAssign) -> bool:\n",
      "    self.subtract_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.subtract_assign_count\n",
      "\n",
      "\n",
      "\n",
      "# Miscellaneous Operators\n",
      "class AssignEqualOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.assign_equal_count = []\n",
      "\n",
      "    def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "        self.assign_equal_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.assign_equal_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.assign_equal_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "    self.assign_equal_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.assign_equal_count\n",
      "\n",
      "\n",
      "\n",
      "class ColonOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.colon_count = []\n",
      "\n",
      "    def visit_Colon(self, node: cst.Colon) -> bool:\n",
      "        self.colon_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.colon_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.colon_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Colon(self, node: cst.Colon) -> bool:\n",
      "    self.colon_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.colon_count\n",
      "\n",
      "\n",
      "\n",
      "class CommaOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.comma_count = []\n",
      "\n",
      "    def visit_Comma(self, node: cst.Comma) -> bool:\n",
      "        self.comma_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.comma_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.comma_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Comma(self, node: cst.Comma) -> bool:\n",
      "    self.comma_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.comma_count\n",
      "\n",
      "\n",
      "\n",
      "class DotOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.dot_count = []\n",
      "\n",
      "    def visit_Dot(self, node: cst.Dot) -> bool:\n",
      "        self.dot_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.dot_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.dot_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Dot(self, node: cst.Dot) -> bool:\n",
      "    self.dot_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.dot_count\n",
      "\n",
      "\n",
      "\n",
      "class ImportStarOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.import_star_count = []\n",
      "\n",
      "    def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n",
      "        self.import_star_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.import_star_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.import_star_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n",
      "    self.import_star_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.import_star_count\n",
      "\n",
      "\n",
      "\n",
      "class SemicolonOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.semicolon_count = []\n",
      "\n",
      "    def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n",
      "        self.semicolon_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.semicolon_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.semicolon_count = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n",
      "    self.semicolon_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.semicolon_count\n",
      "\n",
      "\n",
      "\n",
      "class DocstringCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.docstrings = []\n",
      "\n",
      "    def visit_Module(self, node: cst.Module) -> bool:\n",
      "        if node.body and isinstance(node.body[0].body, cst.SimpleStatementLine):\n",
      "            for stmt in node.body[0].body.body:\n",
      "                if isinstance(stmt, cst.Expr) and isinstance(stmt.value, cst.SimpleString):\n",
      "                    self.docstrings.append(stmt.value.value)\n",
      "        return True\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n",
      "        docstring = node.get_docstring()\n",
      "        if docstring is not None:\n",
      "            self.docstrings.append(docstring)\n",
      "        return True\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n",
      "        docstring = node.get_docstring()\n",
      "        if docstring is not None:\n",
      "            self.docstrings.append(docstring)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.docstrings\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.docstrings = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Module(self, node: cst.Module) -> bool:\n",
      "    if node.body and isinstance(node.body[0].body, cst.SimpleStatementLine):\n",
      "        for stmt in node.body[0].body.body:\n",
      "            if isinstance(stmt, cst.Expr) and isinstance(stmt.value, cst.SimpleString):\n",
      "                self.docstrings.append(stmt.value.value)\n",
      "    return True\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n",
      "    docstring = node.get_docstring()\n",
      "    if docstring is not None:\n",
      "        self.docstrings.append(docstring)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n",
      "    docstring = node.get_docstring()\n",
      "    if docstring is not None:\n",
      "        self.docstrings.append(docstring)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.docstrings\n",
      "\n",
      "\n",
      "\n",
      "class FunctionCallCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.function_calls = []\n",
      "\n",
      "    def visit_Call(self, node: cst.Call) -> bool:\n",
      "        if isinstance(node.func, cst.Name):\n",
      "            # Add the function name to the list\n",
      "            self.function_calls.append(node.func.value)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.function_calls\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.function_calls = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Call(self, node: cst.Call) -> bool:\n",
      "    if isinstance(node.func, cst.Name):\n",
      "        # Add the function name to the list\n",
      "        self.function_calls.append(node.func.value)\n",
      "    return True\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.function_calls\n",
      "\n",
      "\n",
      "\n",
      "class ArgumentTypeCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.argument_types = []\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n",
      "        # Collect argument types for functions\n",
      "        arg_types = []\n",
      "        for param in node.params.params:\n",
      "            if isinstance(param.annotation, cst.Annotation):\n",
      "                arg_types.append(param.annotation.annotation.value)\n",
      "            else:\n",
      "                arg_types.append(None)\n",
      "\n",
      "        self.argument_types.append(arg_types)\n",
      "        return True\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n",
      "        # Collect argument types for methods\n",
      "        for stmt in node.body.body:\n",
      "            if isinstance(stmt, cst.FunctionDef):\n",
      "                arg_types = []\n",
      "                for param in stmt.params.params:\n",
      "                    if isinstance(param.annotation, cst.Annotation):\n",
      "                        arg_types.append(param.annotation.annotation.value)\n",
      "                    else:\n",
      "                        arg_types.append(None)\n",
      "\n",
      "                self.argument_types.append(arg_types)\n",
      "\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.argument_types\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.argument_types = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n",
      "    # Collect argument types for functions\n",
      "    arg_types = []\n",
      "    for param in node.params.params:\n",
      "        if isinstance(param.annotation, cst.Annotation):\n",
      "            arg_types.append(param.annotation.annotation.value)\n",
      "        else:\n",
      "            arg_types.append(None)\n",
      "\n",
      "    self.argument_types.append(arg_types)\n",
      "    return True\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n",
      "    # Collect argument types for methods\n",
      "    for stmt in node.body.body:\n",
      "        if isinstance(stmt, cst.FunctionDef):\n",
      "            arg_types = []\n",
      "            for param in stmt.params.params:\n",
      "                if isinstance(param.annotation, cst.Annotation):\n",
      "                    arg_types.append(param.annotation.annotation.value)\n",
      "                else:\n",
      "                    arg_types.append(None)\n",
      "\n",
      "            self.argument_types.append(arg_types)\n",
      "\n",
      "    return True\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.argument_types\n",
      "\n",
      "\n",
      "\n",
      "class ImportCollector(cst.CSTVisitor):\n",
      "    def __init__(self, filename: str):\n",
      "        with open(filename, \"r\") as file:\n",
      "            self.module = cst.parse_module(file.read())\n",
      "        self.imports = []\n",
      "\n",
      "    def visit_Import(self, node: cst.Import) -> bool:\n",
      "        for name in node.names:\n",
      "            self.imports.append(cst.Module([node]))\n",
      "        return True\n",
      "\n",
      "    def visit_ImportFrom(self, node: cst.ImportFrom) -> bool:\n",
      "        module = node.module.value if node.module else \"\"\n",
      "        for name in node.names:\n",
      "            self.imports.append(cst.Module([node]))\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        import_code = [import_statement.code for import_statement in self.imports]\n",
      "        return import_code\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "]\n",
      "\n",
      "def __init__(self, filename: str):\n",
      "    with open(filename, \"r\") as file:\n",
      "        self.module = cst.parse_module(file.read())\n",
      "    self.imports = []\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "]\n",
      "\n",
      "\n",
      "def visit_Import(self, node: cst.Import) -> bool:\n",
      "    for name in node.names:\n",
      "        self.imports.append(cst.Module([node]))\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def visit_ImportFrom(self, node: cst.ImportFrom) -> bool:\n",
      "    module = node.module.value if node.module else \"\"\n",
      "    for name in node.names:\n",
      "        self.imports.append(cst.Module([node]))\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    import_code = [import_statement.code for import_statement in self.imports]\n",
      "    return import_code\n",
      "\n",
      "\n",
      "\n",
      "class IfStatementCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.if_statements = []\n",
      "\n",
      "    def visit_If(self, node: cst.If) -> bool:\n",
      "        self.if_statements.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.if_statements\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.if_statements = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_If(self, node: cst.If) -> bool:\n",
      "    self.if_statements.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.if_statements\n",
      "\n",
      "\n",
      "\n",
      "class BaseCompoundStatementCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.compound_statements = []\n",
      "\n",
      "    def visit_BaseCompoundStatement(self, node: cst.BaseCompoundStatement) -> bool:\n",
      "        self.compound_statements.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.compound_statements\n",
      "    \n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.compound_statements = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_BaseCompoundStatement(self, node: cst.BaseCompoundStatement) -> bool:\n",
      "    self.compound_statements.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.compound_statements\n",
      "\n",
      "\n",
      "class ForLoopCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.for_loops = []\n",
      "\n",
      "    def visit_For(self, node: cst.For) -> bool:\n",
      "        self.for_loops.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.for_loops\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.for_loops = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_For(self, node: cst.For) -> bool:\n",
      "    self.for_loops.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.for_loops\n",
      "\n",
      "\n",
      "\n",
      "class WhileLoopCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.while_loops = []\n",
      "\n",
      "    def visit_While(self, node: cst.While) -> bool:\n",
      "        self.while_loops.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.while_loops\n",
      "    \n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.while_loops = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_While(self, node: cst.While) -> bool:\n",
      "    self.while_loops.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.while_loops\n",
      "\n",
      "\n",
      "class TryExceptCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.try_excepts = []\n",
      "\n",
      "    def visit_Try(self, node: cst.Try) -> bool:\n",
      "        self.try_excepts.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.try_excepts\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.try_excepts = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Try(self, node: cst.Try) -> bool:\n",
      "    self.try_excepts.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.try_excepts\n",
      "\n",
      "\n",
      "\n",
      "class WithCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.withs = []\n",
      "\n",
      "    def visit_With(self, node: cst.With) -> bool:\n",
      "        self.withs.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.withs\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.withs = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_With(self, node: cst.With) -> bool:\n",
      "    self.withs.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.withs\n",
      "\n",
      "\n",
      "\n",
      "class VariableDeclarationCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.variable_declarations = []\n",
      "\n",
      "    def visit_Assign(self, node: cst.Assign) -> bool:\n",
      "        self.variable_declarations.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.variable_declarations\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.variable_declarations = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Assign(self, node: cst.Assign) -> bool:\n",
      "    self.variable_declarations.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.variable_declarations\n",
      "\n",
      "\n",
      "\n",
      "class ListComprehensionCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.list_comprehensions = []\n",
      "\n",
      "    def visit_ListComp(self, node: cst.ListComp) -> bool:\n",
      "        self.list_comprehensions.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.list_comprehensions\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.list_comprehensions = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_ListComp(self, node: cst.ListComp) -> bool:\n",
      "    self.list_comprehensions.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.list_comprehensions\n",
      "\n",
      "\n",
      "\n",
      "class DictComprehensionCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.dict_comprehensions = []\n",
      "\n",
      "    def visit_DictComp(self, node: cst.DictComp) -> bool:\n",
      "        self.dict_comprehensions.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.dict_comprehensions\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.dict_comprehensions = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_DictComp(self, node: cst.DictComp) -> bool:\n",
      "    self.dict_comprehensions.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.dict_comprehensions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Set Comprehension Collector\n",
      "class SetComprehensionCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.set_comprehensions = []\n",
      "\n",
      "    def visit_SetComp(self, node: cst.SetComp) -> bool:\n",
      "        self.set_comprehensions.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.set_comprehensions\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.set_comprehensions = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_SetComp(self, node: cst.SetComp) -> bool:\n",
      "    self.set_comprehensions.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.set_comprehensions\n",
      "\n",
      "\n",
      "\n",
      "# Generator Expression Collector\n",
      "class GeneratorExpressionCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.generator_expressions = []\n",
      "\n",
      "    def visit_GeneratorExp(self, node: cst.GeneratorExp) -> bool:\n",
      "        self.generator_expressions.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.generator_expressions\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.generator_expressions = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_GeneratorExp(self, node: cst.GeneratorExp) -> bool:\n",
      "    self.generator_expressions.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.generator_expressions\n",
      "\n",
      "\n",
      "\n",
      "# Yield Statement Collector\n",
      "class YieldCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.yields = []\n",
      "\n",
      "    def visit_Yield(self, node: cst.Yield) -> bool:\n",
      "        self.yields.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.yields\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.yields = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Yield(self, node: cst.Yield) -> bool:\n",
      "    self.yields.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.yields\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Return Statement Collector\n",
      "class ReturnCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.returns = []\n",
      "\n",
      "    def visit_Return(self, node: cst.Return) -> bool:\n",
      "        self.returns.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.returns\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.returns = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Return(self, node: cst.Return) -> bool:\n",
      "    self.returns.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.returns\n",
      "\n",
      "\n",
      "\n",
      "# Raise Statement Collector\n",
      "class RaiseCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.raises = []\n",
      "\n",
      "    def visit_Raise(self, node: cst.Raise) -> bool:\n",
      "        self.raises.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.raises\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.raises = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Raise(self, node: cst.Raise) -> bool:\n",
      "    self.raises.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.raises\n",
      "\n",
      "\n",
      "\n",
      "# Assert Statement Collector\n",
      "class AssertCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.asserts = []\n",
      "\n",
      "    def visit_Assert(self, node: cst.Assert) -> bool:\n",
      "        self.asserts.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.asserts\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.asserts = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Assert(self, node: cst.Assert) -> bool:\n",
      "    self.asserts.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.asserts\n",
      "\n",
      "\n",
      "\n",
      "# Break Statement Collector\n",
      "class BreakCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.breaks = []\n",
      "\n",
      "    def visit_Break(self, node: cst.Break) -> bool:\n",
      "        self.breaks.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.breaks\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.breaks = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Break(self, node: cst.Break) -> bool:\n",
      "    self.breaks.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.breaks\n",
      "\n",
      "\n",
      "\n",
      "# Continue Statement Collector\n",
      "class ContinueCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.continues = []\n",
      "\n",
      "    def visit_Continue(self, node: cst.Continue) -> bool:\n",
      "        self.continues.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.continues\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.continues = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Continue(self, node: cst.Continue) -> bool:\n",
      "    self.continues.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.continues\n",
      "\n",
      "\n",
      "\n",
      "# Pass Statement Collector\n",
      "class PassCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.passes = []\n",
      "\n",
      "    def visit_Pass(self, node: cst.Pass) -> bool:\n",
      "        self.passes.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.passes\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.passes = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Pass(self, node: cst.Pass) -> bool:\n",
      "    self.passes.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.passes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# With Statement Collector\n",
      "class WithStatementCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.with_statements = []\n",
      "\n",
      "    def visit_With(self, node: cst.With) -> bool:\n",
      "        self.with_statements.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.with_statements\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.with_statements = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_With(self, node: cst.With) -> bool:\n",
      "    self.with_statements.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.with_statements\n",
      "\n",
      "\n",
      "\n",
      "# Try Statement Collector\n",
      "class TryStatementCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.try_statements = []\n",
      "\n",
      "    def visit_Try(self, node: cst.Try) -> bool:\n",
      "        self.try_statements.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.try_statements\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.try_statements = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Try(self, node: cst.Try) -> bool:\n",
      "    self.try_statements.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.try_statements\n",
      "\n",
      "\n",
      "\n",
      "# Except Clause Collector\n",
      "class ExceptClauseCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.except_clauses = []\n",
      "\n",
      "    def visit_ExceptHandler(self, node: cst.ExceptHandler) -> bool:\n",
      "        self.except_clauses.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.except_clauses\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.except_clauses = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_ExceptHandler(self, node: cst.ExceptHandler) -> bool:\n",
      "    self.except_clauses.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.except_clauses\n",
      "\n",
      "\n",
      "\n",
      "# Lambda Function Collector\n",
      "class LambdaFunctionCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.lambda_functions = []\n",
      "\n",
      "    def visit_Lambda(self, node: cst.Lambda) -> bool:\n",
      "        self.lambda_functions.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.lambda_functions\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.lambda_functions = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Lambda(self, node: cst.Lambda) -> bool:\n",
      "    self.lambda_functions.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.lambda_functions\n",
      "\n",
      "\n",
      "\n",
      "# Global Statement Collector\n",
      "class GlobalStatementCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.global_statements = []\n",
      "\n",
      "    def visit_Global(self, node: cst.Global) -> bool:\n",
      "        self.global_statements.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.global_statements\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.global_statements = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Global(self, node: cst.Global) -> bool:\n",
      "    self.global_statements.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.global_statements\n",
      "\n",
      "\n",
      "\n",
      "# Nonlocal Statement Collector\n",
      "class NonlocalStatementCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.nonlocal_statements = []\n",
      "\n",
      "    def visit_Nonlocal(self, node: cst.Nonlocal) -> bool:\n",
      "        self.nonlocal_statements.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.nonlocal_statements\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.nonlocal_statements = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_Nonlocal(self, node: cst.Nonlocal) -> bool:\n",
      "    self.nonlocal_statements.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.nonlocal_statements\n",
      "\n",
      "\n",
      "\n",
      "class PassInserter(cst.CSTTransformer):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.inside_class = False\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef):\n",
      "        self.inside_class = True\n",
      "        return super().visit_ClassDef(node)\n",
      "\n",
      "    def leave_ClassDef(self, original_node: cst.ClassDef, updated_node: cst.ClassDef) -> cst.ClassDef:\n",
      "        self.inside_class = False\n",
      "        return updated_node\n",
      "\n",
      "    def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\n",
      "        body = updated_node.body\n",
      "        body_elements = body.body\n",
      "\n",
      "        # Check if the first element is a docstring\n",
      "\n",
      "        #if body_elements and matchers.matches(body_elements[0], matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\n",
      "            #docstring = body_elements[0]\n",
      "        docstrings = []\n",
      "        for element in body_elements:\n",
      "            if matchers.matches(element, matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\n",
      "                docstrings.append(element)\n",
      "\n",
      "        # Prepare new body\n",
      "        new_body = [cst.SimpleStatementLine(body=(cst.Pass(),))]\n",
      "        if docstrings[0] is not None:\n",
      "            new_body.insert(0, docstrings[0])\n",
      "\n",
      "        return updated_node.with_changes(\n",
      "            body=cst.IndentedBlock(body=tuple(new_body))\n",
      "        )\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"super\"\n",
      "\t\"tuple\"\n",
      "]\n",
      "\n",
      "def __init__(self):\n",
      "    super().__init__()\n",
      "    self.inside_class = False\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "]\n",
      "\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef):\n",
      "    self.inside_class = True\n",
      "    return super().visit_ClassDef(node)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "]\n",
      "\n",
      "\n",
      "def leave_ClassDef(self, original_node: cst.ClassDef, updated_node: cst.ClassDef) -> cst.ClassDef:\n",
      "    self.inside_class = False\n",
      "    return updated_node\n",
      "\n",
      "\n",
      "\n",
      "def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\n",
      "    body = updated_node.body\n",
      "    body_elements = body.body\n",
      "\n",
      "    # Check if the first element is a docstring\n",
      "\n",
      "    #if body_elements and matchers.matches(body_elements[0], matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\n",
      "        #docstring = body_elements[0]\n",
      "    docstrings = []\n",
      "    for element in body_elements:\n",
      "        if matchers.matches(element, matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\n",
      "            docstrings.append(element)\n",
      "\n",
      "    # Prepare new body\n",
      "    new_body = [cst.SimpleStatementLine(body=(cst.Pass(),))]\n",
      "    if docstrings[0] is not None:\n",
      "        new_body.insert(0, docstrings[0])\n",
      "\n",
      "    return updated_node.with_changes(\n",
      "        body=cst.IndentedBlock(body=tuple(new_body))\n",
      "    )\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"tuple\"\n",
      "]\n",
      "\n",
      "\n",
      "def generate_skeleton(code: str) -> str:\n",
      "    \"\"\"\n",
      "    Generate a skeleton for the given code, replacing function and class bodies with a pass statement.\n",
      "    \"\"\"\n",
      "    # Parse the code into a CST\n",
      "    module = cst.parse_module(code)\n",
      "\n",
      "    # Apply the transformer\n",
      "    transformer = PassInserter()\n",
      "    transformed_module = module.visit(transformer)\n",
      "\n",
      "    # Convert the transformed CST back to code\n",
      "    skeleton_code = transformed_module.code\n",
      "\n",
      "    return skeleton_code\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"PassInserter\"\n",
      "]\n",
      "\n",
      "\n",
      "class CodeReplacerVisitor(cst.CSTTransformer):\n",
      "    def __init__(self, filename_column: str, original_code_column: str, replacing_code_column: str):\n",
      "        self.filename_column = filename_column\n",
      "        self.original_code_column = original_code_column\n",
      "        self.replacing_code_column = replacing_code_column\n",
      "\n",
      "    def visit_Module(self, node: cst.Module) -> cst.Module:\n",
      "        # Get the filename from the node metadata\n",
      "        filename = node.metadata.get(self.filename_column)\n",
      "        if filename is None:\n",
      "            return node\n",
      "\n",
      "        # Load the content of the file\n",
      "        with open(filename, \"r\") as f:\n",
      "            file_content = f.read()\n",
      "\n",
      "        # Get the original code and replacing code from the node metadata\n",
      "        original_code = node.metadata.get(self.original_code_column)\n",
      "        replacing_code = node.metadata.get(self.replacing_code_column)\n",
      "\n",
      "        if original_code is None or replacing_code is None:\n",
      "            return node\n",
      "\n",
      "        # Replace the original code with the replacing code in the file content\n",
      "        modified_content = file_content.replace(original_code, replacing_code)\n",
      "\n",
      "        # Save the modified content back to the file\n",
      "        with open(filename, \"w\") as f:\n",
      "            f.write(modified_content)\n",
      "\n",
      "        # Parse the modified content to update the node\n",
      "        return cst.parse_module(modified_content)\n",
      "\n",
      "    def leave_Module(self, original_node: cst.Module, updated_node: cst.Module) -> cst.Module:\n",
      "        # Copy the metadata from the original node to the updated node\n",
      "        updated_node.metadata = original_node.metadata\n",
      "        return updated_node\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"open\"\n",
      "]\n",
      "\n",
      "def __init__(self, filename_column: str, original_code_column: str, replacing_code_column: str):\n",
      "    self.filename_column = filename_column\n",
      "    self.original_code_column = original_code_column\n",
      "    self.replacing_code_column = replacing_code_column\n",
      "\n",
      "\n",
      "\n",
      "def visit_Module(self, node: cst.Module) -> cst.Module:\n",
      "    # Get the filename from the node metadata\n",
      "    filename = node.metadata.get(self.filename_column)\n",
      "    if filename is None:\n",
      "        return node\n",
      "\n",
      "    # Load the content of the file\n",
      "    with open(filename, \"r\") as f:\n",
      "        file_content = f.read()\n",
      "\n",
      "    # Get the original code and replacing code from the node metadata\n",
      "    original_code = node.metadata.get(self.original_code_column)\n",
      "    replacing_code = node.metadata.get(self.replacing_code_column)\n",
      "\n",
      "    if original_code is None or replacing_code is None:\n",
      "        return node\n",
      "\n",
      "    # Replace the original code with the replacing code in the file content\n",
      "    modified_content = file_content.replace(original_code, replacing_code)\n",
      "\n",
      "    # Save the modified content back to the file\n",
      "    with open(filename, \"w\") as f:\n",
      "        f.write(modified_content)\n",
      "\n",
      "    # Parse the modified content to update the node\n",
      "    return cst.parse_module(modified_content)\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"open\"\n",
      "]\n",
      "\n",
      "\n",
      "def leave_Module(self, original_node: cst.Module, updated_node: cst.Module) -> cst.Module:\n",
      "    # Copy the metadata from the original node to the updated node\n",
      "    updated_node.metadata = original_node.metadata\n",
      "    return updated_node\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Unary Operators\n",
      "class BitInvertOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_invert_operator_count = 0\n",
      "\n",
      "    def visit_BitInvert(self, node: cst.BitInvert) -> bool:\n",
      "        self.bit_invert_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_invert_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_invert_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitInvert(self, node: cst.BitInvert) -> bool:\n",
      "    self.bit_invert_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_invert_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class MinusOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.minus_operator_count = 0\n",
      "\n",
      "    def visit_Minus(self, node: cst.Minus) -> bool:\n",
      "        self.minus_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.minus_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.minus_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Minus(self, node: cst.Minus) -> bool:\n",
      "    self.minus_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.minus_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class NotOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.not_operator_count = 0\n",
      "\n",
      "    def visit_Not(self, node: cst.Not) -> bool:\n",
      "        self.not_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.not_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.not_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Not(self, node: cst.Not) -> bool:\n",
      "    self.not_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.not_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class PlusOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.plus_operator_count = 0\n",
      "\n",
      "    def visit_Plus(self, node: cst.Plus) -> bool:\n",
      "        self.plus_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.plus_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.plus_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Plus(self, node: cst.Plus) -> bool:\n",
      "    self.plus_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.plus_operator_count\n",
      "\n",
      "\n",
      "\n",
      "# Boolean Operators\n",
      "class AndOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.and_operator_count = 0\n",
      "\n",
      "    def visit_And(self, node: cst.And) -> bool:\n",
      "        self.and_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.and_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.and_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_And(self, node: cst.And) -> bool:\n",
      "    self.and_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.and_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class OrOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.or_operator_count = 0\n",
      "\n",
      "    def visit_Or(self, node: cst.Or) -> bool:\n",
      "        self.or_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.or_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.or_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Or(self, node: cst.Or) -> bool:\n",
      "    self.or_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.or_operator_count\n",
      "\n",
      "\n",
      "\n",
      "# Binary Operators\n",
      "class AddOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.add_operator_count = 0\n",
      "\n",
      "    def visit_Add(self, node: cst.Add) -> bool:\n",
      "        self.add_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.add_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.add_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Add(self, node: cst.Add) -> bool:\n",
      "    self.add_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.add_operator_count\n",
      "\n",
      "\n",
      "\n",
      "# Binary Operators\n",
      "class BitAndOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_and_operator_count = 0\n",
      "\n",
      "    def visit_BitAnd(self, node: cst.BitAnd) -> bool:\n",
      "        self.bit_and_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_and_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_and_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitAnd(self, node: cst.BitAnd) -> bool:\n",
      "    self.bit_and_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_and_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class BitOrOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_or_operator_count = 0\n",
      "\n",
      "    def visit_BitOr(self, node: cst.BitOr) -> bool:\n",
      "        self.bit_or_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_or_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_or_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitOr(self, node: cst.BitOr) -> bool:\n",
      "    self.bit_or_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_or_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class BitXorOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_xor_operator_count = 0\n",
      "\n",
      "    def visit_BitXor(self, node: cst.BitXor) -> bool:\n",
      "        self.bit_xor_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_xor_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_xor_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitXor(self, node: cst.BitXor) -> bool:\n",
      "    self.bit_xor_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_xor_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class DivideOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.divide_operator_count = 0\n",
      "\n",
      "    def visit_Divide(self, node: cst.Divide) -> bool:\n",
      "        self.divide_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.divide_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.divide_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Divide(self, node: cst.Divide) -> bool:\n",
      "    self.divide_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.divide_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class FloorDivideOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.floor_divide_operator_count = 0\n",
      "\n",
      "    def visit_FloorDivide(self, node: cst.FloorDivide) -> bool:\n",
      "        self.floor_divide_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.floor_divide_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.floor_divide_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_FloorDivide(self, node: cst.FloorDivide) -> bool:\n",
      "    self.floor_divide_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.floor_divide_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class LeftShiftOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.left_shift_operator_count = 0\n",
      "\n",
      "    def visit_LeftShift(self, node: cst.LeftShift) -> bool:\n",
      "        self.left_shift_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.left_shift_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.left_shift_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_LeftShift(self, node: cst.LeftShift) -> bool:\n",
      "    self.left_shift_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.left_shift_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class MatrixMultiplyOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.matrix_multiply_operator_count = 0\n",
      "\n",
      "    def visit_MatrixMultiply(self, node: cst.MatrixMultiply) -> bool:\n",
      "        self.matrix_multiply_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.matrix_multiply_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.matrix_multiply_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_MatrixMultiply(self, node: cst.MatrixMultiply) -> bool:\n",
      "    self.matrix_multiply_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.matrix_multiply_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class ModuloOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.modulo_operator_count = 0\n",
      "\n",
      "    def visit_Modulo(self, node: cst.Modulo) -> bool:\n",
      "        self.modulo_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.modulo_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.modulo_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Modulo(self, node: cst.Modulo) -> bool:\n",
      "    self.modulo_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.modulo_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class MultiplyOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.multiply_operator_count = 0\n",
      "\n",
      "    def visit_Multiply(self, node: cst.Multiply) -> bool:\n",
      "        self.multiply_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.multiply_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.multiply_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Multiply(self, node: cst.Multiply) -> bool:\n",
      "    self.multiply_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.multiply_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class PowerOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.power_operator_count = 0\n",
      "\n",
      "    def visit_Power(self, node: cst.Power) -> bool:\n",
      "        self.power_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.power_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.power_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Power(self, node: cst.Power) -> bool:\n",
      "    self.power_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.power_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class RightShiftOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.right_shift_operator_count = 0\n",
      "\n",
      "    def visit_RightShift(self, node: cst.RightShift) -> bool:\n",
      "        self.right_shift_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.right_shift_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.right_shift_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_RightShift(self, node: cst.RightShift) -> bool:\n",
      "    self.right_shift_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.right_shift_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class SubtractOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.subtract_operator_count = 0\n",
      "\n",
      "    def visit_Subtract(self, node: cst.Subtract) -> bool:\n",
      "        self.subtract_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.subtract_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.subtract_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Subtract(self, node: cst.Subtract) -> bool:\n",
      "    self.subtract_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.subtract_operator_count\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Comparison Operators\n",
      "class EqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.equal_operator_count = 0\n",
      "\n",
      "    def visit_Equal(self, node: cst.Equal) -> bool:\n",
      "        self.equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.equal_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.equal_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Equal(self, node: cst.Equal) -> bool:\n",
      "    self.equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.equal_operator_count\n",
      "\n",
      "\n",
      "\n",
      "# Comparison Operators\n",
      "class EqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.equal_operator_count = 0\n",
      "\n",
      "    def visit_Equal(self, node: cst.Equal) -> bool:\n",
      "        self.equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.equal_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.equal_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Equal(self, node: cst.Equal) -> bool:\n",
      "    self.equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.equal_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class GreaterThanOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.greater_than_operator_count = 0\n",
      "\n",
      "    def visit_GreaterThan(self, node: cst.GreaterThan) -> bool:\n",
      "        self.greater_than_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.greater_than_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.greater_than_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_GreaterThan(self, node: cst.GreaterThan) -> bool:\n",
      "    self.greater_than_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.greater_than_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class GreaterThanEqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.greater_than_equal_operator_count = 0\n",
      "\n",
      "    def visit_GreaterThanEqual(self, node: cst.GreaterThanEqual) -> bool:\n",
      "        self.greater_than_equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.greater_than_equal_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.greater_than_equal_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_GreaterThanEqual(self, node: cst.GreaterThanEqual) -> bool:\n",
      "    self.greater_than_equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.greater_than_equal_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class InOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.in_operator_count = 0\n",
      "\n",
      "    def visit_In(self, node: cst.In) -> bool:\n",
      "        self.in_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.in_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.in_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_In(self, node: cst.In) -> bool:\n",
      "    self.in_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.in_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class IsOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.is_operator_count = 0\n",
      "\n",
      "    def visit_Is(self, node: cst.Is) -> bool:\n",
      "        self.is_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.is_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.is_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Is(self, node: cst.Is) -> bool:\n",
      "    self.is_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.is_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class LessThanOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.less_than_operator_count = 0\n",
      "\n",
      "    def visit_LessThan(self, node: cst.LessThan) -> bool:\n",
      "        self.less_than_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.less_than_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.less_than_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_LessThan(self, node: cst.LessThan) -> bool:\n",
      "    self.less_than_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.less_than_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class LessThanEqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.less_than_equal_operator_count = 0\n",
      "\n",
      "    def visit_LessThanEqual(self, node: cst.LessThanEqual) -> bool:\n",
      "        self.less_than_equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.less_than_equal_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.less_than_equal_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_LessThanEqual(self, node: cst.LessThanEqual) -> bool:\n",
      "    self.less_than_equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.less_than_equal_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class NotEqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.not_equal_operator_count = 0\n",
      "\n",
      "    def visit_NotEqual(self, node: cst.NotEqual) -> bool:\n",
      "        self.not_equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.not_equal_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.not_equal_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_NotEqual(self, node: cst.NotEqual) -> bool:\n",
      "    self.not_equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.not_equal_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class IsNotOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.is_not_operator_count = 0\n",
      "\n",
      "    def visit_IsNot(self, node: cst.IsNot) -> bool:\n",
      "        self.is_not_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.is_not_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.is_not_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_IsNot(self, node: cst.IsNot) -> bool:\n",
      "    self.is_not_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.is_not_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class NotInOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.not_in_operator_count = 0\n",
      "\n",
      "    def visit_NotIn(self, node: cst.NotIn) -> bool:\n",
      "        self.not_in_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.not_in_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.not_in_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_NotIn(self, node: cst.NotIn) -> bool:\n",
      "    self.not_in_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.not_in_operator_count\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Augmented Assignment Operators\n",
      "class AddAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.add_assign_operator_count = 0\n",
      "\n",
      "    def visit_AddAssign(self, node: cst.AddAssign) -> bool:\n",
      "        self.add_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.add_assign_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.add_assign_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_AddAssign(self, node: cst.AddAssign) -> bool:\n",
      "    self.add_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.add_assign_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class BitAndAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_and_assign_operator_count = 0\n",
      "\n",
      "    def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n",
      "        self.bit_and_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_and_assign_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_and_assign_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n",
      "    self.bit_and_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_and_assign_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class BitOrAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_or_assign_operator_count = 0\n",
      "\n",
      "    def visit_BitOrAssign(self, node: cst.BitOrAssign) -> bool:\n",
      "        self.bit_or_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_or_assign_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_or_assign_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitOrAssign(self, node: cst.BitOrAssign) -> bool:\n",
      "    self.bit_or_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_or_assign_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class BitXorAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_xor_assign_operator_count = 0\n",
      "\n",
      "    def visit_BitXorAssign(self, node: cst.BitXorAssign) -> bool:\n",
      "        self.bit_xor_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_xor_assign_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_xor_assign_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_BitXorAssign(self, node: cst.BitXorAssign) -> bool:\n",
      "    self.bit_xor_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_xor_assign_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class DivideAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.divide_assign_operator_count = 0\n",
      "\n",
      "    def visit_DivideAssign(self, node: cst.DivideAssign) -> bool:\n",
      "        self.divide_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.divide_assign_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.divide_assign_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_DivideAssign(self, node: cst.DivideAssign) -> bool:\n",
      "    self.divide_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.divide_assign_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class FloorDivideAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.floor_divide_assign_operator_count = 0\n",
      "\n",
      "    def visit_FloorDivideAssign(self, node: cst.FloorDivideAssign) -> bool:\n",
      "        self.floor_divide_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.floor_divide_assign_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.floor_divide_assign_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_FloorDivideAssign(self, node: cst.FloorDivideAssign) -> bool:\n",
      "    self.floor_divide_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.floor_divide_assign_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class LeftShiftAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.left_shift_assign_operator_count = 0\n",
      "\n",
      "    def visit_LeftShiftAssign(self, node: cst.LeftShiftAssign) -> bool:\n",
      "        self.left_shift_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.left_shift_assign_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.left_shift_assign_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_LeftShiftAssign(self, node: cst.LeftShiftAssign) -> bool:\n",
      "    self.left_shift_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.left_shift_assign_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class MatrixMultiplyAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.matrix_multiply_assign_operator_count = 0\n",
      "\n",
      "    def visit_MatrixMultiplyAssign(self, node: cst.MatrixMultiplyAssign) -> bool:\n",
      "        self.matrix_multiply_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.matrix_multiply_assign_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.matrix_multiply_assign_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_MatrixMultiplyAssign(self, node: cst.MatrixMultiplyAssign) -> bool:\n",
      "    self.matrix_multiply_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.matrix_multiply_assign_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class ModuloAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.modulo_assign_operator_count = 0\n",
      "\n",
      "    def visit_ModuloAssign(self, node: cst.ModuloAssign) -> bool:\n",
      "        self.modulo_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.modulo_assign_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.modulo_assign_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_ModuloAssign(self, node: cst.ModuloAssign) -> bool:\n",
      "    self.modulo_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.modulo_assign_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class MultiplyAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.multiply_assign_operator_count = 0\n",
      "\n",
      "    def visit_MultiplyAssign(self, node: cst.MultiplyAssign) -> bool:\n",
      "        self.multiply_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.multiply_assign_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.multiply_assign_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_MultiplyAssign(self, node: cst.MultiplyAssign) -> bool:\n",
      "    self.multiply_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.multiply_assign_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class PowerAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.power_assign_operator_count = 0\n",
      "\n",
      "    def visit_PowerAssign(self, node: cst.PowerAssign) -> bool:\n",
      "        self.power_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.power_assign_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.power_assign_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_PowerAssign(self, node: cst.PowerAssign) -> bool:\n",
      "    self.power_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.power_assign_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class RightShiftAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.right_shift_assign_operator_count = 0\n",
      "\n",
      "    def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n",
      "        self.right_shift_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.right_shift_assign_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.right_shift_assign_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n",
      "    self.right_shift_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.right_shift_assign_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class SubtractAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.subtract_assign_operator_count = 0\n",
      "\n",
      "    def visit_SubtractAssign(self, node: cst.SubtractAssign) -> bool:\n",
      "        self.subtract_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.subtract_assign_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.subtract_assign_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_SubtractAssign(self, node: cst.SubtractAssign) -> bool:\n",
      "    self.subtract_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.subtract_assign_operator_count\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Miscellaneous Operators\n",
      "class AssignEqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.assign_equal_operator_count = 0\n",
      "\n",
      "    def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "        self.assign_equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.assign_equal_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.assign_equal_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "    self.assign_equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.assign_equal_operator_count\n",
      "\n",
      "\n",
      "\n",
      "# Miscellaneous Operators\n",
      "class AssignEqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.assign_equal_operator_count = 0\n",
      "\n",
      "    def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "        self.assign_equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.assign_equal_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.assign_equal_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "    self.assign_equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.assign_equal_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class ColonOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.colon_operator_count = 0\n",
      "\n",
      "    def visit_Colon(self, node: cst.Colon) -> bool:\n",
      "        self.colon_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.colon_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.colon_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Colon(self, node: cst.Colon) -> bool:\n",
      "    self.colon_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.colon_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class CommaOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.comma_operator_count = 0\n",
      "\n",
      "    def visit_Comma(self, node: cst.Comma) -> bool:\n",
      "        self.comma_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.comma_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.comma_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Comma(self, node: cst.Comma) -> bool:\n",
      "    self.comma_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.comma_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class DotOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.dot_operator_count = 0\n",
      "\n",
      "    def visit_Dot(self, node: cst.Dot) -> bool:\n",
      "        self.dot_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.dot_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.dot_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Dot(self, node: cst.Dot) -> bool:\n",
      "    self.dot_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.dot_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class ImportStarOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.import_star_operator_count = 0\n",
      "\n",
      "    def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n",
      "        self.import_star_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.import_star_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.import_star_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n",
      "    self.import_star_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.import_star_operator_count\n",
      "\n",
      "\n",
      "\n",
      "class SemicolonOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.semicolon_operator_count = 0\n",
      "\n",
      "    def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n",
      "        self.semicolon_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.semicolon_operator_count\n",
      "\n",
      "\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.semicolon_operator_count = 0\n",
      "\n",
      "\n",
      "\n",
      "def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n",
      "    self.semicolon_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.semicolon_operator_count\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class VectorThread(BaseThread, MemoryIndex):\n",
      "    \"\"\"vector BaseThread, creates a faiss index with the messages and allows to search for similar messages, memory BaseThread can return messages in either similarity or chronological order\n",
      "    add a parameter to choose the order of the messages\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\n",
      "        BaseThread.__init__(self, name=name, max_memory=None)\n",
      "        MemoryIndex.__init__(self, index=None, name=name)\n",
      "        self.max_context = max_context\n",
      "        self.use_mark = use_mark\n",
      "        self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "\n",
      "    def index_message(self, message: str, verbose: bool = False):\n",
      "        \"\"\"index a message in the faiss index, the message is embedded and added to the index\n",
      "        self.values and self.embeddings and self.index are updated\n",
      "        \"\"\"\n",
      "\n",
      "        self.add_to_index(value=message, verbose=verbose)\n",
      "\n",
      "    def add_message(self, message_dict: dict, verbose: bool = False):\n",
      "        \"\"\"add a message to the memory thread, the message is embedded and added to the index\n",
      "        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\n",
      "        \"\"\"\n",
      "        # print(\"checking the dict\")\n",
      "        message_dict = check_dict(message_dict)\n",
      "        # print(\"trying to add the message\")\n",
      "        BaseThread.add_message(self, message_dict)\n",
      "        # print(message_dict)\n",
      "        message = message_dict[\"content\"]\n",
      "        self.index_message(message, verbose=verbose)\n",
      "        return True\n",
      "\n",
      "    def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\n",
      "        \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\n",
      "        if self.use_mark:\n",
      "            query = mark_question(query)\n",
      "        return MemoryIndex.token_bound_query(self, query, k, max_tokens)\n",
      "\n",
      "    def sorted_query(\n",
      "        self,\n",
      "        query,\n",
      "        k: int = 10,\n",
      "        max_tokens: int = 4000,\n",
      "        reverse: bool = False,\n",
      "        return_from_thread=True,\n",
      "    ) -> Tuple[List[str], List[float], List[int]]:\n",
      "        \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\n",
      "        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\n",
      "        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\n",
      "        \"\"\"\n",
      "        unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\n",
      "\n",
      "        num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\n",
      "        # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\n",
      "        unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\n",
      "\n",
      "        # Sort the indices\n",
      "        sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\n",
      "        \n",
      "        print(sorted_indices)\n",
      "        print(type(sorted_indices))\n",
      "\n",
      "        if reverse:\n",
      "            sorted_indices.reverse()\n",
      "\n",
      "        # Fetch the sorted messages, scores, and indices based on sorted_indices\n",
      "        sorted_messages = [unsorted_messages[i] for i in sorted_indices]\n",
      "        sorted_scores = [unsorted_scores[i] for i in sorted_indices]\n",
      "        sorted_indices = [unsorted_indices[i] for i in sorted_indices]\n",
      "\n",
      "        if return_from_thread:\n",
      "            sorted_messages = [self.memory_thread[i] for i in sorted_indices]\n",
      "\n",
      "        return sorted_messages, sorted_scores, sorted_indices\n",
      "    def weighted_query(\n",
      "        self,\n",
      "        query,\n",
      "        k: int = 10,\n",
      "        max_tokens: int = 4000,\n",
      "        decay_factor: float = 0.1,\n",
      "        temporal_weight: float = 0.5,\n",
      "        order_by: str = \"chronological\",\n",
      "        reverse: bool = False,\n",
      "    ) -> list:\n",
      "        \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\n",
      "        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\n",
      "        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\n",
      "        The order_by parameter controls the order of the results. If it is set to 'similarity', the results are sorted in similarity order. If it is set to 'chronological', the results are sorted in chronological order with the most recent message first.\n",
      "        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\n",
      "        \"\"\"\n",
      "        # Validate order_by parameter\n",
      "        if order_by not in (\"similarity\", \"chronological\"):\n",
      "            raise ValueError(\n",
      "                \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
      "            )\n",
      "\n",
      "        # Get similarity-based results\n",
      "        sim_messages, sim_scores, sim_indices = self.sorted_query(\n",
      "            query, k, max_tokens=max_tokens\n",
      "        )\n",
      "\n",
      "        # Get token-bound history\n",
      "        hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\n",
      "\n",
      "        # Combine messages and indices\n",
      "        combined_messages = sim_messages + hist_messages\n",
      "        combined_indices = sim_indices + hist_indices\n",
      "\n",
      "        # Create the local_index and populate it\n",
      "        self.local_index = MemoryIndex(name=\"local_index\")\n",
      "        for message in combined_messages:\n",
      "            self.local_index.add_to_index(value=message, verbose=False)\n",
      "\n",
      "        # Perform a new query on the combined index\n",
      "        (\n",
      "            new_query_results,\n",
      "            new_query_scores,\n",
      "            new_query_indices,\n",
      "        ) = self.local_index.token_bound_query(\n",
      "            query, k=len(combined_messages), max_tokens=max_tokens\n",
      "        )\n",
      "\n",
      "        # Compute temporal weights\n",
      "        temporal_weights = [\n",
      "            np.exp(-decay_factor * i) for i in range(len(combined_messages))\n",
      "        ]\n",
      "        temporal_weights = [\n",
      "            w / sum(temporal_weights) for w in temporal_weights\n",
      "        ]  # Normalize the temporal weights\n",
      "\n",
      "        # Combine similarity scores and temporal weights\n",
      "        weighted_scores = []\n",
      "        for i in range(len(new_query_scores)):\n",
      "            sim_score = new_query_scores[i]\n",
      "            temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\n",
      "            weighted_score = (\n",
      "                1 - temporal_weight\n",
      "            ) * sim_score + temporal_weight * temp_weight\n",
      "            weighted_scores.append(weighted_score)\n",
      "\n",
      "        # Sort the results based on the order_by parameter\n",
      "        if order_by == \"similarity\":\n",
      "            sorting_key = lambda k: weighted_scores[k]\n",
      "        elif order_by == \"chronological\":  # order_by == 'chronological'\n",
      "            sorting_key = lambda k: new_query_indices[k]\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
      "            )\n",
      "\n",
      "        sorted_indices = [\n",
      "            new_query_indices[i]\n",
      "            for i in sorted(\n",
      "                range(len(new_query_indices)), key=sorting_key, reverse=not reverse\n",
      "            )\n",
      "        ]\n",
      "        sorted_results = [\n",
      "            new_query_results[i]\n",
      "            for i in sorted(\n",
      "                range(len(new_query_results)), key=sorting_key, reverse=not reverse\n",
      "            )\n",
      "        ]\n",
      "        sorted_scores = [\n",
      "            weighted_scores[i]\n",
      "            for i in sorted(\n",
      "                range(len(weighted_scores)), key=sorting_key, reverse=not reverse\n",
      "            )\n",
      "        ]\n",
      "\n",
      "        # Return only the top k results without exceeding max_tokens\n",
      "        final_results, final_scores, final_indices = [], [], []\n",
      "        current_tokens = 0\n",
      "        for i in range(min(k, len(sorted_results))):\n",
      "            message_tokens = self.get_message_tokens(sorted_results[i])\n",
      "            if current_tokens + message_tokens <= max_tokens:\n",
      "                final_results.append(sorted_results[i])\n",
      "                final_scores.append(sorted_scores[i])\n",
      "                final_indices.append(sorted_indices[i])\n",
      "                current_tokens += message_tokens\n",
      "            else:\n",
      "                break\n",
      "\n",
      "        return final_results, final_scores, final_indices\n",
      "\n",
      "Function calls: shape: (33,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"check_dict\"\n",
      "\t\"mark_question\"\n",
      "\t\"min\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"int\"\n",
      "\t\"sorted\"\n",
      "\t\"range\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"type\"\n",
      "\t…\n",
      "\t\"ValueError\"\n",
      "\t\"sorted\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"sorted\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"sorted\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"range\"\n",
      "\t\"min\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\n",
      "    BaseThread.__init__(self, name=name, max_memory=None)\n",
      "    MemoryIndex.__init__(self, index=None, name=name)\n",
      "    self.max_context = max_context\n",
      "    self.use_mark = use_mark\n",
      "    self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "\n",
      "\n",
      "\n",
      "def index_message(self, message: str, verbose: bool = False):\n",
      "    \"\"\"index a message in the faiss index, the message is embedded and added to the index\n",
      "        self.values and self.embeddings and self.index are updated\n",
      "        \"\"\"\n",
      "\n",
      "    self.add_to_index(value=message, verbose=verbose)\n",
      "\n",
      "\n",
      "\n",
      "def add_message(self, message_dict: dict, verbose: bool = False):\n",
      "    \"\"\"add a message to the memory thread, the message is embedded and added to the index\n",
      "        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\n",
      "        \"\"\"\n",
      "    # print(\"checking the dict\")\n",
      "    message_dict = check_dict(message_dict)\n",
      "    # print(\"trying to add the message\")\n",
      "    BaseThread.add_message(self, message_dict)\n",
      "    # print(message_dict)\n",
      "    message = message_dict[\"content\"]\n",
      "    self.index_message(message, verbose=verbose)\n",
      "    return True\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"check_dict\"\n",
      "]\n",
      "\n",
      "\n",
      "def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\n",
      "    \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\n",
      "    if self.use_mark:\n",
      "        query = mark_question(query)\n",
      "    return MemoryIndex.token_bound_query(self, query, k, max_tokens)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_question\"\n",
      "]\n",
      "\n",
      "\n",
      "def sorted_query(\n",
      "    self,\n",
      "    query,\n",
      "    k: int = 10,\n",
      "    max_tokens: int = 4000,\n",
      "    reverse: bool = False,\n",
      "    return_from_thread=True,\n",
      ") -> Tuple[List[str], List[float], List[int]]:\n",
      "    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\n",
      "        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\n",
      "        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\n",
      "        \"\"\"\n",
      "    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\n",
      "\n",
      "    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\n",
      "    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\n",
      "    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\n",
      "\n",
      "    # Sort the indices\n",
      "    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\n",
      "    \n",
      "    print(sorted_indices)\n",
      "    print(type(sorted_indices))\n",
      "\n",
      "    if reverse:\n",
      "        sorted_indices.reverse()\n",
      "\n",
      "    # Fetch the sorted messages, scores, and indices based on sorted_indices\n",
      "    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\n",
      "    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\n",
      "    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\n",
      "\n",
      "    if return_from_thread:\n",
      "        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\n",
      "\n",
      "    return sorted_messages, sorted_scores, sorted_indices\n",
      "\n",
      "Function calls: shape: (10,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"min\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"int\"\n",
      "\t\"sorted\"\n",
      "\t\"range\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"type\"\n",
      "]\n",
      "\n",
      "def weighted_query(\n",
      "    self,\n",
      "    query,\n",
      "    k: int = 10,\n",
      "    max_tokens: int = 4000,\n",
      "    decay_factor: float = 0.1,\n",
      "    temporal_weight: float = 0.5,\n",
      "    order_by: str = \"chronological\",\n",
      "    reverse: bool = False,\n",
      ") -> list:\n",
      "    \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\n",
      "        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\n",
      "        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\n",
      "        The order_by parameter controls the order of the results. If it is set to 'similarity', the results are sorted in similarity order. If it is set to 'chronological', the results are sorted in chronological order with the most recent message first.\n",
      "        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\n",
      "        \"\"\"\n",
      "    # Validate order_by parameter\n",
      "    if order_by not in (\"similarity\", \"chronological\"):\n",
      "        raise ValueError(\n",
      "            \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
      "        )\n",
      "\n",
      "    # Get similarity-based results\n",
      "    sim_messages, sim_scores, sim_indices = self.sorted_query(\n",
      "        query, k, max_tokens=max_tokens\n",
      "    )\n",
      "\n",
      "    # Get token-bound history\n",
      "    hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\n",
      "\n",
      "    # Combine messages and indices\n",
      "    combined_messages = sim_messages + hist_messages\n",
      "    combined_indices = sim_indices + hist_indices\n",
      "\n",
      "    # Create the local_index and populate it\n",
      "    self.local_index = MemoryIndex(name=\"local_index\")\n",
      "    for message in combined_messages:\n",
      "        self.local_index.add_to_index(value=message, verbose=False)\n",
      "\n",
      "    # Perform a new query on the combined index\n",
      "    (\n",
      "        new_query_results,\n",
      "        new_query_scores,\n",
      "        new_query_indices,\n",
      "    ) = self.local_index.token_bound_query(\n",
      "        query, k=len(combined_messages), max_tokens=max_tokens\n",
      "    )\n",
      "\n",
      "    # Compute temporal weights\n",
      "    temporal_weights = [\n",
      "        np.exp(-decay_factor * i) for i in range(len(combined_messages))\n",
      "    ]\n",
      "    temporal_weights = [\n",
      "        w / sum(temporal_weights) for w in temporal_weights\n",
      "    ]  # Normalize the temporal weights\n",
      "\n",
      "    # Combine similarity scores and temporal weights\n",
      "    weighted_scores = []\n",
      "    for i in range(len(new_query_scores)):\n",
      "        sim_score = new_query_scores[i]\n",
      "        temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\n",
      "        weighted_score = (\n",
      "            1 - temporal_weight\n",
      "        ) * sim_score + temporal_weight * temp_weight\n",
      "        weighted_scores.append(weighted_score)\n",
      "\n",
      "    # Sort the results based on the order_by parameter\n",
      "    if order_by == \"similarity\":\n",
      "        sorting_key = lambda k: weighted_scores[k]\n",
      "    elif order_by == \"chronological\":  # order_by == 'chronological'\n",
      "        sorting_key = lambda k: new_query_indices[k]\n",
      "    else:\n",
      "        raise ValueError(\n",
      "            \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
      "        )\n",
      "\n",
      "    sorted_indices = [\n",
      "        new_query_indices[i]\n",
      "        for i in sorted(\n",
      "            range(len(new_query_indices)), key=sorting_key, reverse=not reverse\n",
      "        )\n",
      "    ]\n",
      "    sorted_results = [\n",
      "        new_query_results[i]\n",
      "        for i in sorted(\n",
      "            range(len(new_query_results)), key=sorting_key, reverse=not reverse\n",
      "        )\n",
      "    ]\n",
      "    sorted_scores = [\n",
      "        weighted_scores[i]\n",
      "        for i in sorted(\n",
      "            range(len(weighted_scores)), key=sorting_key, reverse=not reverse\n",
      "        )\n",
      "    ]\n",
      "\n",
      "    # Return only the top k results without exceeding max_tokens\n",
      "    final_results, final_scores, final_indices = [], [], []\n",
      "    current_tokens = 0\n",
      "    for i in range(min(k, len(sorted_results))):\n",
      "        message_tokens = self.get_message_tokens(sorted_results[i])\n",
      "        if current_tokens + message_tokens <= max_tokens:\n",
      "            final_results.append(sorted_results[i])\n",
      "            final_scores.append(sorted_scores[i])\n",
      "            final_indices.append(sorted_indices[i])\n",
      "            current_tokens += message_tokens\n",
      "        else:\n",
      "            break\n",
      "\n",
      "    return final_results, final_scores, final_indices\n",
      "\n",
      "Function calls: shape: (21,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"ValueError\"\n",
      "\t\"MemoryIndex\"\n",
      "\t\"len\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"sum\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"sorted\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"sorted\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"sorted\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"range\"\n",
      "\t\"min\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class BaseThread:\n",
      "    \"\"\"\n",
      "    This class is used to keep track of the memory thread of a conversation and the total number of tokens.\n",
      "    All conversation memories should subclass this class. If max_memory is None, it has\n",
      "    no limit to the number of tokens that can be stored in the memory thread.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        name: str = \"memory\",\n",
      "        max_memory: Optional[int] = None,\n",
      "        tokenizer: Optional[Any] = None,\n",
      "        save_path: str = 'threads'\n",
      "\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        Initialize the BaseThread instance.\n",
      "\n",
      "        :param name: The name of the memory thread. Defaults to 'memory'.\n",
      "        :param max_memory: The maximum number of tokens allowed in the memory thread.\n",
      "                           Defaults to None, which means no limit.\n",
      "        :param tokenizer: The tokenizer to be used for tokenizing messages.\n",
      "                          Defaults to None, which means using the tiktoken encoding for the 'gpt-3.5-turbo' model.\n",
      "        \"\"\"\n",
      "        self.name = name\n",
      "        self.max_memory = max_memory\n",
      "        self.memory_schema = {\"role\": pl.Utf8, \"content\": pl.Utf8,\"timestamp\":pl.Float64,\"tokens_count\":pl.UInt16}\n",
      "        self.memory_thread: pl.DataFrame = pl.DataFrame(schema=self.memory_schema)\n",
      "        \"\"\" self.time_stamps = [] \"\"\"\n",
      "        \"\"\" self.message_tokens = [] \"\"\"\n",
      "        self.total_tokens = self.get_total_tokens_from_thread()\n",
      "        self.save_path = save_path\n",
      "        if tokenizer is None:\n",
      "            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        return self.memory_thread[idx]\n",
      "\n",
      "    def __len__(self):\n",
      "        return self.memory_thread.shape[0]\n",
      "\n",
      "    def save(self, path: Union[str,None]) -> None:\n",
      "        if path is None:\n",
      "            path = os.path.join(self.save_path,f'{self.name}.parquet')\n",
      "        self.memory_thread.write_parquet(\n",
      "            file=path,\n",
      "            compression='zstd',\n",
      "            compression_level=None,\n",
      "            statistics=False,\n",
      "            row_group_size=None,\n",
      "            use_pyarrow=True,\n",
      "            pyarrow_options=None\n",
      "        )\n",
      "\n",
      "    def load(self, path: Union[str,None]) -> None:\n",
      "        if path is None:\n",
      "            path = os.path.join(self.save_path,f'{self.name}.parquet')\n",
      "        self.memory_thread = pl.read_parquet(source = path,\n",
      "                        use_pyarrow= True,\n",
      "                        memory_map = True,\n",
      "                        )\n",
      "        \n",
      "    def get_total_tokens_from_thread(self):\n",
      "        return self.memory_thread[\"tokens_count\"].sum()\n",
      "\n",
      "    def reset_memory(self) -> None:\n",
      "        self.memory_thread = pl.DataFrame(schema=self.memory_schema) \n",
      "\n",
      "    def dict_to_row(self, message_dict:Dict[str,str]) -> pl.DataFrame:\n",
      "        timestamp = message_dict['timestamp'] if 'timestamp' in message_dict else [now()]\n",
      "        return pl.DataFrame(schema=self.memory_schema,\n",
      "                            data={ \"role\":[message_dict[\"role\"]],\n",
      "                                  \"content\":[message_dict[\"content\"]],\n",
      "                                  \"timestamp\": timestamp,\n",
      "                                  \"tokens_count\":[len(self.tokenizer.encode(message_dict[\"content\"]))+7]\n",
      "                                  })\n",
      "\n",
      "    def get_message_tokens_from_dict(self, message_dict: dict) -> int:\n",
      "        \"\"\"\n",
      "        Calculate the number of tokens in a message, including the role token.\n",
      "\n",
      "        :param message_dict: A dictionary containing the role and content of the message.\n",
      "        :return: The total number of tokens in the message.\n",
      "        \"\"\"\n",
      "        message_dict = check_dict(message_dict)\n",
      "        message = message_dict[\"content\"]\n",
      "        return len(self.tokenizer.encode(message)) + 7  # +7 for the role token\n",
      "\n",
      "    def get_message_role_from_dict(self, message_dict: dict) -> str:\n",
      "        \"\"\"\n",
      "        Get the role of the message from a message dictionary.\n",
      "\n",
      "        :param message_dict: A dictionary containing the role and content of the message.\n",
      "        :return: The role of the message.\n",
      "        \"\"\"\n",
      "        message_dict = check_dict(message_dict)\n",
      "        return message_dict[\"role\"]\n",
      "\n",
      "    def add_dict_to_thread(self, message_dict: dict) -> None:\n",
      "        \"\"\"\n",
      "        Add a message to the memory thread.\n",
      "\n",
      "        :param message_dict: A dictionary containing the role and content of the message.\n",
      "        \"\"\"\n",
      "        \n",
      "        new_message_row = self.dict_to_row(message_dict)\n",
      "\n",
      "        if (\n",
      "            self.max_memory is None\n",
      "            or self.total_tokens + new_message_row['tokens_count'] <= self.max_memory\n",
      "        ):\n",
      "            self.memory_thread = pl.concat([self.memory_thread, new_message_row], rechunk=True)\n",
      "            self.total_tokens = self.get_total_tokens_from_thread()\n",
      "        else:\n",
      "            print(\"The memory BaseThread is full, the last message was not added\")\n",
      "            \n",
      "\n",
      "    def remove_dict_from_thread(\n",
      "        self, message_dict: Union[Dict, None] = None, idx: Union[int, None] = None\n",
      "    ) -> None:\n",
      "        \n",
      "        if message_dict is None and idx is None:\n",
      "            raise Exception(\"You need to provide either a message_dict or an idx\")\n",
      "\n",
      "        elif idx is not None and idx < len(self.memory_thread):\n",
      "            \n",
      "            self.memory_thread = self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\"index\") != idx).drop(\"index\").collect()\n",
      "            self.total_tokens = self.get_total_tokens_from_thread() \n",
      "            return\n",
      "\n",
      "        elif message_dict is not None:\n",
      "            message_dict = check_dict(message_dict)\n",
      "            self.memory_thread = self.memory_thread.lazy().filter(self.memory_thread[\"content\"] != message_dict[\"content\"]).collect()\n",
      "            self.total_tokens = self.get_total_tokens_from_thread()\n",
      "            return\n",
      "\n",
      "        else:\n",
      "            raise Exception(\"Index was out bound and no corresponding content found.\")\n",
      "\n",
      "\n",
      "    def find_message(\n",
      "        self, message: Union[dict, str]\n",
      "        ) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\n",
      "        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\n",
      "\n",
      "        message = message if isinstance(message, str) else check_dict(message)\n",
      "        \n",
      "        if isinstance(message,str):\n",
      "            return self.memory_thread.lazy().filter(pl.col(\"content\") == message).collect()\n",
      "\n",
      "        else:\n",
      "            return self.memory_thread.lazy().filter((pl.col(\"content\") == message[\"content\"]) & (pl.col(\"role\") == message['role'])).collect()\n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "    def last_message(self, role: Union[str, None] = None) ->  pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get the last message in the memory thread with a specific role.\"\"\"\n",
      "        if role is None:\n",
      "            return self.memory_thread[-1]\n",
      "        else:\n",
      "            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[-1]\n",
      "\n",
      "    def first_message(self, role: Union[str, None] = None) -> pl.DataFrame: \n",
      "        \"\"\"\n",
      "        Get the first message in the memory thread with a specific role.\"\"\"\n",
      "        if role is None:\n",
      "            return self.memory_thread[0]\n",
      "        else:\n",
      "            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[0]\n",
      "\n",
      "\n",
      "    def messages_before( self, message: dict   ) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages before a specific message in the memory thread.\"\"\"\n",
      "        \n",
      "        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==message['content']) & (pl.col('role')==message['role'])).select('index').collect()[0][0]\n",
      "        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col('index')<index).collect()\n",
      "    \n",
      "    def messages_after( self, message: dict   ) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages after a specific message in the memory thread.\"\"\"\n",
      "        \n",
      "        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==message['content']) & (pl.col('role')==message['role'])).select('index').collect()[0][0]\n",
      "        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col('index')>index).collect()\n",
      "\n",
      "    def messages_between(\n",
      "        self, start_message: dict, end_message: dict ) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\n",
      "        start_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==start_message['content']) & (pl.col('role')==start_message['role'])).select('index').collect()[0][0]\n",
      "        end_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==end_message['content']) & (pl.col('role')==end_message['role'])).select('index').collect()[0][0]\n",
      "        return self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('index')<end_index) & (pl.col('index'))>start_index).collect()\n",
      "\n",
      "    def messages_more_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "        \n",
      "        return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('tokens_count'))>tokens).collect()\n",
      "\n",
      "    def messages_less_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "        \n",
      "        return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('tokens_count'))<tokens).collect()\n",
      "\n",
      "\n",
      "    def messages_after_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "        \n",
      "        return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('timestamp')>timestamp)).collect()\n",
      "\n",
      "    def messages_before_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "        \n",
      "        return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('timestamp')<timestamp)).collect()\n",
      "\n",
      "    def select_col(self, feature: Union[List[str],str]):\n",
      "        return self.memory_thread[feature]\n",
      "\n",
      "    def filter_col(self, feature: str, filter: str):\n",
      "        try:\n",
      "            return self.memory_thread.lazy().filter(pl.col(feature) == filter).collect()\n",
      "        except Exception as e:\n",
      "            return str(e)\n",
      "\n",
      "\n",
      "\n",
      "    def token_bound_history(\n",
      "        self, max_tokens: int, role: Union[str, None] = None\n",
      "    ):\n",
      "\n",
      "        reversed_df = self.memory_thread.lazy().with_row_count(\"index\").reverse()   \n",
      "        reversed_df = reversed_df.with_columns(pl.col(\"tokens_count\").cumsum().alias(\"cum_tokens_count\"))  \n",
      "        filtered_df = reversed_df.filter((pl.col(\"cum_tokens_count\") <= max_tokens) & (pl.col(\"role\") != role)).collect()\n",
      "        messages = filtered_df['content']\n",
      "        indexes = filtered_df['index']\n",
      "\n",
      "        return messages,indexes\n",
      "\n",
      "    \n",
      "    def load_from_gpt_url(self, url: str):\n",
      "\n",
      "        response = requests.get(url)\n",
      "        if response.status_code == 200:\n",
      "        \n",
      "            soup = BeautifulSoup(response.text, 'html.parser')\n",
      "        else:\n",
      "            raise ValueError(f\"Non è stato possibile accedere alla pagina. Codice di stato: {response.status_code}\")\n",
      "            \n",
      "        \n",
      "        next_data = soup.find('script', id='__NEXT_DATA__')\n",
      "\n",
      "        if next_data is not None:\n",
      "\n",
      "            data_string = next_data.string  # pyright: ignore \n",
      "\n",
      "            json_obj = json.loads(data_string)  # pyright: ignore\n",
      "\n",
      "            conversation_data = json_obj['props']['pageProps']['serverResponse']['data']\n",
      "\n",
      "            messages = conversation_data['mapping']\n",
      "\n",
      "            for _,value in messages.items():\n",
      "                if ('parent' in value.keys()) and (value['message']['content']['parts'][0] != ''):\n",
      "                    message_dict = {'role':value['message']['author']['role'],\n",
      "                                    'content': value['message']['content']['parts'][0],\n",
      "                                    'timestamp': value['message']['create_time']}\n",
      "                    self.add_dict_to_thread(message_dict)\n",
      "            print(\"Conversation loaded succesfully\")\n",
      "        else:\n",
      "            raise ValueError(f\"Nessuna conversazione trovata a questo link!\")\n",
      "\n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "Function calls: shape: (18,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"now\"\n",
      "\t\"len\"\n",
      "\t\"check_dict\"\n",
      "\t\"len\"\n",
      "\t\"check_dict\"\n",
      "\t\"print\"\n",
      "\t\"Exception\"\n",
      "\t\"len\"\n",
      "\t\"check_dict\"\n",
      "\t\"Exception\"\n",
      "\t\"isinstance\"\n",
      "\t\"check_dict\"\n",
      "\t\"isinstance\"\n",
      "\t\"str\"\n",
      "\t\"BeautifulSoup\"\n",
      "\t\"ValueError\"\n",
      "\t\"print\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    name: str = \"memory\",\n",
      "    max_memory: Optional[int] = None,\n",
      "    tokenizer: Optional[Any] = None,\n",
      "    save_path: str = 'threads'\n",
      "\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "        Initialize the BaseThread instance.\n",
      "\n",
      "        :param name: The name of the memory thread. Defaults to 'memory'.\n",
      "        :param max_memory: The maximum number of tokens allowed in the memory thread.\n",
      "                           Defaults to None, which means no limit.\n",
      "        :param tokenizer: The tokenizer to be used for tokenizing messages.\n",
      "                          Defaults to None, which means using the tiktoken encoding for the 'gpt-3.5-turbo' model.\n",
      "        \"\"\"\n",
      "    self.name = name\n",
      "    self.max_memory = max_memory\n",
      "    self.memory_schema = {\"role\": pl.Utf8, \"content\": pl.Utf8,\"timestamp\":pl.Float64,\"tokens_count\":pl.UInt16}\n",
      "    self.memory_thread: pl.DataFrame = pl.DataFrame(schema=self.memory_schema)\n",
      "    \"\"\" self.time_stamps = [] \"\"\"\n",
      "    \"\"\" self.message_tokens = [] \"\"\"\n",
      "    self.total_tokens = self.get_total_tokens_from_thread()\n",
      "    self.save_path = save_path\n",
      "    if tokenizer is None:\n",
      "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "\n",
      "\n",
      "\n",
      "def __getitem__(self, idx):\n",
      "    return self.memory_thread[idx]\n",
      "\n",
      "\n",
      "\n",
      "def __len__(self):\n",
      "    return self.memory_thread.shape[0]\n",
      "\n",
      "\n",
      "\n",
      "def save(self, path: Union[str,None]) -> None:\n",
      "    if path is None:\n",
      "        path = os.path.join(self.save_path,f'{self.name}.parquet')\n",
      "    self.memory_thread.write_parquet(\n",
      "        file=path,\n",
      "        compression='zstd',\n",
      "        compression_level=None,\n",
      "        statistics=False,\n",
      "        row_group_size=None,\n",
      "        use_pyarrow=True,\n",
      "        pyarrow_options=None\n",
      "    )\n",
      "\n",
      "\n",
      "\n",
      "def load(self, path: Union[str,None]) -> None:\n",
      "    if path is None:\n",
      "        path = os.path.join(self.save_path,f'{self.name}.parquet')\n",
      "    self.memory_thread = pl.read_parquet(source = path,\n",
      "                    use_pyarrow= True,\n",
      "                    memory_map = True,\n",
      "                    )\n",
      "    \n",
      "\n",
      "\n",
      "def get_total_tokens_from_thread(self):\n",
      "    return self.memory_thread[\"tokens_count\"].sum()\n",
      "\n",
      "\n",
      "\n",
      "def reset_memory(self) -> None:\n",
      "    self.memory_thread = pl.DataFrame(schema=self.memory_schema) \n",
      "\n",
      "\n",
      "\n",
      "def dict_to_row(self, message_dict:Dict[str,str]) -> pl.DataFrame:\n",
      "    timestamp = message_dict['timestamp'] if 'timestamp' in message_dict else [now()]\n",
      "    return pl.DataFrame(schema=self.memory_schema,\n",
      "                        data={ \"role\":[message_dict[\"role\"]],\n",
      "                              \"content\":[message_dict[\"content\"]],\n",
      "                              \"timestamp\": timestamp,\n",
      "                              \"tokens_count\":[len(self.tokenizer.encode(message_dict[\"content\"]))+7]\n",
      "                              })\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"now\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_message_tokens_from_dict(self, message_dict: dict) -> int:\n",
      "    \"\"\"\n",
      "        Calculate the number of tokens in a message, including the role token.\n",
      "\n",
      "        :param message_dict: A dictionary containing the role and content of the message.\n",
      "        :return: The total number of tokens in the message.\n",
      "        \"\"\"\n",
      "    message_dict = check_dict(message_dict)\n",
      "    message = message_dict[\"content\"]\n",
      "    return len(self.tokenizer.encode(message)) + 7  # +7 for the role token\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"check_dict\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_message_role_from_dict(self, message_dict: dict) -> str:\n",
      "    \"\"\"\n",
      "        Get the role of the message from a message dictionary.\n",
      "\n",
      "        :param message_dict: A dictionary containing the role and content of the message.\n",
      "        :return: The role of the message.\n",
      "        \"\"\"\n",
      "    message_dict = check_dict(message_dict)\n",
      "    return message_dict[\"role\"]\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"check_dict\"\n",
      "]\n",
      "\n",
      "\n",
      "def add_dict_to_thread(self, message_dict: dict) -> None:\n",
      "    \"\"\"\n",
      "        Add a message to the memory thread.\n",
      "\n",
      "        :param message_dict: A dictionary containing the role and content of the message.\n",
      "        \"\"\"\n",
      "    \n",
      "    new_message_row = self.dict_to_row(message_dict)\n",
      "\n",
      "    if (\n",
      "        self.max_memory is None\n",
      "        or self.total_tokens + new_message_row['tokens_count'] <= self.max_memory\n",
      "    ):\n",
      "        self.memory_thread = pl.concat([self.memory_thread, new_message_row], rechunk=True)\n",
      "        self.total_tokens = self.get_total_tokens_from_thread()\n",
      "    else:\n",
      "        print(\"The memory BaseThread is full, the last message was not added\")\n",
      "        \n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def remove_dict_from_thread(\n",
      "    self, message_dict: Union[Dict, None] = None, idx: Union[int, None] = None\n",
      ") -> None:\n",
      "    \n",
      "    if message_dict is None and idx is None:\n",
      "        raise Exception(\"You need to provide either a message_dict or an idx\")\n",
      "\n",
      "    elif idx is not None and idx < len(self.memory_thread):\n",
      "        \n",
      "        self.memory_thread = self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\"index\") != idx).drop(\"index\").collect()\n",
      "        self.total_tokens = self.get_total_tokens_from_thread() \n",
      "        return\n",
      "\n",
      "    elif message_dict is not None:\n",
      "        message_dict = check_dict(message_dict)\n",
      "        self.memory_thread = self.memory_thread.lazy().filter(self.memory_thread[\"content\"] != message_dict[\"content\"]).collect()\n",
      "        self.total_tokens = self.get_total_tokens_from_thread()\n",
      "        return\n",
      "\n",
      "    else:\n",
      "        raise Exception(\"Index was out bound and no corresponding content found.\")\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Exception\"\n",
      "\t\"len\"\n",
      "\t\"check_dict\"\n",
      "\t\"Exception\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def find_message(\n",
      "    self, message: Union[dict, str]\n",
      "    ) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\n",
      "        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\n",
      "\n",
      "    message = message if isinstance(message, str) else check_dict(message)\n",
      "    \n",
      "    if isinstance(message,str):\n",
      "        return self.memory_thread.lazy().filter(pl.col(\"content\") == message).collect()\n",
      "\n",
      "    else:\n",
      "        return self.memory_thread.lazy().filter((pl.col(\"content\") == message[\"content\"]) & (pl.col(\"role\") == message['role'])).collect()\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"check_dict\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def last_message(self, role: Union[str, None] = None) ->  pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get the last message in the memory thread with a specific role.\"\"\"\n",
      "    if role is None:\n",
      "        return self.memory_thread[-1]\n",
      "    else:\n",
      "        return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[-1]\n",
      "\n",
      "\n",
      "\n",
      "def first_message(self, role: Union[str, None] = None) -> pl.DataFrame: \n",
      "    \"\"\"\n",
      "        Get the first message in the memory thread with a specific role.\"\"\"\n",
      "    if role is None:\n",
      "        return self.memory_thread[0]\n",
      "    else:\n",
      "        return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def messages_before( self, message: dict   ) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages before a specific message in the memory thread.\"\"\"\n",
      "    \n",
      "    index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==message['content']) & (pl.col('role')==message['role'])).select('index').collect()[0][0]\n",
      "    return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col('index')<index).collect()\n",
      "\n",
      "\n",
      "\n",
      "def messages_after( self, message: dict   ) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages after a specific message in the memory thread.\"\"\"\n",
      "    \n",
      "    index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==message['content']) & (pl.col('role')==message['role'])).select('index').collect()[0][0]\n",
      "    return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col('index')>index).collect()\n",
      "\n",
      "\n",
      "\n",
      "def messages_between(\n",
      "    self, start_message: dict, end_message: dict ) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\n",
      "    start_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==start_message['content']) & (pl.col('role')==start_message['role'])).select('index').collect()[0][0]\n",
      "    end_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==end_message['content']) & (pl.col('role')==end_message['role'])).select('index').collect()[0][0]\n",
      "    return self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('index')<end_index) & (pl.col('index'))>start_index).collect()\n",
      "\n",
      "\n",
      "\n",
      "def messages_more_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "    \n",
      "    return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('tokens_count'))>tokens).collect()\n",
      "\n",
      "\n",
      "\n",
      "def messages_less_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "    \n",
      "    return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('tokens_count'))<tokens).collect()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def messages_after_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "    \n",
      "    return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('timestamp')>timestamp)).collect()\n",
      "\n",
      "\n",
      "\n",
      "def messages_before_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "    \n",
      "    return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('timestamp')<timestamp)).collect()\n",
      "\n",
      "\n",
      "\n",
      "def select_col(self, feature: Union[List[str],str]):\n",
      "    return self.memory_thread[feature]\n",
      "\n",
      "\n",
      "\n",
      "def filter_col(self, feature: str, filter: str):\n",
      "    try:\n",
      "        return self.memory_thread.lazy().filter(pl.col(feature) == filter).collect()\n",
      "    except Exception as e:\n",
      "        return str(e)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"str\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def token_bound_history(\n",
      "    self, max_tokens: int, role: Union[str, None] = None\n",
      "):\n",
      "\n",
      "    reversed_df = self.memory_thread.lazy().with_row_count(\"index\").reverse()   \n",
      "    reversed_df = reversed_df.with_columns(pl.col(\"tokens_count\").cumsum().alias(\"cum_tokens_count\"))  \n",
      "    filtered_df = reversed_df.filter((pl.col(\"cum_tokens_count\") <= max_tokens) & (pl.col(\"role\") != role)).collect()\n",
      "    messages = filtered_df['content']\n",
      "    indexes = filtered_df['index']\n",
      "\n",
      "    return messages,indexes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def load_from_gpt_url(self, url: str):\n",
      "\n",
      "    response = requests.get(url)\n",
      "    if response.status_code == 200:\n",
      "        \n",
      "        soup = BeautifulSoup(response.text, 'html.parser')\n",
      "    else:\n",
      "        raise ValueError(f\"Non è stato possibile accedere alla pagina. Codice di stato: {response.status_code}\")\n",
      "        \n",
      "    \n",
      "    next_data = soup.find('script', id='__NEXT_DATA__')\n",
      "\n",
      "    if next_data is not None:\n",
      "\n",
      "        data_string = next_data.string  # pyright: ignore \n",
      "\n",
      "        json_obj = json.loads(data_string)  # pyright: ignore\n",
      "\n",
      "        conversation_data = json_obj['props']['pageProps']['serverResponse']['data']\n",
      "\n",
      "        messages = conversation_data['mapping']\n",
      "\n",
      "        for _,value in messages.items():\n",
      "            if ('parent' in value.keys()) and (value['message']['content']['parts'][0] != ''):\n",
      "                message_dict = {'role':value['message']['author']['role'],\n",
      "                                'content': value['message']['content']['parts'][0],\n",
      "                                'timestamp': value['message']['create_time']}\n",
      "                self.add_dict_to_thread(message_dict)\n",
      "        print(\"Conversation loaded succesfully\")\n",
      "    else:\n",
      "        raise ValueError(f\"Nessuna conversazione trovata a questo link!\")\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"BeautifulSoup\"\n",
      "\t\"ValueError\"\n",
      "\t\"print\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class FifoThread(BaseThread):\n",
      "    \"\"\"FIFO Memory BaseThread, the oldest messages are removed first when reaching the max_memory limit, the memory is defined in terms of tokens,\n",
      "    outs are passe to the longterm_memory, lucid_memory is a redundant memory that stores all the messages\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self, name=\"fifo_memory\", max_memory=None, longterm_thread=None, redundant=True\n",
      "    ):\n",
      "\n",
      "        BaseThread.__init__(self, name=name, max_memory=None)\n",
      "        if redundant is True:\n",
      "            self.redundant_thread = BaseThread(name=\"lucid_memory\", max_memory=None)\n",
      "        else:\n",
      "            self.redundant_thread = None\n",
      "        if longterm_thread is None:\n",
      "            self.longterm_thread = BaseThread(name=\"longterm_memory\", max_memory=None)\n",
      "        else:\n",
      "            self.longterm_thread = longterm_thread\n",
      "        # create an alias for the memory_thread to make the code more readable\n",
      "        self.fifo_thread = self.memory_thread\n",
      "        self.max_memory = max_memory\n",
      "\n",
      "    def to_longterm(self, idx: int):\n",
      "        \"\"\"move the message at the index idx to the longterm_memory\"\"\"\n",
      "        # move the message at the index idx to the longterm_memory\n",
      "        display(\n",
      "            Markdown(\n",
      "                \"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(\n",
      "                    idx\n",
      "                )\n",
      "            )\n",
      "        )\n",
      "        # this should not remove everything\n",
      "        # there should be a check to make sure the thread isnt left empty\n",
      "        message = copy.deepcopy(self.memory_thread[idx])\n",
      "        # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\n",
      "        self.longterm_thread.add_message(message)\n",
      "        self.remove_message(idx=idx)\n",
      "\n",
      "    def add_message(self, message_dict: dict):\n",
      "        \"\"\"add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\n",
      "        # message_dict = {\"role\": role, \"content\": content}\n",
      "        # chek that the message_dict is a dictionary or a list of dictionaries\n",
      "        message_dict = check_dict(message_dict)\n",
      "        if self.redundant_thread is not None:\n",
      "            self.redundant_thread.add_message(message_dict)\n",
      "        message_tokens = self.get_message_tokens(message_dict)\n",
      "\n",
      "        if self.total_tokens + message_tokens > self.max_memory:\n",
      "            while self.total_tokens + message_tokens > self.max_memory:\n",
      "                if len(self.memory_thread) > 0:\n",
      "                    self.to_longterm(idx=0)\n",
      "                    message_tokens = self.get_message_tokens(message_dict)  # Update message_tokens\n",
      "                    self.total_tokens -= message_tokens  # Update self.total_tokens\n",
      "\n",
      "            super().add_message(message_dict)\n",
      "\n",
      "        else:\n",
      "            # add the message_dict to the memory_thread\n",
      "            # update the total number of tokens\n",
      "            super().add_message(message_dict)\n",
      "\n",
      "Function calls: shape: (8,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"BaseThread\"\n",
      "\t\"BaseThread\"\n",
      "\t\"display\"\n",
      "\t\"Markdown\"\n",
      "\t\"check_dict\"\n",
      "\t\"len\"\n",
      "\t\"super\"\n",
      "\t\"super\"\n",
      "]\n",
      "\n",
      "\n",
      "def __init__(\n",
      "    self, name=\"fifo_memory\", max_memory=None, longterm_thread=None, redundant=True\n",
      "):\n",
      "\n",
      "    BaseThread.__init__(self, name=name, max_memory=None)\n",
      "    if redundant is True:\n",
      "        self.redundant_thread = BaseThread(name=\"lucid_memory\", max_memory=None)\n",
      "    else:\n",
      "        self.redundant_thread = None\n",
      "    if longterm_thread is None:\n",
      "        self.longterm_thread = BaseThread(name=\"longterm_memory\", max_memory=None)\n",
      "    else:\n",
      "        self.longterm_thread = longterm_thread\n",
      "    # create an alias for the memory_thread to make the code more readable\n",
      "    self.fifo_thread = self.memory_thread\n",
      "    self.max_memory = max_memory\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"BaseThread\"\n",
      "\t\"BaseThread\"\n",
      "]\n",
      "\n",
      "\n",
      "def to_longterm(self, idx: int):\n",
      "    \"\"\"move the message at the index idx to the longterm_memory\"\"\"\n",
      "    # move the message at the index idx to the longterm_memory\n",
      "    display(\n",
      "        Markdown(\n",
      "            \"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(\n",
      "                idx\n",
      "            )\n",
      "        )\n",
      "    )\n",
      "    # this should not remove everything\n",
      "    # there should be a check to make sure the thread isnt left empty\n",
      "    message = copy.deepcopy(self.memory_thread[idx])\n",
      "    # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\n",
      "    self.longterm_thread.add_message(message)\n",
      "    self.remove_message(idx=idx)\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"display\"\n",
      "\t\"Markdown\"\n",
      "]\n",
      "\n",
      "\n",
      "def add_message(self, message_dict: dict):\n",
      "    \"\"\"add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\n",
      "    # message_dict = {\"role\": role, \"content\": content}\n",
      "    # chek that the message_dict is a dictionary or a list of dictionaries\n",
      "    message_dict = check_dict(message_dict)\n",
      "    if self.redundant_thread is not None:\n",
      "        self.redundant_thread.add_message(message_dict)\n",
      "    message_tokens = self.get_message_tokens(message_dict)\n",
      "\n",
      "    if self.total_tokens + message_tokens > self.max_memory:\n",
      "        while self.total_tokens + message_tokens > self.max_memory:\n",
      "            if len(self.memory_thread) > 0:\n",
      "                self.to_longterm(idx=0)\n",
      "                message_tokens = self.get_message_tokens(message_dict)  # Update message_tokens\n",
      "                self.total_tokens -= message_tokens  # Update self.total_tokens\n",
      "\n",
      "        super().add_message(message_dict)\n",
      "\n",
      "    else:\n",
      "        # add the message_dict to the memory_thread\n",
      "        # update the total number of tokens\n",
      "        super().add_message(message_dict)\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"check_dict\"\n",
      "\t\"len\"\n",
      "\t\"super\"\n",
      "\t\"super\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class MemoryIndex(NpIndex):\n",
      "    \"\"\"\n",
      "    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\n",
      "    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\n",
      "    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \n",
      "    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\n",
      "    \"\"\"\n",
      "    @staticmethod\n",
      "    def check_uniform_context_type(context: List[Any]) -> None:\n",
      "        \"\"\"Check if all context elements are of the same type.\"\"\"\n",
      "        if not all(isinstance(x, type(context[0])) for x in context):\n",
      "            raise ValueError(\"All context elements must be of the same type.\")\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        values: Optional[List[str]] = None,\n",
      "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "        context: Optional[List[Any]] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        load: bool = False,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "        markdown: str = \"text/markdown\",\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ):\n",
      "        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\n",
      "        if context is not None and len(context) != len(values):\n",
      "            raise ValueError(\"The context must have the same length as the values\")\n",
      "\n",
      "\n",
      "        self.markdown = markdown\n",
      "        if context is not None and values is not None:\n",
      "            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\n",
      "\n",
      "        if context is not None:\n",
      "            self.check_uniform_context_type(context)\n",
      "            self.context_type = type(context[0])\n",
      "\n",
      "\n",
      "    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\n",
      "        \"\"\" get the context of a value id or embedding or a list of \"\"\"\n",
      "        if isinstance(identifier, list):\n",
      "            return [self.get_context(value) for value in identifier]\n",
      "        else:\n",
      "            id = self.identify_input(identifier)\n",
      "            value = self.values[id]\n",
      "            return self.context[value]\n",
      "\n",
      "    def clean_context(self):\n",
      "        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\n",
      "        self.context = {value: self.context[value] for value in self.values}\n",
      "\n",
      "    def add_to_context(self, value: str, context: Any):\n",
      "        \"\"\" add a context to a value \"\"\"\n",
      "        if not isinstance(context, self.context_type):\n",
      "            raise ValueError(\"The context must be of the same type as the other contexts\")\n",
      "        if value in self.values:\n",
      "            if value not in self.context:\n",
      "                self.context[value] = []\n",
      "            self.context[value].append(context)\n",
      "\n",
      "    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\n",
      "        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\n",
      "        if isinstance(values, str):\n",
      "            values = [values]\n",
      "        NpIndex.add(self, values, embedding)\n",
      "        if context is not None:\n",
      "            for value, cont in zip(values, context):\n",
      "                self.add_to_context(value, cont)\n",
      "\n",
      "    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\n",
      "        if not isinstance(identifier, list):\n",
      "            id = self.identify_input(identifier)\n",
      "            value = self.values[id]\n",
      "        NpIndex.remove(self, identifier)\n",
      "        if not isinstance(identifier, list):\n",
      "            self.context.pop(value)\n",
      "\n",
      "    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n",
      "        #recover value from old_identifier\n",
      "        if new_context is not None:\n",
      "            if not isinstance(new_context, self.context_type):\n",
      "                raise ValueError(\"The context must be of the same type as the other contexts\")\n",
      "        if not isinstance(old_identifier, list):\n",
      "            old_id = self.identify_input(old_identifier)\n",
      "            old_value = self.values[old_id]\n",
      "            # Only perform the update if the old_value is not the same as the new_value.\n",
      "            if old_value != new_value:\n",
      "                NpIndex.update(self, old_identifier, new_value, new_embedding)\n",
      "\n",
      "            if new_context is not None:\n",
      "                self.context[new_value] = [new_context]\n",
      "            else:\n",
      "                self.context[new_value] = self.context.pop(old_value)\n",
      "        else:\n",
      "            self.context[new_value] = self.context.pop(old_value)\n",
      "\n",
      "    @classmethod\n",
      "    def from_pandas(\n",
      "        cls,\n",
      "        data_frame: Union[pd.DataFrame, str],\n",
      "        value_column: str,\n",
      "        embeddings_column: Optional[str] = None,\n",
      "        context_columns: Optional[List[str]] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "        markdown: str = \"text/markdown\",\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ) -> \"MemoryIndex\":\n",
      "        if (\n",
      "            isinstance(data_frame, str)\n",
      "            and data_frame.endswith(\".csv\")\n",
      "            and os.path.isfile(data_frame)\n",
      "        ):\n",
      "            logger.info(\"Loading the CSV file\")\n",
      "            data_frame = pd.read_csv(data_frame)\n",
      "            name = os.path.basename(data_frame).split(\".\")[0]\n",
      "        elif isinstance(data_frame, pd.core.frame.DataFrame):\n",
      "            logger.info(\"Loading the pandas DataFrame\")\n",
      "        else:\n",
      "            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\n",
      "\n",
      "        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\n",
      "        if context_columns is not None:\n",
      "            context = get_context_from_pandas(data_frame, context_columns)\n",
      "\n",
      "        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "    @classmethod\n",
      "    def from_hf_dataset(\n",
      "        cls,\n",
      "        dataset_url: str,\n",
      "        value_column: str,\n",
      "        data_split: str = \"train\",\n",
      "        embeddings_column: Optional[str] = None,\n",
      "        context_columns: Optional[List[str]] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "        markdown: str = \"text/markdown\",\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ) -> \"MemoryIndex\":\n",
      "        dataset = load_dataset(dataset_url)[data_split]\n",
      "        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\n",
      "        if context_columns is not None:\n",
      "            context = get_context_from_hf(dataset, context_columns)\n",
      "        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "    @classmethod\n",
      "    def from_polars(\n",
      "        cls,\n",
      "        data_frame: pl.DataFrame,\n",
      "        value_column: str,\n",
      "        embeddings_column: Optional[str] = None,\n",
      "        context_columns: Optional[List[str]] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "        markdown: str = \"text/markdown\",\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ) -> \"MemoryIndex\":\n",
      "        print(\"Loading the Polars DataFrame\")\n",
      "        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\n",
      "        if context_columns is not None:\n",
      "            context = get_context_from_polars(data_frame, context_columns)\n",
      "\n",
      "        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "\n",
      "    @classmethod\n",
      "    def from_python(\n",
      "        cls,\n",
      "        directory_path: str,\n",
      "        minify_code: bool = False,\n",
      "        remove_docstrings: bool = False,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        markdown: str = \"python/markdown\",\n",
      "        resolution: str = \"both\",\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ) -> \"MemoryIndex\":\n",
      "        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n",
      "        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n",
      "        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "Function calls: shape: (35,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"all\"\n",
      "\t\"isinstance\"\n",
      "\t\"type\"\n",
      "\t\"ValueError\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"type\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t…\n",
      "\t\"get_context_fr…\n",
      "\t\"cls\"\n",
      "\t\"load_dataset\"\n",
      "\t\"extract_values…\n",
      "\t\"get_context_fr…\n",
      "\t\"cls\"\n",
      "\t\"print\"\n",
      "\t\"extract_values…\n",
      "\t\"get_context_fr…\n",
      "\t\"cls\"\n",
      "\t\"extract_values…\n",
      "\t\"len\"\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "@staticmethod\n",
      "def check_uniform_context_type(context: List[Any]) -> None:\n",
      "    \"\"\"Check if all context elements are of the same type.\"\"\"\n",
      "    if not all(isinstance(x, type(context[0])) for x in context):\n",
      "        raise ValueError(\"All context elements must be of the same type.\")\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"all\"\n",
      "\t\"isinstance\"\n",
      "\t\"type\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    values: Optional[List[str]] = None,\n",
      "    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "    context: Optional[List[Any]] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    load: bool = False,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "    markdown: str = \"text/markdown\",\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      "):\n",
      "    NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\n",
      "    if context is not None and len(context) != len(values):\n",
      "        raise ValueError(\"The context must have the same length as the values\")\n",
      "\n",
      "\n",
      "    self.markdown = markdown\n",
      "    if context is not None and values is not None:\n",
      "        self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\n",
      "\n",
      "    if context is not None:\n",
      "        self.check_uniform_context_type(context)\n",
      "        self.context_type = type(context[0])\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"type\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\n",
      "    \"\"\" get the context of a value id or embedding or a list of \"\"\"\n",
      "    if isinstance(identifier, list):\n",
      "        return [self.get_context(value) for value in identifier]\n",
      "    else:\n",
      "        id = self.identify_input(identifier)\n",
      "        value = self.values[id]\n",
      "        return self.context[value]\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def clean_context(self):\n",
      "    \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\n",
      "    self.context = {value: self.context[value] for value in self.values}\n",
      "\n",
      "\n",
      "\n",
      "def add_to_context(self, value: str, context: Any):\n",
      "    \"\"\" add a context to a value \"\"\"\n",
      "    if not isinstance(context, self.context_type):\n",
      "        raise ValueError(\"The context must be of the same type as the other contexts\")\n",
      "    if value in self.values:\n",
      "        if value not in self.context:\n",
      "            self.context[value] = []\n",
      "        self.context[value].append(context)\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\n",
      "    \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\n",
      "    if isinstance(values, str):\n",
      "        values = [values]\n",
      "    NpIndex.add(self, values, embedding)\n",
      "    if context is not None:\n",
      "        for value, cont in zip(values, context):\n",
      "            self.add_to_context(value, cont)\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"zip\"\n",
      "]\n",
      "\n",
      "\n",
      "def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\n",
      "    if not isinstance(identifier, list):\n",
      "        id = self.identify_input(identifier)\n",
      "        value = self.values[id]\n",
      "    NpIndex.remove(self, identifier)\n",
      "    if not isinstance(identifier, list):\n",
      "        self.context.pop(value)\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n",
      "    #recover value from old_identifier\n",
      "    if new_context is not None:\n",
      "        if not isinstance(new_context, self.context_type):\n",
      "            raise ValueError(\"The context must be of the same type as the other contexts\")\n",
      "    if not isinstance(old_identifier, list):\n",
      "        old_id = self.identify_input(old_identifier)\n",
      "        old_value = self.values[old_id]\n",
      "        # Only perform the update if the old_value is not the same as the new_value.\n",
      "        if old_value != new_value:\n",
      "            NpIndex.update(self, old_identifier, new_value, new_embedding)\n",
      "\n",
      "        if new_context is not None:\n",
      "            self.context[new_value] = [new_context]\n",
      "        else:\n",
      "            self.context[new_value] = self.context.pop(old_value)\n",
      "    else:\n",
      "        self.context[new_value] = self.context.pop(old_value)\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "@classmethod\n",
      "def from_pandas(\n",
      "    cls,\n",
      "    data_frame: Union[pd.DataFrame, str],\n",
      "    value_column: str,\n",
      "    embeddings_column: Optional[str] = None,\n",
      "    context_columns: Optional[List[str]] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "    markdown: str = \"text/markdown\",\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      ") -> \"MemoryIndex\":\n",
      "    if (\n",
      "        isinstance(data_frame, str)\n",
      "        and data_frame.endswith(\".csv\")\n",
      "        and os.path.isfile(data_frame)\n",
      "    ):\n",
      "        logger.info(\"Loading the CSV file\")\n",
      "        data_frame = pd.read_csv(data_frame)\n",
      "        name = os.path.basename(data_frame).split(\".\")[0]\n",
      "    elif isinstance(data_frame, pd.core.frame.DataFrame):\n",
      "        logger.info(\"Loading the pandas DataFrame\")\n",
      "    else:\n",
      "        raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\n",
      "\n",
      "    values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\n",
      "    if context_columns is not None:\n",
      "        context = get_context_from_pandas(data_frame, context_columns)\n",
      "\n",
      "    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"extract_values…\n",
      "\t\"get_context_fr…\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "\n",
      "@classmethod\n",
      "def from_hf_dataset(\n",
      "    cls,\n",
      "    dataset_url: str,\n",
      "    value_column: str,\n",
      "    data_split: str = \"train\",\n",
      "    embeddings_column: Optional[str] = None,\n",
      "    context_columns: Optional[List[str]] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "    markdown: str = \"text/markdown\",\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      ") -> \"MemoryIndex\":\n",
      "    dataset = load_dataset(dataset_url)[data_split]\n",
      "    values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\n",
      "    if context_columns is not None:\n",
      "        context = get_context_from_hf(dataset, context_columns)\n",
      "    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"load_dataset\"\n",
      "\t\"extract_values…\n",
      "\t\"get_context_fr…\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "\n",
      "@classmethod\n",
      "def from_polars(\n",
      "    cls,\n",
      "    data_frame: pl.DataFrame,\n",
      "    value_column: str,\n",
      "    embeddings_column: Optional[str] = None,\n",
      "    context_columns: Optional[List[str]] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "    markdown: str = \"text/markdown\",\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      ") -> \"MemoryIndex\":\n",
      "    print(\"Loading the Polars DataFrame\")\n",
      "    values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\n",
      "    if context_columns is not None:\n",
      "        context = get_context_from_polars(data_frame, context_columns)\n",
      "\n",
      "    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"extract_values…\n",
      "\t\"get_context_fr…\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "@classmethod\n",
      "def from_python(\n",
      "    cls,\n",
      "    directory_path: str,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    markdown: str = \"python/markdown\",\n",
      "    resolution: str = \"both\",\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      ") -> \"MemoryIndex\":\n",
      "    values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n",
      "    logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n",
      "    return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"extract_values…\n",
      "\t\"len\"\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class PythonIndex(MemoryIndex, PythonParser):\n",
      "    def __init__(\n",
      "        self,\n",
      "        directory_path: str,\n",
      "        name: str = \"python_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        load: bool = False,\n",
      "        minify_code: bool = False,\n",
      "        remove_docstrings: bool = False,\n",
      "        tokenizer: Optional[tiktoken.Encoding] = None,\n",
      "        max_workers: int = 1,\n",
      "        backup: bool = False,\n",
      "        filter: str = \"class_function\"\n",
      "    ):\n",
      "\n",
      "        PythonParser.__init__(\n",
      "            self,\n",
      "            directory_path=directory_path,\n",
      "            minify_code=minify_code,\n",
      "            remove_docstrings=remove_docstrings,\n",
      "        )\n",
      "        #check if load folder exists\n",
      "        if save_path is None:\n",
      "            save_path = \"storage\"\n",
      "        load_directory = os.path.join(save_path, name)\n",
      "        loadcheck = not load or not os.path.exists(load_directory)\n",
      "        if load and not os.path.exists(load_directory):\n",
      "            print(\"No python-index found even if load=True, indexing from scratch\")\n",
      "        if loadcheck:\n",
      "            # Extract functions and classes source code\n",
      "            function_source_codes, class_source_codes, _, _ = self.process_directory()\n",
      "            print(\n",
      "                \"Indexing {} functions and {} classes\".format(\n",
      "                    len(function_source_codes), len(class_source_codes)\n",
      "                )\n",
      "            )\n",
      "            # Concatenate function and class source code and index them\n",
      "            if filter == \"function\":\n",
      "                codes = function_source_codes\n",
      "            elif filter == \"class\":\n",
      "                codes = class_source_codes\n",
      "            elif filter == \"class_function\":\n",
      "                codes = function_source_codes + class_source_codes\n",
      "            load = False\n",
      "            self.function_source_codes = function_source_codes\n",
      "            self.class_source_codes = class_source_codes\n",
      "\n",
      "         # Initialize the MemoryIndex\n",
      "        MemoryIndex.__init__(\n",
      "            self,\n",
      "            name=name,\n",
      "            values=codes if loadcheck else None,\n",
      "            save_path=save_path,\n",
      "            load=load,\n",
      "            tokenizer=tokenizer,\n",
      "            max_workers=max_workers,\n",
      "            backup=backup,\n",
      "        )\n",
      "        self.markdown = \"python\"\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    directory_path: str,\n",
      "    name: str = \"python_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    load: bool = False,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "    tokenizer: Optional[tiktoken.Encoding] = None,\n",
      "    max_workers: int = 1,\n",
      "    backup: bool = False,\n",
      "    filter: str = \"class_function\"\n",
      "):\n",
      "\n",
      "    PythonParser.__init__(\n",
      "        self,\n",
      "        directory_path=directory_path,\n",
      "        minify_code=minify_code,\n",
      "        remove_docstrings=remove_docstrings,\n",
      "    )\n",
      "    #check if load folder exists\n",
      "    if save_path is None:\n",
      "        save_path = \"storage\"\n",
      "    load_directory = os.path.join(save_path, name)\n",
      "    loadcheck = not load or not os.path.exists(load_directory)\n",
      "    if load and not os.path.exists(load_directory):\n",
      "        print(\"No python-index found even if load=True, indexing from scratch\")\n",
      "    if loadcheck:\n",
      "        # Extract functions and classes source code\n",
      "        function_source_codes, class_source_codes, _, _ = self.process_directory()\n",
      "        print(\n",
      "            \"Indexing {} functions and {} classes\".format(\n",
      "                len(function_source_codes), len(class_source_codes)\n",
      "            )\n",
      "        )\n",
      "        # Concatenate function and class source code and index them\n",
      "        if filter == \"function\":\n",
      "            codes = function_source_codes\n",
      "        elif filter == \"class\":\n",
      "            codes = class_source_codes\n",
      "        elif filter == \"class_function\":\n",
      "            codes = function_source_codes + class_source_codes\n",
      "        load = False\n",
      "        self.function_source_codes = function_source_codes\n",
      "        self.class_source_codes = class_source_codes\n",
      "\n",
      "     # Initialize the MemoryIndex\n",
      "    MemoryIndex.__init__(\n",
      "        self,\n",
      "        name=name,\n",
      "        values=codes if loadcheck else None,\n",
      "        save_path=save_path,\n",
      "        load=load,\n",
      "        tokenizer=tokenizer,\n",
      "        max_workers=max_workers,\n",
      "        backup=backup,\n",
      "    )\n",
      "    self.markdown = \"python\"\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def prune_index(\n",
      "    cls: \"MemoryIndex\",\n",
      "    constraint: Optional[str] = None,\n",
      "    regex_pattern: Optional[str] = None,\n",
      "    length_constraint: Optional[Tuple[int,int]] = None,\n",
      "    tokenizer: Optional[tiktoken.Encoding] = None,\n",
      ") -> \"MemoryIndex\" :\n",
      "    if constraint is not None:\n",
      "        if constraint == \"regex\":\n",
      "            if regex_pattern is None:\n",
      "                raise ValueError(\"regex_pattern must be provided for regex constraint.\")\n",
      "            pruned_values, pruned_embeddings = _prune_by_regex(cls, regex_pattern)\n",
      "        elif constraint == \"length\":\n",
      "            if length_constraint is None:\n",
      "                raise ValueError(\"length_constraint must be provided for length constraint.\")\n",
      "            pruned_values, pruned_embeddings = _prune_by_length(cls, length_constraint, tokenizer)\n",
      "        else:\n",
      "            raise ValueError(\"Invalid constraint type provided.\")\n",
      "    else:\n",
      "        raise ValueError(\"constraint must be provided for pruning the index.\")\n",
      "\n",
      "    pruned_memory_index = cls.__class__(\n",
      "        values=pruned_values,\n",
      "        embeddings=pruned_embeddings,\n",
      "        name=cls.name + \"_pruned\",\n",
      "    )\n",
      "\n",
      "    return pruned_memory_index\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"ValueError\"\n",
      "\t\"_prune_by_rege…\n",
      "\t\"ValueError\"\n",
      "\t\"_prune_by_leng…\n",
      "\t\"ValueError\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def _prune_by_regex(cls: \"MemoryIndex\", regex_pattern: str) -> Tuple[List[str], List[np.ndarray]]:\n",
      "    pruned_values = []\n",
      "    pruned_embeddings = []\n",
      "\n",
      "    for value in cls.values:\n",
      "        if re.search(regex_pattern, value):\n",
      "            pruned_values.append(value)\n",
      "            pruned_embeddings.append(cls.get_embedding_by_value(value))\n",
      "\n",
      "    return pruned_values, pruned_embeddings\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def _prune_by_length(cls: \"MemoryIndex\", length_constraint: Tuple[int,int], tokenizer) -> Tuple[List[str], List[np.ndarray]]:\n",
      "    pruned_values = []\n",
      "    pruned_embeddings = []\n",
      "    if tokenizer is None:\n",
      "        len_func = len\n",
      "    else:\n",
      "        def len_func(value):\n",
      "            return len(tokenizer.encode(value))\n",
      "    print(\"Pruning by length\")\n",
      "    print(\"Length constraint: \", length_constraint)\n",
      "    print(\"Number of values: \", len(cls.values))\n",
      "    print(\"tokenizer: \", tokenizer)\n",
      "    for value in cls.values:\n",
      "        if len_func(value) <= length_constraint[1] and len_func(value) >= length_constraint[0]:\n",
      "            print(f\"value {value} is in range {length_constraint}\")\n",
      "            pruned_values.append(value)\n",
      "            pruned_embeddings.append(cls.get_embedding_by_value(value))\n",
      "\n",
      "    return pruned_values, pruned_embeddings\n",
      "\n",
      "Function calls: shape: (9,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"len_func\"\n",
      "\t\"len_func\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "def len_func(value):\n",
      "    return len(tokenizer.encode(value))\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def save(cls):\n",
      "    save_directory = os.path.join(cls.save_path, cls.name)\n",
      "    os.makedirs(save_directory, exist_ok=True)\n",
      "\n",
      "    index_filename = os.path.join(save_directory, f\"{cls.name}_index.faiss\")\n",
      "    faiss.write_index(cls.index, index_filename)\n",
      "\n",
      "    values_filename = os.path.join(save_directory, f\"{cls.name}_values.json\")\n",
      "    with open(values_filename, \"w\") as f:\n",
      "        json.dump(cls.values, f)\n",
      "\n",
      "    # embeddings_filename = os.path.join(save_directory, f\"{cls.name}_embeddings.npz\")\n",
      "    # np.savez_compressed(embeddings_filename, cls.get_all_embeddings())\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "]\n",
      "\n",
      "\n",
      "def load(cls):\n",
      "    load_directory = os.path.join(cls.save_path, cls.name)\n",
      "    if not os.path.exists(load_directory):\n",
      "        cls.loaded = False\n",
      "        print(\"I did not find the directory to load the index from.\", load_directory)\n",
      "        return\n",
      "\n",
      "    print(f\"Loading index from {load_directory}\")\n",
      "\n",
      "    index_filename = os.path.join(load_directory, f\"{cls.name}_index.faiss\")\n",
      "    cls.index = faiss.read_index(index_filename)\n",
      "\n",
      "    values_filename = os.path.join(load_directory, f\"{cls.name}_values.json\")\n",
      "    with open(values_filename, \"r\") as f:\n",
      "        cls.values = json.load(f)\n",
      "    # embeddings_filename = os.path.join(load_directory, f\"{cls.name}_embeddings.npz\")\n",
      "    # print(embeddings_filename)\n",
      "    # embeddings_data = np.load(embeddings_filename)\n",
      "    # cls.embeddings = embeddings_data[\"arr_0\"]\n",
      "    cls.loaded = True\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"open\"\n",
      "]\n",
      "\n",
      "\n",
      "class MemoryIndex:\n",
      "    \"\"\"\n",
      "    this class is a wrapper for a faiss index, it contains information about the format of the index the faiss index itself\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        index: Optional[faiss.Index] = None,\n",
      "        values: Optional[List[str]] = None,\n",
      "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        load: bool = False,\n",
      "        tokenizer: Optional[tiktoken.Encoding] = None,\n",
      "        max_workers: int = 1,\n",
      "        backup: bool = False,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "        is_batched: bool = False,\n",
      "        markdown: str = \"text/markdown\",\n",
      "    ):\n",
      "\n",
      "        self.name = name\n",
      "        self.embedder = embedder()\n",
      "        self.save_path = save_path if save_path is not None else \"storage\"\n",
      "        os.makedirs(self.save_path, exist_ok=True)\n",
      "        self.values = []\n",
      "        self.embeddings = []\n",
      "        self.max_workers = max_workers\n",
      "        self.is_batched = is_batched\n",
      "        self.markdown = markdown\n",
      "\n",
      "        if load is True:\n",
      "            self.load()\n",
      "        else:\n",
      "            self.loaded = False\n",
      "        if not self.loaded:\n",
      "            if (\n",
      "                self.max_workers > 1\n",
      "                and values is not None\n",
      "                and embeddings is None\n",
      "                and index is None\n",
      "            ):\n",
      "                embeddings = parallel_embeddings(self.embedder,\n",
      "                    values, max_workers, backup=backup, name=name\n",
      "                )\n",
      "            self.init_index(index, values, embeddings, is_embed_batched=is_batched)\n",
      "        if tokenizer is None:\n",
      "            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "        else:\n",
      "            self.tokenizer = tokenizer\n",
      "        self.query_history = []\n",
      "        if not self.loaded:\n",
      "            self.save()\n",
      "\n",
      "    def init_index(\n",
      "        self,\n",
      "        index: Optional[faiss.Index] = None,\n",
      "        values: Optional[List[str]] = None,\n",
      "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "        is_embed_batched: bool = False,\n",
      "    ) -> None:\n",
      "\n",
      "        \"\"\"\n",
      "        initializes the index, there are 4 cases:\n",
      "        1. we create a new index from scratch\n",
      "        2. we create a new index from a list of embeddings and values\n",
      "        3. we create a new index from a faiss index and values list\n",
      "        4. we load an index from a file\n",
      "        \"\"\"\n",
      "        if index is None and values is None and embeddings is None:\n",
      "            print(\"Creating a new index\")\n",
      "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "            self.values = []\n",
      "        elif (\n",
      "            index is None\n",
      "            and values is not None\n",
      "            and embeddings is not None\n",
      "            and len(values) == len(embeddings)\n",
      "        ):\n",
      "            print(\"Creating a new index from a list of embeddings and values\")\n",
      "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "            #add all the embeddings to the index\n",
      "            if is_embed_batched:\n",
      "                print(\"Adding batched embeddings to index\")\n",
      "                print(type(embeddings))\n",
      "                embeddings = np.array(embeddings)\n",
      "                self.add_batch_to_index(values=values, embeddings=embeddings)\n",
      "            else:\n",
      "                for embedding, value in zip(embeddings, values):\n",
      "                    self.add_to_index(value, embedding)\n",
      "\n",
      "        elif (\n",
      "            isinstance(index, faiss.Index)\n",
      "            and index.d == self.embedder.get_embedding_size()\n",
      "            and type(values) == list\n",
      "            and len(values) == index.ntotal\n",
      "        ):\n",
      "            print(\"Creating a new index from a faiss index and values list\")\n",
      "            self.index = index\n",
      "            self.values = values\n",
      "        elif index is None and values is not None and embeddings is None:\n",
      "            print(\"Creating a new index from a list of values\")\n",
      "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "            if is_embed_batched:\n",
      "                batch = []\n",
      "                i = 0\n",
      "                for value in values:\n",
      "                    batch.append(value)\n",
      "                    if len(batch) == 1000:\n",
      "                        start = time.time()\n",
      "                        self.add_batch_to_index(values=batch)\n",
      "                        print(f\"Embedding batch {i} took \", time.time() - start, \" seconds\")\n",
      "                        print(f\"Batch {i} of {len(values)//1000}\")\n",
      "                        i +=1\n",
      "                        batch = []\n",
      "                if len(batch) > 0:\n",
      "                    self.add_batch_to_index(values=batch)\n",
      "            else:\n",
      "                i = 0\n",
      "                for value in values:\n",
      "                    print(\"Embedding value \", i, \" of \", len(values))\n",
      "                    start = time.time()\n",
      "                    self.add_to_index(value)\n",
      "                    print(\"Embedding value \", i, \" took \", time.time() - start, \" seconds\")\n",
      "                    i += 1\n",
      "        else:\n",
      "            print(type(values))\n",
      "            print(type(embeddings))\n",
      "            print(type(index))\n",
      "            raise ValueError(\n",
      "                \"The index is not a valid faiss index or the embedding dimension is not correct\"\n",
      "            )\n",
      "        print(len(self.values), \" values in the index\")\n",
      "        print(self.index.ntotal, \" embeddings in the index\")\n",
      "\n",
      "    def __getstate__(self):\n",
      "        state = self.__dict__.copy()\n",
      "        del state[\"index\"]\n",
      "\n",
      "        index_buffer = io.BytesIO()\n",
      "        faiss.write_index(state[\"index\"], index_buffer)\n",
      "        state[\"index_bytes\"] = index_buffer.getvalue()\n",
      "\n",
      "        return state\n",
      "\n",
      "    def __setstate__(self, state):\n",
      "        index_buffer = io.BytesIO(state[\"index_bytes\"])\n",
      "        state[\"index\"] = faiss.read_index(index_buffer)\n",
      "\n",
      "        del state[\"index_bytes\"]\n",
      "\n",
      "        self.__dict__.update(state)\n",
      "\n",
      "    @classmethod\n",
      "    def from_pandas(\n",
      "        cls,\n",
      "        data_frame: Union[pd.DataFrame, str],\n",
      "        columns: Optional[Union[str, List[str]]] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        in_place: bool = True,\n",
      "        embeddings_col: Optional[str] = None,\n",
      "        markdown: str = \"text/markdown\",\n",
      "    ) -> \"MemoryIndex\":\n",
      "        \"\"\"\n",
      "        Initialize a MemoryIndex object from a pandas DataFrame.\n",
      "\n",
      "        Args:\n",
      "            data_frame: The DataFrame or path to a CSV file.\n",
      "            columns: The columns of the DataFrame to use as values.\n",
      "            name: The name of the index.\n",
      "            save_path: The path to save the index.\n",
      "            in_place: Whether to work on the DataFrame in place or create a copy.\n",
      "            embeddings_col: The column name containing the embeddings.\n",
      "\n",
      "        Returns:\n",
      "            A MemoryIndex object initialized with values and embeddings from the DataFrame.\n",
      "        \"\"\"\n",
      "\n",
      "        if (\n",
      "            isinstance(data_frame, str)\n",
      "            and data_frame.endswith(\".csv\")\n",
      "            and os.path.isfile(data_frame)\n",
      "        ):\n",
      "            print(\"Loading the CSV file\")\n",
      "            try:\n",
      "                data_frame = pd.read_csv(data_frame)\n",
      "            except:\n",
      "                raise ValueError(\"The CSV file is not valid\")\n",
      "            name = data_frame.split(\"/\")[-1].split(\".\")[0]\n",
      "        elif isinstance(data_frame, pd.core.frame.DataFrame) and columns is not None:\n",
      "            print(\"Loading the DataFrame\")\n",
      "            if not in_place:\n",
      "                data_frame = copy.deepcopy(data_frame)\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                \"The data_frame is not a valid pandas dataframe or the columns are not valid or the path is not valid\"\n",
      "            )\n",
      "\n",
      "        values, embeddings = extract_values_and_embeddings(\n",
      "            data_frame, columns, embeddings_col\n",
      "        )\n",
      "        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown)\n",
      "\n",
      "    @classmethod\n",
      "    def from_hf_dataset(\n",
      "        cls,\n",
      "        dataset_url: str,\n",
      "        value_column: str,\n",
      "        embeddings_column: Optional[str] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= CohereEmbedder,\n",
      "        is_batched: bool = False,\n",
      "        markdown: str = \"text/markdown\"\n",
      "    ) -> \"MemoryIndex\":\n",
      "        \"\"\"\n",
      "        Initialize a MemoryIndex object from a Hugging Face dataset.\n",
      "\n",
      "        Args:\n",
      "            dataset_url: The URL of the Hugging Face dataset.\n",
      "            value_column: The column of the dataset to use as values.\n",
      "            embeddings_column: The column of the dataset containing the embeddings.\n",
      "            name: The name of the index.\n",
      "            save_path: The path to save the index.\n",
      "\n",
      "        Returns:\n",
      "            A MemoryIndex object initialized with values and embeddings from the Hugging Face dataset.\n",
      "        \"\"\"\n",
      "        dataset = load_dataset(dataset_url)['train']\n",
      "        if embeddings_column is not None:\n",
      "            values, embeddings = extract_values_and_embeddings_hf(\n",
      "                dataset, value_column, embeddings_column\n",
      "            )\n",
      "        elif embeddings_column is None:\n",
      "            values = extract_values_hf(dataset, value_column)\n",
      "            embeddings = None\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                \"The dataset is not a valid Hugging Face dataset or the columns are not valid\"\n",
      "            )\n",
      "        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, is_batched=is_batched, markdown=markdown)\n",
      "\n",
      "    def add_to_index(\n",
      "        self,\n",
      "        value: str,\n",
      "        embedding: Optional[Union[List[float], np.ndarray, str]] = None,\n",
      "        verbose: bool = False,\n",
      "        default_save: bool = False,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n",
      "        \"\"\"\n",
      "        if value not in self.values:\n",
      "            if embedding is None:\n",
      "                embedding = self.embedder.embed(value)\n",
      "                if verbose:\n",
      "                    display(\n",
      "                        Markdown(\"The value {value} was embedded\".format(value=value))\n",
      "                    )\n",
      "                if type(embedding) is list:\n",
      "                    embedding = np.array([embedding])\n",
      "                self.index.add(embedding)\n",
      "                self.values.append(value)\n",
      "            elif embedding is not None:\n",
      "                if type(embedding) is list:\n",
      "                    embedding = np.array([embedding])\n",
      "                elif type(embedding) is str:\n",
      "                    try:\n",
      "                        embedding = eval(embedding)\n",
      "                        embedding = np.array([embedding]).astype(np.float32)\n",
      "                    except (SyntaxError, ValueError):\n",
      "                        print(\"The string is not a valid list, probably an error:\", embedding)\n",
      "                        return\n",
      "                elif type(embedding) is not np.ndarray:\n",
      "                    raise ValueError(\"The embedding is not a valid type\")\n",
      "\n",
      "                if embedding.ndim == 1:\n",
      "                    embedding = embedding.reshape(1, -1)\n",
      "                self.index.add(embedding)\n",
      "                self.values.append(value)\n",
      "                if default_save:\n",
      "                    self.save()  # we should check here the save time is not too long\n",
      "        else:\n",
      "            if verbose:\n",
      "                display(\n",
      "                    Markdown(\n",
      "                        \"The value {value} was already in the index\".format(value=value)\n",
      "                    )\n",
      "                )\n",
      "    def add_batch_to_index(\n",
      "        self,\n",
      "        values: List[str],\n",
      "        embeddings: Optional[Union[List[float], np.ndarray, str]] = None,\n",
      "        verbose: bool = False,\n",
      "        default_save: bool = False,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n",
      "        \"\"\"\n",
      "\n",
      "        if embeddings is None:\n",
      "            embeddings = self.embedder.batch_embed(values)\n",
      "            if verbose:\n",
      "                display(\n",
      "                    Markdown(\"The value batch was embedded\")\n",
      "                )\n",
      "            if type(embeddings) is list:\n",
      "                embeddings = np.array(embeddings)\n",
      "            self.index.add(embeddings)\n",
      "            self.values.extend(values)\n",
      "        elif embeddings is not None:\n",
      "            if type(embeddings) is list:\n",
      "                embeddings = np.array([embeddings])\n",
      "            elif type(embeddings) is str:\n",
      "                try:\n",
      "                    embeddings = eval(embeddings)\n",
      "                    embeddings = np.array([embeddings]).astype(np.float32)\n",
      "                except (SyntaxError, ValueError):\n",
      "                    print(\"The string is not a valid list, probably an error:\", embeddings)\n",
      "                    return\n",
      "            elif type(embeddings) is not np.ndarray:\n",
      "                raise ValueError(\"The embedding is not a valid type\")\n",
      "\n",
      "            self.index.add(embeddings)\n",
      "            self.values.extend(values)\n",
      "            if default_save:\n",
      "                self.save()  # we should check here the save time is not too long\n",
      "\n",
      "    def remove_from_id(self, id: int) -> None:\n",
      "        \"\"\"\n",
      "        Remove a value from the index and the values list.\n",
      "        Args:\n",
      "            value: The value to remove from the index.\n",
      "        \"\"\"\n",
      "        if id is not None:\n",
      "            self.values.pop(id)\n",
      "            self.embeddings = np.array(list(self.embeddings).pop(id))\n",
      "            id_selector = faiss.IDSelectorArray(n=1, ids=np.array([id], dtype=np.int64))\n",
      "            self.index.remove_ids(id_selector)            \n",
      "            self.save()\n",
      "        else:\n",
      "            print(f\"The value '{id}' was not found in the index.\")\n",
      "\n",
      "    def remove_from_index(self, value: str) -> None:\n",
      "        \"\"\"\n",
      "        Remove a value from the index and the values list.\n",
      "        Args:\n",
      "            value: The value to remove from the index.\n",
      "        \"\"\"\n",
      "        index = self.get_index_by_value(value)\n",
      "        if index is not None:\n",
      "            self.values.pop(index)\n",
      "\n",
      "            id_selector = faiss.IDSelectorArray(np.array([index], dtype=np.int64))\n",
      "            self.index.remove_ids(id_selector)\n",
      "\n",
      "            self.save()\n",
      "        else:\n",
      "            print(f\"The value '{value}' was not found in the index.\")\n",
      "\n",
      "    def get_embedding_by_index(self, index: int) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Get the embedding corresponding to a certain index value.\n",
      "        \"\"\"\n",
      "        if index < 0 or index >= len(self.values):\n",
      "            raise ValueError(\"The index is out of range\")\n",
      "\n",
      "        embedding = self.index.reconstruct(index)\n",
      "\n",
      "        return embedding\n",
      "\n",
      "    def get_index_by_value(self, value: str) -> Optional[int]:\n",
      "        \"\"\"\n",
      "        Get the index corresponding to a value in self.values.\n",
      "        \"\"\"\n",
      "        if value in self.values:\n",
      "            index = self.values.index(value)\n",
      "            return index\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    def get_embedding_by_value(self, value: str) -> Optional[np.ndarray]:\n",
      "        \"\"\"\n",
      "        Get the embedding corresponding to a certain value in self.values.\n",
      "        \"\"\"\n",
      "        index = self.get_index_by_value(value)\n",
      "        if index is not None:\n",
      "            embedding = self.get_embedding_by_index(index)\n",
      "            return embedding\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    def get_all_embeddings(self) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Get all the embeddings in the index.\n",
      "        \"\"\"\n",
      "        embeddings = []\n",
      "        for i in range(len(self.values)):\n",
      "            embeddings.append(self.get_embedding_by_index(i))\n",
      "        self.embeddings = np.array(embeddings)\n",
      "        return self.embeddings\n",
      "\n",
      "    def faiss_query(self, query: str, k: int = 10) -> Tuple[List[str], List[float]]:\n",
      "        \"\"\"Query the faiss index for the top-k most similar values to the query\"\"\"\n",
      "\n",
      "        embedding = self.embedder.embed(query)\n",
      "        if k > len(self.values):\n",
      "            k = len(self.values)\n",
      "        D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\n",
      "        values = [self.values[i] for i in I[0]]\n",
      "        scores = [d for d in D[0]]\n",
      "        return values, scores, I\n",
      "\n",
      "    def token_bound_query(self, query, k=10, max_tokens=4000):\n",
      "        \"\"\"Query the faiss index for the top-k most similar values to the query, but bound the number of tokens retrieved by the max_tokens parameter\"\"\"\n",
      "        returned_tokens = 0\n",
      "        top_k_hint = []\n",
      "        scores = []\n",
      "        tokens = []\n",
      "        indices = []\n",
      "\n",
      "        if len(self.values) > 0:\n",
      "            top_k, scores, indices = self.faiss_query(query, k=min(k, len(self.values)))\n",
      "\n",
      "            for hint in top_k:\n",
      "                message_tokens = len(self.tokenizer.encode(hint))\n",
      "                tokens.append(message_tokens)\n",
      "                if returned_tokens + message_tokens <= max_tokens:\n",
      "                    top_k_hint += [hint]\n",
      "                    returned_tokens += message_tokens\n",
      "\n",
      "            self.query_history.append(\n",
      "                {\n",
      "                    \"query\": query,\n",
      "                    \"hints\": top_k_hint,\n",
      "                    \"scores\": scores,\n",
      "                    \"indices\": indices,\n",
      "                    \"hints_tokens\": tokens,\n",
      "                    \"returned_tokens\": returned_tokens,\n",
      "                    \"max_tokens\": max_tokens,\n",
      "                    \"k\": k,\n",
      "                }\n",
      "            )\n",
      "\n",
      "        return top_k_hint, scores, indices\n",
      "\n",
      "    def save(self):\n",
      "        save(self)\n",
      "\n",
      "    def load(self):\n",
      "        load(self)\n",
      "\n",
      "    def prune(\n",
      "        self,\n",
      "        constraint: Optional[str] = None,\n",
      "        regex_pattern: Optional[str] = None,\n",
      "        length_constraint: Optional[int] = None,\n",
      "        tokenizer: Optional[tiktoken.Encoding] = None,\n",
      "    ) -> \"MemoryIndex\":\n",
      "        if tokenizer is None:\n",
      "            tokenizer = self.tokenizer\n",
      "        return prune_index(self, constraint, regex_pattern, length_constraint,tokenizer)\n",
      "\n",
      "Function calls: shape: (82,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"embedder\"\n",
      "\t\"parallel_embed…\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"type\"\n",
      "\t\"zip\"\n",
      "\t\"isinstance\"\n",
      "\t\"type\"\n",
      "\t…\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"min\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"save\"\n",
      "\t\"load\"\n",
      "\t\"prune_index\"\n",
      "]\n",
      "\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    index: Optional[faiss.Index] = None,\n",
      "    values: Optional[List[str]] = None,\n",
      "    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    load: bool = False,\n",
      "    tokenizer: Optional[tiktoken.Encoding] = None,\n",
      "    max_workers: int = 1,\n",
      "    backup: bool = False,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "    is_batched: bool = False,\n",
      "    markdown: str = \"text/markdown\",\n",
      "):\n",
      "\n",
      "    self.name = name\n",
      "    self.embedder = embedder()\n",
      "    self.save_path = save_path if save_path is not None else \"storage\"\n",
      "    os.makedirs(self.save_path, exist_ok=True)\n",
      "    self.values = []\n",
      "    self.embeddings = []\n",
      "    self.max_workers = max_workers\n",
      "    self.is_batched = is_batched\n",
      "    self.markdown = markdown\n",
      "\n",
      "    if load is True:\n",
      "        self.load()\n",
      "    else:\n",
      "        self.loaded = False\n",
      "    if not self.loaded:\n",
      "        if (\n",
      "            self.max_workers > 1\n",
      "            and values is not None\n",
      "            and embeddings is None\n",
      "            and index is None\n",
      "        ):\n",
      "            embeddings = parallel_embeddings(self.embedder,\n",
      "                values, max_workers, backup=backup, name=name\n",
      "            )\n",
      "        self.init_index(index, values, embeddings, is_embed_batched=is_batched)\n",
      "    if tokenizer is None:\n",
      "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "    else:\n",
      "        self.tokenizer = tokenizer\n",
      "    self.query_history = []\n",
      "    if not self.loaded:\n",
      "        self.save()\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"embedder\"\n",
      "\t\"parallel_embed…\n",
      "]\n",
      "\n",
      "\n",
      "def init_index(\n",
      "    self,\n",
      "    index: Optional[faiss.Index] = None,\n",
      "    values: Optional[List[str]] = None,\n",
      "    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "    is_embed_batched: bool = False,\n",
      ") -> None:\n",
      "\n",
      "    \"\"\"\n",
      "        initializes the index, there are 4 cases:\n",
      "        1. we create a new index from scratch\n",
      "        2. we create a new index from a list of embeddings and values\n",
      "        3. we create a new index from a faiss index and values list\n",
      "        4. we load an index from a file\n",
      "        \"\"\"\n",
      "    if index is None and values is None and embeddings is None:\n",
      "        print(\"Creating a new index\")\n",
      "        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "        self.values = []\n",
      "    elif (\n",
      "        index is None\n",
      "        and values is not None\n",
      "        and embeddings is not None\n",
      "        and len(values) == len(embeddings)\n",
      "    ):\n",
      "        print(\"Creating a new index from a list of embeddings and values\")\n",
      "        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "        #add all the embeddings to the index\n",
      "        if is_embed_batched:\n",
      "            print(\"Adding batched embeddings to index\")\n",
      "            print(type(embeddings))\n",
      "            embeddings = np.array(embeddings)\n",
      "            self.add_batch_to_index(values=values, embeddings=embeddings)\n",
      "        else:\n",
      "            for embedding, value in zip(embeddings, values):\n",
      "                self.add_to_index(value, embedding)\n",
      "\n",
      "    elif (\n",
      "        isinstance(index, faiss.Index)\n",
      "        and index.d == self.embedder.get_embedding_size()\n",
      "        and type(values) == list\n",
      "        and len(values) == index.ntotal\n",
      "    ):\n",
      "        print(\"Creating a new index from a faiss index and values list\")\n",
      "        self.index = index\n",
      "        self.values = values\n",
      "    elif index is None and values is not None and embeddings is None:\n",
      "        print(\"Creating a new index from a list of values\")\n",
      "        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "        if is_embed_batched:\n",
      "            batch = []\n",
      "            i = 0\n",
      "            for value in values:\n",
      "                batch.append(value)\n",
      "                if len(batch) == 1000:\n",
      "                    start = time.time()\n",
      "                    self.add_batch_to_index(values=batch)\n",
      "                    print(f\"Embedding batch {i} took \", time.time() - start, \" seconds\")\n",
      "                    print(f\"Batch {i} of {len(values)//1000}\")\n",
      "                    i +=1\n",
      "                    batch = []\n",
      "            if len(batch) > 0:\n",
      "                self.add_batch_to_index(values=batch)\n",
      "        else:\n",
      "            i = 0\n",
      "            for value in values:\n",
      "                print(\"Embedding value \", i, \" of \", len(values))\n",
      "                start = time.time()\n",
      "                self.add_to_index(value)\n",
      "                print(\"Embedding value \", i, \" took \", time.time() - start, \" seconds\")\n",
      "                i += 1\n",
      "    else:\n",
      "        print(type(values))\n",
      "        print(type(embeddings))\n",
      "        print(type(index))\n",
      "        raise ValueError(\n",
      "            \"The index is not a valid faiss index or the embedding dimension is not correct\"\n",
      "        )\n",
      "    print(len(self.values), \" values in the index\")\n",
      "    print(self.index.ntotal, \" embeddings in the index\")\n",
      "\n",
      "Function calls: shape: (31,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"type\"\n",
      "\t\"zip\"\n",
      "\t\"isinstance\"\n",
      "\t\"type\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t…\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"type\"\n",
      "\t\"print\"\n",
      "\t\"type\"\n",
      "\t\"print\"\n",
      "\t\"type\"\n",
      "\t\"ValueError\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def __getstate__(self):\n",
      "    state = self.__dict__.copy()\n",
      "    del state[\"index\"]\n",
      "\n",
      "    index_buffer = io.BytesIO()\n",
      "    faiss.write_index(state[\"index\"], index_buffer)\n",
      "    state[\"index_bytes\"] = index_buffer.getvalue()\n",
      "\n",
      "    return state\n",
      "\n",
      "\n",
      "\n",
      "def __setstate__(self, state):\n",
      "    index_buffer = io.BytesIO(state[\"index_bytes\"])\n",
      "    state[\"index\"] = faiss.read_index(index_buffer)\n",
      "\n",
      "    del state[\"index_bytes\"]\n",
      "\n",
      "    self.__dict__.update(state)\n",
      "\n",
      "\n",
      "\n",
      "@classmethod\n",
      "def from_pandas(\n",
      "    cls,\n",
      "    data_frame: Union[pd.DataFrame, str],\n",
      "    columns: Optional[Union[str, List[str]]] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    in_place: bool = True,\n",
      "    embeddings_col: Optional[str] = None,\n",
      "    markdown: str = \"text/markdown\",\n",
      ") -> \"MemoryIndex\":\n",
      "    \"\"\"\n",
      "        Initialize a MemoryIndex object from a pandas DataFrame.\n",
      "\n",
      "        Args:\n",
      "            data_frame: The DataFrame or path to a CSV file.\n",
      "            columns: The columns of the DataFrame to use as values.\n",
      "            name: The name of the index.\n",
      "            save_path: The path to save the index.\n",
      "            in_place: Whether to work on the DataFrame in place or create a copy.\n",
      "            embeddings_col: The column name containing the embeddings.\n",
      "\n",
      "        Returns:\n",
      "            A MemoryIndex object initialized with values and embeddings from the DataFrame.\n",
      "        \"\"\"\n",
      "\n",
      "    if (\n",
      "        isinstance(data_frame, str)\n",
      "        and data_frame.endswith(\".csv\")\n",
      "        and os.path.isfile(data_frame)\n",
      "    ):\n",
      "        print(\"Loading the CSV file\")\n",
      "        try:\n",
      "            data_frame = pd.read_csv(data_frame)\n",
      "        except:\n",
      "            raise ValueError(\"The CSV file is not valid\")\n",
      "        name = data_frame.split(\"/\")[-1].split(\".\")[0]\n",
      "    elif isinstance(data_frame, pd.core.frame.DataFrame) and columns is not None:\n",
      "        print(\"Loading the DataFrame\")\n",
      "        if not in_place:\n",
      "            data_frame = copy.deepcopy(data_frame)\n",
      "    else:\n",
      "        raise ValueError(\n",
      "            \"The data_frame is not a valid pandas dataframe or the columns are not valid or the path is not valid\"\n",
      "        )\n",
      "\n",
      "    values, embeddings = extract_values_and_embeddings(\n",
      "        data_frame, columns, embeddings_col\n",
      "    )\n",
      "    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown)\n",
      "\n",
      "Function calls: shape: (8,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"print\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"print\"\n",
      "\t\"ValueError\"\n",
      "\t\"extract_values…\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "\n",
      "@classmethod\n",
      "def from_hf_dataset(\n",
      "    cls,\n",
      "    dataset_url: str,\n",
      "    value_column: str,\n",
      "    embeddings_column: Optional[str] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= CohereEmbedder,\n",
      "    is_batched: bool = False,\n",
      "    markdown: str = \"text/markdown\"\n",
      ") -> \"MemoryIndex\":\n",
      "    \"\"\"\n",
      "        Initialize a MemoryIndex object from a Hugging Face dataset.\n",
      "\n",
      "        Args:\n",
      "            dataset_url: The URL of the Hugging Face dataset.\n",
      "            value_column: The column of the dataset to use as values.\n",
      "            embeddings_column: The column of the dataset containing the embeddings.\n",
      "            name: The name of the index.\n",
      "            save_path: The path to save the index.\n",
      "\n",
      "        Returns:\n",
      "            A MemoryIndex object initialized with values and embeddings from the Hugging Face dataset.\n",
      "        \"\"\"\n",
      "    dataset = load_dataset(dataset_url)['train']\n",
      "    if embeddings_column is not None:\n",
      "        values, embeddings = extract_values_and_embeddings_hf(\n",
      "            dataset, value_column, embeddings_column\n",
      "        )\n",
      "    elif embeddings_column is None:\n",
      "        values = extract_values_hf(dataset, value_column)\n",
      "        embeddings = None\n",
      "    else:\n",
      "        raise ValueError(\n",
      "            \"The dataset is not a valid Hugging Face dataset or the columns are not valid\"\n",
      "        )\n",
      "    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, is_batched=is_batched, markdown=markdown)\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"load_dataset\"\n",
      "\t\"extract_values…\n",
      "\t\"extract_values…\n",
      "\t\"ValueError\"\n",
      "\t\"cls\"\n",
      "]\n",
      "\n",
      "\n",
      "def add_to_index(\n",
      "    self,\n",
      "    value: str,\n",
      "    embedding: Optional[Union[List[float], np.ndarray, str]] = None,\n",
      "    verbose: bool = False,\n",
      "    default_save: bool = False,\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n",
      "        \"\"\"\n",
      "    if value not in self.values:\n",
      "        if embedding is None:\n",
      "            embedding = self.embedder.embed(value)\n",
      "            if verbose:\n",
      "                display(\n",
      "                    Markdown(\"The value {value} was embedded\".format(value=value))\n",
      "                )\n",
      "            if type(embedding) is list:\n",
      "                embedding = np.array([embedding])\n",
      "            self.index.add(embedding)\n",
      "            self.values.append(value)\n",
      "        elif embedding is not None:\n",
      "            if type(embedding) is list:\n",
      "                embedding = np.array([embedding])\n",
      "            elif type(embedding) is str:\n",
      "                try:\n",
      "                    embedding = eval(embedding)\n",
      "                    embedding = np.array([embedding]).astype(np.float32)\n",
      "                except (SyntaxError, ValueError):\n",
      "                    print(\"The string is not a valid list, probably an error:\", embedding)\n",
      "                    return\n",
      "            elif type(embedding) is not np.ndarray:\n",
      "                raise ValueError(\"The embedding is not a valid type\")\n",
      "\n",
      "            if embedding.ndim == 1:\n",
      "                embedding = embedding.reshape(1, -1)\n",
      "            self.index.add(embedding)\n",
      "            self.values.append(value)\n",
      "            if default_save:\n",
      "                self.save()  # we should check here the save time is not too long\n",
      "    else:\n",
      "        if verbose:\n",
      "            display(\n",
      "                Markdown(\n",
      "                    \"The value {value} was already in the index\".format(value=value)\n",
      "                )\n",
      "            )\n",
      "\n",
      "Function calls: shape: (11,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"display\"\n",
      "\t\"Markdown\"\n",
      "\t\"type\"\n",
      "\t\"type\"\n",
      "\t\"type\"\n",
      "\t\"eval\"\n",
      "\t\"print\"\n",
      "\t\"type\"\n",
      "\t\"ValueError\"\n",
      "\t\"display\"\n",
      "\t\"Markdown\"\n",
      "]\n",
      "\n",
      "def add_batch_to_index(\n",
      "    self,\n",
      "    values: List[str],\n",
      "    embeddings: Optional[Union[List[float], np.ndarray, str]] = None,\n",
      "    verbose: bool = False,\n",
      "    default_save: bool = False,\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n",
      "        \"\"\"\n",
      "\n",
      "    if embeddings is None:\n",
      "        embeddings = self.embedder.batch_embed(values)\n",
      "        if verbose:\n",
      "            display(\n",
      "                Markdown(\"The value batch was embedded\")\n",
      "            )\n",
      "        if type(embeddings) is list:\n",
      "            embeddings = np.array(embeddings)\n",
      "        self.index.add(embeddings)\n",
      "        self.values.extend(values)\n",
      "    elif embeddings is not None:\n",
      "        if type(embeddings) is list:\n",
      "            embeddings = np.array([embeddings])\n",
      "        elif type(embeddings) is str:\n",
      "            try:\n",
      "                embeddings = eval(embeddings)\n",
      "                embeddings = np.array([embeddings]).astype(np.float32)\n",
      "            except (SyntaxError, ValueError):\n",
      "                print(\"The string is not a valid list, probably an error:\", embeddings)\n",
      "                return\n",
      "        elif type(embeddings) is not np.ndarray:\n",
      "            raise ValueError(\"The embedding is not a valid type\")\n",
      "\n",
      "        self.index.add(embeddings)\n",
      "        self.values.extend(values)\n",
      "        if default_save:\n",
      "            self.save()  # we should check here the save time is not too long\n",
      "\n",
      "Function calls: shape: (9,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"display\"\n",
      "\t\"Markdown\"\n",
      "\t\"type\"\n",
      "\t\"type\"\n",
      "\t\"type\"\n",
      "\t\"eval\"\n",
      "\t\"print\"\n",
      "\t\"type\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def remove_from_id(self, id: int) -> None:\n",
      "    \"\"\"\n",
      "        Remove a value from the index and the values list.\n",
      "        Args:\n",
      "            value: The value to remove from the index.\n",
      "        \"\"\"\n",
      "    if id is not None:\n",
      "        self.values.pop(id)\n",
      "        self.embeddings = np.array(list(self.embeddings).pop(id))\n",
      "        id_selector = faiss.IDSelectorArray(n=1, ids=np.array([id], dtype=np.int64))\n",
      "        self.index.remove_ids(id_selector)            \n",
      "        self.save()\n",
      "    else:\n",
      "        print(f\"The value '{id}' was not found in the index.\")\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"list\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def remove_from_index(self, value: str) -> None:\n",
      "    \"\"\"\n",
      "        Remove a value from the index and the values list.\n",
      "        Args:\n",
      "            value: The value to remove from the index.\n",
      "        \"\"\"\n",
      "    index = self.get_index_by_value(value)\n",
      "    if index is not None:\n",
      "        self.values.pop(index)\n",
      "\n",
      "        id_selector = faiss.IDSelectorArray(np.array([index], dtype=np.int64))\n",
      "        self.index.remove_ids(id_selector)\n",
      "\n",
      "        self.save()\n",
      "    else:\n",
      "        print(f\"The value '{value}' was not found in the index.\")\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_embedding_by_index(self, index: int) -> np.ndarray:\n",
      "    \"\"\"\n",
      "        Get the embedding corresponding to a certain index value.\n",
      "        \"\"\"\n",
      "    if index < 0 or index >= len(self.values):\n",
      "        raise ValueError(\"The index is out of range\")\n",
      "\n",
      "    embedding = self.index.reconstruct(index)\n",
      "\n",
      "    return embedding\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_index_by_value(self, value: str) -> Optional[int]:\n",
      "    \"\"\"\n",
      "        Get the index corresponding to a value in self.values.\n",
      "        \"\"\"\n",
      "    if value in self.values:\n",
      "        index = self.values.index(value)\n",
      "        return index\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "\n",
      "def get_embedding_by_value(self, value: str) -> Optional[np.ndarray]:\n",
      "    \"\"\"\n",
      "        Get the embedding corresponding to a certain value in self.values.\n",
      "        \"\"\"\n",
      "    index = self.get_index_by_value(value)\n",
      "    if index is not None:\n",
      "        embedding = self.get_embedding_by_index(index)\n",
      "        return embedding\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "\n",
      "def get_all_embeddings(self) -> np.ndarray:\n",
      "    \"\"\"\n",
      "        Get all the embeddings in the index.\n",
      "        \"\"\"\n",
      "    embeddings = []\n",
      "    for i in range(len(self.values)):\n",
      "        embeddings.append(self.get_embedding_by_index(i))\n",
      "    self.embeddings = np.array(embeddings)\n",
      "    return self.embeddings\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def faiss_query(self, query: str, k: int = 10) -> Tuple[List[str], List[float]]:\n",
      "    \"\"\"Query the faiss index for the top-k most similar values to the query\"\"\"\n",
      "\n",
      "    embedding = self.embedder.embed(query)\n",
      "    if k > len(self.values):\n",
      "        k = len(self.values)\n",
      "    D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\n",
      "    values = [self.values[i] for i in I[0]]\n",
      "    scores = [d for d in D[0]]\n",
      "    return values, scores, I\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def token_bound_query(self, query, k=10, max_tokens=4000):\n",
      "    \"\"\"Query the faiss index for the top-k most similar values to the query, but bound the number of tokens retrieved by the max_tokens parameter\"\"\"\n",
      "    returned_tokens = 0\n",
      "    top_k_hint = []\n",
      "    scores = []\n",
      "    tokens = []\n",
      "    indices = []\n",
      "\n",
      "    if len(self.values) > 0:\n",
      "        top_k, scores, indices = self.faiss_query(query, k=min(k, len(self.values)))\n",
      "\n",
      "        for hint in top_k:\n",
      "            message_tokens = len(self.tokenizer.encode(hint))\n",
      "            tokens.append(message_tokens)\n",
      "            if returned_tokens + message_tokens <= max_tokens:\n",
      "                top_k_hint += [hint]\n",
      "                returned_tokens += message_tokens\n",
      "\n",
      "        self.query_history.append(\n",
      "            {\n",
      "                \"query\": query,\n",
      "                \"hints\": top_k_hint,\n",
      "                \"scores\": scores,\n",
      "                \"indices\": indices,\n",
      "                \"hints_tokens\": tokens,\n",
      "                \"returned_tokens\": returned_tokens,\n",
      "                \"max_tokens\": max_tokens,\n",
      "                \"k\": k,\n",
      "            }\n",
      "        )\n",
      "\n",
      "    return top_k_hint, scores, indices\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"min\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def save(self):\n",
      "    save(self)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"save\"\n",
      "]\n",
      "\n",
      "\n",
      "def load(self):\n",
      "    load(self)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"load\"\n",
      "]\n",
      "\n",
      "\n",
      "def prune(\n",
      "    self,\n",
      "    constraint: Optional[str] = None,\n",
      "    regex_pattern: Optional[str] = None,\n",
      "    length_constraint: Optional[int] = None,\n",
      "    tokenizer: Optional[tiktoken.Encoding] = None,\n",
      ") -> \"MemoryIndex\":\n",
      "    if tokenizer is None:\n",
      "        tokenizer = self.tokenizer\n",
      "    return prune_index(self, constraint, regex_pattern, length_constraint,tokenizer)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"prune_index\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class PandasIndex(MemoryIndex):\n",
      "    \"\"\"\n",
      "    A class to create an index of a pandas DataFrame, allowing querying on specified columns.\n",
      "    Inherits from MemoryIndex class.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        df: pd.DataFrame,\n",
      "        row_func: Optional[Callable[[pd.Series], str]] = None,\n",
      "        name=\"pandas_index\",\n",
      "        columns: Optional[List[str]] = None,\n",
      "        load=False,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize a PandasIndex object.\n",
      "\n",
      "        Args:\n",
      "            df: A pandas DataFrame to index.\n",
      "            row_func: An optional function to process rows before adding them to the index.\n",
      "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
      "        \"\"\"\n",
      "        if row_func is None:\n",
      "            row_func = lambda row: str(row)\n",
      "        self.row_func = row_func\n",
      "\n",
      "        self.df = df\n",
      "        MemoryIndex.__init__(\n",
      "            self, name=name, load=load\n",
      "        )  # Initialize the parent MemoryIndex class\n",
      "\n",
      "        for _, row in df.iterrows():\n",
      "            self.add_to_index(row_func(row))\n",
      "\n",
      "        self.columns: Dict[str, MemoryIndex] = {}\n",
      "\n",
      "        # Set up columns during initialization\n",
      "        if columns is None:\n",
      "            self.setup_columns()\n",
      "        else:\n",
      "            self.setup_columns(columns)\n",
      "        self.save()\n",
      "        for col in self.columns:\n",
      "            self.columns[col].save()\n",
      "        self.executed_tasks = []\n",
      "\n",
      "    def setup_columns(self, columns: Optional[List[str]] = None):\n",
      "        \"\"\"\n",
      "        Set up columns for indexing.\n",
      "\n",
      "        Args:\n",
      "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
      "        \"\"\"\n",
      "        if columns is None:\n",
      "            # Use string columns or columns with lists containing a single string by default\n",
      "            columns = []\n",
      "\n",
      "        for col in columns:\n",
      "            self.columns[col] = MemoryIndex.from_pandas(\n",
      "                self.df, columns=col, name=f\"{self.name}_{col}\"\n",
      "            )\n",
      "\n",
      "    def query_columns(self, query: str, columns: List[str]) -> List[Tuple[str, float]]:\n",
      "        \"\"\"\n",
      "        Query the indexed columns of the DataFrame.\n",
      "\n",
      "        Args:\n",
      "            query: The search query as a string.\n",
      "            columns: A list of column names to query.\n",
      "\n",
      "        Returns:\n",
      "            A list of tuples containing the matched value and its similarity score.\n",
      "        \"\"\"\n",
      "        results = []\n",
      "        for col in columns:\n",
      "            if col in self.columns:\n",
      "                results.extend(self.columns[col].faiss_query(query))\n",
      "            else:\n",
      "                raise KeyError(\n",
      "                    f\"Column '{col}' not found in PandaDb columns dictionary.\"\n",
      "                )\n",
      "        return results\n",
      "\n",
      "    def add_row(self, row: pd.Series) -> None:\n",
      "        \"\"\"\n",
      "        Add a row to the DataFrame and update the row and column indexes.\n",
      "\n",
      "        Args:\n",
      "            row: A pandas Series representing the row to add.\n",
      "        \"\"\"\n",
      "        self.df = pd.concat([self.df, row.to_frame().T], ignore_index=True)\n",
      "        self.add_to_index(self.row_func(row))\n",
      "\n",
      "        for col in self.columns:\n",
      "            if col in row:\n",
      "                self.columns[col].add_to_index(row[col])\n",
      "\n",
      "    def remove_row(self, index: int) -> None:\n",
      "        \"\"\"\n",
      "        Remove a row from the DataFrame and update the row and column indexes.\n",
      "\n",
      "        Args:\n",
      "            index: The index of the row to remove.\n",
      "        \"\"\"\n",
      "        if 0 <= index < len(self.df):\n",
      "            self.remove_from_index(self.values[index])\n",
      "\n",
      "            for col in self.columns:\n",
      "                self.columns[col].remove_from_index(self.columns[col].values[index])\n",
      "\n",
      "            self.df.drop(index, inplace=True)\n",
      "            self.df.reset_index(drop=True, inplace=True)\n",
      "        else:\n",
      "            raise IndexError(\n",
      "                f\"Index {index} is out of bounds for DataFrame with length {len(self.df)}\"\n",
      "            )\n",
      "\n",
      "    def rows_from_value(\n",
      "        self, value: Union[str, int, float], column: Optional[str] = None\n",
      "    ) -> pd.DataFrame:\n",
      "        \"\"\"\n",
      "        Return all rows of the DataFrame that have a particular value in the row index or a column index.\n",
      "\n",
      "        Args:\n",
      "            value: The value to search for in the DataFrame.\n",
      "            column: The name of the column to search in. If None, search in the row index.\n",
      "\n",
      "        Returns:\n",
      "            A pandas DataFrame containing the rows with the specified value.\n",
      "        \"\"\"\n",
      "        if column is None:\n",
      "            return self.df.loc[self.df.index == value]\n",
      "        else:\n",
      "            if column in self.df.columns:\n",
      "                return self.df.loc[self.df[column] == value]\n",
      "            else:\n",
      "                raise KeyError(f\"Column '{column}' not found in the DataFrame.\")\n",
      "\n",
      "    def apply_llmtask(\n",
      "        self,\n",
      "        path: List[List[int]],\n",
      "        chatbot: Chat,\n",
      "        task_name=None,\n",
      "        write_func=None,\n",
      "        columns: Optional[List[str]] = None,\n",
      "        task_id=None,\n",
      "        max_workers=1,\n",
      "        calls_per_minute: int = 20,\n",
      "    ) -> pd.DataFrame:\n",
      "        \"\"\"\n",
      "        Apply a writing task to the specified columns or the main index, and create new modified indexes and a corresponding DataFrame with new values.\n",
      "\n",
      "        Args:\n",
      "            write_task: An instance of a writing task (subclass of BaseTask).\n",
      "            columns: A list of column names to apply the writing task to, or None (default) to apply the task to the main index.\n",
      "\n",
      "        Returns:\n",
      "            A pandas DataFrame containing the modified values in the specified columns or a new column with the modified values of the main index.\n",
      "        \"\"\"\n",
      "        modified_df = self.df.copy()\n",
      "        if task_name is None and task_id is None:\n",
      "            task_name = \"llm_task\"\n",
      "        elif task_name is None:\n",
      "            task_name = task_id\n",
      "\n",
      "        if columns is None:\n",
      "            # Apply the writing task to the main index\n",
      "            write_index = self\n",
      "            write_task = LLMWriter(\n",
      "                write_index,\n",
      "                path,\n",
      "                chatbot,\n",
      "                task_name=task_name,\n",
      "                write_func=write_func,\n",
      "                context=self.df,\n",
      "                task_id=task_id,\n",
      "                max_workers=max_workers,\n",
      "                calls_per_minute=calls_per_minute,\n",
      "            )\n",
      "\n",
      "            new_index = write_task.write()\n",
      "\n",
      "            # Create a mapping of old values to new values\n",
      "            old_to_new_values = dict(zip(self.values, new_index.values))\n",
      "\n",
      "            # Update the row values in the modified DataFrame\n",
      "            modified_df[\"new_column\"] = modified_df.apply(\n",
      "                lambda row: old_to_new_values.get(\n",
      "                    self.row_func(row), self.row_func(row)\n",
      "                ),\n",
      "                axis=1,\n",
      "            )\n",
      "        else:\n",
      "            # Iterate over the specified columns\n",
      "            for col in columns:\n",
      "                if col in self.columns:\n",
      "                    # Apply the writing task to the column\n",
      "                    write_index = self.columns[col]\n",
      "                    write_task = LLMWriter(\n",
      "                        write_index,\n",
      "                        path,\n",
      "                        chatbot,\n",
      "                        write_func=write_func,\n",
      "                        context=self.df,\n",
      "                        task_id=task_id,\n",
      "                        max_workers=max_workers,\n",
      "                        calls_per_minute=calls_per_minute,\n",
      "                    )\n",
      "                    new_index = write_task.write()\n",
      "\n",
      "                    # Create a mapping of old values to new values\n",
      "                    old_to_new_values = dict(\n",
      "                        zip(self.columns[col].values, new_index.values)\n",
      "                    )\n",
      "\n",
      "                    # Update the column values in the modified DataFrame\n",
      "                    modified_df[col] = modified_df[col].apply(\n",
      "                        lambda x: old_to_new_values.get(x, x)\n",
      "                    )\n",
      "\n",
      "                    # Update the column's MemoryIndex\n",
      "                    self.columns[col] = new_index\n",
      "                    self.columns[col].save()\n",
      "                else:\n",
      "                    raise KeyError(\n",
      "                        f\"Column '{col}' not found in PandasIndex columns dictionary.\"\n",
      "                    )\n",
      "        # remove context from the write_task to avoid memory leak\n",
      "        write_task.context = None\n",
      "        self.executed_tasks.append({\"task\": write_task, \"output\": modified_df})\n",
      "\n",
      "        return modified_df\n",
      "\n",
      "Function calls: shape: (14,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"str\"\n",
      "\t\"row_func\"\n",
      "\t\"KeyError\"\n",
      "\t\"len\"\n",
      "\t\"IndexError\"\n",
      "\t\"len\"\n",
      "\t\"KeyError\"\n",
      "\t\"LLMWriter\"\n",
      "\t\"dict\"\n",
      "\t\"zip\"\n",
      "\t\"LLMWriter\"\n",
      "\t\"dict\"\n",
      "\t\"zip\"\n",
      "\t\"KeyError\"\n",
      "]\n",
      "\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    df: pd.DataFrame,\n",
      "    row_func: Optional[Callable[[pd.Series], str]] = None,\n",
      "    name=\"pandas_index\",\n",
      "    columns: Optional[List[str]] = None,\n",
      "    load=False,\n",
      "):\n",
      "    \"\"\"\n",
      "        Initialize a PandasIndex object.\n",
      "\n",
      "        Args:\n",
      "            df: A pandas DataFrame to index.\n",
      "            row_func: An optional function to process rows before adding them to the index.\n",
      "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
      "        \"\"\"\n",
      "    if row_func is None:\n",
      "        row_func = lambda row: str(row)\n",
      "    self.row_func = row_func\n",
      "\n",
      "    self.df = df\n",
      "    MemoryIndex.__init__(\n",
      "        self, name=name, load=load\n",
      "    )  # Initialize the parent MemoryIndex class\n",
      "\n",
      "    for _, row in df.iterrows():\n",
      "        self.add_to_index(row_func(row))\n",
      "\n",
      "    self.columns: Dict[str, MemoryIndex] = {}\n",
      "\n",
      "    # Set up columns during initialization\n",
      "    if columns is None:\n",
      "        self.setup_columns()\n",
      "    else:\n",
      "        self.setup_columns(columns)\n",
      "    self.save()\n",
      "    for col in self.columns:\n",
      "        self.columns[col].save()\n",
      "    self.executed_tasks = []\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"str\"\n",
      "\t\"row_func\"\n",
      "]\n",
      "\n",
      "\n",
      "def setup_columns(self, columns: Optional[List[str]] = None):\n",
      "    \"\"\"\n",
      "        Set up columns for indexing.\n",
      "\n",
      "        Args:\n",
      "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
      "        \"\"\"\n",
      "    if columns is None:\n",
      "        # Use string columns or columns with lists containing a single string by default\n",
      "        columns = []\n",
      "\n",
      "    for col in columns:\n",
      "        self.columns[col] = MemoryIndex.from_pandas(\n",
      "            self.df, columns=col, name=f\"{self.name}_{col}\"\n",
      "        )\n",
      "\n",
      "\n",
      "\n",
      "def query_columns(self, query: str, columns: List[str]) -> List[Tuple[str, float]]:\n",
      "    \"\"\"\n",
      "        Query the indexed columns of the DataFrame.\n",
      "\n",
      "        Args:\n",
      "            query: The search query as a string.\n",
      "            columns: A list of column names to query.\n",
      "\n",
      "        Returns:\n",
      "            A list of tuples containing the matched value and its similarity score.\n",
      "        \"\"\"\n",
      "    results = []\n",
      "    for col in columns:\n",
      "        if col in self.columns:\n",
      "            results.extend(self.columns[col].faiss_query(query))\n",
      "        else:\n",
      "            raise KeyError(\n",
      "                f\"Column '{col}' not found in PandaDb columns dictionary.\"\n",
      "            )\n",
      "    return results\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"KeyError\"\n",
      "]\n",
      "\n",
      "\n",
      "def add_row(self, row: pd.Series) -> None:\n",
      "    \"\"\"\n",
      "        Add a row to the DataFrame and update the row and column indexes.\n",
      "\n",
      "        Args:\n",
      "            row: A pandas Series representing the row to add.\n",
      "        \"\"\"\n",
      "    self.df = pd.concat([self.df, row.to_frame().T], ignore_index=True)\n",
      "    self.add_to_index(self.row_func(row))\n",
      "\n",
      "    for col in self.columns:\n",
      "        if col in row:\n",
      "            self.columns[col].add_to_index(row[col])\n",
      "\n",
      "\n",
      "\n",
      "def remove_row(self, index: int) -> None:\n",
      "    \"\"\"\n",
      "        Remove a row from the DataFrame and update the row and column indexes.\n",
      "\n",
      "        Args:\n",
      "            index: The index of the row to remove.\n",
      "        \"\"\"\n",
      "    if 0 <= index < len(self.df):\n",
      "        self.remove_from_index(self.values[index])\n",
      "\n",
      "        for col in self.columns:\n",
      "            self.columns[col].remove_from_index(self.columns[col].values[index])\n",
      "\n",
      "        self.df.drop(index, inplace=True)\n",
      "        self.df.reset_index(drop=True, inplace=True)\n",
      "    else:\n",
      "        raise IndexError(\n",
      "            f\"Index {index} is out of bounds for DataFrame with length {len(self.df)}\"\n",
      "        )\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"IndexError\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def rows_from_value(\n",
      "    self, value: Union[str, int, float], column: Optional[str] = None\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "        Return all rows of the DataFrame that have a particular value in the row index or a column index.\n",
      "\n",
      "        Args:\n",
      "            value: The value to search for in the DataFrame.\n",
      "            column: The name of the column to search in. If None, search in the row index.\n",
      "\n",
      "        Returns:\n",
      "            A pandas DataFrame containing the rows with the specified value.\n",
      "        \"\"\"\n",
      "    if column is None:\n",
      "        return self.df.loc[self.df.index == value]\n",
      "    else:\n",
      "        if column in self.df.columns:\n",
      "            return self.df.loc[self.df[column] == value]\n",
      "        else:\n",
      "            raise KeyError(f\"Column '{column}' not found in the DataFrame.\")\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"KeyError\"\n",
      "]\n",
      "\n",
      "\n",
      "def apply_llmtask(\n",
      "    self,\n",
      "    path: List[List[int]],\n",
      "    chatbot: Chat,\n",
      "    task_name=None,\n",
      "    write_func=None,\n",
      "    columns: Optional[List[str]] = None,\n",
      "    task_id=None,\n",
      "    max_workers=1,\n",
      "    calls_per_minute: int = 20,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "        Apply a writing task to the specified columns or the main index, and create new modified indexes and a corresponding DataFrame with new values.\n",
      "\n",
      "        Args:\n",
      "            write_task: An instance of a writing task (subclass of BaseTask).\n",
      "            columns: A list of column names to apply the writing task to, or None (default) to apply the task to the main index.\n",
      "\n",
      "        Returns:\n",
      "            A pandas DataFrame containing the modified values in the specified columns or a new column with the modified values of the main index.\n",
      "        \"\"\"\n",
      "    modified_df = self.df.copy()\n",
      "    if task_name is None and task_id is None:\n",
      "        task_name = \"llm_task\"\n",
      "    elif task_name is None:\n",
      "        task_name = task_id\n",
      "\n",
      "    if columns is None:\n",
      "        # Apply the writing task to the main index\n",
      "        write_index = self\n",
      "        write_task = LLMWriter(\n",
      "            write_index,\n",
      "            path,\n",
      "            chatbot,\n",
      "            task_name=task_name,\n",
      "            write_func=write_func,\n",
      "            context=self.df,\n",
      "            task_id=task_id,\n",
      "            max_workers=max_workers,\n",
      "            calls_per_minute=calls_per_minute,\n",
      "        )\n",
      "\n",
      "        new_index = write_task.write()\n",
      "\n",
      "        # Create a mapping of old values to new values\n",
      "        old_to_new_values = dict(zip(self.values, new_index.values))\n",
      "\n",
      "        # Update the row values in the modified DataFrame\n",
      "        modified_df[\"new_column\"] = modified_df.apply(\n",
      "            lambda row: old_to_new_values.get(\n",
      "                self.row_func(row), self.row_func(row)\n",
      "            ),\n",
      "            axis=1,\n",
      "        )\n",
      "    else:\n",
      "        # Iterate over the specified columns\n",
      "        for col in columns:\n",
      "            if col in self.columns:\n",
      "                # Apply the writing task to the column\n",
      "                write_index = self.columns[col]\n",
      "                write_task = LLMWriter(\n",
      "                    write_index,\n",
      "                    path,\n",
      "                    chatbot,\n",
      "                    write_func=write_func,\n",
      "                    context=self.df,\n",
      "                    task_id=task_id,\n",
      "                    max_workers=max_workers,\n",
      "                    calls_per_minute=calls_per_minute,\n",
      "                )\n",
      "                new_index = write_task.write()\n",
      "\n",
      "                # Create a mapping of old values to new values\n",
      "                old_to_new_values = dict(\n",
      "                    zip(self.columns[col].values, new_index.values)\n",
      "                )\n",
      "\n",
      "                # Update the column values in the modified DataFrame\n",
      "                modified_df[col] = modified_df[col].apply(\n",
      "                    lambda x: old_to_new_values.get(x, x)\n",
      "                )\n",
      "\n",
      "                # Update the column's MemoryIndex\n",
      "                self.columns[col] = new_index\n",
      "                self.columns[col].save()\n",
      "            else:\n",
      "                raise KeyError(\n",
      "                    f\"Column '{col}' not found in PandasIndex columns dictionary.\"\n",
      "                )\n",
      "    # remove context from the write_task to avoid memory leak\n",
      "    write_task.context = None\n",
      "    self.executed_tasks.append({\"task\": write_task, \"output\": modified_df})\n",
      "\n",
      "    return modified_df\n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"LLMWriter\"\n",
      "\t\"dict\"\n",
      "\t\"zip\"\n",
      "\t\"LLMWriter\"\n",
      "\t\"dict\"\n",
      "\t\"zip\"\n",
      "\t\"KeyError\"\n",
      "]\n",
      "\n",
      "\n",
      "class BaseIndex(ABC):\n",
      "    def __init__(\n",
      "            self,\n",
      "            values: Optional[List[str]] = None,\n",
      "            embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "            name: str = \"np_index\",\n",
      "            save_path: Optional[str] = None,\n",
      "            load: bool = False,\n",
      "            embedder: Optional[Union[OpenAiEmbedder, CohereEmbedder]] = OpenAiEmbedder,\n",
      "            token_overflow_strategy: str = \"ignore\",\n",
      "    ):\n",
      "        self.name = name\n",
      "        self.embedder = embedder()\n",
      "        self.save_path = save_path or \"storage\"\n",
      "        os.makedirs(self.save_path, exist_ok=True)\n",
      "        self.values = []\n",
      "        self.embeddings = None  # initialize embeddings as None\n",
      "        self.queries_embeddings = None  # initialize query embeddings as None\n",
      "        self.token_overflow_strategy = token_overflow_strategy\n",
      "        self.queries = []\n",
      "        self.queries_set = set()  # add this to quickly check for duplicates\n",
      "        self.index_set = set()  # add this to quickly check for duplicates\n",
      "        self.loaded = False\n",
      "        self.setup_index(values, embeddings, load)\n",
      "    \n",
      "    @staticmethod\n",
      "    @abstractmethod\n",
      "    def compare_embeddings(query: Any, targets: Any) -> Any:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abstractmethod\n",
      "    def batched_l2_distance(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abstractmethod\n",
      "    def batched_cosine_similarity(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def get(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Union[str, List[str]]:\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def add(self, values: List[str], embeddings: Optional[List[Union[List[float], np.ndarray]]] = None):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def search(self, query: Optional[str] = None, query_embedding: Optional[np.ndarray] = None, top_k: int = 10, metric: str = \"cosine\", filter_mask: Optional[np.ndarray] = None) -> Tuple[List[str], Optional[List[float]], List[int]]:\n",
      "        pass\n",
      "    \n",
      "    @abstractmethod\n",
      "    def setup_index(self, values: Optional[List[str]] = None, embeddings: Optional[List[Union[List[float], np.ndarray]]] = None, load: bool = False):\n",
      "        pass\n",
      "    \n",
      "    # Non-abstract method\n",
      "    def save_index(self):\n",
      "        save_directory = os.path.join(self.save_path, self.name)\n",
      "        os.makedirs(save_directory, exist_ok=True)\n",
      "\n",
      "        with open(os.path.join(save_directory, f\"{self.name}_values.json\"), \"w\") as f:\n",
      "            json.dump(self.values, f)\n",
      "        #check if queries exist\n",
      "        if len(self.queries) > 0:\n",
      "            with open(os.path.join(save_directory, f\"{self.name}_queries.json\"), \"w\") as f:\n",
      "                json.dump(self.queries, f)\n",
      "\n",
      "        # Save embeddings in a subclass-specific way\n",
      "        self._save_embeddings(save_directory)\n",
      "\n",
      "    def load_index(self):\n",
      "        load_directory = os.path.join(self.save_path, self.name)\n",
      "        if not os.path.exists(load_directory):\n",
      "            print(f\"I did not find the directory to load the index from: {load_directory}\")\n",
      "            return\n",
      "\n",
      "        print(f\"Loading index from {load_directory}\")\n",
      "\n",
      "        with open(os.path.join(load_directory, f\"{self.name}_values.json\"), \"r\") as f:\n",
      "            self.values = json.load(f)\n",
      "        self.values_set = set(self.values)\n",
      "        #check that queries exist\n",
      "        if os.path.exists(os.path.join(load_directory, f\"{self.name}_queries.json\")):\n",
      "            with open(os.path.join(load_directory, f\"{self.name}_queries.json\"), \"r\") as f:\n",
      "                self.queries = json.load(f)\n",
      "            self.queries_set = set(self.queries)\n",
      "\n",
      "        # Load embeddings in a subclass-specific way\n",
      "        self._load_embeddings(load_directory)\n",
      "        self.loaded = True\n",
      "\n",
      "    @abstractmethod\n",
      "    def _save_embeddings(self, directory: str):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def _load_embeddings(self, directory: str):\n",
      "        pass\n",
      "\n",
      "Function calls: shape: (12,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"embedder\"\n",
      "\t\"set\"\n",
      "\t\"set\"\n",
      "\t\"open\"\n",
      "\t\"len\"\n",
      "\t\"open\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"open\"\n",
      "\t\"set\"\n",
      "\t\"open\"\n",
      "\t\"set\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "        self,\n",
      "        values: Optional[List[str]] = None,\n",
      "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "        name: str = \"np_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        load: bool = False,\n",
      "        embedder: Optional[Union[OpenAiEmbedder, CohereEmbedder]] = OpenAiEmbedder,\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "):\n",
      "    self.name = name\n",
      "    self.embedder = embedder()\n",
      "    self.save_path = save_path or \"storage\"\n",
      "    os.makedirs(self.save_path, exist_ok=True)\n",
      "    self.values = []\n",
      "    self.embeddings = None  # initialize embeddings as None\n",
      "    self.queries_embeddings = None  # initialize query embeddings as None\n",
      "    self.token_overflow_strategy = token_overflow_strategy\n",
      "    self.queries = []\n",
      "    self.queries_set = set()  # add this to quickly check for duplicates\n",
      "    self.index_set = set()  # add this to quickly check for duplicates\n",
      "    self.loaded = False\n",
      "    self.setup_index(values, embeddings, load)\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"embedder\"\n",
      "\t\"set\"\n",
      "\t\"set\"\n",
      "]\n",
      "\n",
      "\n",
      "@staticmethod\n",
      "@abstractmethod\n",
      "def compare_embeddings(query: Any, targets: Any) -> Any:\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@staticmethod\n",
      "@abstractmethod\n",
      "def batched_l2_distance(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@staticmethod\n",
      "@abstractmethod\n",
      "def batched_cosine_similarity(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def get(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Union[str, List[str]]:\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def add(self, values: List[str], embeddings: Optional[List[Union[List[float], np.ndarray]]] = None):\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def search(self, query: Optional[str] = None, query_embedding: Optional[np.ndarray] = None, top_k: int = 10, metric: str = \"cosine\", filter_mask: Optional[np.ndarray] = None) -> Tuple[List[str], Optional[List[float]], List[int]]:\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def setup_index(self, values: Optional[List[str]] = None, embeddings: Optional[List[Union[List[float], np.ndarray]]] = None, load: bool = False):\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "# Non-abstract method\n",
      "def save_index(self):\n",
      "    save_directory = os.path.join(self.save_path, self.name)\n",
      "    os.makedirs(save_directory, exist_ok=True)\n",
      "\n",
      "    with open(os.path.join(save_directory, f\"{self.name}_values.json\"), \"w\") as f:\n",
      "        json.dump(self.values, f)\n",
      "    #check if queries exist\n",
      "    if len(self.queries) > 0:\n",
      "        with open(os.path.join(save_directory, f\"{self.name}_queries.json\"), \"w\") as f:\n",
      "            json.dump(self.queries, f)\n",
      "\n",
      "    # Save embeddings in a subclass-specific way\n",
      "    self._save_embeddings(save_directory)\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"len\"\n",
      "\t\"open\"\n",
      "]\n",
      "\n",
      "\n",
      "def load_index(self):\n",
      "    load_directory = os.path.join(self.save_path, self.name)\n",
      "    if not os.path.exists(load_directory):\n",
      "        print(f\"I did not find the directory to load the index from: {load_directory}\")\n",
      "        return\n",
      "\n",
      "    print(f\"Loading index from {load_directory}\")\n",
      "\n",
      "    with open(os.path.join(load_directory, f\"{self.name}_values.json\"), \"r\") as f:\n",
      "        self.values = json.load(f)\n",
      "    self.values_set = set(self.values)\n",
      "    #check that queries exist\n",
      "    if os.path.exists(os.path.join(load_directory, f\"{self.name}_queries.json\")):\n",
      "        with open(os.path.join(load_directory, f\"{self.name}_queries.json\"), \"r\") as f:\n",
      "            self.queries = json.load(f)\n",
      "        self.queries_set = set(self.queries)\n",
      "\n",
      "    # Load embeddings in a subclass-specific way\n",
      "    self._load_embeddings(load_directory)\n",
      "    self.loaded = True\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"open\"\n",
      "\t\"set\"\n",
      "\t\"open\"\n",
      "\t\"set\"\n",
      "]\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def _save_embeddings(self, directory: str):\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "@abstractmethod\n",
      "def _load_embeddings(self, directory: str):\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class NpIndex(BaseIndex):\n",
      "    def __init__(\n",
      "            self,\n",
      "            values: Optional[List[str]] = None,\n",
      "            embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "            name: str = \"np_index\",\n",
      "            save_path: Optional[str] = None,\n",
      "            load: bool = False,\n",
      "            embedder: Optional[Union[OpenAiEmbedder, CohereEmbedder]] = OpenAiEmbedder,\n",
      "            token_overflow_strategy: str = \"ignore\",\n",
      "    ):\n",
      "        self.old_ids = collections.OrderedDict()\n",
      "        BaseIndex.__init__(self,values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "\n",
      "    @staticmethod\n",
      "    def compare_embeddings(query: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
      "        return np.array([np.allclose(query, target, rtol=1e-05, atol=1e-08) for target in targets])\n",
      "\n",
      "    @staticmethod\n",
      "    def batched_l2_distance(query_embedding: np.ndarray, embeddings: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
      "        scores = np.linalg.norm(embeddings - query_embedding, axis=1)\n",
      "        if mask is not None:\n",
      "            scores[~mask.astype(bool)] = np.inf  # set scores of excluded embeddings to infinity\n",
      "        return scores\n",
      "\n",
      "    @staticmethod\n",
      "    def batched_cosine_similarity(query_embedding: np.ndarray, embeddings: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
      "        scores = np.dot(embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True),\n",
      "                        query_embedding / np.linalg.norm(query_embedding))\n",
      "        if mask is not None:\n",
      "            scores[~mask.astype(bool)] = -np.inf  # set scores of excluded embeddings to negative infinity\n",
      "        return scores\n",
      "\n",
      "    def _save_embeddings(self, directory: str):\n",
      "        np.save(os.path.join(directory, f\"{self.name}_embeddings.npy\"), self.embeddings)\n",
      "        if self.queries_embeddings is not None:\n",
      "            np.save(os.path.join(directory, f\"{self.name}_queries_embeddings.npy\"), self.queries_embeddings)\n",
      "\n",
      "\n",
      "    def _load_embeddings(self, directory: str):\n",
      "        load_directory = os.path.join(self.save_path, self.name)\n",
      "        if not os.path.exists(load_directory):\n",
      "            print(f\"I did not find the directory to load the embe from: {load_directory}\")\n",
      "            return\n",
      "\n",
      "        self.embeddings = np.load(os.path.join(load_directory, f\"{self.name}_embeddings.npy\"))\n",
      "        if len(self.values) != len(self.embeddings):\n",
      "            raise ValueError(\"Loaded values and embeddings are not the same length.\")\n",
      "        #check that queries embeddings exist\n",
      "        if os.path.exists(os.path.join(load_directory, f\"{self.name}_queries_embeddings.npy\")):\n",
      "            self.queries_embeddings = np.load(os.path.join(load_directory, f\"{self.name}_queries_embeddings.npy\"),allow_pickle=True)\n",
      "            print(self.embeddings.shape, len(self.values))\n",
      "            print(self.queries_embeddings.shape, len(self.queries))\n",
      "            print(self.queries, self.queries_embeddings, self.queries_embeddings is not None, type(self.queries_embeddings), self.queries_embeddings.shape)\n",
      "\n",
      "            if self.queries_embeddings is not None and len(self.queries) != len(self.queries_embeddings):\n",
      "                raise ValueError(\"Loaded queries and queries embeddings are not the same length.\")\n",
      "\n",
      "\n",
      "    def setup_index(self, input_values: Optional[List[str]], embeddings: Optional[List[Union[List[float], np.ndarray]]], load: bool):\n",
      "        if load and os.path.exists(os.path.join(self.save_path, self.name)):\n",
      "            self.load_index()\n",
      "\n",
      "        elif input_values and embeddings and len(input_values) == len(embeddings):\n",
      "            # Check that input_values and embeddings are the same length\n",
      "            unique_dict = collections.defaultdict(list)\n",
      "            for val, emb in zip(input_values, embeddings):\n",
      "                unique_dict[val].append(emb)\n",
      "\n",
      "            # Ensure that all embeddings for each value are identical\n",
      "            for val in unique_dict.keys():\n",
      "                if len(unique_dict[val]) > 1 and not all(np.array_equal(unique_dict[val][0], emb) for emb in unique_dict[val]):\n",
      "                    raise ValueError(f'Different embeddings for the same value \"{val}\" found.')\n",
      "\n",
      "            self.add(list(unique_dict.keys()), [unique_dict[val][0] for val in unique_dict.keys()])\n",
      "            self.save_index()\n",
      "\n",
      "        elif input_values:\n",
      "            # Embed the input_values\n",
      "            self.add(list(set(input_values)))\n",
      "            self.save_index()\n",
      "\n",
      "    def get(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], output_type: str = \"value\") -> Union[int, str, np.ndarray, Dict[str, Union[int, str, np.ndarray]]]:\n",
      "\n",
      "        index = self.identify_input(identifier)\n",
      "        # Define output types\n",
      "        output_types = {\n",
      "            'index': index,\n",
      "            'value': self.values[index],\n",
      "            'embedding': self.embeddings[index],\n",
      "            'all': {\n",
      "                'index': index,\n",
      "                'value': self.values[index],\n",
      "                'embedding': self.embeddings[index]}\n",
      "        }\n",
      "\n",
      "        # Check output type is valid\n",
      "        if output_type not in output_types:\n",
      "            raise ValueError(\"Invalid output_type. Expected 'index', 'value', or 'embedding'.\")\n",
      "\n",
      "        return output_types[output_type]\n",
      "\n",
      "    def validate_value_length(self, value: str, tokens: List[int]) -> Union[bool, str]:\n",
      "        if not isinstance(value, str):\n",
      "            raise TypeError(\"Value must be a string.\")\n",
      "        token_len = len(tokens)\n",
      "        overflow_check = token_len > MAX_CONTEXT_LENGTH\n",
      "        match self.token_overflow_strategy:\n",
      "            case \"ignore\":\n",
      "                if overflow_check:\n",
      "                    return False, value\n",
      "                else:\n",
      "                    return True, value\n",
      "            case \"truncate\":\n",
      "                if overflow_check:\n",
      "                    return True, TOKENIZER.decode(tokens[:MAX_CONTEXT_LENGTH])\n",
      "                else:\n",
      "                    return True, value\n",
      "            case \"error\":\n",
      "                if overflow_check:\n",
      "                    raise ValueError(f\" The input is too long for OpenAI, num tokens is {token_len}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "                else:\n",
      "                    return True, value\n",
      "            case _:\n",
      "                raise ValueError(\"Invalid token_overflow_strategy. Expected 'ignore', 'truncate', or 'error'.\")\n",
      "\n",
      "    def add(self, values: List[str], embeddings: Optional[List[Union[List[float], np.ndarray]]] = None):\n",
      "\n",
      "        if embeddings is not None and len(values) != len(embeddings):\n",
      "            raise ValueError(\"values and embeddings must be the same length\")\n",
      "\n",
      "        # Check for duplicates and only add unique values\n",
      "        unique_values = []\n",
      "        unique_embeddings = []\n",
      "        token_batch = TOKENIZER.encode_batch(values, allowed_special=\"all\")\n",
      "        #extract the max value in old_ids consider that each old_ids is a list take the max of the list itself\n",
      "        for i, (val, tokens) in enumerate(zip(values, token_batch)):\n",
      "            is_valid, value = self.validate_value_length(val, tokens)\n",
      "            if not is_valid:\n",
      "                print(f\"Value '{value[:50]}...' is too long and will be ignored.\")\n",
      "                continue\n",
      "            if value not in self.index_set:\n",
      "                unique_values.append(value)\n",
      "                self.index_set.add(value)\n",
      "\n",
      "                self.old_ids[value] = [i]\n",
      "                if embeddings is not None:\n",
      "                    unique_embeddings.append(embeddings[i])\n",
      "            else:\n",
      "                self.old_ids[value].append(i)\n",
      "\n",
      "        if not unique_values:\n",
      "            print(\"All values already exist in the index. No values were added.\")\n",
      "            return\n",
      "        if embeddings is None:\n",
      "            unique_embeddings = self.embedder.embed(unique_values)\n",
      "\n",
      "        # Add unique values to the set\n",
      "        self.index_set.update(unique_values)\n",
      "\n",
      "        # If embeddings array is not yet created, initialize it, else append to it\n",
      "        if self.embeddings is None:\n",
      "            logger.info(\"Initializing embeddings array\")\n",
      "            self.embeddings = np.vstack(unique_embeddings)\n",
      "        else:\n",
      "            self.embeddings = np.vstack((self.embeddings, unique_embeddings))\n",
      "\n",
      "        self.values.extend(unique_values)\n",
      "\n",
      "    def identify_input(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Union[int, str, np.ndarray]:\n",
      "        if isinstance(identifier, int):  # if given an index\n",
      "            if identifier < len(self.values):  # if valid index\n",
      "                index = identifier\n",
      "            else:\n",
      "                raise ValueError(\"Invalid index given.\")\n",
      "        elif isinstance(identifier, str):  # if given a value\n",
      "            if identifier in self.values:\n",
      "                index = self.values.index(identifier)\n",
      "            else:\n",
      "                raise ValueError(\"Value not found.\")\n",
      "        elif isinstance(identifier, np.ndarray):  # if given an embedding\n",
      "            indices = np.where(self.compare_embeddings(identifier, self.embeddings))[0]\n",
      "            if len(indices) == 0:\n",
      "                raise ValueError(\"Embedding not found.\")\n",
      "            index = indices[0]\n",
      "        else:\n",
      "            raise TypeError(\"Invalid identifier type. Expected int, str, np.ndarray, or list of these types\")\n",
      "\n",
      "        return index\n",
      "\n",
      "\n",
      "    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[str, np.ndarray]]]) -> None:\n",
      "\n",
      "        if isinstance(identifier, list):\n",
      "            if all(isinstance(i, type(identifier[0])) for i in identifier) and not isinstance(identifier[0], list) and not isinstance(identifier[0], int):\n",
      "                for i in identifier:\n",
      "                    self.remove(i)\n",
      "            else:\n",
      "                raise TypeError(\"All elements in the list must be of the same type.\")\n",
      "            return\n",
      "        id = self.identify_input(identifier)\n",
      "        value = self.values[id]\n",
      "        self.index_set.remove(value)\n",
      "        self.old_ids.pop(value)\n",
      "        self.values.pop(id)\n",
      "        self.embeddings = np.delete(self.embeddings, [id], axis=0)\n",
      "\n",
      "\n",
      "    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[str, np.ndarray]]], new_value: Union[str, List[str]], new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n",
      "        if isinstance(new_value,str) and new_value in self.index_set:\n",
      "            raise ValueError(\"new_value already exists in the index. Please remove it first.\")\n",
      "        elif isinstance(new_value, list) and any(v in self.index_set for v in new_value):\n",
      "            raise ValueError(\"One or more new_value already exists in the index. Please remove them first.\")\n",
      "        if isinstance(old_identifier, list) and not isinstance(old_identifier[0], list) and not isinstance(old_identifier[0], int):\n",
      "            if not isinstance(new_value, list) or len(old_identifier) != len(new_value):\n",
      "                raise ValueError(\"For list inputs, old_identifier and new_value must all be lists of the same length.\")\n",
      "            if new_embedding is not None:\n",
      "                if not isinstance(new_embedding, list) or len(old_identifier) != len(new_embedding):\n",
      "                    raise ValueError(\"new_embedding must be a list of same length as old_identifier and new_value.\")\n",
      "                # If new_embedding is a 2D array or list of lists\n",
      "                if isinstance(new_embedding[0], list) or isinstance(new_embedding[0], np.ndarray):\n",
      "                    if len(new_embedding[0]) != len(self.embeddings[0]):\n",
      "                        raise ValueError(\"Each row in new_embedding must have the same dimension as the original embeddings.\")\n",
      "            else:\n",
      "                new_embedding = self.embedder.embed(new_value)\n",
      "            for old_id, new_val, new_emb in zip(old_identifier, new_value, new_embedding):\n",
      "                self.update(old_id, new_val, new_emb)\n",
      "            return\n",
      "        old_id = self.identify_input(old_identifier)\n",
      "        self.index_set.remove(self.values[old_id])\n",
      "        self.old_ids[new_value] = self.old_ids.pop(self.values[old_id])\n",
      "        self.values[old_id] = new_value\n",
      "        self.index_set.add(new_value)\n",
      "        self.embeddings[old_id] = self.embedder.embed([new_value]) if new_embedding is None else new_embedding\n",
      "\n",
      "    def search(self, query: Optional[str] = None, query_embedding: Optional[np.ndarray] = None, top_k: int = 10, metric: str = \"cosine\", filter_mask: Optional[np.ndarray] = None) -> Tuple[List[str], Optional[List[float]], List[int]]:\n",
      "\n",
      "        # create a 2D numpy array from the embeddings list\n",
      "        embeddings_array = self.embeddings\n",
      "\n",
      "        # if no query or query embedding is provided, return random samples\n",
      "        if query is None and query_embedding is None:\n",
      "            indices = np.random.choice(len(self.values), size=top_k, replace=False)\n",
      "            return [self.values[i] for i in indices],None, indices  # return indices with dummy scores\n",
      "\n",
      "        # if the query is in the queries set, use the stored embedding\n",
      "        if query in self.queries_set:\n",
      "            print(\"Query is in queries set.\")\n",
      "            query_embedding = self.queries_embeddings[self.queries.index(query)]\n",
      "        # else if a query string is provided but not in queries set, compute its embedding\n",
      "        elif query is not None:\n",
      "            print(\"Query is not in queries set. Computing embedding...\")\n",
      "            query_embedding = self.embedder.embed([query])\n",
      "            # print(query_embedding)\n",
      "            self.queries_set.add(query)\n",
      "            self.queries.append(query)\n",
      "            # If queries_embeddings array is not yet created, initialize it, else append to it\n",
      "            if self.queries_embeddings is None:\n",
      "                self.queries_embeddings = np.array([query_embedding])\n",
      "            else:\n",
      "                self.queries_embeddings = np.vstack((self.queries_embeddings, query_embedding))\n",
      "\n",
      "        # compute distances or similarities\n",
      "        if metric == \"l2\":\n",
      "            scores = self.batched_l2_distance(query_embedding, embeddings_array, filter_mask)\n",
      "        elif metric == \"cosine\":\n",
      "            scores = self.batched_cosine_similarity(query_embedding, embeddings_array, filter_mask)\n",
      "        else:\n",
      "            raise ValueError(\"Invalid metric. Expected 'l2' or 'cosine'.\")\n",
      "\n",
      "        # sort by scores\n",
      "        sorted_indices = np.argsort(scores)\n",
      "\n",
      "        # for L2 distance, closer vectors are better (smaller distance)\n",
      "        # for cosine similarity, further vectors are better (larger similarity)\n",
      "        top_k = min(top_k, len(self.values))\n",
      "        top_indices = sorted_indices[:top_k] if metric == \"l2\" else sorted_indices[-top_k:][::-1]\n",
      "\n",
      "        # return indices and scores\n",
      "        return  [self.values[i] for i in top_indices],[scores[i] for i in top_indices], [i for i in top_indices]\n",
      "\n",
      "Function calls: shape: (79,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"type\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t…\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"zip\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"ValueError\"\n",
      "\t\"min\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "        self,\n",
      "        values: Optional[List[str]] = None,\n",
      "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "        name: str = \"np_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        load: bool = False,\n",
      "        embedder: Optional[Union[OpenAiEmbedder, CohereEmbedder]] = OpenAiEmbedder,\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "):\n",
      "    self.old_ids = collections.OrderedDict()\n",
      "    BaseIndex.__init__(self,values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@staticmethod\n",
      "def compare_embeddings(query: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
      "    return np.array([np.allclose(query, target, rtol=1e-05, atol=1e-08) for target in targets])\n",
      "\n",
      "\n",
      "\n",
      "@staticmethod\n",
      "def batched_l2_distance(query_embedding: np.ndarray, embeddings: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
      "    scores = np.linalg.norm(embeddings - query_embedding, axis=1)\n",
      "    if mask is not None:\n",
      "        scores[~mask.astype(bool)] = np.inf  # set scores of excluded embeddings to infinity\n",
      "    return scores\n",
      "\n",
      "\n",
      "\n",
      "@staticmethod\n",
      "def batched_cosine_similarity(query_embedding: np.ndarray, embeddings: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
      "    scores = np.dot(embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True),\n",
      "                    query_embedding / np.linalg.norm(query_embedding))\n",
      "    if mask is not None:\n",
      "        scores[~mask.astype(bool)] = -np.inf  # set scores of excluded embeddings to negative infinity\n",
      "    return scores\n",
      "\n",
      "\n",
      "\n",
      "def _save_embeddings(self, directory: str):\n",
      "    np.save(os.path.join(directory, f\"{self.name}_embeddings.npy\"), self.embeddings)\n",
      "    if self.queries_embeddings is not None:\n",
      "        np.save(os.path.join(directory, f\"{self.name}_queries_embeddings.npy\"), self.queries_embeddings)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def _load_embeddings(self, directory: str):\n",
      "    load_directory = os.path.join(self.save_path, self.name)\n",
      "    if not os.path.exists(load_directory):\n",
      "        print(f\"I did not find the directory to load the embe from: {load_directory}\")\n",
      "        return\n",
      "\n",
      "    self.embeddings = np.load(os.path.join(load_directory, f\"{self.name}_embeddings.npy\"))\n",
      "    if len(self.values) != len(self.embeddings):\n",
      "        raise ValueError(\"Loaded values and embeddings are not the same length.\")\n",
      "    #check that queries embeddings exist\n",
      "    if os.path.exists(os.path.join(load_directory, f\"{self.name}_queries_embeddings.npy\")):\n",
      "        self.queries_embeddings = np.load(os.path.join(load_directory, f\"{self.name}_queries_embeddings.npy\"),allow_pickle=True)\n",
      "        print(self.embeddings.shape, len(self.values))\n",
      "        print(self.queries_embeddings.shape, len(self.queries))\n",
      "        print(self.queries, self.queries_embeddings, self.queries_embeddings is not None, type(self.queries_embeddings), self.queries_embeddings.shape)\n",
      "\n",
      "        if self.queries_embeddings is not None and len(self.queries) != len(self.queries_embeddings):\n",
      "            raise ValueError(\"Loaded queries and queries embeddings are not the same length.\")\n",
      "\n",
      "Function calls: shape: (13,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"type\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def setup_index(self, input_values: Optional[List[str]], embeddings: Optional[List[Union[List[float], np.ndarray]]], load: bool):\n",
      "    if load and os.path.exists(os.path.join(self.save_path, self.name)):\n",
      "        self.load_index()\n",
      "\n",
      "    elif input_values and embeddings and len(input_values) == len(embeddings):\n",
      "        # Check that input_values and embeddings are the same length\n",
      "        unique_dict = collections.defaultdict(list)\n",
      "        for val, emb in zip(input_values, embeddings):\n",
      "            unique_dict[val].append(emb)\n",
      "\n",
      "        # Ensure that all embeddings for each value are identical\n",
      "        for val in unique_dict.keys():\n",
      "            if len(unique_dict[val]) > 1 and not all(np.array_equal(unique_dict[val][0], emb) for emb in unique_dict[val]):\n",
      "                raise ValueError(f'Different embeddings for the same value \"{val}\" found.')\n",
      "\n",
      "        self.add(list(unique_dict.keys()), [unique_dict[val][0] for val in unique_dict.keys()])\n",
      "        self.save_index()\n",
      "\n",
      "    elif input_values:\n",
      "        # Embed the input_values\n",
      "        self.add(list(set(input_values)))\n",
      "        self.save_index()\n",
      "\n",
      "Function calls: shape: (9,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"zip\"\n",
      "\t\"len\"\n",
      "\t\"all\"\n",
      "\t\"ValueError\"\n",
      "\t\"list\"\n",
      "\t\"list\"\n",
      "\t\"set\"\n",
      "]\n",
      "\n",
      "\n",
      "def get(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], output_type: str = \"value\") -> Union[int, str, np.ndarray, Dict[str, Union[int, str, np.ndarray]]]:\n",
      "\n",
      "    index = self.identify_input(identifier)\n",
      "    # Define output types\n",
      "    output_types = {\n",
      "        'index': index,\n",
      "        'value': self.values[index],\n",
      "        'embedding': self.embeddings[index],\n",
      "        'all': {\n",
      "            'index': index,\n",
      "            'value': self.values[index],\n",
      "            'embedding': self.embeddings[index]}\n",
      "    }\n",
      "\n",
      "    # Check output type is valid\n",
      "    if output_type not in output_types:\n",
      "        raise ValueError(\"Invalid output_type. Expected 'index', 'value', or 'embedding'.\")\n",
      "\n",
      "    return output_types[output_type]\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def validate_value_length(self, value: str, tokens: List[int]) -> Union[bool, str]:\n",
      "    if not isinstance(value, str):\n",
      "        raise TypeError(\"Value must be a string.\")\n",
      "    token_len = len(tokens)\n",
      "    overflow_check = token_len > MAX_CONTEXT_LENGTH\n",
      "    match self.token_overflow_strategy:\n",
      "        case \"ignore\":\n",
      "            if overflow_check:\n",
      "                return False, value\n",
      "            else:\n",
      "                return True, value\n",
      "        case \"truncate\":\n",
      "            if overflow_check:\n",
      "                return True, TOKENIZER.decode(tokens[:MAX_CONTEXT_LENGTH])\n",
      "            else:\n",
      "                return True, value\n",
      "        case \"error\":\n",
      "            if overflow_check:\n",
      "                raise ValueError(f\" The input is too long for OpenAI, num tokens is {token_len}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "            else:\n",
      "                return True, value\n",
      "        case _:\n",
      "            raise ValueError(\"Invalid token_overflow_strategy. Expected 'ignore', 'truncate', or 'error'.\")\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"TypeError\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def add(self, values: List[str], embeddings: Optional[List[Union[List[float], np.ndarray]]] = None):\n",
      "\n",
      "    if embeddings is not None and len(values) != len(embeddings):\n",
      "        raise ValueError(\"values and embeddings must be the same length\")\n",
      "\n",
      "    # Check for duplicates and only add unique values\n",
      "    unique_values = []\n",
      "    unique_embeddings = []\n",
      "    token_batch = TOKENIZER.encode_batch(values, allowed_special=\"all\")\n",
      "    #extract the max value in old_ids consider that each old_ids is a list take the max of the list itself\n",
      "    for i, (val, tokens) in enumerate(zip(values, token_batch)):\n",
      "        is_valid, value = self.validate_value_length(val, tokens)\n",
      "        if not is_valid:\n",
      "            print(f\"Value '{value[:50]}...' is too long and will be ignored.\")\n",
      "            continue\n",
      "        if value not in self.index_set:\n",
      "            unique_values.append(value)\n",
      "            self.index_set.add(value)\n",
      "\n",
      "            self.old_ids[value] = [i]\n",
      "            if embeddings is not None:\n",
      "                unique_embeddings.append(embeddings[i])\n",
      "        else:\n",
      "            self.old_ids[value].append(i)\n",
      "\n",
      "    if not unique_values:\n",
      "        print(\"All values already exist in the index. No values were added.\")\n",
      "        return\n",
      "    if embeddings is None:\n",
      "        unique_embeddings = self.embedder.embed(unique_values)\n",
      "\n",
      "    # Add unique values to the set\n",
      "    self.index_set.update(unique_values)\n",
      "\n",
      "    # If embeddings array is not yet created, initialize it, else append to it\n",
      "    if self.embeddings is None:\n",
      "        logger.info(\"Initializing embeddings array\")\n",
      "        self.embeddings = np.vstack(unique_embeddings)\n",
      "    else:\n",
      "        self.embeddings = np.vstack((self.embeddings, unique_embeddings))\n",
      "\n",
      "    self.values.extend(unique_values)\n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"enumerate\"\n",
      "\t\"zip\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def identify_input(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Union[int, str, np.ndarray]:\n",
      "    if isinstance(identifier, int):  # if given an index\n",
      "        if identifier < len(self.values):  # if valid index\n",
      "            index = identifier\n",
      "        else:\n",
      "            raise ValueError(\"Invalid index given.\")\n",
      "    elif isinstance(identifier, str):  # if given a value\n",
      "        if identifier in self.values:\n",
      "            index = self.values.index(identifier)\n",
      "        else:\n",
      "            raise ValueError(\"Value not found.\")\n",
      "    elif isinstance(identifier, np.ndarray):  # if given an embedding\n",
      "        indices = np.where(self.compare_embeddings(identifier, self.embeddings))[0]\n",
      "        if len(indices) == 0:\n",
      "            raise ValueError(\"Embedding not found.\")\n",
      "        index = indices[0]\n",
      "    else:\n",
      "        raise TypeError(\"Invalid identifier type. Expected int, str, np.ndarray, or list of these types\")\n",
      "\n",
      "    return index\n",
      "\n",
      "Function calls: shape: (9,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"TypeError\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def remove(self, identifier: Union[int, str, np.ndarray, List[Union[str, np.ndarray]]]) -> None:\n",
      "\n",
      "    if isinstance(identifier, list):\n",
      "        if all(isinstance(i, type(identifier[0])) for i in identifier) and not isinstance(identifier[0], list) and not isinstance(identifier[0], int):\n",
      "            for i in identifier:\n",
      "                self.remove(i)\n",
      "        else:\n",
      "            raise TypeError(\"All elements in the list must be of the same type.\")\n",
      "        return\n",
      "    id = self.identify_input(identifier)\n",
      "    value = self.values[id]\n",
      "    self.index_set.remove(value)\n",
      "    self.old_ids.pop(value)\n",
      "    self.values.pop(id)\n",
      "    self.embeddings = np.delete(self.embeddings, [id], axis=0)\n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"all\"\n",
      "\t\"isinstance\"\n",
      "\t\"type\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"TypeError\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[str, np.ndarray]]], new_value: Union[str, List[str]], new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n",
      "    if isinstance(new_value,str) and new_value in self.index_set:\n",
      "        raise ValueError(\"new_value already exists in the index. Please remove it first.\")\n",
      "    elif isinstance(new_value, list) and any(v in self.index_set for v in new_value):\n",
      "        raise ValueError(\"One or more new_value already exists in the index. Please remove them first.\")\n",
      "    if isinstance(old_identifier, list) and not isinstance(old_identifier[0], list) and not isinstance(old_identifier[0], int):\n",
      "        if not isinstance(new_value, list) or len(old_identifier) != len(new_value):\n",
      "            raise ValueError(\"For list inputs, old_identifier and new_value must all be lists of the same length.\")\n",
      "        if new_embedding is not None:\n",
      "            if not isinstance(new_embedding, list) or len(old_identifier) != len(new_embedding):\n",
      "                raise ValueError(\"new_embedding must be a list of same length as old_identifier and new_value.\")\n",
      "            # If new_embedding is a 2D array or list of lists\n",
      "            if isinstance(new_embedding[0], list) or isinstance(new_embedding[0], np.ndarray):\n",
      "                if len(new_embedding[0]) != len(self.embeddings[0]):\n",
      "                    raise ValueError(\"Each row in new_embedding must have the same dimension as the original embeddings.\")\n",
      "        else:\n",
      "            new_embedding = self.embedder.embed(new_value)\n",
      "        for old_id, new_val, new_emb in zip(old_identifier, new_value, new_embedding):\n",
      "            self.update(old_id, new_val, new_emb)\n",
      "        return\n",
      "    old_id = self.identify_input(old_identifier)\n",
      "    self.index_set.remove(self.values[old_id])\n",
      "    self.old_ids[new_value] = self.old_ids.pop(self.values[old_id])\n",
      "    self.values[old_id] = new_value\n",
      "    self.index_set.add(new_value)\n",
      "    self.embeddings[old_id] = self.embedder.embed([new_value]) if new_embedding is None else new_embedding\n",
      "\n",
      "Function calls: shape: (22,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"any\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"zip\"\n",
      "]\n",
      "\n",
      "\n",
      "def search(self, query: Optional[str] = None, query_embedding: Optional[np.ndarray] = None, top_k: int = 10, metric: str = \"cosine\", filter_mask: Optional[np.ndarray] = None) -> Tuple[List[str], Optional[List[float]], List[int]]:\n",
      "\n",
      "    # create a 2D numpy array from the embeddings list\n",
      "    embeddings_array = self.embeddings\n",
      "\n",
      "    # if no query or query embedding is provided, return random samples\n",
      "    if query is None and query_embedding is None:\n",
      "        indices = np.random.choice(len(self.values), size=top_k, replace=False)\n",
      "        return [self.values[i] for i in indices],None, indices  # return indices with dummy scores\n",
      "\n",
      "    # if the query is in the queries set, use the stored embedding\n",
      "    if query in self.queries_set:\n",
      "        print(\"Query is in queries set.\")\n",
      "        query_embedding = self.queries_embeddings[self.queries.index(query)]\n",
      "    # else if a query string is provided but not in queries set, compute its embedding\n",
      "    elif query is not None:\n",
      "        print(\"Query is not in queries set. Computing embedding...\")\n",
      "        query_embedding = self.embedder.embed([query])\n",
      "        # print(query_embedding)\n",
      "        self.queries_set.add(query)\n",
      "        self.queries.append(query)\n",
      "        # If queries_embeddings array is not yet created, initialize it, else append to it\n",
      "        if self.queries_embeddings is None:\n",
      "            self.queries_embeddings = np.array([query_embedding])\n",
      "        else:\n",
      "            self.queries_embeddings = np.vstack((self.queries_embeddings, query_embedding))\n",
      "\n",
      "    # compute distances or similarities\n",
      "    if metric == \"l2\":\n",
      "        scores = self.batched_l2_distance(query_embedding, embeddings_array, filter_mask)\n",
      "    elif metric == \"cosine\":\n",
      "        scores = self.batched_cosine_similarity(query_embedding, embeddings_array, filter_mask)\n",
      "    else:\n",
      "        raise ValueError(\"Invalid metric. Expected 'l2' or 'cosine'.\")\n",
      "\n",
      "    # sort by scores\n",
      "    sorted_indices = np.argsort(scores)\n",
      "\n",
      "    # for L2 distance, closer vectors are better (smaller distance)\n",
      "    # for cosine similarity, further vectors are better (larger similarity)\n",
      "    top_k = min(top_k, len(self.values))\n",
      "    top_indices = sorted_indices[:top_k] if metric == \"l2\" else sorted_indices[-top_k:][::-1]\n",
      "\n",
      "    # return indices and scores\n",
      "    return  [self.values[i] for i in top_indices],[scores[i] for i in top_indices], [i for i in top_indices]\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"ValueError\"\n",
      "\t\"min\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "class FifoChat(FifoThread, Chat):\n",
      "    \"\"\"\n",
      "    A chatbot class that combines FIFO Memory Thread, BaseChat, and Prompter. The oldest messages are removed first\n",
      "    when reaching the max_memory limit. The memory is defined in terms of tokens, and outs are passed to the\n",
      "    longterm_memory. The lucid_memory is a redundant memory that stores all the messages.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        model: Optional[str] = None,\n",
      "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "        system_prompt: Optional[str] = None,\n",
      "        user_prompt: Optional[str] = None,\n",
      "        name: str = \"fifo_memory\",\n",
      "        max_index_memory: int = 400,\n",
      "        max_fifo_memory: int = 2048,\n",
      "        max_output_tokens: int = 1000,\n",
      "        longterm_thread: Optional[BaseThread] = None,\n",
      "    ):\n",
      "\n",
      "        FifoThread.__init__(\n",
      "            self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread\n",
      "        )\n",
      "        Chat.__init__(\n",
      "            self,\n",
      "            model=model,\n",
      "            index_dict=index_dict,\n",
      "            max_output_tokens=max_output_tokens,\n",
      "            max_index_memory=max_index_memory,\n",
      "            system_prompt=system_prompt,\n",
      "            user_prompt=user_prompt,\n",
      "            name=name,\n",
      "        )\n",
      "\n",
      "        self.prompt_func = self.fifo_memory_prompt\n",
      "\n",
      "    def fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\n",
      "        \"\"\"\n",
      "        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "        marked_question = mark_question(self.user_prompt(message))\n",
      "        prompt = (\n",
      "            [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\n",
      "        )\n",
      "        return prompt, marked_question\n",
      "\n",
      "    def query(self, question: str, verbose: bool = True, stream: bool = False) -> Union[Generator,str]:\n",
      "        \"\"\"\n",
      "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
      "        and added to the memory.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "        marked_question = mark_question(question)\n",
      "        self.add_message(marked_question)\n",
      "        answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n",
      "        if stream:\n",
      "            return answer\n",
      "        else:\n",
      "            self.add_message(answer)\n",
      "            return answer\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_question\"\n",
      "\t\"mark_system\"\n",
      "\t\"mark_question\"\n",
      "]\n",
      "\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    model: Optional[str] = None,\n",
      "    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "    system_prompt: Optional[str] = None,\n",
      "    user_prompt: Optional[str] = None,\n",
      "    name: str = \"fifo_memory\",\n",
      "    max_index_memory: int = 400,\n",
      "    max_fifo_memory: int = 2048,\n",
      "    max_output_tokens: int = 1000,\n",
      "    longterm_thread: Optional[BaseThread] = None,\n",
      "):\n",
      "\n",
      "    FifoThread.__init__(\n",
      "        self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread\n",
      "    )\n",
      "    Chat.__init__(\n",
      "        self,\n",
      "        model=model,\n",
      "        index_dict=index_dict,\n",
      "        max_output_tokens=max_output_tokens,\n",
      "        max_index_memory=max_index_memory,\n",
      "        system_prompt=system_prompt,\n",
      "        user_prompt=user_prompt,\n",
      "        name=name,\n",
      "    )\n",
      "\n",
      "    self.prompt_func = self.fifo_memory_prompt\n",
      "\n",
      "\n",
      "\n",
      "def fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\n",
      "    \"\"\"\n",
      "        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "    marked_question = mark_question(self.user_prompt(message))\n",
      "    prompt = (\n",
      "        [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\n",
      "    )\n",
      "    return prompt, marked_question\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_question\"\n",
      "\t\"mark_system\"\n",
      "]\n",
      "\n",
      "\n",
      "def query(self, question: str, verbose: bool = True, stream: bool = False) -> Union[Generator,str]:\n",
      "    \"\"\"\n",
      "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
      "        and added to the memory.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "    marked_question = mark_question(question)\n",
      "    self.add_message(marked_question)\n",
      "    answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n",
      "    if stream:\n",
      "        return answer\n",
      "    else:\n",
      "        self.add_message(answer)\n",
      "        return answer\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_question\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class VectorChat(VectorThread, Chat):\n",
      "    \"\"\"\n",
      "    A chatbot class that combines Vector Memory Thread, BaseChat, and Prompter. Memory prompt is constructed by\n",
      "    filling the memory with the k most similar messages to the question until the max prompt memory tokens are reached.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        model: Optional[str] = None,\n",
      "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "        name: str = \"vector_memory\",\n",
      "        max_index_memory: int = 400,\n",
      "        max_vector_memory: int = 2048,\n",
      "        max_output_tokens: int = 1000,\n",
      "        system_prompt: str = None,\n",
      "        user_prompt: str = None,\n",
      "    \n",
      "    ):\n",
      "        VectorThread.__init__(self, name=name, max_context=max_vector_memory)\n",
      "        Chat.__init__(\n",
      "            self,\n",
      "            model=model,\n",
      "            index_dict=index_dict,\n",
      "            max_output_tokens=max_output_tokens,\n",
      "            max_index_memory=max_index_memory,\n",
      "            system_prompt=system_prompt,\n",
      "            user_prompt=user_prompt,\n",
      "            name=name,\n",
      "        )\n",
      "        self.max_vector_memory = self.max_context\n",
      "        self.prompt_func = self.vector_memory_prompt\n",
      "\n",
      "    def vector_memory_prompt(\n",
      "        self, message: str, k: int = 10\n",
      "    ) -> Tuple[List[dict], dict]:\n",
      "        \"\"\"\n",
      "        Combine system prompt, k most similar messages to the question, and the user prompt.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include in the prompt.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "        sorted_messages, sorted_scores, sorted_indices = self.sorted_query(\n",
      "            message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
      "        )\n",
      "        marked_question = mark_question(self.user_prompt(message))\n",
      "        prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\n",
      "        return prompt, marked_question\n",
      "\n",
      "    def weighted_memory_prompt(\n",
      "        self,\n",
      "        message: str,\n",
      "        k: int = 10,\n",
      "        decay_factor: float = 0.1,\n",
      "        temporal_weight: float = 0.5,\n",
      "    ) -> Tuple[List[dict], dict]:\n",
      "        \"\"\"\n",
      "        Combine system prompt, weighted k most similar messages to the question, and the user prompt.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include in the prompt.\n",
      "        :param decay_factor: A float representing the decay factor for weighting.\n",
      "        :param temporal_weight: A float representing the weight of the temporal aspect.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "        weighted_messages, weighted_scores, weighted_indices = self.weighted_query(\n",
      "            message,\n",
      "            k=k,\n",
      "            max_tokens=self.max_vector_memory,\n",
      "            decay_factor=decay_factor,\n",
      "            temporal_weight=temporal_weight,\n",
      "            order_by=\"chronological\",\n",
      "            reverse=True,\n",
      "        )\n",
      "        marked_question = mark_question(self.user_prompt(message))\n",
      "        prompt = (\n",
      "            [mark_system(self.system_prompt)] + weighted_messages + [marked_question]\n",
      "        )\n",
      "        return prompt, marked_question\n",
      "\n",
      "    def query(self, question: str, verbose: bool = False, stream:bool = False) -> Union[Generator,str]:\n",
      "        \"\"\"\n",
      "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
      "        and added to the memory.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "                \n",
      "        marked_question = mark_question(question)\n",
      "        self.add_message(marked_question)\n",
      "        answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n",
      "        if stream:\n",
      "            return answer\n",
      "        else:\n",
      "            self.add_message(answer)\n",
      "            return answer\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_question\"\n",
      "\t\"mark_system\"\n",
      "\t\"mark_question\"\n",
      "\t\"mark_system\"\n",
      "\t\"mark_question\"\n",
      "]\n",
      "\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    model: Optional[str] = None,\n",
      "    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "    name: str = \"vector_memory\",\n",
      "    max_index_memory: int = 400,\n",
      "    max_vector_memory: int = 2048,\n",
      "    max_output_tokens: int = 1000,\n",
      "    system_prompt: str = None,\n",
      "    user_prompt: str = None,\n",
      "\n",
      "):\n",
      "    VectorThread.__init__(self, name=name, max_context=max_vector_memory)\n",
      "    Chat.__init__(\n",
      "        self,\n",
      "        model=model,\n",
      "        index_dict=index_dict,\n",
      "        max_output_tokens=max_output_tokens,\n",
      "        max_index_memory=max_index_memory,\n",
      "        system_prompt=system_prompt,\n",
      "        user_prompt=user_prompt,\n",
      "        name=name,\n",
      "    )\n",
      "    self.max_vector_memory = self.max_context\n",
      "    self.prompt_func = self.vector_memory_prompt\n",
      "\n",
      "\n",
      "\n",
      "def vector_memory_prompt(\n",
      "    self, message: str, k: int = 10\n",
      ") -> Tuple[List[dict], dict]:\n",
      "    \"\"\"\n",
      "        Combine system prompt, k most similar messages to the question, and the user prompt.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include in the prompt.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "    sorted_messages, sorted_scores, sorted_indices = self.sorted_query(\n",
      "        message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
      "    )\n",
      "    marked_question = mark_question(self.user_prompt(message))\n",
      "    prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\n",
      "    return prompt, marked_question\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_question\"\n",
      "\t\"mark_system\"\n",
      "]\n",
      "\n",
      "\n",
      "def weighted_memory_prompt(\n",
      "    self,\n",
      "    message: str,\n",
      "    k: int = 10,\n",
      "    decay_factor: float = 0.1,\n",
      "    temporal_weight: float = 0.5,\n",
      ") -> Tuple[List[dict], dict]:\n",
      "    \"\"\"\n",
      "        Combine system prompt, weighted k most similar messages to the question, and the user prompt.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include in the prompt.\n",
      "        :param decay_factor: A float representing the decay factor for weighting.\n",
      "        :param temporal_weight: A float representing the weight of the temporal aspect.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "    weighted_messages, weighted_scores, weighted_indices = self.weighted_query(\n",
      "        message,\n",
      "        k=k,\n",
      "        max_tokens=self.max_vector_memory,\n",
      "        decay_factor=decay_factor,\n",
      "        temporal_weight=temporal_weight,\n",
      "        order_by=\"chronological\",\n",
      "        reverse=True,\n",
      "    )\n",
      "    marked_question = mark_question(self.user_prompt(message))\n",
      "    prompt = (\n",
      "        [mark_system(self.system_prompt)] + weighted_messages + [marked_question]\n",
      "    )\n",
      "    return prompt, marked_question\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_question\"\n",
      "\t\"mark_system\"\n",
      "]\n",
      "\n",
      "\n",
      "def query(self, question: str, verbose: bool = False, stream:bool = False) -> Union[Generator,str]:\n",
      "    \"\"\"\n",
      "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
      "        and added to the memory.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "            \n",
      "    marked_question = mark_question(question)\n",
      "    self.add_message(marked_question)\n",
      "    answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n",
      "    if stream:\n",
      "        return answer\n",
      "    else:\n",
      "        self.add_message(answer)\n",
      "        return answer\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_question\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class FifoVectorChat(FifoThread, Chat):\n",
      "    \"\"\"\n",
      "    A chatbot class that combines FIFO Memory Thread, Vector Memory Thread, BaseChat, and Prompter.\n",
      "    The memory prompt is constructed by including both FIFO memory and Vector memory.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        model: str = None,\n",
      "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "        system_prompt: str = None,\n",
      "        user_prompt: str = None,\n",
      "        name: str = \"fifo_vector_memory\",\n",
      "        max_memory: int = 2048,\n",
      "        max_index_memory: int = 400,\n",
      "        max_output_tokens: int = 1000,\n",
      "        longterm_thread: Optional[VectorThread] = None,\n",
      "        longterm_frac: float = 0.5,\n",
      "    ):\n",
      "        self.total_max_memory = max_memory\n",
      "\n",
      "        self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n",
      "        FifoThread.__init__(\n",
      "            self,\n",
      "            name=name,\n",
      "            max_memory=self.max_fifo_memory,\n",
      "            longterm_thread=self.longterm_thread,\n",
      "        )\n",
      "        Chat.__init__(\n",
      "            self,\n",
      "            model=model,\n",
      "            index_dict=index_dict,\n",
      "            max_output_tokens=max_output_tokens,\n",
      "            max_index_memory=max_index_memory,\n",
      "            system_prompt=system_prompt,\n",
      "            user_prompt=user_prompt,\n",
      "            name=name,\n",
      "        )\n",
      "        \n",
      "        self.prompt_func = self.fifovector_memory_prompt\n",
      "        self.prompt_list = []\n",
      "\n",
      "    def setup_longterm_memory(\n",
      "        self,\n",
      "        longterm_thread: Optional[VectorThread],\n",
      "        max_memory: int,\n",
      "        longterm_frac: float,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n",
      "\n",
      "        :param longterm_thread: An optional VectorThread for long-term memory.\n",
      "        :param max_memory: The maximum amount of memory for the chatbot.\n",
      "        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n",
      "        \"\"\"\n",
      "        if longterm_thread is None:\n",
      "            self.longterm_frac = longterm_frac\n",
      "            self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n",
      "            self.max_vector_memory = max_memory - self.max_fifo_memory\n",
      "            self.longterm_thread = VectorThread(\n",
      "                name=\"longterm_memory\", max_context=self.max_vector_memory\n",
      "            )\n",
      "        else:\n",
      "            self.longterm_thread = longterm_thread\n",
      "            self.max_vector_memory = self.longterm_thread.max_context\n",
      "            self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n",
      "            self.longterm_frac = self.max_vector_memory / self.total_max_memory\n",
      "\n",
      "    def fifovector_memory_prompt(\n",
      "        self, message: str, k: int = 10\n",
      "    ) -> Tuple[List[dict], dict]:\n",
      "        \"\"\"\n",
      "        Combine the system prompt, long-term memory (vector memory), short-term memory (FIFO memory), and the user prompt.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include from the long-term memory.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "        prompt = [mark_system(self.system_prompt)]\n",
      "        if (\n",
      "            len(self.longterm_thread.memory_thread) > 0\n",
      "            and self.longterm_thread.total_tokens <= self.max_vector_memory\n",
      "        ):\n",
      "            prompt += self.longterm_thread.memory_thread\n",
      "        elif (\n",
      "            len(self.longterm_thread.memory_thread) > 0\n",
      "            and self.longterm_thread.total_tokens > self.max_vector_memory\n",
      "        ):\n",
      "            (\n",
      "                sorted_messages,\n",
      "                sorted_scores,\n",
      "                sorted_indices,\n",
      "            ) = self.longterm_thread.sorted_query(\n",
      "                message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
      "            )\n",
      "            prompt += sorted_messages\n",
      "\n",
      "        prompt += self.memory_thread\n",
      "        marked_question = mark_question(self.user_prompt(message))\n",
      "        prompt += [marked_question]\n",
      "        return prompt, marked_question\n",
      "\n",
      "    def query(self, question: str, verbose: bool = False, stream:bool = False) -> Union[Generator,str]:\n",
      "        \"\"\"\n",
      "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
      "        and added to the memory.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "        prompt, marked_question = self.fifovector_memory_prompt(question)\n",
      "\n",
      "        self.add_message(marked_question)\n",
      "\n",
      "        answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n",
      "        if stream:\n",
      "            return answer\n",
      "        else:\n",
      "            self.add_message(answer)\n",
      "            return answer\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"int\"\n",
      "\t\"VectorThread\"\n",
      "\t\"mark_system\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"mark_question\"\n",
      "]\n",
      "\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    model: str = None,\n",
      "    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "    system_prompt: str = None,\n",
      "    user_prompt: str = None,\n",
      "    name: str = \"fifo_vector_memory\",\n",
      "    max_memory: int = 2048,\n",
      "    max_index_memory: int = 400,\n",
      "    max_output_tokens: int = 1000,\n",
      "    longterm_thread: Optional[VectorThread] = None,\n",
      "    longterm_frac: float = 0.5,\n",
      "):\n",
      "    self.total_max_memory = max_memory\n",
      "\n",
      "    self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n",
      "    FifoThread.__init__(\n",
      "        self,\n",
      "        name=name,\n",
      "        max_memory=self.max_fifo_memory,\n",
      "        longterm_thread=self.longterm_thread,\n",
      "    )\n",
      "    Chat.__init__(\n",
      "        self,\n",
      "        model=model,\n",
      "        index_dict=index_dict,\n",
      "        max_output_tokens=max_output_tokens,\n",
      "        max_index_memory=max_index_memory,\n",
      "        system_prompt=system_prompt,\n",
      "        user_prompt=user_prompt,\n",
      "        name=name,\n",
      "    )\n",
      "    \n",
      "    self.prompt_func = self.fifovector_memory_prompt\n",
      "    self.prompt_list = []\n",
      "\n",
      "\n",
      "\n",
      "def setup_longterm_memory(\n",
      "    self,\n",
      "    longterm_thread: Optional[VectorThread],\n",
      "    max_memory: int,\n",
      "    longterm_frac: float,\n",
      "):\n",
      "    \"\"\"\n",
      "        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n",
      "\n",
      "        :param longterm_thread: An optional VectorThread for long-term memory.\n",
      "        :param max_memory: The maximum amount of memory for the chatbot.\n",
      "        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n",
      "        \"\"\"\n",
      "    if longterm_thread is None:\n",
      "        self.longterm_frac = longterm_frac\n",
      "        self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n",
      "        self.max_vector_memory = max_memory - self.max_fifo_memory\n",
      "        self.longterm_thread = VectorThread(\n",
      "            name=\"longterm_memory\", max_context=self.max_vector_memory\n",
      "        )\n",
      "    else:\n",
      "        self.longterm_thread = longterm_thread\n",
      "        self.max_vector_memory = self.longterm_thread.max_context\n",
      "        self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n",
      "        self.longterm_frac = self.max_vector_memory / self.total_max_memory\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"int\"\n",
      "\t\"VectorThread\"\n",
      "]\n",
      "\n",
      "\n",
      "def fifovector_memory_prompt(\n",
      "    self, message: str, k: int = 10\n",
      ") -> Tuple[List[dict], dict]:\n",
      "    \"\"\"\n",
      "        Combine the system prompt, long-term memory (vector memory), short-term memory (FIFO memory), and the user prompt.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include from the long-term memory.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "    prompt = [mark_system(self.system_prompt)]\n",
      "    if (\n",
      "        len(self.longterm_thread.memory_thread) > 0\n",
      "        and self.longterm_thread.total_tokens <= self.max_vector_memory\n",
      "    ):\n",
      "        prompt += self.longterm_thread.memory_thread\n",
      "    elif (\n",
      "        len(self.longterm_thread.memory_thread) > 0\n",
      "        and self.longterm_thread.total_tokens > self.max_vector_memory\n",
      "    ):\n",
      "        (\n",
      "            sorted_messages,\n",
      "            sorted_scores,\n",
      "            sorted_indices,\n",
      "        ) = self.longterm_thread.sorted_query(\n",
      "            message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
      "        )\n",
      "        prompt += sorted_messages\n",
      "\n",
      "    prompt += self.memory_thread\n",
      "    marked_question = mark_question(self.user_prompt(message))\n",
      "    prompt += [marked_question]\n",
      "    return prompt, marked_question\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_system\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"mark_question\"\n",
      "]\n",
      "\n",
      "\n",
      "def query(self, question: str, verbose: bool = False, stream:bool = False) -> Union[Generator,str]:\n",
      "    \"\"\"\n",
      "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
      "        and added to the memory.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "    prompt, marked_question = self.fifovector_memory_prompt(question)\n",
      "\n",
      "    self.add_message(marked_question)\n",
      "\n",
      "    answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n",
      "    if stream:\n",
      "        return answer\n",
      "    else:\n",
      "        self.add_message(answer)\n",
      "        return answer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class ContextManagedFifoVectorChat(FifoThread, Chat):\n",
      "    def __init__(\n",
      "        self,\n",
      "        model: str = None,\n",
      "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "        system_prompt: str = None,\n",
      "        user_prompt: str = None,\n",
      "        name: str = \"fifo_vector_memory\",\n",
      "        max_memory: int = 2048,\n",
      "        max_index_memory: int = 400,\n",
      "        max_output_tokens: int = 1000,\n",
      "        longterm_thread: Optional[VectorThread] = None,\n",
      "        longterm_frac: float = 0.5,\n",
      "    ):\n",
      "        self.total_max_memory = max_memory\n",
      "        \n",
      "\n",
      "        self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n",
      "        FifoThread.__init__(\n",
      "            self,\n",
      "            name=name,\n",
      "            max_memory=self.max_fifo_memory,\n",
      "            longterm_thread=self.longterm_thread,\n",
      "        )\n",
      "        Chat.__init__(\n",
      "            self,\n",
      "            model=model,\n",
      "            index_dict=index_dict,\n",
      "            max_output_tokens=max_output_tokens,\n",
      "            max_index_memory=max_index_memory,\n",
      "            system_prompt=system_prompt,\n",
      "            user_prompt=user_prompt,\n",
      "            name=name,\n",
      "        )\n",
      "        #self.context_manager = ContextManager(index_dict)\n",
      "        #deep copy of index_dict\n",
      "        '''\n",
      "        deep_copied_index_dict = copy.deepcopy(self.index_dict)\n",
      "        \n",
      "        self.multi_kernel = CreateMultiKernel(deep_copied_index_dict).create_multi_kernel()\n",
      "        self.memory_kernel_dict = self.multi_kernel.memory_kernel_dict\n",
      "        self.path_group = self.multi_kernel.path_group\n",
      "        \n",
      "        self.prompt_func = self.fifovector_memory_prompt\n",
      "        '''\n",
      "        self.prompt_list = []\n",
      "    \n",
      "    def setup_longterm_memory(\n",
      "        self,\n",
      "        longterm_thread: Optional[VectorThread],\n",
      "        max_memory: int,\n",
      "        longterm_frac: float,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n",
      "\n",
      "        :param longterm_thread: An optional VectorThread for long-term memory.\n",
      "        :param max_memory: The maximum amount of memory for the chatbot.\n",
      "        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n",
      "        \"\"\"\n",
      "        #TODO preload longterm memory with index summaries\n",
      "        if longterm_thread is None:\n",
      "            self.longterm_frac = longterm_frac\n",
      "            self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n",
      "            self.max_vector_memory = max_memory - self.max_fifo_memory\n",
      "            self.longterm_thread = VectorThread(\n",
      "                name=\"longterm_memory\", max_context=self.max_vector_memory\n",
      "            )\n",
      "        else:\n",
      "            self.longterm_thread = longterm_thread\n",
      "            self.max_vector_memory = self.longterm_thread.max_context\n",
      "            self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n",
      "            self.longterm_frac = self.max_vector_memory / self.total_max_memory\n",
      "\n",
      "    def calculate_stability(self, boundaries, adjacency_matrix_with_message):\n",
      "        boundary_stability = np.zeros(len(boundaries) - 1)\n",
      "        for i in range(len(boundaries) - 1):\n",
      "            boundary_connections = adjacency_matrix_with_message[boundaries[i]:boundaries[i+1], :][:, boundaries[i]:boundaries[i+1]]\n",
      "            boundary_stability[i] = np.mean(boundary_connections) - np.std(boundary_connections)**2\n",
      "        logging.info(f\"Boundary Stability: {boundary_stability}\")\n",
      "        return boundary_stability\n",
      "\n",
      "    def query_hints(self, message, index, k):\n",
      "        top_k, _, indices = index.token_bound_query(message, k=k, max_tokens=5000)\n",
      "        top_k_embeddings = [index.embeddings[i] for i in indices]\n",
      "        return top_k, top_k_embeddings\n",
      "\n",
      "    def query_mk_hints(self, message, index_key, index, k):\n",
      "        top_k, _, indices = index.token_bound_query(message, k=k, max_tokens=3000)\n",
      "        top_k_embeddings = [self.memory_kernel_dict[index_key].node_embeddings[i] for i in indices]\n",
      "        return top_k, top_k_embeddings\n",
      "\n",
      "\n",
      "    def heat_trajectory(self, message, k=20):\n",
      "        \"\"\"\n",
      "        This function gets the top k embeddings from all indexes including longterm, merges into numpy matrix,\n",
      "        record boundaries, computes kernel adj matrix, and sums all connections in each index section.\n",
      "        It returns the heat trajectory.\n",
      "        \"\"\"\n",
      "        embeddings = []\n",
      "        boundaries = [0]  # Start boundary\n",
      "        top_k_hints = {}\n",
      "        # Gather top k embeddings from each index\n",
      "        logging.info(\"Computing Trajectory for the current state of memory.\")\n",
      "        for index_key, index in self.index_dict.items():\n",
      "            top_k, top_k_embeddings = self.query_hints(message, index, k)\n",
      "            embeddings.extend(top_k_embeddings)\n",
      "            boundaries.append(boundaries[-1] + k)\n",
      "            top_k_hints[index_key] = top_k\n",
      "\n",
      "\n",
      "        if len(self.longterm_thread.values) >= k:\n",
      "            if len(self.longterm_thread.embeddings) != len(self.longterm_thread.values):\n",
      "                self.longterm_thread.compute_embeddings()\n",
      "            top_k, top_k_embeddings = self.query_hints(message, index, k)\n",
      "            embeddings.extend(top_k_embeddings)\n",
      "            boundaries.append(boundaries[-1] + k)\n",
      "            top_k_hints['longterm_thread'] = top_k\n",
      "        mean = np.sum(embeddings, axis=1)\n",
      "        mean = mean / np.linalg.norm(mean)\n",
      "        #should be shape (,1536) currently (4, 1536)\n",
      "        mean = np.sum(mean, axis=0)\n",
      "\n",
      "\n",
      "        logging.info(f\"Mean: {mean.shape}\")\n",
      "        embeddings_matrix = np.vstack(embeddings)\n",
      "        adjacency_matrix = cosine_similarity(embeddings_matrix)\n",
      "        message_embedding = EMBEDDER.embed(data=message)\n",
      "        #get residuals\n",
      "        message_embedding = message_embedding - mean\n",
      "        top_k_embeddings_with_message = embeddings_matrix.copy()\n",
      "        top_k_embeddings_with_message[:-1] += message_embedding\n",
      "        norms = np.linalg.norm(top_k_embeddings_with_message, axis=1)\n",
      "        normalized_embeddings = top_k_embeddings_with_message / norms[:, np.newaxis]\n",
      "        adjacency_matrix_with_message = cosine_similarity(normalized_embeddings)\n",
      "\n",
      "        adjacency_matrix_with_message = adjacency_matrix_with_message - adjacency_matrix\n",
      "        adjacency_matrix_with_message =  adjacency_matrix_with_message**3\n",
      "\n",
      "        degrees = np.sum(adjacency_matrix_with_message, axis=1)\n",
      "        heat_trajectory = [np.sum(degrees[boundaries[i]:boundaries[i + 1]]) for i in range(len(boundaries) - 1)]\n",
      "        logging.info(f\"Heat Trajectory: {heat_trajectory}\")\n",
      "        heat_dict = dict(zip(list(self.index_dict.keys()) + ['longterm_thread'], heat_trajectory))\n",
      "        sum_of_vals = sum(heat_dict.values())\n",
      "        heat_dict = {k: v / sum_of_vals for k, v in heat_dict.items()}\n",
      "        hdict = {k: v for k, v in sorted(heat_dict.items(), key=lambda item: item[1])}\n",
      "        logging.info(f\"Heat dict: {heat_dict}\")\n",
      "        return hdict, top_k_hints\n",
      "\n",
      "\n",
      "    def fifovector_memory_prompt(\n",
      "        self, message: str, k: int = 5\n",
      "    ) -> Tuple[List[dict], dict]:\n",
      "        hdict, top_k_hint_dict = self.heat_trajectory(message)\n",
      "        min_heat_index = list(hdict.keys())[0]\n",
      "        second_min_heat_index = list(hdict.keys())[1]\n",
      "        if len(hdict) > 2:\n",
      "            third_min_heat_index = list(hdict.keys())[2]\n",
      "        if min_heat_index == 'longterm_thread':\n",
      "            logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from long-term memory.\")\n",
      "            logging.info(f\"Number of values in Index: {len(self.longterm_thread.values)}\")\n",
      "            if len(hdict) > 2:\n",
      "                if hdict[second_min_heat_index] - hdict[third_min_heat_index] < 0.20:\n",
      "                    top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-2] + top_k_hint_dict[third_min_heat_index][:1]\n",
      "                else:\n",
      "                    top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-1]\n",
      "            else:\n",
      "                top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-1]\n",
      "\n",
      "            logging.info(f\"Top K Hint: {top_k_hint}\")\n",
      "            prompt =f'[LONG TERM MEMORY]{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n",
      "        elif min_heat_index in self.index_dict.keys():\n",
      "            # if the difference between min and second min is less than 0.25, merge hints from both indexes\n",
      "            if hdict[min_heat_index] - hdict[second_min_heat_index] < 0.25:\n",
      "                logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from index.\")\n",
      "                logging.info(f\"Number of values in Index {self.index_dict[min_heat_index].name}: {len(self.index_dict[min_heat_index].values)}\")\n",
      "                #take 2 from first index topk and 1 from second index topk\n",
      "                top_k_hint = top_k_hint_dict[min_heat_index][:k-2] + top_k_hint_dict[second_min_heat_index][:2]\n",
      "                logging.info(f\"Top K Hint: {top_k_hint}\")\n",
      "                prompt =f'{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n",
      "            else:\n",
      "                logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from index.\")\n",
      "                logging.info(f\"Number of values in Index {self.index_dict[min_heat_index].name}: {len(self.index_dict[min_heat_index].values)}\")\n",
      "                top_k_hint = top_k_hint_dict[min_heat_index][:k]\n",
      "                logging.info(f\"Top K Hint: {top_k_hint}\")\n",
      "                prompt =f'{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n",
      "        else:\n",
      "            raise ValueError(\"The provided index name is not available.\")\n",
      "\n",
      "        return prompt\n",
      "\n",
      "    def context_query(self, question: str, verbose: bool = False, stream: bool = False) -> Union[Generator, str]:\n",
      "        prompt = self.fifovector_memory_prompt(question)\n",
      "        modified_question = mark_question(prompt)\n",
      "        self.add_message(modified_question)\n",
      "\n",
      "        answer = BaseChat.query(self, message=prompt, verbose=verbose, stream=stream)\n",
      "\n",
      "        if stream:\n",
      "            return answer\n",
      "        else:\n",
      "            self.add_message(answer)\n",
      "            return answer\n",
      "\n",
      "Function calls: shape: (30,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"int\"\n",
      "\t\"VectorThread\"\n",
      "\t\"len\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"cosine_similar…\n",
      "\t\"cosine_similar…\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t…\n",
      "\t\"list\"\n",
      "\t\"list\"\n",
      "\t\"len\"\n",
      "\t\"list\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"str\"\n",
      "\t\"len\"\n",
      "\t\"str\"\n",
      "\t\"len\"\n",
      "\t\"str\"\n",
      "\t\"ValueError\"\n",
      "\t\"mark_question\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    model: str = None,\n",
      "    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "    system_prompt: str = None,\n",
      "    user_prompt: str = None,\n",
      "    name: str = \"fifo_vector_memory\",\n",
      "    max_memory: int = 2048,\n",
      "    max_index_memory: int = 400,\n",
      "    max_output_tokens: int = 1000,\n",
      "    longterm_thread: Optional[VectorThread] = None,\n",
      "    longterm_frac: float = 0.5,\n",
      "):\n",
      "    self.total_max_memory = max_memory\n",
      "    \n",
      "\n",
      "    self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n",
      "    FifoThread.__init__(\n",
      "        self,\n",
      "        name=name,\n",
      "        max_memory=self.max_fifo_memory,\n",
      "        longterm_thread=self.longterm_thread,\n",
      "    )\n",
      "    Chat.__init__(\n",
      "        self,\n",
      "        model=model,\n",
      "        index_dict=index_dict,\n",
      "        max_output_tokens=max_output_tokens,\n",
      "        max_index_memory=max_index_memory,\n",
      "        system_prompt=system_prompt,\n",
      "        user_prompt=user_prompt,\n",
      "        name=name,\n",
      "    )\n",
      "    #self.context_manager = ContextManager(index_dict)\n",
      "    #deep copy of index_dict\n",
      "    '''\n",
      "        deep_copied_index_dict = copy.deepcopy(self.index_dict)\n",
      "        \n",
      "        self.multi_kernel = CreateMultiKernel(deep_copied_index_dict).create_multi_kernel()\n",
      "        self.memory_kernel_dict = self.multi_kernel.memory_kernel_dict\n",
      "        self.path_group = self.multi_kernel.path_group\n",
      "        \n",
      "        self.prompt_func = self.fifovector_memory_prompt\n",
      "        '''\n",
      "    self.prompt_list = []\n",
      "\n",
      "\n",
      "\n",
      "def setup_longterm_memory(\n",
      "    self,\n",
      "    longterm_thread: Optional[VectorThread],\n",
      "    max_memory: int,\n",
      "    longterm_frac: float,\n",
      "):\n",
      "    \"\"\"\n",
      "        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n",
      "\n",
      "        :param longterm_thread: An optional VectorThread for long-term memory.\n",
      "        :param max_memory: The maximum amount of memory for the chatbot.\n",
      "        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n",
      "        \"\"\"\n",
      "    #TODO preload longterm memory with index summaries\n",
      "    if longterm_thread is None:\n",
      "        self.longterm_frac = longterm_frac\n",
      "        self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n",
      "        self.max_vector_memory = max_memory - self.max_fifo_memory\n",
      "        self.longterm_thread = VectorThread(\n",
      "            name=\"longterm_memory\", max_context=self.max_vector_memory\n",
      "        )\n",
      "    else:\n",
      "        self.longterm_thread = longterm_thread\n",
      "        self.max_vector_memory = self.longterm_thread.max_context\n",
      "        self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n",
      "        self.longterm_frac = self.max_vector_memory / self.total_max_memory\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"int\"\n",
      "\t\"VectorThread\"\n",
      "]\n",
      "\n",
      "\n",
      "def calculate_stability(self, boundaries, adjacency_matrix_with_message):\n",
      "    boundary_stability = np.zeros(len(boundaries) - 1)\n",
      "    for i in range(len(boundaries) - 1):\n",
      "        boundary_connections = adjacency_matrix_with_message[boundaries[i]:boundaries[i+1], :][:, boundaries[i]:boundaries[i+1]]\n",
      "        boundary_stability[i] = np.mean(boundary_connections) - np.std(boundary_connections)**2\n",
      "    logging.info(f\"Boundary Stability: {boundary_stability}\")\n",
      "    return boundary_stability\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def query_hints(self, message, index, k):\n",
      "    top_k, _, indices = index.token_bound_query(message, k=k, max_tokens=5000)\n",
      "    top_k_embeddings = [index.embeddings[i] for i in indices]\n",
      "    return top_k, top_k_embeddings\n",
      "\n",
      "\n",
      "\n",
      "def query_mk_hints(self, message, index_key, index, k):\n",
      "    top_k, _, indices = index.token_bound_query(message, k=k, max_tokens=3000)\n",
      "    top_k_embeddings = [self.memory_kernel_dict[index_key].node_embeddings[i] for i in indices]\n",
      "    return top_k, top_k_embeddings\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def heat_trajectory(self, message, k=20):\n",
      "    \"\"\"\n",
      "        This function gets the top k embeddings from all indexes including longterm, merges into numpy matrix,\n",
      "        record boundaries, computes kernel adj matrix, and sums all connections in each index section.\n",
      "        It returns the heat trajectory.\n",
      "        \"\"\"\n",
      "    embeddings = []\n",
      "    boundaries = [0]  # Start boundary\n",
      "    top_k_hints = {}\n",
      "    # Gather top k embeddings from each index\n",
      "    logging.info(\"Computing Trajectory for the current state of memory.\")\n",
      "    for index_key, index in self.index_dict.items():\n",
      "        top_k, top_k_embeddings = self.query_hints(message, index, k)\n",
      "        embeddings.extend(top_k_embeddings)\n",
      "        boundaries.append(boundaries[-1] + k)\n",
      "        top_k_hints[index_key] = top_k\n",
      "\n",
      "\n",
      "    if len(self.longterm_thread.values) >= k:\n",
      "        if len(self.longterm_thread.embeddings) != len(self.longterm_thread.values):\n",
      "            self.longterm_thread.compute_embeddings()\n",
      "        top_k, top_k_embeddings = self.query_hints(message, index, k)\n",
      "        embeddings.extend(top_k_embeddings)\n",
      "        boundaries.append(boundaries[-1] + k)\n",
      "        top_k_hints['longterm_thread'] = top_k\n",
      "    mean = np.sum(embeddings, axis=1)\n",
      "    mean = mean / np.linalg.norm(mean)\n",
      "    #should be shape (,1536) currently (4, 1536)\n",
      "    mean = np.sum(mean, axis=0)\n",
      "\n",
      "\n",
      "    logging.info(f\"Mean: {mean.shape}\")\n",
      "    embeddings_matrix = np.vstack(embeddings)\n",
      "    adjacency_matrix = cosine_similarity(embeddings_matrix)\n",
      "    message_embedding = EMBEDDER.embed(data=message)\n",
      "    #get residuals\n",
      "    message_embedding = message_embedding - mean\n",
      "    top_k_embeddings_with_message = embeddings_matrix.copy()\n",
      "    top_k_embeddings_with_message[:-1] += message_embedding\n",
      "    norms = np.linalg.norm(top_k_embeddings_with_message, axis=1)\n",
      "    normalized_embeddings = top_k_embeddings_with_message / norms[:, np.newaxis]\n",
      "    adjacency_matrix_with_message = cosine_similarity(normalized_embeddings)\n",
      "\n",
      "    adjacency_matrix_with_message = adjacency_matrix_with_message - adjacency_matrix\n",
      "    adjacency_matrix_with_message =  adjacency_matrix_with_message**3\n",
      "\n",
      "    degrees = np.sum(adjacency_matrix_with_message, axis=1)\n",
      "    heat_trajectory = [np.sum(degrees[boundaries[i]:boundaries[i + 1]]) for i in range(len(boundaries) - 1)]\n",
      "    logging.info(f\"Heat Trajectory: {heat_trajectory}\")\n",
      "    heat_dict = dict(zip(list(self.index_dict.keys()) + ['longterm_thread'], heat_trajectory))\n",
      "    sum_of_vals = sum(heat_dict.values())\n",
      "    heat_dict = {k: v / sum_of_vals for k, v in heat_dict.items()}\n",
      "    hdict = {k: v for k, v in sorted(heat_dict.items(), key=lambda item: item[1])}\n",
      "    logging.info(f\"Heat dict: {heat_dict}\")\n",
      "    return hdict, top_k_hints\n",
      "\n",
      "Function calls: shape: (12,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"cosine_similar…\n",
      "\t\"cosine_similar…\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"dict\"\n",
      "\t\"zip\"\n",
      "\t\"list\"\n",
      "\t\"sum\"\n",
      "\t\"sorted\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def fifovector_memory_prompt(\n",
      "    self, message: str, k: int = 5\n",
      ") -> Tuple[List[dict], dict]:\n",
      "    hdict, top_k_hint_dict = self.heat_trajectory(message)\n",
      "    min_heat_index = list(hdict.keys())[0]\n",
      "    second_min_heat_index = list(hdict.keys())[1]\n",
      "    if len(hdict) > 2:\n",
      "        third_min_heat_index = list(hdict.keys())[2]\n",
      "    if min_heat_index == 'longterm_thread':\n",
      "        logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from long-term memory.\")\n",
      "        logging.info(f\"Number of values in Index: {len(self.longterm_thread.values)}\")\n",
      "        if len(hdict) > 2:\n",
      "            if hdict[second_min_heat_index] - hdict[third_min_heat_index] < 0.20:\n",
      "                top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-2] + top_k_hint_dict[third_min_heat_index][:1]\n",
      "            else:\n",
      "                top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-1]\n",
      "        else:\n",
      "            top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-1]\n",
      "\n",
      "        logging.info(f\"Top K Hint: {top_k_hint}\")\n",
      "        prompt =f'[LONG TERM MEMORY]{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n",
      "    elif min_heat_index in self.index_dict.keys():\n",
      "        # if the difference between min and second min is less than 0.25, merge hints from both indexes\n",
      "        if hdict[min_heat_index] - hdict[second_min_heat_index] < 0.25:\n",
      "            logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from index.\")\n",
      "            logging.info(f\"Number of values in Index {self.index_dict[min_heat_index].name}: {len(self.index_dict[min_heat_index].values)}\")\n",
      "            #take 2 from first index topk and 1 from second index topk\n",
      "            top_k_hint = top_k_hint_dict[min_heat_index][:k-2] + top_k_hint_dict[second_min_heat_index][:2]\n",
      "            logging.info(f\"Top K Hint: {top_k_hint}\")\n",
      "            prompt =f'{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n",
      "        else:\n",
      "            logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from index.\")\n",
      "            logging.info(f\"Number of values in Index {self.index_dict[min_heat_index].name}: {len(self.index_dict[min_heat_index].values)}\")\n",
      "            top_k_hint = top_k_hint_dict[min_heat_index][:k]\n",
      "            logging.info(f\"Top K Hint: {top_k_hint}\")\n",
      "            prompt =f'{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n",
      "    else:\n",
      "        raise ValueError(\"The provided index name is not available.\")\n",
      "\n",
      "    return prompt\n",
      "\n",
      "Function calls: shape: (12,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"list\"\n",
      "\t\"list\"\n",
      "\t\"len\"\n",
      "\t\"list\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"str\"\n",
      "\t\"len\"\n",
      "\t\"str\"\n",
      "\t\"len\"\n",
      "\t\"str\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def context_query(self, question: str, verbose: bool = False, stream: bool = False) -> Union[Generator, str]:\n",
      "    prompt = self.fifovector_memory_prompt(question)\n",
      "    modified_question = mark_question(prompt)\n",
      "    self.add_message(modified_question)\n",
      "\n",
      "    answer = BaseChat.query(self, message=prompt, verbose=verbose, stream=stream)\n",
      "\n",
      "    if stream:\n",
      "        return answer\n",
      "    else:\n",
      "        self.add_message(answer)\n",
      "        return answer\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_question\"\n",
      "]\n",
      "\n",
      "\n",
      "# Set the variables\n",
      "\n",
      "\n",
      "async def process_chat_requests(\n",
      "    request_data: List[Dict],\n",
      "    save_filepath: str,\n",
      "    request_url: str,\n",
      "    api_key: str,\n",
      "    max_requests_per_minute: float,\n",
      "    max_tokens_per_minute: float,\n",
      "    token_encoding_name: str,\n",
      "    max_attempts: int,\n",
      "    logging_level: int,\n",
      "):\n",
      "    \"\"\"Processes chat requests in parallel, throttling to stay under rate limits.\"\"\"\n",
      "    # constants\n",
      "    seconds_to_pause_after_rate_limit_error = 15\n",
      "    seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n",
      "\n",
      "    # initialize logging\n",
      "    logging.basicConfig(level=logging_level)\n",
      "    logging.debug(f\"Logging initialized at level {logging_level}\")\n",
      "\n",
      "    # infer API endpoint and construct request header\n",
      "    api_endpoint = api_endpoint_from_url(request_url)\n",
      "    request_header = {\"Authorization\": f\"Bearer {api_key}\"}\n",
      "\n",
      "    # initialize trackers\n",
      "    queue_of_requests_to_retry = asyncio.Queue()\n",
      "    task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...\n",
      "    status_tracker = StatusTracker()  # single instance to track a collection of variables\n",
      "    next_request = None  # variable to hold the next request to call\n",
      "\n",
      "    # initialize available capacity counts\n",
      "    available_request_capacity = max_requests_per_minute\n",
      "    available_token_capacity = max_tokens_per_minute\n",
      "    last_update_time = time.time()\n",
      "\n",
      "    # initialize flags\n",
      "    file_not_finished = True  # after file is empty, we'll skip reading it\n",
      "    logging.debug(f\"Initialization complete.\")\n",
      "\n",
      "    # `requests` will provide requests one at a time\n",
      "    requests = iter(request_data)\n",
      "    logging.debug(f\"List initialized. Entering main loop\")\n",
      "\n",
      "    while True:\n",
      "        # get next request (if one is not already waiting for capacity)\n",
      "        if next_request is None:\n",
      "            if not queue_of_requests_to_retry.empty():\n",
      "                next_request = queue_of_requests_to_retry.get_nowait()\n",
      "                logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")\n",
      "            elif file_not_finished:\n",
      "                try:\n",
      "                    # get new request\n",
      "                    request_json = next(requests)\n",
      "                    next_request = APIRequest(\n",
      "                        task_id=next(task_id_generator),\n",
      "                        request_json=request_json,\n",
      "                        token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),\n",
      "                        attempts_left=max_attempts,\n",
      "                        metadata=request_json.pop(\"metadata\", None)\n",
      "                    )\n",
      "                    status_tracker.num_tasks_started += 1\n",
      "                    status_tracker.num_tasks_in_progress += 1\n",
      "                    logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")\n",
      "                except StopIteration:\n",
      "                    # if file runs out, set flag to stop reading it\n",
      "                    logging.debug(\"Read file exhausted\")\n",
      "                    file_not_finished = False\n",
      "\n",
      "        # update available capacity\n",
      "        current_time = time.time()\n",
      "        seconds_since_update = current_time - last_update_time\n",
      "        available_request_capacity = min(\n",
      "            available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,\n",
      "            max_requests_per_minute,\n",
      "        )\n",
      "        available_token_capacity = min(\n",
      "            available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,\n",
      "            max_tokens_per_minute,\n",
      "        )\n",
      "        last_update_time = current_time\n",
      "\n",
      "        # if enough capacity available, call API\n",
      "        if next_request:\n",
      "            next_request_tokens = next_request.token_consumption\n",
      "            if (\n",
      "                available_request_capacity >= 1\n",
      "                and available_token_capacity >= next_request_tokens\n",
      "            ):\n",
      "                # update counters\n",
      "                available_request_capacity -= 1\n",
      "                available_token_capacity -= next_request_tokens\n",
      "                next_request.attempts_left -= 1\n",
      "\n",
      "                # call API\n",
      "                asyncio.create_task(\n",
      "                    next_request.call_api(\n",
      "                        request_url=request_url,\n",
      "                        request_header=request_header,\n",
      "                        retry_queue=queue_of_requests_to_retry,\n",
      "                        save_filepath=save_filepath,\n",
      "                        status_tracker=status_tracker,\n",
      "                    )\n",
      "                )\n",
      "                next_request = None  # reset next_request to empty\n",
      "\n",
      "        # if all tasks are finished, break\n",
      "        if status_tracker.num_tasks_in_progress == 0:\n",
      "            break\n",
      "\n",
      "        # main loop sleeps briefly so concurrent tasks can run\n",
      "        await asyncio.sleep(seconds_to_sleep_each_loop)\n",
      "\n",
      "        # if a rate limit error was hit recently, pause to cool down\n",
      "        seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)\n",
      "        if seconds_since_rate_limit_error < seconds_to_pause_after_rate_limit_error:\n",
      "            remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n",
      "            await asyncio.sleep(remaining_seconds_to_pause)\n",
      "            # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n",
      "            logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")\n",
      "\n",
      "    # after finishing, log final status\n",
      "    logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")\n",
      "    if status_tracker.num_tasks_failed > 0:\n",
      "        logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")\n",
      "    if status_tracker.num_rate_limit_errors > 0:\n",
      "        logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n",
      "\n",
      "Function calls: shape: (10,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"api_endpoint_f…\n",
      "\t\"task_id_genera…\n",
      "\t\"StatusTracker\"\n",
      "\t\"iter\"\n",
      "\t\"next\"\n",
      "\t\"APIRequest\"\n",
      "\t\"next\"\n",
      "\t\"num_tokens_con…\n",
      "\t\"min\"\n",
      "\t\"min\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class BatchChat(BaseChat):\n",
      "    def __init__(self, model: str = \"gpt-3.5-turbo\", max_output_tokens: int = 200, save_filepath: str = None, max_attempts: int = 5, api_key: str = None ):\n",
      "        #super init\n",
      "        super().__init__(model=model, max_output_tokens=max_output_tokens)\n",
      "        # Set the variables\n",
      "        if save_filepath is None:\n",
      "            self.save_filepath = \"results.jsonl\"  # Default filename if not using input file\n",
      "        else:\n",
      "            self.save_filepath = save_filepath\n",
      "        self.request_url = \"https://api.openai.com/v1/chat/completions\" # URL for the chat endpoint\n",
      "        self.max_requests_per_minute = 3_000 * 0.5\n",
      "        self.max_tokens_per_minute = 250_000 * 0.5\n",
      "        self.token_encoding_name = \"cl100k_base\"\n",
      "        self.api_key = api_key\n",
      "        self.max_attempts = max_attempts\n",
      "        self.logging_level = logging.INFO\n",
      "\n",
      "    async def batch_query(self, messages: List[str], system_prompts:List[str]) -> List[str]:\n",
      "        request_data = []\n",
      "        for message, system_prompt in zip(messages, system_prompts):\n",
      "            prompt, _ = self.prompt_func(message)\n",
      "            print(prompt)\n",
      "            request_data.append({\n",
      "                \"model\": self.model,\n",
      "                \"max_tokens\": self.max_output_tokens,\n",
      "                \"messages\": [\n",
      "                    {\"role\": \"system\", \"content\": system_prompt},\n",
      "                    {\"role\": \"user\", \"content\": message}\n",
      "                ]\n",
      "            })\n",
      "        responses = await process_chat_requests(request_data=request_data, request_url=self.request_url, api_key=self.api_key, save_filepath=self.save_filepath, max_requests_per_minute=self.max_requests_per_minute, max_tokens_per_minute=self.max_tokens_per_minute, token_encoding_name=self.token_encoding_name, max_attempts=self.max_attempts, logging_level=self.logging_level)\n",
      "        return responses\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"zip\"\n",
      "\t\"print\"\n",
      "\t\"process_chat_r…\n",
      "]\n",
      "\n",
      "def __init__(self, model: str = \"gpt-3.5-turbo\", max_output_tokens: int = 200, save_filepath: str = None, max_attempts: int = 5, api_key: str = None ):\n",
      "    #super init\n",
      "    super().__init__(model=model, max_output_tokens=max_output_tokens)\n",
      "    # Set the variables\n",
      "    if save_filepath is None:\n",
      "        self.save_filepath = \"results.jsonl\"  # Default filename if not using input file\n",
      "    else:\n",
      "        self.save_filepath = save_filepath\n",
      "    self.request_url = \"https://api.openai.com/v1/chat/completions\" # URL for the chat endpoint\n",
      "    self.max_requests_per_minute = 3_000 * 0.5\n",
      "    self.max_tokens_per_minute = 250_000 * 0.5\n",
      "    self.token_encoding_name = \"cl100k_base\"\n",
      "    self.api_key = api_key\n",
      "    self.max_attempts = max_attempts\n",
      "    self.logging_level = logging.INFO\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "]\n",
      "\n",
      "\n",
      "async def batch_query(self, messages: List[str], system_prompts:List[str]) -> List[str]:\n",
      "    request_data = []\n",
      "    for message, system_prompt in zip(messages, system_prompts):\n",
      "        prompt, _ = self.prompt_func(message)\n",
      "        print(prompt)\n",
      "        request_data.append({\n",
      "            \"model\": self.model,\n",
      "            \"max_tokens\": self.max_output_tokens,\n",
      "            \"messages\": [\n",
      "                {\"role\": \"system\", \"content\": system_prompt},\n",
      "                {\"role\": \"user\", \"content\": message}\n",
      "            ]\n",
      "        })\n",
      "    responses = await process_chat_requests(request_data=request_data, request_url=self.request_url, api_key=self.api_key, save_filepath=self.save_filepath, max_requests_per_minute=self.max_requests_per_minute, max_tokens_per_minute=self.max_tokens_per_minute, token_encoding_name=self.token_encoding_name, max_attempts=self.max_attempts, logging_level=self.logging_level)\n",
      "    return responses\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"zip\"\n",
      "\t\"print\"\n",
      "\t\"process_chat_r…\n",
      "]\n",
      "\n",
      "\n",
      "class ContextManager:\n",
      "    \"\"\"\n",
      "    A class that manages the context of user interactions and maps them to the appropriate memory index or thread.\n",
      "    \"\"\"\n",
      "    def __init__(\n",
      "        self, \n",
      "        index_dict: Dict[str, MemoryIndex], \n",
      "        longterm_thread_keywords: List[str] = [\"long ago\", \"in the past\", \"long term\", \"long-term\", \"longterm\"]\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        Initialize the ContextManager with the available indexes.\n",
      "\n",
      "        :param index_dict: A dictionary mapping index names to MemoryIndex instances.\n",
      "        :param fifo_thread_keywords: A list of keywords indicating user reference to recent conversation context.\n",
      "        :param longterm_thread_keywords: A list of keywords indicating user reference to long-term conversation context.\n",
      "        \"\"\"\n",
      "        self.index_dict = index_dict\n",
      "        self.longterm_thread_keywords = longterm_thread_keywords\n",
      "        self.keyword_to_index_map = self.create_keyword_to_index_map()\n",
      "\n",
      "    def create_keyword_to_index_map(self) -> Dict[str, str]:\n",
      "        \"\"\"\n",
      "        Create a mapping from keywords to index names.\n",
      "\n",
      "        :return: A dictionary mapping keywords to index names or threads.\n",
      "        \"\"\"\n",
      "        keyword_to_index_map = {index_name.split('_')[0]: index_name for index_name in self.index_dict.keys()}\n",
      "        for keyword in self.longterm_thread_keywords:\n",
      "            keyword_to_index_map[keyword] = \"longterm_thread\"\n",
      "        return keyword_to_index_map\n",
      "\n",
      "    def get_context_for_user_input(self, user_input: str) -> str:\n",
      "        \"\"\"\n",
      "        Get the appropriate context (index or thread) for the given user input.\n",
      "\n",
      "        :param user_input: A string representing the user's input.\n",
      "        :return: The name of the appropriate context (index or thread), or None if no specific context is appropriate.\n",
      "        \"\"\"\n",
      "        user_input_str = str(user_input)  # Ensure user_input is a string\n",
      "        for keyword, context in self.keyword_to_index_map.items():\n",
      "            if re.search(r'\\b' + keyword + r'\\b', user_input_str, re.I):\n",
      "                return context\n",
      "        return None\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"str\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self, \n",
      "    index_dict: Dict[str, MemoryIndex], \n",
      "    longterm_thread_keywords: List[str] = [\"long ago\", \"in the past\", \"long term\", \"long-term\", \"longterm\"]\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "        Initialize the ContextManager with the available indexes.\n",
      "\n",
      "        :param index_dict: A dictionary mapping index names to MemoryIndex instances.\n",
      "        :param fifo_thread_keywords: A list of keywords indicating user reference to recent conversation context.\n",
      "        :param longterm_thread_keywords: A list of keywords indicating user reference to long-term conversation context.\n",
      "        \"\"\"\n",
      "    self.index_dict = index_dict\n",
      "    self.longterm_thread_keywords = longterm_thread_keywords\n",
      "    self.keyword_to_index_map = self.create_keyword_to_index_map()\n",
      "\n",
      "\n",
      "\n",
      "def create_keyword_to_index_map(self) -> Dict[str, str]:\n",
      "    \"\"\"\n",
      "        Create a mapping from keywords to index names.\n",
      "\n",
      "        :return: A dictionary mapping keywords to index names or threads.\n",
      "        \"\"\"\n",
      "    keyword_to_index_map = {index_name.split('_')[0]: index_name for index_name in self.index_dict.keys()}\n",
      "    for keyword in self.longterm_thread_keywords:\n",
      "        keyword_to_index_map[keyword] = \"longterm_thread\"\n",
      "    return keyword_to_index_map\n",
      "\n",
      "\n",
      "\n",
      "def get_context_for_user_input(self, user_input: str) -> str:\n",
      "    \"\"\"\n",
      "        Get the appropriate context (index or thread) for the given user input.\n",
      "\n",
      "        :param user_input: A string representing the user's input.\n",
      "        :return: The name of the appropriate context (index or thread), or None if no specific context is appropriate.\n",
      "        \"\"\"\n",
      "    user_input_str = str(user_input)  # Ensure user_input is a string\n",
      "    for keyword, context in self.keyword_to_index_map.items():\n",
      "        if re.search(r'\\b' + keyword + r'\\b', user_input_str, re.I):\n",
      "            return context\n",
      "    return None\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"str\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class Chat(BaseChat, Prompter):\n",
      "    \"\"\"\n",
      "    This class combines the BaseChat and Prompter classes to create a oneshot chatbot with a system and user prompt,\n",
      "    and the ability to handle queries to multiple MemoryIndex through the index_dict.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        model: str = None,\n",
      "        max_output_tokens: int = 1000,\n",
      "        system_prompt: str = None,\n",
      "        user_prompt: str = None,\n",
      "        index_dict: Optional[Dict[str, MemoryIndex]] = None,\n",
      "        max_index_memory: int = 1000,\n",
      "        name: str = \"Chat\",\n",
      "    ) -> None:\n",
      "        BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\n",
      "        Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\n",
      "        self.index_dict = index_dict\n",
      "        self.setup_indices(max_index_memory)\n",
      "        self.name = name\n",
      "    \n",
      "    def add_user_defined_ids(self, id_dict: Dict[str, list]):\n",
      "        self.user_defined_ids.append(id_dict)\n",
      "        self.use_user_defined_ids = True\n",
      "        self.setup_index_prompts()\n",
      "\n",
      "\n",
      "    def setup_index_prompts(self):\n",
      "        if self.current_index is not None or self.use_user_defined_ids:\n",
      "            print(\"Index is available so using index prompts\")\n",
      "            self.system_prompt = (\n",
      "                INDEX_SYSTEM_PROMPT \n",
      "                if self.user_defined_system_prompt is None \n",
      "                else self.user_defined_system_prompt\n",
      "            )\n",
      "            self.user_prompt = (\n",
      "                self.get_index_hints\n",
      "                if self.user_defined_user_prompt is None\n",
      "                else self.user_defined_user_prompt\n",
      "            )\n",
      "        else:\n",
      "            if self.user_defined_system_prompt is None:\n",
      "                print(\"No user defined system prompt defaulting to default prompts\")\n",
      "                self.set_default_prompts()\n",
      "            else:\n",
      "                print(\"User defined system prompt and default user prompt\")\n",
      "                self.system_prompt = self.user_defined_system_prompt\n",
      "                self.user_prompt = self.default_user_prompt\n",
      "\n",
      "\n",
      "                \n",
      "\n",
      "    def setup_indices(self, max_index_memory):\n",
      "        \"\"\"setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index.\"\"\"\n",
      "        if self.index_dict is not None:\n",
      "            self.max_index_memory = max_index_memory\n",
      "            # set the last index to be the current index\n",
      "            self.current_index = list(self.index_dict.keys())[-1]\n",
      "            self.setup_index_prompts()    \n",
      "        else:\n",
      "            self.current_index = None\n",
      "\n",
      "    def get_index_hints(\n",
      "        self, question: str, k: int = 10, max_tokens: int = None\n",
      "    ) -> str:\n",
      "        \"\"\"\n",
      "        Get hints from the current index for the given question.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include from the index.\n",
      "        :param max_tokens: The maximum number of tokens to be retrieved from the index.\n",
      "        :return: A string representing the hint prompt with the question.\n",
      "        \"\"\"\n",
      "        if max_tokens is None:\n",
      "            max_tokens = self.max_index_memory\n",
      "        hints = []\n",
      "        if self.use_user_defined_ids is True:\n",
      "            user_defined_id = self.user_defined_ids[-1]\n",
      "            for index, ids in user_defined_id.items():\n",
      "                for i in ids:\n",
      "                    hints.append(self.index_dict[index].values[i])\n",
      "            self.use_user_defined_ids = False\n",
      "            self.setup_index_prompts()\n",
      "        elif self.current_index is not None:\n",
      "            index_instance = self.index_dict[self.current_index]\n",
      "            if isinstance(index_instance, MemoryIndex):\n",
      "                hints, _, _ = index_instance.token_bound_query(\n",
      "                    question, k=k, max_tokens=max_tokens\n",
      "                )\n",
      "            else:\n",
      "                raise ValueError(\"The current index is not a valid index instance.\")\n",
      "        if len(hints) == 0:\n",
      "            return question\n",
      "        else:\n",
      "            hints_string = \"\\n\".join(hints)\n",
      "            hint_prompt = INDEX_HINT_PROMPT\n",
      "            question_intro = QUESTION_INTRO\n",
      "            return hint_prompt.format(\n",
      "                hints_string=hints_string\n",
      "            ) + question_intro.format(question=question)\n",
      "\n",
      "    def set_current_index(self, index_name: Optional[str]) -> None:\n",
      "        \"\"\"\n",
      "        Set the current index to be used for hints.\n",
      "\n",
      "        :param index_name: A string representing the index name or None to clear the current index.\n",
      "        :raise ValueError: If the provided index name is not available.\n",
      "        \"\"\"\n",
      "        if self.index_dict is None:\n",
      "            raise ValueError(\"No index_dict are available.\")\n",
      "        elif index_name in self.index_dict:\n",
      "            self.current_index = index_name\n",
      "        elif index_name is None:\n",
      "            self.current_index = None\n",
      "        else:\n",
      "            raise ValueError(\"The provided index name is not available.\")\n",
      "        self.setup_index_prompts()\n",
      "\n",
      "Function calls: shape: (9,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"list\"\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    model: str = None,\n",
      "    max_output_tokens: int = 1000,\n",
      "    system_prompt: str = None,\n",
      "    user_prompt: str = None,\n",
      "    index_dict: Optional[Dict[str, MemoryIndex]] = None,\n",
      "    max_index_memory: int = 1000,\n",
      "    name: str = \"Chat\",\n",
      ") -> None:\n",
      "    BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\n",
      "    Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\n",
      "    self.index_dict = index_dict\n",
      "    self.setup_indices(max_index_memory)\n",
      "    self.name = name\n",
      "\n",
      "\n",
      "\n",
      "def add_user_defined_ids(self, id_dict: Dict[str, list]):\n",
      "    self.user_defined_ids.append(id_dict)\n",
      "    self.use_user_defined_ids = True\n",
      "    self.setup_index_prompts()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def setup_index_prompts(self):\n",
      "    if self.current_index is not None or self.use_user_defined_ids:\n",
      "        print(\"Index is available so using index prompts\")\n",
      "        self.system_prompt = (\n",
      "            INDEX_SYSTEM_PROMPT \n",
      "            if self.user_defined_system_prompt is None \n",
      "            else self.user_defined_system_prompt\n",
      "        )\n",
      "        self.user_prompt = (\n",
      "            self.get_index_hints\n",
      "            if self.user_defined_user_prompt is None\n",
      "            else self.user_defined_user_prompt\n",
      "        )\n",
      "    else:\n",
      "        if self.user_defined_system_prompt is None:\n",
      "            print(\"No user defined system prompt defaulting to default prompts\")\n",
      "            self.set_default_prompts()\n",
      "        else:\n",
      "            print(\"User defined system prompt and default user prompt\")\n",
      "            self.system_prompt = self.user_defined_system_prompt\n",
      "            self.user_prompt = self.default_user_prompt\n",
      "\n",
      "\n",
      "            \n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def setup_indices(self, max_index_memory):\n",
      "    \"\"\"setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index.\"\"\"\n",
      "    if self.index_dict is not None:\n",
      "        self.max_index_memory = max_index_memory\n",
      "        # set the last index to be the current index\n",
      "        self.current_index = list(self.index_dict.keys())[-1]\n",
      "        self.setup_index_prompts()    \n",
      "    else:\n",
      "        self.current_index = None\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"list\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_index_hints(\n",
      "    self, question: str, k: int = 10, max_tokens: int = None\n",
      ") -> str:\n",
      "    \"\"\"\n",
      "        Get hints from the current index for the given question.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include from the index.\n",
      "        :param max_tokens: The maximum number of tokens to be retrieved from the index.\n",
      "        :return: A string representing the hint prompt with the question.\n",
      "        \"\"\"\n",
      "    if max_tokens is None:\n",
      "        max_tokens = self.max_index_memory\n",
      "    hints = []\n",
      "    if self.use_user_defined_ids is True:\n",
      "        user_defined_id = self.user_defined_ids[-1]\n",
      "        for index, ids in user_defined_id.items():\n",
      "            for i in ids:\n",
      "                hints.append(self.index_dict[index].values[i])\n",
      "        self.use_user_defined_ids = False\n",
      "        self.setup_index_prompts()\n",
      "    elif self.current_index is not None:\n",
      "        index_instance = self.index_dict[self.current_index]\n",
      "        if isinstance(index_instance, MemoryIndex):\n",
      "            hints, _, _ = index_instance.token_bound_query(\n",
      "                question, k=k, max_tokens=max_tokens\n",
      "            )\n",
      "        else:\n",
      "            raise ValueError(\"The current index is not a valid index instance.\")\n",
      "    if len(hints) == 0:\n",
      "        return question\n",
      "    else:\n",
      "        hints_string = \"\\n\".join(hints)\n",
      "        hint_prompt = INDEX_HINT_PROMPT\n",
      "        question_intro = QUESTION_INTRO\n",
      "        return hint_prompt.format(\n",
      "            hints_string=hints_string\n",
      "        ) + question_intro.format(question=question)\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def set_current_index(self, index_name: Optional[str]) -> None:\n",
      "    \"\"\"\n",
      "        Set the current index to be used for hints.\n",
      "\n",
      "        :param index_name: A string representing the index name or None to clear the current index.\n",
      "        :raise ValueError: If the provided index name is not available.\n",
      "        \"\"\"\n",
      "    if self.index_dict is None:\n",
      "        raise ValueError(\"No index_dict are available.\")\n",
      "    elif index_name in self.index_dict:\n",
      "        self.current_index = index_name\n",
      "    elif index_name is None:\n",
      "        self.current_index = None\n",
      "    else:\n",
      "        raise ValueError(\"The provided index name is not available.\")\n",
      "    self.setup_index_prompts()\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"ValueError\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class Prompter:\n",
      "    \"\"\"\n",
      "    This class handles the system and user prompts and the prompt_func. By subclassing and overriding the\n",
      "    prompt_func, you can change the way the prompts are composed.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, system_prompt: Union[str,None] = None, user_prompt: Union[str,None] = None):\n",
      "        \"\"\"\n",
      "        Initialize the Prompter with system and user prompts.\n",
      "\n",
      "        :param system_prompt: A string representing the system prompt.\n",
      "        :param user_prompt: A string representing the user prompt.\n",
      "        \"\"\"\n",
      "        \n",
      "        if system_prompt is None:\n",
      "            self.system_prompt = DEFAULT_SYSTEM_PROMPT\n",
      "            self.user_defined_system_prompt = None\n",
      "        else:\n",
      "            self.system_prompt = system_prompt\n",
      "            self.user_defined_system_prompt = system_prompt\n",
      "        if user_prompt is None:\n",
      "            self.user_prompt = self.default_user_prompt\n",
      "            self.user_defined_user_prompt = None\n",
      "        else:\n",
      "            self.user_prompt = user_prompt\n",
      "            self.user_defined_user_prompt = user_prompt\n",
      "\n",
      "        self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\n",
      "        self.user_defined_ids = []\n",
      "        self.user_defined_values = []\n",
      "        self.use_user_defined_ids = False\n",
      "\n",
      "    def set_default_prompts(self):\n",
      "        self.system_prompt = DEFAULT_SYSTEM_PROMPT\n",
      "        self.user_prompt = self.default_user_prompt\n",
      "\n",
      "\n",
      "    def default_user_prompt(self, message: str) -> str:\n",
      "        return DEFAULT_USER_PROMPT.format(question=message)\n",
      "\n",
      "    def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\n",
      "        \"\"\"\n",
      "        Compose the prompt for the chat-gpt API.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\n",
      "        \"\"\"\n",
      "        marked_question = mark_question(self.user_prompt(message))\n",
      "        prompt = [mark_system(self.system_prompt)] + [marked_question]\n",
      "        return prompt, marked_question\n",
      "\n",
      "    def update_system_prompt(self, new_prompt: str) -> None:\n",
      "        \"\"\"\n",
      "        Update the system prompt.\n",
      "\n",
      "        :param new_prompt: A string representing the new system prompt.\n",
      "        \"\"\"\n",
      "        self.system_prompt = new_prompt\n",
      "\n",
      "    def update_user_prompt(self, new_prompt ) -> None:\n",
      "        \"\"\"\n",
      "        Update the user prompt.\n",
      "\n",
      "        :param new_prompt: A string representing the new user prompt.\n",
      "        \"\"\"\n",
      "        self.user_prompt = new_prompt\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_question\"\n",
      "\t\"mark_system\"\n",
      "]\n",
      "\n",
      "\n",
      "def __init__(self, system_prompt: Union[str,None] = None, user_prompt: Union[str,None] = None):\n",
      "    \"\"\"\n",
      "        Initialize the Prompter with system and user prompts.\n",
      "\n",
      "        :param system_prompt: A string representing the system prompt.\n",
      "        :param user_prompt: A string representing the user prompt.\n",
      "        \"\"\"\n",
      "    \n",
      "    if system_prompt is None:\n",
      "        self.system_prompt = DEFAULT_SYSTEM_PROMPT\n",
      "        self.user_defined_system_prompt = None\n",
      "    else:\n",
      "        self.system_prompt = system_prompt\n",
      "        self.user_defined_system_prompt = system_prompt\n",
      "    if user_prompt is None:\n",
      "        self.user_prompt = self.default_user_prompt\n",
      "        self.user_defined_user_prompt = None\n",
      "    else:\n",
      "        self.user_prompt = user_prompt\n",
      "        self.user_defined_user_prompt = user_prompt\n",
      "\n",
      "    self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\n",
      "    self.user_defined_ids = []\n",
      "    self.user_defined_values = []\n",
      "    self.use_user_defined_ids = False\n",
      "\n",
      "\n",
      "\n",
      "def set_default_prompts(self):\n",
      "    self.system_prompt = DEFAULT_SYSTEM_PROMPT\n",
      "    self.user_prompt = self.default_user_prompt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def default_user_prompt(self, message: str) -> str:\n",
      "    return DEFAULT_USER_PROMPT.format(question=message)\n",
      "\n",
      "\n",
      "\n",
      "def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\n",
      "    \"\"\"\n",
      "        Compose the prompt for the chat-gpt API.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\n",
      "        \"\"\"\n",
      "    marked_question = mark_question(self.user_prompt(message))\n",
      "    prompt = [mark_system(self.system_prompt)] + [marked_question]\n",
      "    return prompt, marked_question\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_question\"\n",
      "\t\"mark_system\"\n",
      "]\n",
      "\n",
      "\n",
      "def update_system_prompt(self, new_prompt: str) -> None:\n",
      "    \"\"\"\n",
      "        Update the system prompt.\n",
      "\n",
      "        :param new_prompt: A string representing the new system prompt.\n",
      "        \"\"\"\n",
      "    self.system_prompt = new_prompt\n",
      "\n",
      "\n",
      "\n",
      "def update_user_prompt(self, new_prompt ) -> None:\n",
      "    \"\"\"\n",
      "        Update the user prompt.\n",
      "\n",
      "        :param new_prompt: A string representing the new user prompt.\n",
      "        \"\"\"\n",
      "    self.user_prompt = new_prompt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class BaseChat:\n",
      "    \"\"\"\n",
      "    This is the base class for chatbots, defining the basic functions that a chatbot should have, mainly the calls to\n",
      "    chat-gpt API, and a basic Gradio interface. It has a prompt_func that acts as a placeholder for a call to chat-gpt\n",
      "    API without any additional messages. It can be overridden by subclasses to add additional messages to the prompt.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, model: Union[str,None] = None, max_output_tokens: int = 200):\n",
      "        \"\"\"\n",
      "        Initialize the BaseChat with a model and max_output_tokens.\n",
      "\n",
      "        :param model: A string representing the chat model to be used.\n",
      "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
      "        \"\"\"\n",
      "        if model is None:\n",
      "            self.model = \"gpt-3.5-turbo\"\n",
      "        else:\n",
      "            self.model = model\n",
      "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "        self.max_output_tokens = max_output_tokens\n",
      "        self.failed_responses = []\n",
      "        self.outputs = []\n",
      "        self.inputs = []\n",
      "        self.prompts = []\n",
      "        self.prompt_func = self.identity_prompter\n",
      "\n",
      "    def __getstate__(self):\n",
      "        state = self.__dict__.copy()\n",
      "        # Remove the tokenizer attribute from the state\n",
      "        del state[\"tokenizer\"]\n",
      "        return state\n",
      "\n",
      "    def __setstate__(self, state):\n",
      "        self.__dict__.update(state)\n",
      "        # Reinitialize the tokenizer attribute after unpickling\n",
      "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "\n",
      "    def identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\n",
      "        \"\"\"\n",
      "        A simple identity prompter that takes a message and returns the message marked as a question.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A tuple containing the marked question and the original message.\n",
      "        \"\"\"\n",
      "        return [mark_question(message)], mark_question(message)\n",
      "\n",
      "    def chat_response( self, prompt: List[dict], max_tokens: Union[int,None] = None, stream:bool = False ) -> Union[Generator,Tuple[Dict, bool]]:\n",
      "        if max_tokens is None:\n",
      "            max_tokens = self.max_output_tokens\n",
      "        if \"gpt\" in self.model:\n",
      "            logging.info(prompt)\n",
      "            response, status = chatgpt_response(prompt=prompt,model=self.model, max_tokens = 1000, stream=stream)\n",
      "            if status:\n",
      "                return response, True\n",
      "            else:\n",
      "                self.failed_responses.append(response)\n",
      "                return response, False\n",
      "\n",
      "        elif \"command\" in self.model:\n",
      "            response, status = cohere_response(prompt=prompt,model=self.model, max_tokens = 1000)  \n",
      "            if status:\n",
      "                return response, True\n",
      "            else:\n",
      "                self.failed_responses.append(response)\n",
      "                return response, False\n",
      "        else:\n",
      "            return {}, False \n",
      "\n",
      "    def reply(self, message: str, verbose: bool = True, stream: bool = False) -> Union[Generator, str]:\n",
      "        \"\"\"\n",
      "        Reply to a given message using the chatbot.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "        if stream:\n",
      "            return self.query(message, verbose, stream)\n",
      "        else:\n",
      "            return self.query(message, verbose)[\"content\"]\n",
      "\n",
      "    def query(self, message: str, verbose: bool = True, stream: bool = False) -> Union[Generator, str]:\n",
      "        \"\"\"\n",
      "        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "\n",
      "        prompt, _ = self.prompt_func(message)\n",
      "\n",
      "        if stream:\n",
      "            return self.chat_response(prompt=prompt, stream=stream)\n",
      "\n",
      "        response, success = self.chat_response(prompt)\n",
      "        if verbose:\n",
      "            display(Markdown(\"#### Question: \\n {question}\".format(question=message)))\n",
      "        if success:\n",
      "            answer = get_mark_from_response(response, self.model)\n",
      "            self.outputs.append(answer)\n",
      "            self.inputs.append(message)\n",
      "            self.prompts.append(prompt)\n",
      "            if verbose:\n",
      "                display(\n",
      "                    Markdown(\n",
      "                        \" #### Anwser: \\n {answer}\".format(\n",
      "                            answer=get_str_from_response(response, self.model)\n",
      "                        )\n",
      "                    )\n",
      "                )\n",
      "            return answer\n",
      "        else:\n",
      "            raise Exception(\"OpenAI API Error inside query function\")\n",
      "\n",
      "    def reset_logs(self):\n",
      "        \"\"\"\n",
      "        Reset the chatbot's memory.\n",
      "        \"\"\"\n",
      "        self.outputs = []\n",
      "        self.inputs = []\n",
      "        self.prompts = []\n",
      "\n",
      "    def get_logs(self):\n",
      "        \"\"\"\n",
      "        Get the chatbot's memory.\n",
      "\n",
      "        :return: A tuple containing the chatbot's memory as three lists of strings.\n",
      "        \"\"\"\n",
      "        return self.inputs, self.outputs, self.prompts\n",
      "\n",
      "    def run_text(\n",
      "        self, text: str, state: List[Tuple[str, str]]\n",
      "    ) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\n",
      "        \"\"\"\n",
      "        Process the user's text input and update the chat state.\n",
      "\n",
      "        :param text: A string representing the user input.\n",
      "        :param state: A list of tuples representing the current chat state.\n",
      "        :return: A tuple containing the updated chat state as two lists of tuples.\n",
      "        \"\"\"\n",
      "        print(\"===============Running run_text =============\")\n",
      "        print(\"Inputs:\", text)\n",
      "        try:\n",
      "            print(\"======>Current memory:\\n %s\" % self.memory_thread)\n",
      "        except:\n",
      "            print(\"======>No memory\")\n",
      "        print(\"failed here\")\n",
      "        response = self.reply(text)\n",
      "        state = state + [(text, response)]\n",
      "        print(\"Outputs:\", state)\n",
      "        return state, state\n",
      "\n",
      "    def gradio(self):\n",
      "        \"\"\"\n",
      "        Create and launch a Gradio interface for the chatbot.\n",
      "        \"\"\"\n",
      "        with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n",
      "            chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\n",
      "            state = gr.State([])\n",
      "            with gr.Row():\n",
      "                with gr.Column(scale=1):\n",
      "                    txt = gr.Textbox(\n",
      "                        show_label=False,\n",
      "                        placeholder=\"Enter text and press enter, or upload an image\",\n",
      "                    ).style(container=False)\n",
      "                with gr.Column(scale=0.15, min_width=0):\n",
      "                    clear = gr.Button(\"Clear️\")\n",
      "\n",
      "            txt.submit(lambda text, state: self.run_text(text, state), [txt, state], [chatbot, state])\n",
      "            txt.submit(lambda: \"\", None, txt)\n",
      "            demo.launch(server_name=\"localhost\", server_port=7860)\n",
      "\n",
      "Function calls: shape: (17,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_question\"\n",
      "\t\"mark_question\"\n",
      "\t\"chatgpt_respon…\n",
      "\t\"cohere_respons…\n",
      "\t\"display\"\n",
      "\t\"Markdown\"\n",
      "\t\"get_mark_from_…\n",
      "\t\"display\"\n",
      "\t\"Markdown\"\n",
      "\t\"get_str_from_r…\n",
      "\t\"Exception\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def __init__(self, model: Union[str,None] = None, max_output_tokens: int = 200):\n",
      "    \"\"\"\n",
      "        Initialize the BaseChat with a model and max_output_tokens.\n",
      "\n",
      "        :param model: A string representing the chat model to be used.\n",
      "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
      "        \"\"\"\n",
      "    if model is None:\n",
      "        self.model = \"gpt-3.5-turbo\"\n",
      "    else:\n",
      "        self.model = model\n",
      "    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "    self.max_output_tokens = max_output_tokens\n",
      "    self.failed_responses = []\n",
      "    self.outputs = []\n",
      "    self.inputs = []\n",
      "    self.prompts = []\n",
      "    self.prompt_func = self.identity_prompter\n",
      "\n",
      "\n",
      "\n",
      "def __getstate__(self):\n",
      "    state = self.__dict__.copy()\n",
      "    # Remove the tokenizer attribute from the state\n",
      "    del state[\"tokenizer\"]\n",
      "    return state\n",
      "\n",
      "\n",
      "\n",
      "def __setstate__(self, state):\n",
      "    self.__dict__.update(state)\n",
      "    # Reinitialize the tokenizer attribute after unpickling\n",
      "    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "\n",
      "\n",
      "\n",
      "def identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\n",
      "    \"\"\"\n",
      "        A simple identity prompter that takes a message and returns the message marked as a question.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A tuple containing the marked question and the original message.\n",
      "        \"\"\"\n",
      "    return [mark_question(message)], mark_question(message)\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"mark_question\"\n",
      "\t\"mark_question\"\n",
      "]\n",
      "\n",
      "\n",
      "def chat_response( self, prompt: List[dict], max_tokens: Union[int,None] = None, stream:bool = False ) -> Union[Generator,Tuple[Dict, bool]]:\n",
      "    if max_tokens is None:\n",
      "        max_tokens = self.max_output_tokens\n",
      "    if \"gpt\" in self.model:\n",
      "        logging.info(prompt)\n",
      "        response, status = chatgpt_response(prompt=prompt,model=self.model, max_tokens = 1000, stream=stream)\n",
      "        if status:\n",
      "            return response, True\n",
      "        else:\n",
      "            self.failed_responses.append(response)\n",
      "            return response, False\n",
      "\n",
      "    elif \"command\" in self.model:\n",
      "        response, status = cohere_response(prompt=prompt,model=self.model, max_tokens = 1000)  \n",
      "        if status:\n",
      "            return response, True\n",
      "        else:\n",
      "            self.failed_responses.append(response)\n",
      "            return response, False\n",
      "    else:\n",
      "        return {}, False \n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"chatgpt_respon…\n",
      "\t\"cohere_respons…\n",
      "]\n",
      "\n",
      "\n",
      "def reply(self, message: str, verbose: bool = True, stream: bool = False) -> Union[Generator, str]:\n",
      "    \"\"\"\n",
      "        Reply to a given message using the chatbot.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "    if stream:\n",
      "        return self.query(message, verbose, stream)\n",
      "    else:\n",
      "        return self.query(message, verbose)[\"content\"]\n",
      "\n",
      "\n",
      "\n",
      "def query(self, message: str, verbose: bool = True, stream: bool = False) -> Union[Generator, str]:\n",
      "    \"\"\"\n",
      "        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "\n",
      "    prompt, _ = self.prompt_func(message)\n",
      "\n",
      "    if stream:\n",
      "        return self.chat_response(prompt=prompt, stream=stream)\n",
      "\n",
      "    response, success = self.chat_response(prompt)\n",
      "    if verbose:\n",
      "        display(Markdown(\"#### Question: \\n {question}\".format(question=message)))\n",
      "    if success:\n",
      "        answer = get_mark_from_response(response, self.model)\n",
      "        self.outputs.append(answer)\n",
      "        self.inputs.append(message)\n",
      "        self.prompts.append(prompt)\n",
      "        if verbose:\n",
      "            display(\n",
      "                Markdown(\n",
      "                    \" #### Anwser: \\n {answer}\".format(\n",
      "                        answer=get_str_from_response(response, self.model)\n",
      "                    )\n",
      "                )\n",
      "            )\n",
      "        return answer\n",
      "    else:\n",
      "        raise Exception(\"OpenAI API Error inside query function\")\n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"display\"\n",
      "\t\"Markdown\"\n",
      "\t\"get_mark_from_…\n",
      "\t\"display\"\n",
      "\t\"Markdown\"\n",
      "\t\"get_str_from_r…\n",
      "\t\"Exception\"\n",
      "]\n",
      "\n",
      "\n",
      "def reset_logs(self):\n",
      "    \"\"\"\n",
      "        Reset the chatbot's memory.\n",
      "        \"\"\"\n",
      "    self.outputs = []\n",
      "    self.inputs = []\n",
      "    self.prompts = []\n",
      "\n",
      "\n",
      "\n",
      "def get_logs(self):\n",
      "    \"\"\"\n",
      "        Get the chatbot's memory.\n",
      "\n",
      "        :return: A tuple containing the chatbot's memory as three lists of strings.\n",
      "        \"\"\"\n",
      "    return self.inputs, self.outputs, self.prompts\n",
      "\n",
      "\n",
      "\n",
      "def run_text(\n",
      "    self, text: str, state: List[Tuple[str, str]]\n",
      ") -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\n",
      "    \"\"\"\n",
      "        Process the user's text input and update the chat state.\n",
      "\n",
      "        :param text: A string representing the user input.\n",
      "        :param state: A list of tuples representing the current chat state.\n",
      "        :return: A tuple containing the updated chat state as two lists of tuples.\n",
      "        \"\"\"\n",
      "    print(\"===============Running run_text =============\")\n",
      "    print(\"Inputs:\", text)\n",
      "    try:\n",
      "        print(\"======>Current memory:\\n %s\" % self.memory_thread)\n",
      "    except:\n",
      "        print(\"======>No memory\")\n",
      "    print(\"failed here\")\n",
      "    response = self.reply(text)\n",
      "    state = state + [(text, response)]\n",
      "    print(\"Outputs:\", state)\n",
      "    return state, state\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def gradio(self):\n",
      "    \"\"\"\n",
      "        Create and launch a Gradio interface for the chatbot.\n",
      "        \"\"\"\n",
      "    with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n",
      "        chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\n",
      "        state = gr.State([])\n",
      "        with gr.Row():\n",
      "            with gr.Column(scale=1):\n",
      "                txt = gr.Textbox(\n",
      "                    show_label=False,\n",
      "                    placeholder=\"Enter text and press enter, or upload an image\",\n",
      "                ).style(container=False)\n",
      "            with gr.Column(scale=0.15, min_width=0):\n",
      "                clear = gr.Button(\"Clear️\")\n",
      "\n",
      "        txt.submit(lambda text, state: self.run_text(text, state), [txt, state], [chatbot, state])\n",
      "        txt.submit(lambda: \"\", None, txt)\n",
      "        demo.launch(server_name=\"localhost\", server_port=7860)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def list_subjects_and_perspective():\n",
      "    # read from the file\n",
      "    prompts = json.load(open(filename))\n",
      "    subject_and_perspective = list(prompts.keys())\n",
      "    subjects = set()\n",
      "    perspectives = set()\n",
      "    for item in subject_and_perspective:\n",
      "        subject, perspective = item.split('\\\\')\n",
      "        subjects.add(subject)\n",
      "        perspectives.add(perspective)\n",
      "    return subjects, perspectives\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"list\"\n",
      "\t\"set\"\n",
      "\t\"set\"\n",
      "]\n",
      "\n",
      "\n",
      "def list_subjects():\n",
      "    # read from the file\n",
      "    prompts = json.load(open(filename))\n",
      "    subject_and_perspective = list(prompts.keys())\n",
      "    subjects = set()\n",
      "    for item in subject_and_perspective:\n",
      "        subject, perspective = item.split('\\\\')\n",
      "        subjects.add(subject)\n",
      "    return subjects\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"list\"\n",
      "\t\"set\"\n",
      "]\n",
      "\n",
      "\n",
      "def list_perspectives():\n",
      "    # read from the file\n",
      "    prompts = json.load(open(filename))\n",
      "    subject_and_perspective = list(prompts.keys())\n",
      "    perspectives = set()\n",
      "    for item in subject_and_perspective:\n",
      "        subject, perspective = item.split('\\\\')\n",
      "        perspectives.add(perspective)\n",
      "    return perspectives\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"list\"\n",
      "\t\"set\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_perspective_prompt(subject, perspective):\n",
      "    # read from the file\n",
      "    prompts = json.load(open(filename))\n",
      "    key = subject + '\\\\' + perspective\n",
      "    if key in prompts:\n",
      "        return prompts[key]\n",
      "    else:\n",
      "        raise Exception('No prompt found for subject: ' + subject + ' and perspective: ' + perspective +' use list_subjects() and list_perspectives() to see available prompts')\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"Exception\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_random_perspective_prompt():\n",
      "    # read from the file\n",
      "    prompts = json.load(open(filename))\n",
      "    key = random.choice(list(prompts.keys()))\n",
      "    subject = key.split('\\\\')[0]\n",
      "    perspective = key.split('\\\\')[1]\n",
      "    return subject, perspective, prompts[key]\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"list\"\n",
      "]\n",
      "\n",
      "\n",
      "def extract_values_and_embeddings(\n",
      "        data_frame: pd.DataFrame,\n",
      "        columns: Union[str, List[str]],\n",
      "        embeddings_col: Optional[str],\n",
      "    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\n",
      "    \"\"\"\n",
      "        Extract values and embeddings from a pandas DataFrame.\n",
      "\n",
      "        Args:\n",
      "            data_frame: The DataFrame to extract values and embeddings from.\n",
      "            columns: The columns of the DataFrame to use as values.\n",
      "            embeddings_col: The column name containing the embeddings.\n",
      "\n",
      "        Returns:\n",
      "            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n",
      "        \"\"\"\n",
      "\n",
      "    if isinstance(columns, list) and len(columns) > 1:\n",
      "        data_frame[\"values_combined\"] = data_frame[columns].apply(\n",
      "            lambda x: \" \".join(x), axis=1\n",
      "        )\n",
      "        columns = \"values_combined\"\n",
      "    elif isinstance(columns, list) and len(columns) == 1:\n",
      "        columns = columns[0]\n",
      "    elif not isinstance(columns, str):\n",
      "        raise ValueError(\"The columns are not valid\")\n",
      "\n",
      "    values = []\n",
      "    embeddings = []\n",
      "\n",
      "    for _, row in data_frame.iterrows():\n",
      "        value = row[columns]\n",
      "        values.append(value)\n",
      "\n",
      "        if embeddings_col is not None:\n",
      "            embedding = row[embeddings_col]\n",
      "            embeddings.append(embedding)\n",
      "\n",
      "    return values, embeddings if embeddings_col is not None else None\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def convert_mark_to_str_prompt(messages: List[dict], prompt: str = \"\") -> str:\n",
      "    prompt = \"\"\n",
      "    for message in messages:\n",
      "        role = message[\"role\"].upper()\n",
      "        content = message[\"content\"]\n",
      "        prompt += f\" #{role}: {content}\"\n",
      "\n",
      "    return prompt\n",
      "\n",
      "\n",
      "\n",
      "def mark_system(system_prompt):\n",
      "    return {\"role\": \"system\", \"content\": system_prompt}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def mark_answer(answer):\n",
      "    return {\"role\": \"assistant\", \"content\": answer}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def mark_question(question):\n",
      "    return {\"role\": \"user\", \"content\": question}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def check_dict(message_dict):\n",
      "    if (\n",
      "        type(message_dict) is list\n",
      "        and len(message_dict) == 1\n",
      "        and type(message_dict[0]) is dict\n",
      "    ):\n",
      "        message_dict = message_dict[0]\n",
      "    elif type(message_dict) is not dict:\n",
      "        raise Exception(\n",
      "            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\n",
      "            message_dict,\n",
      "            type(message_dict),\n",
      "        )\n",
      "    return message_dict\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"type\"\n",
      "\t\"len\"\n",
      "\t\"type\"\n",
      "\t\"type\"\n",
      "\t\"Exception\"\n",
      "\t\"type\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def get_mark_from_response(response, model = \"gpt\"):\n",
      "    # return the answer from the response\n",
      "    if \"gpt\" in model:\n",
      "        role = response[\"choices\"][0][\"message\"][\"role\"]\n",
      "        message = response[\"choices\"][0][\"message\"][\"content\"]\n",
      "    elif \"command\" in model:\n",
      "        role = \"assistant\"\n",
      "        message = response[0].text \n",
      "    else:\n",
      "        raise Exception(\"Unknown model type\")\n",
      "    return {\"role\": role, \"content\": message}\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Exception\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def get_str_from_response(response, model = \"gpt\"):\n",
      "    # return the answer from the response\n",
      "    if \"gpt\" in model:\n",
      "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
      "    elif \"command\" in model:\n",
      "        text = response[0].text\n",
      "        text_without_assistant = text.replace(\"#ASSISTANT\", \"\")\n",
      "        return text_without_assistant\n",
      "    else:\n",
      "        raise Exception(\"Unknown model type\")\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Exception\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def apply_sigmoid(matrix:np.ndarray):\n",
      "    \"\"\"\n",
      "    This function applies a sigmoid non-linearity to the matrix elements.\n",
      "    The sigmoid function maps any value to a value between 0 and 1.\n",
      "    \"\"\"\n",
      "    return expit(matrix)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"expit\"\n",
      "]\n",
      "\n",
      "\n",
      "def apply_threshold(matrix, threshold=0.5):\n",
      "    \"\"\"\n",
      "    This function applies a threshold to the matrix elements.\n",
      "    All values above the threshold are set to 1, all values below or equal to the threshold are set to 0.\n",
      "    \"\"\"\n",
      "    matrix[matrix > threshold] = 1\n",
      "    matrix[matrix <= threshold] = 0\n",
      "    return matrix\n",
      "\n",
      "\n",
      "\n",
      "def concat_columns(example, index=None):\n",
      "    column1='title'\n",
      "    column2='text'\n",
      "    example['merged_column'] = example[column1] +\" - \" + example[column2]\n",
      "    return example\n",
      "\n",
      "\n",
      "\n",
      "def extract_values_and_embeddings_hf(\n",
      "    dataset: datasets.Dataset,\n",
      "    value_column: Union[str, List[str]],\n",
      "    embeddings_column: Optional[str],\n",
      ") -> Tuple[List[str], Optional[List[np.ndarray]]]:\n",
      "    \"\"\"\n",
      "    Extract values and embeddings from a Hugging Face dataset.\n",
      "\n",
      "    Args:\n",
      "        dataset: The Hugging Face dataset to extract values and embeddings from.\n",
      "        value_column: The column(s) of the dataset to use as values.\n",
      "        embeddings_column: The column name containing the embeddings.\n",
      "\n",
      "    Returns:\n",
      "        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n",
      "    \"\"\"\n",
      "    if isinstance(value_column, str):\n",
      "        value_column = [value_column]\n",
      "    print(\"Merging values: Start\")\n",
      "    merged_docs = dataset.map(concat_columns, with_indices=True)\n",
      "    print(\"Merging values: Done\")\n",
      "    values = merged_docs['merged_column']\n",
      "    embeddings = dataset[embeddings_column]\n",
      "    return values, embeddings if embeddings_column is not None else None\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def extract_values_hf(dataset: datasets.Dataset, value_column: Union[str, List[str]]) -> List[str]:\n",
      "    \"\"\"\n",
      "    Extract values from a Hugging Face dataset.\n",
      "\n",
      "    Args:\n",
      "        dataset: The Hugging Face dataset to extract values from.\n",
      "        value_column: The column(s) of the dataset to use as values.\n",
      "\n",
      "    Returns:\n",
      "        A list with the extracted values.\n",
      "    \"\"\"\n",
      "    if isinstance(value_column, str):\n",
      "        value_column = [value_column]\n",
      "    if len(value_column) == 1:\n",
      "        return dataset[value_column[0]]\n",
      "    elif len(value_column) > 1:\n",
      "        merged_docs = dataset.map(concat_columns)\n",
      "        return merged_docs\n",
      "    else:\n",
      "        raise ValueError(\"No value column specified.\")\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def extract_values_and_embeddings_python(\n",
      "    directory_path: str,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "    resolution: str = \"function\"\n",
      ") -> Tuple[List[str], List[Dict[str, str]]]:\n",
      "    values = []\n",
      "    context = []\n",
      "\n",
      "    parser = PythonParser(\n",
      "        directory_path=directory_path,\n",
      "        minify_code=minify_code,\n",
      "        remove_docstrings=remove_docstrings\n",
      "    )\n",
      "\n",
      "    results_dict = parser.process_directory()\n",
      "\n",
      "    if resolution == \"function\":\n",
      "        source_codes = results_dict['function_source_codes']\n",
      "        nodes = results_dict['function_nodes']\n",
      "    elif resolution == \"class\":\n",
      "        source_codes = results_dict['class_source_codes']\n",
      "        nodes = results_dict['class_nodes']\n",
      "    elif resolution == \"both\":\n",
      "        source_codes = results_dict['full_source']\n",
      "        nodes = results_dict['full_nodes']\n",
      "    else:\n",
      "        raise ValueError(f\"Invalid resolution: {resolution}\")\n",
      "    if resolution in ['function', 'class']:\n",
      "        for source_code, node in zip(source_codes, nodes):\n",
      "            values.append(source_code)\n",
      "            context.append({\"libcst tree\": str(node)})\n",
      "    elif resolution == \"both\":\n",
      "        for source_code, node, filename in zip(source_codes, nodes, results_dict['file_map']):\n",
      "            values.append(source_code)\n",
      "            context.append({\"libcst tree\": str(node), \"filename\": filename})\n",
      "    else:\n",
      "        raise ValueError(f\"Invalid resolution: {resolution}\")\n",
      "    return values, context\n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"PythonParser\"\n",
      "\t\"ValueError\"\n",
      "\t\"zip\"\n",
      "\t\"str\"\n",
      "\t\"zip\"\n",
      "\t\"str\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "async def process_api_requests_from_file(\n",
      "    requests_filepath: str,\n",
      "    save_filepath: str,\n",
      "    request_url: str,\n",
      "    api_key: str,\n",
      "    max_requests_per_minute: float,\n",
      "    max_tokens_per_minute: float,\n",
      "    token_encoding_name: str,\n",
      "    max_attempts: int,\n",
      "    logging_level: int,\n",
      "):\n",
      "    \"\"\"Processes API requests in parallel, throttling to stay under rate limits.\"\"\"\n",
      "    # constants\n",
      "    seconds_to_pause_after_rate_limit_error = 15\n",
      "    seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n",
      "\n",
      "    # initialize logging\n",
      "    logging.basicConfig(level=logging_level)\n",
      "    logging.debug(f\"Logging initialized at level {logging_level}\")\n",
      "\n",
      "    # infer API endpoint and construct request header\n",
      "    api_endpoint = api_endpoint_from_url(request_url)\n",
      "    request_header = {\"Authorization\": f\"Bearer {api_key}\"}\n",
      "\n",
      "    # initialize trackers\n",
      "    queue_of_requests_to_retry = asyncio.Queue()\n",
      "    task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...\n",
      "    status_tracker = StatusTracker()  # single instance to track a collection of variables\n",
      "    next_request = None  # variable to hold the next request to call\n",
      "\n",
      "    # initialize available capacity counts\n",
      "    available_request_capacity = max_requests_per_minute\n",
      "    available_token_capacity = max_tokens_per_minute\n",
      "    last_update_time = time.time()\n",
      "\n",
      "    # initialize flags\n",
      "    file_not_finished = True  # after file is empty, we'll skip reading it\n",
      "    logging.debug(f\"Initialization complete.\")\n",
      "\n",
      "    # initialize file reading\n",
      "    with open(requests_filepath) as file:\n",
      "        # `requests` will provide requests one at a time\n",
      "        requests = file.__iter__()\n",
      "        logging.debug(f\"File opened. Entering main loop\")\n",
      "\n",
      "        while True:\n",
      "            # get next request (if one is not already waiting for capacity)\n",
      "            if next_request is None:\n",
      "                if not queue_of_requests_to_retry.empty():\n",
      "                    next_request = queue_of_requests_to_retry.get_nowait()\n",
      "                    logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")\n",
      "                elif file_not_finished:\n",
      "                    try:\n",
      "                        # get new request\n",
      "                        request_json = json.loads(next(requests))\n",
      "                        next_request = APIRequest(\n",
      "                            task_id=next(task_id_generator),\n",
      "                            request_json=request_json,\n",
      "                            token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),\n",
      "                            attempts_left=max_attempts,\n",
      "                            metadata=request_json.pop(\"metadata\", None)\n",
      "                        )\n",
      "                        status_tracker.num_tasks_started += 1\n",
      "                        status_tracker.num_tasks_in_progress += 1\n",
      "                        logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")\n",
      "                    except StopIteration:\n",
      "                        # if file runs out, set flag to stop reading it\n",
      "                        logging.debug(\"Read file exhausted\")\n",
      "                        file_not_finished = False\n",
      "\n",
      "            # update available capacity\n",
      "            current_time = time.time()\n",
      "            seconds_since_update = current_time - last_update_time\n",
      "            available_request_capacity = min(\n",
      "                available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,\n",
      "                max_requests_per_minute,\n",
      "            )\n",
      "            available_token_capacity = min(\n",
      "                available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,\n",
      "                max_tokens_per_minute,\n",
      "            )\n",
      "            last_update_time = current_time\n",
      "\n",
      "            # if enough capacity available, call API\n",
      "            if next_request:\n",
      "                next_request_tokens = next_request.token_consumption\n",
      "                if (\n",
      "                    available_request_capacity >= 1\n",
      "                    and available_token_capacity >= next_request_tokens\n",
      "                ):\n",
      "                    # update counters\n",
      "                    available_request_capacity -= 1\n",
      "                    available_token_capacity -= next_request_tokens\n",
      "                    next_request.attempts_left -= 1\n",
      "\n",
      "                    # call API\n",
      "                    asyncio.create_task(\n",
      "                        next_request.call_api(\n",
      "                            request_url=request_url,\n",
      "                            request_header=request_header,\n",
      "                            retry_queue=queue_of_requests_to_retry,\n",
      "                            save_filepath=save_filepath,\n",
      "                            status_tracker=status_tracker,\n",
      "                        )\n",
      "                    )\n",
      "                    next_request = None  # reset next_request to empty\n",
      "\n",
      "            # if all tasks are finished, break\n",
      "            if status_tracker.num_tasks_in_progress == 0:\n",
      "                break\n",
      "\n",
      "            # main loop sleeps briefly so concurrent tasks can run\n",
      "            await asyncio.sleep(seconds_to_sleep_each_loop)\n",
      "\n",
      "            # if a rate limit error was hit recently, pause to cool down\n",
      "            seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)\n",
      "            if seconds_since_rate_limit_error < seconds_to_pause_after_rate_limit_error:\n",
      "                remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n",
      "                await asyncio.sleep(remaining_seconds_to_pause)\n",
      "                # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n",
      "                logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")\n",
      "\n",
      "        # after finishing, log final status\n",
      "        logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")\n",
      "        if status_tracker.num_tasks_failed > 0:\n",
      "            logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")\n",
      "        if status_tracker.num_rate_limit_errors > 0:\n",
      "            logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n",
      "\n",
      "Function calls: shape: (10,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"api_endpoint_f…\n",
      "\t\"task_id_genera…\n",
      "\t\"StatusTracker\"\n",
      "\t\"open\"\n",
      "\t\"next\"\n",
      "\t\"APIRequest\"\n",
      "\t\"next\"\n",
      "\t\"num_tokens_con…\n",
      "\t\"min\"\n",
      "\t\"min\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "# dataclasses\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class StatusTracker:\n",
      "    \"\"\"Stores metadata about the script's progress. Only one instance is created.\"\"\"\n",
      "\n",
      "    num_tasks_started: int = 0\n",
      "    num_tasks_in_progress: int = 0  # script ends when this reaches 0\n",
      "    num_tasks_succeeded: int = 0\n",
      "    num_tasks_failed: int = 0\n",
      "    num_rate_limit_errors: int = 0\n",
      "    num_api_errors: int = 0  # excluding rate limit errors, counted above\n",
      "    num_other_errors: int = 0\n",
      "    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class APIRequest:\n",
      "    \"\"\"Stores an API request's inputs, outputs, and other metadata. Contains a method to make an API call.\"\"\"\n",
      "\n",
      "    task_id: int\n",
      "    request_json: dict\n",
      "    token_consumption: int\n",
      "    attempts_left: int\n",
      "    metadata: dict\n",
      "    result: list = field(default_factory=list)\n",
      "\n",
      "    async def call_api(\n",
      "        self,\n",
      "        request_url: str,\n",
      "        request_header: dict,\n",
      "        retry_queue: asyncio.Queue,\n",
      "        save_filepath: str,\n",
      "        status_tracker: StatusTracker,\n",
      "    ):\n",
      "        \"\"\"Calls the OpenAI API and saves results.\"\"\"\n",
      "        logging.info(f\"Starting request #{self.task_id}\")\n",
      "        error = None\n",
      "        try:\n",
      "            async with aiohttp.ClientSession() as session:\n",
      "                async with session.post(\n",
      "                    url=request_url, headers=request_header, json=self.request_json\n",
      "                ) as response:\n",
      "                    response = await response.json()\n",
      "            if \"error\" in response:\n",
      "                logging.warning(\n",
      "                    f\"Request {self.task_id} failed with error {response['error']}\"\n",
      "                )\n",
      "                status_tracker.num_api_errors += 1\n",
      "                error = response\n",
      "                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                    status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \n",
      "                    status_tracker.num_rate_limit_errors += 1\n",
      "                    status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n",
      "\n",
      "        except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "            logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n",
      "            status_tracker.num_other_errors += 1\n",
      "            error = e\n",
      "        if error:\n",
      "            self.result.append(error)\n",
      "            if self.attempts_left:\n",
      "                retry_queue.put_nowait(self)\n",
      "            else:\n",
      "                logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n",
      "                data = (\n",
      "                    [self.request_json, [str(e) for e in self.result], self.metadata]\n",
      "                    if self.metadata\n",
      "                    else [self.request_json, [str(e) for e in self.result]]\n",
      "                )\n",
      "                append_to_jsonl(data, save_filepath)\n",
      "                status_tracker.num_tasks_in_progress -= 1\n",
      "                status_tracker.num_tasks_failed += 1\n",
      "        else:\n",
      "            data = (\n",
      "                [self.request_json, response, self.metadata]\n",
      "                if self.metadata\n",
      "                else [self.request_json, response]\n",
      "            )\n",
      "            append_to_jsonl(data, save_filepath)\n",
      "            status_tracker.num_tasks_in_progress -= 1\n",
      "            status_tracker.num_tasks_succeeded += 1\n",
      "            logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field\"\n",
      "\t\"str\"\n",
      "\t\"str\"\n",
      "\t\"append_to_json…\n",
      "\t\"append_to_json…\n",
      "]\n",
      "\n",
      "\n",
      "async def call_api(\n",
      "    self,\n",
      "    request_url: str,\n",
      "    request_header: dict,\n",
      "    retry_queue: asyncio.Queue,\n",
      "    save_filepath: str,\n",
      "    status_tracker: StatusTracker,\n",
      "):\n",
      "    \"\"\"Calls the OpenAI API and saves results.\"\"\"\n",
      "    logging.info(f\"Starting request #{self.task_id}\")\n",
      "    error = None\n",
      "    try:\n",
      "        async with aiohttp.ClientSession() as session:\n",
      "            async with session.post(\n",
      "                url=request_url, headers=request_header, json=self.request_json\n",
      "            ) as response:\n",
      "                response = await response.json()\n",
      "        if \"error\" in response:\n",
      "            logging.warning(\n",
      "                f\"Request {self.task_id} failed with error {response['error']}\"\n",
      "            )\n",
      "            status_tracker.num_api_errors += 1\n",
      "            error = response\n",
      "            if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \n",
      "                status_tracker.num_rate_limit_errors += 1\n",
      "                status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n",
      "\n",
      "    except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "        logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n",
      "        status_tracker.num_other_errors += 1\n",
      "        error = e\n",
      "    if error:\n",
      "        self.result.append(error)\n",
      "        if self.attempts_left:\n",
      "            retry_queue.put_nowait(self)\n",
      "        else:\n",
      "            logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n",
      "            data = (\n",
      "                [self.request_json, [str(e) for e in self.result], self.metadata]\n",
      "                if self.metadata\n",
      "                else [self.request_json, [str(e) for e in self.result]]\n",
      "            )\n",
      "            append_to_jsonl(data, save_filepath)\n",
      "            status_tracker.num_tasks_in_progress -= 1\n",
      "            status_tracker.num_tasks_failed += 1\n",
      "    else:\n",
      "        data = (\n",
      "            [self.request_json, response, self.metadata]\n",
      "            if self.metadata\n",
      "            else [self.request_json, response]\n",
      "        )\n",
      "        append_to_jsonl(data, save_filepath)\n",
      "        status_tracker.num_tasks_in_progress -= 1\n",
      "        status_tracker.num_tasks_succeeded += 1\n",
      "        logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"str\"\n",
      "\t\"str\"\n",
      "\t\"append_to_json…\n",
      "\t\"append_to_json…\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "# functions\n",
      "\n",
      "\n",
      "def api_endpoint_from_url(request_url):\n",
      "    \"\"\"Extract the API endpoint from the request URL.\"\"\"\n",
      "    match = re.search('^https://[^/]+/v\\\\d+/(.+)$', request_url)\n",
      "    return match[1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def append_to_jsonl(data, filename: str) -> None:\n",
      "    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\n",
      "    json_string = json.dumps(data)\n",
      "    with open(filename, \"a\") as f:\n",
      "        f.write(json_string + \"\\n\")\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def num_tokens_consumed_from_request(\n",
      "    request_json: dict,\n",
      "    api_endpoint: str,\n",
      "    token_encoding_name: str,\n",
      "):\n",
      "    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\n",
      "    encoding = tiktoken.get_encoding(token_encoding_name)\n",
      "    # if completions request, tokens = prompt + n * max_tokens\n",
      "    if api_endpoint.endswith(\"completions\"):\n",
      "        max_tokens = request_json.get(\"max_tokens\", 15)\n",
      "        n = request_json.get(\"n\", 1)\n",
      "        completion_tokens = n * max_tokens\n",
      "\n",
      "        # chat completions\n",
      "        if api_endpoint.startswith(\"chat/\"):\n",
      "            num_tokens = 0\n",
      "            for message in request_json[\"messages\"]:\n",
      "                num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
      "                for key, value in message.items():\n",
      "                    num_tokens += len(encoding.encode(value))\n",
      "                    if key == \"name\":  # if there's a name, the role is omitted\n",
      "                        num_tokens -= 1  # role is always required and always 1 token\n",
      "            num_tokens += 2  # every reply is primed with <im_start>assistant\n",
      "            return num_tokens + completion_tokens\n",
      "        # normal completions\n",
      "        else:\n",
      "            prompt = request_json[\"prompt\"]\n",
      "            if isinstance(prompt, str):  # single prompt\n",
      "                prompt_tokens = len(encoding.encode(prompt))\n",
      "                num_tokens = prompt_tokens + completion_tokens\n",
      "                return num_tokens\n",
      "            elif isinstance(prompt, list):  # multiple prompts\n",
      "                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\n",
      "                num_tokens = prompt_tokens + completion_tokens * len(prompt)\n",
      "                return num_tokens\n",
      "            else:\n",
      "                raise TypeError('Expecting either string or list of strings for \"prompt\" field in completion request')\n",
      "    # if embeddings request, tokens = input tokens\n",
      "    elif api_endpoint == \"embeddings\":\n",
      "        input = request_json[\"input\"]\n",
      "        if isinstance(input, str):  # single input\n",
      "            num_tokens = len(encoding.encode(input))\n",
      "            return num_tokens\n",
      "        elif isinstance(input, list):  # multiple inputs\n",
      "            num_tokens = sum([len(encoding.encode(i)) for i in input])\n",
      "            return num_tokens\n",
      "        else:\n",
      "            raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n",
      "    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\n",
      "    else:\n",
      "        raise NotImplementedError(f'API endpoint \"{api_endpoint}\" not implemented in this script')\n",
      "\n",
      "Function calls: shape: (15,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"sum\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"TypeError\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"sum\"\n",
      "\t\"len\"\n",
      "\t\"TypeError\"\n",
      "\t\"NotImplemented…\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def task_id_generator_function():\n",
      "    \"\"\"Generate integers 0, 1, 2, and so on.\"\"\"\n",
      "    task_id = 0\n",
      "    while True:\n",
      "        yield task_id\n",
      "        task_id += 1\n",
      "\n",
      "\n",
      "\n",
      "def extract_values_and_embeddings_pd(\n",
      "        data_frame: pd.DataFrame,\n",
      "        value_column: str,\n",
      "        embeddings_col: Optional[str],\n",
      "    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\n",
      "    \"\"\"\n",
      "        Extract values and embeddings from a pandas DataFrame.\n",
      "\n",
      "        Args:\n",
      "            data_frame: The DataFrame to extract values and embeddings from.\n",
      "            value_column: The column of the DataFrame to use as values.\n",
      "            embeddings_col: The column name containing the embeddings.\n",
      "\n",
      "        Returns:\n",
      "            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n",
      "        \"\"\"\n",
      "    values = data_frame[value_column].tolist()\n",
      "    embeddings = data_frame[embeddings_col].tolist() if embeddings_col else None\n",
      "\n",
      "    return values, embeddings\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def extract_values_and_embeddings_hf(\n",
      "    dataset: datasets.Dataset,\n",
      "    value_column: str,\n",
      "    embeddings_column: Optional[str],\n",
      ") -> Tuple[List[str], Optional[List[np.ndarray]]]:\n",
      "    \"\"\"\n",
      "    Extract values and embeddings from a Hugging Face dataset.\n",
      "\n",
      "    Args:\n",
      "        dataset: The Hugging Face dataset to extract values and embeddings from.\n",
      "        value_column: The column of the dataset to use as values.\n",
      "        embeddings_column: The column name containing the embeddings.\n",
      "\n",
      "    Returns:\n",
      "        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n",
      "    \"\"\"\n",
      "    values = dataset[value_column]\n",
      "    embeddings = dataset[embeddings_column] if embeddings_column else None\n",
      "\n",
      "    return values, embeddings\n",
      "\n",
      "\n",
      "\n",
      "def extract_values_and_embeddings_polars(\n",
      "        data_frame: pl.DataFrame,\n",
      "        value_column: str,\n",
      "        embeddings_column: Optional[str]\n",
      "    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\n",
      "    \"\"\"\n",
      "    Extract values and embeddings from a Polars DataFrame.\n",
      "\n",
      "    Args:\n",
      "        data_frame: The DataFrame to extract values and embeddings from.\n",
      "        value_column: The column of the DataFrame to use as values.\n",
      "        embeddings_column: The column name containing the embeddings.\n",
      "\n",
      "    Returns:\n",
      "        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n",
      "    \"\"\"\n",
      "    values = data_frame[value_column].to_list()\n",
      "    embeddings = data_frame[embeddings_column].to_list() if embeddings_column else None\n",
      "\n",
      "    return values, embeddings\n",
      "\n",
      "\n",
      "\n",
      "def get_context_from_pandas(\n",
      "          data_frame: pd.DataFrame,\n",
      "          context_columns: List[str]):\n",
      "    \"\"\"Extract context information from a pandas DataFrame.\"\"\"\n",
      "    # This function will convert the row of the DataFrame into a dictionary where the keys are the column names.\n",
      "    def row_to_dict(row: pd.Series) -> Dict[str, Any]:\n",
      "        return {column: row[column] for column in context_columns}\n",
      "\n",
      "    # Apply the function to each row of the DataFrame and convert the result into a list.\n",
      "    context = data_frame.apply(row_to_dict, axis=1).tolist()\n",
      "\n",
      "    return context\n",
      "\n",
      "\n",
      "# This function will convert the row of the DataFrame into a dictionary where the keys are the column names.\n",
      "def row_to_dict(row: pd.Series) -> Dict[str, Any]:\n",
      "    return {column: row[column] for column in context_columns}\n",
      "\n",
      "\n",
      "\n",
      "def get_context_from_hf(\n",
      "          data_frame: datasets.Dataset,\n",
      "          context_columns: List[str]):\n",
      "    \"\"\"return a list dictionaries with the keys the column name and value the context columns values\"\"\"\n",
      "    context_data = {column: data_frame[column] for column in context_columns}\n",
      "    context = []\n",
      "    data_frame_len = len(data_frame)\n",
      "    for row in range(data_frame_len):\n",
      "         context.append({column: context_data[column][row] for column in context_columns})\n",
      "    return context\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"range\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_context_from_polars(\n",
      "          data_frame: pl.DataFrame,\n",
      "          context_columns: List[str]) -> List[Dict[str, Any]]:\n",
      "    \"\"\"Extract context information from a Polars DataFrame.\"\"\"\n",
      "\n",
      "    context = []\n",
      "    \n",
      "    # Convert each row to a dictionary\n",
      "    for i in range(len(data_frame)):\n",
      "        row_dict = data_frame[i]\n",
      "        context.append({column: row_dict[column] for column in context_columns})\n",
      "\n",
      "    return context\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "class EmbeddingAnalysis:\n",
      "    def __init__(self, embedding_matrix: np.ndarray, kernel_matrix: np.ndarray) -> None:\n",
      "        \"\"\"\n",
      "        Initializes an instance of the EmbeddingAnalysis class.\n",
      "\n",
      "        Parameters:\n",
      "        kernel_matrix (np.ndarray): A square kernel matrix.\n",
      "\n",
      "        Raises:\n",
      "        ValueError: If the input kernel matrix is not square or symmetric.\n",
      "        \"\"\"\n",
      "        if kernel_matrix.shape[0] != kernel_matrix.shape[1]:\n",
      "            raise ValueError(\"The input kernel matrix must be square.\")\n",
      "        if not np.allclose(kernel_matrix, kernel_matrix.T, atol=1e-8):\n",
      "            raise ValueError(\"The input kernel matrix must be symmetric.\")\n",
      "        self.embedding_matrix = embedding_matrix\n",
      "        self.kernel_matrix = kernel_matrix\n",
      "        self.eigenvalues = np.linalg.eigvalsh(kernel_matrix)\n",
      "\n",
      "    def mean_center(self, A):\n",
      "        # A.shape = (embedding dim, num of articles)\n",
      "        avg_doc = np.mean(A, axis=1)\n",
      "        # avg_doc.shape = (embedding dim, 1)\n",
      "        # Compute eigenfaces on mean-subtracted training data\n",
      "        X = A - np.tile(avg_doc,(A.shape[1],1)).T\n",
      "        # X.shape = A.shape\n",
      "        return X, avg_doc\n",
      "\n",
      "    def eigen_topic(self, A, r1 = 5, r2=55):\n",
      "        X, avg_doc = self.mean_center(A)\n",
      "        # SVD on Mean-centered articles\n",
      "        U, _, _ = np.linalg.svd(X)\n",
      "        # U.shape = (embedding dim, embedding dim)\n",
      "        # np.diag(S.shape) = (embedding dim, embedding dim)\n",
      "        # VT.shape = (num of articles, num of articles)\n",
      "        econ_UT = U[:, r1:r2].T\n",
      "        # econ_UT.shape = (r2-r1, emmbedding dim)\n",
      "        transformed_matrix = econ_UT @ X\n",
      "        # transformed_matrix.shape = (r2-r1, num of articles)\n",
      "        return transformed_matrix, avg_doc, econ_UT\n",
      "\n",
      "    def check_symmetry(self) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if the kernel matrix is symmetric.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if the kernel matrix is symmetric, False otherwise.\n",
      "        \"\"\"\n",
      "        return np.allclose(self.kernel_matrix, self.kernel_matrix.T, atol=1e-8)\n",
      "\n",
      "    def is_positive_definite(self) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if the kernel matrix is positive definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are positive, False otherwise.\n",
      "        \"\"\"\n",
      "        return np.all(self.eigenvalues > 0)\n",
      "\n",
      "    def is_positive_semi_definite(self) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if the kernel matrix is positive semi-definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are non-negative, False otherwise.\n",
      "        \"\"\"\n",
      "        return np.all(self.eigenvalues >= 0)\n",
      "\n",
      "    def is_negative_definite(self) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if the kernel matrix is negative definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are negative, False otherwise.\n",
      "        \"\"\"\n",
      "        return np.all(self.eigenvalues < 0)\n",
      "\n",
      "    def is_negative_semi_definite(self) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if the kernel matrix is negative semi-definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are non-positive, False otherwise.\n",
      "        \"\"\"\n",
      "        return np.all(self.eigenvalues <= 0)\n",
      "\n",
      "    def is_indefinite(self) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if the kernel matrix is indefinite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if the matrix has both positive and negative eigenvalues, False otherwise.\n",
      "        \"\"\"\n",
      "        has_negative = np.any(self.eigenvalues < 0)\n",
      "        has_non_negative = np.any(self.eigenvalues >= 0)\n",
      "        return has_negative and has_non_negative\n",
      "\n",
      "    def check_definiteness(self, num_random_vectors: int = 1000) -> Dict[str, bool]:\n",
      "        \"\"\"\n",
      "        Checks the definiteness of the kernel matrix using random vectors.\n",
      "\n",
      "        num_random_vectors: Number of random vectors to use for checking.\n",
      "\n",
      "        Returns:\n",
      "        Dict[str, bool]: A dictionary with the results of the analysis.\n",
      "        \"\"\"\n",
      "        n = self.kernel_matrix.shape[0]\n",
      "        is_positive_definite = True\n",
      "        is_negative_definite = True\n",
      "        for _ in range(num_random_vectors):\n",
      "            x = np.random.randn(n)\n",
      "            xTMx = x.T @ self.kernel_matrix @ x\n",
      "            if xTMx <= 0:\n",
      "                is_positive_definite = False\n",
      "            if xTMx >= 0:\n",
      "                is_negative_definite = False\n",
      "        return {\n",
      "            \"is_positive_definite\": is_positive_definite,\n",
      "            \"is_negative_definite\": is_negative_definite,\n",
      "        }\n",
      "\n",
      "    def run_analysis(self) -> Dict[str, bool]:\n",
      "        return {\n",
      "            \"is_symmetric\": self.check_symmetry(),\n",
      "            \"is_positive_semi_definite\": self.is_positive_semi_definite(),\n",
      "            \"is_negative_semi_definite\": self.is_negative_semi_definite(),\n",
      "            \"is_indefinite\": self.is_indefinite(),\n",
      "            **self.check_definiteness()\n",
      "        }\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"ValueError\"\n",
      "\t\"ValueError\"\n",
      "\t\"range\"\n",
      "]\n",
      "\n",
      "def __init__(self, embedding_matrix: np.ndarray, kernel_matrix: np.ndarray) -> None:\n",
      "    \"\"\"\n",
      "        Initializes an instance of the EmbeddingAnalysis class.\n",
      "\n",
      "        Parameters:\n",
      "        kernel_matrix (np.ndarray): A square kernel matrix.\n",
      "\n",
      "        Raises:\n",
      "        ValueError: If the input kernel matrix is not square or symmetric.\n",
      "        \"\"\"\n",
      "    if kernel_matrix.shape[0] != kernel_matrix.shape[1]:\n",
      "        raise ValueError(\"The input kernel matrix must be square.\")\n",
      "    if not np.allclose(kernel_matrix, kernel_matrix.T, atol=1e-8):\n",
      "        raise ValueError(\"The input kernel matrix must be symmetric.\")\n",
      "    self.embedding_matrix = embedding_matrix\n",
      "    self.kernel_matrix = kernel_matrix\n",
      "    self.eigenvalues = np.linalg.eigvalsh(kernel_matrix)\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"ValueError\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def mean_center(self, A):\n",
      "    # A.shape = (embedding dim, num of articles)\n",
      "    avg_doc = np.mean(A, axis=1)\n",
      "    # avg_doc.shape = (embedding dim, 1)\n",
      "    # Compute eigenfaces on mean-subtracted training data\n",
      "    X = A - np.tile(avg_doc,(A.shape[1],1)).T\n",
      "    # X.shape = A.shape\n",
      "    return X, avg_doc\n",
      "\n",
      "\n",
      "\n",
      "def eigen_topic(self, A, r1 = 5, r2=55):\n",
      "    X, avg_doc = self.mean_center(A)\n",
      "    # SVD on Mean-centered articles\n",
      "    U, _, _ = np.linalg.svd(X)\n",
      "    # U.shape = (embedding dim, embedding dim)\n",
      "    # np.diag(S.shape) = (embedding dim, embedding dim)\n",
      "    # VT.shape = (num of articles, num of articles)\n",
      "    econ_UT = U[:, r1:r2].T\n",
      "    # econ_UT.shape = (r2-r1, emmbedding dim)\n",
      "    transformed_matrix = econ_UT @ X\n",
      "    # transformed_matrix.shape = (r2-r1, num of articles)\n",
      "    return transformed_matrix, avg_doc, econ_UT\n",
      "\n",
      "\n",
      "\n",
      "def check_symmetry(self) -> bool:\n",
      "    \"\"\"\n",
      "        Checks if the kernel matrix is symmetric.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if the kernel matrix is symmetric, False otherwise.\n",
      "        \"\"\"\n",
      "    return np.allclose(self.kernel_matrix, self.kernel_matrix.T, atol=1e-8)\n",
      "\n",
      "\n",
      "\n",
      "def is_positive_definite(self) -> bool:\n",
      "    \"\"\"\n",
      "        Checks if the kernel matrix is positive definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are positive, False otherwise.\n",
      "        \"\"\"\n",
      "    return np.all(self.eigenvalues > 0)\n",
      "\n",
      "\n",
      "\n",
      "def is_positive_semi_definite(self) -> bool:\n",
      "    \"\"\"\n",
      "        Checks if the kernel matrix is positive semi-definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are non-negative, False otherwise.\n",
      "        \"\"\"\n",
      "    return np.all(self.eigenvalues >= 0)\n",
      "\n",
      "\n",
      "\n",
      "def is_negative_definite(self) -> bool:\n",
      "    \"\"\"\n",
      "        Checks if the kernel matrix is negative definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are negative, False otherwise.\n",
      "        \"\"\"\n",
      "    return np.all(self.eigenvalues < 0)\n",
      "\n",
      "\n",
      "\n",
      "def is_negative_semi_definite(self) -> bool:\n",
      "    \"\"\"\n",
      "        Checks if the kernel matrix is negative semi-definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are non-positive, False otherwise.\n",
      "        \"\"\"\n",
      "    return np.all(self.eigenvalues <= 0)\n",
      "\n",
      "\n",
      "\n",
      "def is_indefinite(self) -> bool:\n",
      "    \"\"\"\n",
      "        Checks if the kernel matrix is indefinite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if the matrix has both positive and negative eigenvalues, False otherwise.\n",
      "        \"\"\"\n",
      "    has_negative = np.any(self.eigenvalues < 0)\n",
      "    has_non_negative = np.any(self.eigenvalues >= 0)\n",
      "    return has_negative and has_non_negative\n",
      "\n",
      "\n",
      "\n",
      "def check_definiteness(self, num_random_vectors: int = 1000) -> Dict[str, bool]:\n",
      "    \"\"\"\n",
      "        Checks the definiteness of the kernel matrix using random vectors.\n",
      "\n",
      "        num_random_vectors: Number of random vectors to use for checking.\n",
      "\n",
      "        Returns:\n",
      "        Dict[str, bool]: A dictionary with the results of the analysis.\n",
      "        \"\"\"\n",
      "    n = self.kernel_matrix.shape[0]\n",
      "    is_positive_definite = True\n",
      "    is_negative_definite = True\n",
      "    for _ in range(num_random_vectors):\n",
      "        x = np.random.randn(n)\n",
      "        xTMx = x.T @ self.kernel_matrix @ x\n",
      "        if xTMx <= 0:\n",
      "            is_positive_definite = False\n",
      "        if xTMx >= 0:\n",
      "            is_negative_definite = False\n",
      "    return {\n",
      "        \"is_positive_definite\": is_positive_definite,\n",
      "        \"is_negative_definite\": is_negative_definite,\n",
      "    }\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"range\"\n",
      "]\n",
      "\n",
      "\n",
      "def run_analysis(self) -> Dict[str, bool]:\n",
      "    return {\n",
      "        \"is_symmetric\": self.check_symmetry(),\n",
      "        \"is_positive_semi_definite\": self.is_positive_semi_definite(),\n",
      "        \"is_negative_semi_definite\": self.is_negative_semi_definite(),\n",
      "        \"is_indefinite\": self.is_indefinite(),\n",
      "        **self.check_definiteness()\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class RateLimiter:\n",
      "    def __init__(self, calls_per_minute: int, verbose: bool = False):\n",
      "        self.calls_per_minute = calls_per_minute\n",
      "        self.interval = 60 / calls_per_minute\n",
      "        self.lock = Lock()\n",
      "        self.last_call_time = None\n",
      "        self.verbose = verbose\n",
      "\n",
      "    def __call__(self, func):\n",
      "        @functools.wraps(func)\n",
      "        def wrapper(*args, **kwargs):\n",
      "            with self.lock:\n",
      "                if self.last_call_time is not None:\n",
      "                    time_since_last_call = time.time() - self.last_call_time\n",
      "                    if time_since_last_call < self.interval:\n",
      "                        time_to_wait = self.interval - time_since_last_call\n",
      "                        if self.verbose:\n",
      "                            print(\n",
      "                                f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\n",
      "                            )\n",
      "                        time.sleep(time_to_wait)\n",
      "                    elif self.verbose:\n",
      "                        print(\n",
      "                            f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\n",
      "                        )\n",
      "                else:\n",
      "                    if self.verbose:\n",
      "                        print(\"RateLimiter: This is the first call, no wait required.\")\n",
      "                self.last_call_time = time.time()\n",
      "            return func(*args, **kwargs)\n",
      "\n",
      "        return wrapper\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Lock\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"func\"\n",
      "]\n",
      "\n",
      "def __init__(self, calls_per_minute: int, verbose: bool = False):\n",
      "    self.calls_per_minute = calls_per_minute\n",
      "    self.interval = 60 / calls_per_minute\n",
      "    self.lock = Lock()\n",
      "    self.last_call_time = None\n",
      "    self.verbose = verbose\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Lock\"\n",
      "]\n",
      "\n",
      "\n",
      "def __call__(self, func):\n",
      "    @functools.wraps(func)\n",
      "    def wrapper(*args, **kwargs):\n",
      "        with self.lock:\n",
      "            if self.last_call_time is not None:\n",
      "                time_since_last_call = time.time() - self.last_call_time\n",
      "                if time_since_last_call < self.interval:\n",
      "                    time_to_wait = self.interval - time_since_last_call\n",
      "                    if self.verbose:\n",
      "                        print(\n",
      "                            f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\n",
      "                        )\n",
      "                    time.sleep(time_to_wait)\n",
      "                elif self.verbose:\n",
      "                    print(\n",
      "                        f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\n",
      "                    )\n",
      "            else:\n",
      "                if self.verbose:\n",
      "                    print(\"RateLimiter: This is the first call, no wait required.\")\n",
      "            self.last_call_time = time.time()\n",
      "        return func(*args, **kwargs)\n",
      "\n",
      "    return wrapper\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"func\"\n",
      "]\n",
      "\n",
      "@functools.wraps(func)\n",
      "def wrapper(*args, **kwargs):\n",
      "    with self.lock:\n",
      "        if self.last_call_time is not None:\n",
      "            time_since_last_call = time.time() - self.last_call_time\n",
      "            if time_since_last_call < self.interval:\n",
      "                time_to_wait = self.interval - time_since_last_call\n",
      "                if self.verbose:\n",
      "                    print(\n",
      "                        f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\n",
      "                    )\n",
      "                time.sleep(time_to_wait)\n",
      "            elif self.verbose:\n",
      "                print(\n",
      "                    f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\n",
      "                )\n",
      "        else:\n",
      "            if self.verbose:\n",
      "                print(\"RateLimiter: This is the first call, no wait required.\")\n",
      "        self.last_call_time = time.time()\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"func\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class RateLimitedThreadPoolExecutor(ThreadPoolExecutor):\n",
      "    def __init__(self, max_workers=None, *args, **kwargs):\n",
      "        super().__init__(max_workers)\n",
      "        self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\n",
      "\n",
      "    def submit(self, fn, *args, **kwargs):\n",
      "        rate_limited_fn = self.rate_limiter(fn)\n",
      "        return super().submit(rate_limited_fn, *args, **kwargs)\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"RateLimiter\"\n",
      "\t\"super\"\n",
      "]\n",
      "\n",
      "def __init__(self, max_workers=None, *args, **kwargs):\n",
      "    super().__init__(max_workers)\n",
      "    self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"RateLimiter\"\n",
      "]\n",
      "\n",
      "\n",
      "def submit(self, fn, *args, **kwargs):\n",
      "    rate_limited_fn = self.rate_limiter(fn)\n",
      "    return super().submit(rate_limited_fn, *args, **kwargs)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "]\n",
      "\n",
      "@dataclass\n",
      "class StatusTracker:\n",
      "    \"\"\"Stores metadata about the script's progress. Only one instance is created.\"\"\"\n",
      "\n",
      "    num_tasks_started: int = 0\n",
      "    num_tasks_in_progress: int = 0  # script ends when this reaches 0\n",
      "    num_tasks_succeeded: int = 0\n",
      "    num_tasks_failed: int = 0\n",
      "    num_rate_limit_errors: int = 0\n",
      "    num_api_errors: int = 0  # excluding rate limit errors, counted above\n",
      "    num_other_errors: int = 0\n",
      "    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class APIRequest:\n",
      "    \"\"\"Stores an API request's inputs, outputs, and other metadata. Contains a method to make an API call.\"\"\"\n",
      "\n",
      "    task_id: int\n",
      "    request_json: dict\n",
      "    token_consumption: int\n",
      "    attempts_left: int\n",
      "    metadata: dict\n",
      "    result: list = field(default_factory=list)\n",
      "\n",
      "    async def call_api(\n",
      "        self,\n",
      "        request_url: str,\n",
      "        request_header: dict,\n",
      "        retry_queue: asyncio.Queue,\n",
      "        save_filepath: str,\n",
      "        status_tracker: StatusTracker,\n",
      "    ):\n",
      "        \"\"\"Calls the OpenAI API and saves results.\"\"\"\n",
      "        logging.info(f\"Starting request #{self.task_id}\")\n",
      "        error = None\n",
      "        try:\n",
      "            async with aiohttp.ClientSession() as session:\n",
      "                async with session.post(\n",
      "                    url=request_url, headers=request_header, json=self.request_json\n",
      "                ) as response:\n",
      "                    response = await response.json()\n",
      "            if \"error\" in response:\n",
      "                logging.warning(\n",
      "                    f\"Request {self.task_id} failed with error {response['error']}\"\n",
      "                )\n",
      "                status_tracker.num_api_errors += 1\n",
      "                error = response\n",
      "                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                    status_tracker.time_of_last_rate_limit_error = time.time()\n",
      "                    status_tracker.num_rate_limit_errors += 1\n",
      "                    status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n",
      "\n",
      "        except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "            logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n",
      "            status_tracker.num_other_errors += 1\n",
      "            error = e\n",
      "        if error:\n",
      "            self.result.append(error)\n",
      "            if self.attempts_left:\n",
      "                retry_queue.put_nowait(self)\n",
      "            else:\n",
      "                logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n",
      "                data = (\n",
      "                    [self.request_json, [str(e) for e in self.result], self.metadata]\n",
      "                    if self.metadata\n",
      "                    else [self.request_json, [str(e) for e in self.result]]\n",
      "                )\n",
      "                append_to_jsonl(data, save_filepath)\n",
      "                status_tracker.num_tasks_in_progress -= 1\n",
      "                status_tracker.num_tasks_failed += 1\n",
      "        else:\n",
      "            data = (\n",
      "                [self.request_json, response, self.metadata]\n",
      "                if self.metadata\n",
      "                else [self.request_json, response]\n",
      "            )\n",
      "            append_to_jsonl(data, save_filepath)\n",
      "            status_tracker.num_tasks_in_progress -= 1\n",
      "            status_tracker.num_tasks_succeeded += 1\n",
      "            logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"field\"\n",
      "\t\"str\"\n",
      "\t\"str\"\n",
      "\t\"append_to_json…\n",
      "\t\"append_to_json…\n",
      "]\n",
      "\n",
      "\n",
      "async def call_api(\n",
      "    self,\n",
      "    request_url: str,\n",
      "    request_header: dict,\n",
      "    retry_queue: asyncio.Queue,\n",
      "    save_filepath: str,\n",
      "    status_tracker: StatusTracker,\n",
      "):\n",
      "    \"\"\"Calls the OpenAI API and saves results.\"\"\"\n",
      "    logging.info(f\"Starting request #{self.task_id}\")\n",
      "    error = None\n",
      "    try:\n",
      "        async with aiohttp.ClientSession() as session:\n",
      "            async with session.post(\n",
      "                url=request_url, headers=request_header, json=self.request_json\n",
      "            ) as response:\n",
      "                response = await response.json()\n",
      "        if \"error\" in response:\n",
      "            logging.warning(\n",
      "                f\"Request {self.task_id} failed with error {response['error']}\"\n",
      "            )\n",
      "            status_tracker.num_api_errors += 1\n",
      "            error = response\n",
      "            if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                status_tracker.time_of_last_rate_limit_error = time.time()\n",
      "                status_tracker.num_rate_limit_errors += 1\n",
      "                status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n",
      "\n",
      "    except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "        logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n",
      "        status_tracker.num_other_errors += 1\n",
      "        error = e\n",
      "    if error:\n",
      "        self.result.append(error)\n",
      "        if self.attempts_left:\n",
      "            retry_queue.put_nowait(self)\n",
      "        else:\n",
      "            logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n",
      "            data = (\n",
      "                [self.request_json, [str(e) for e in self.result], self.metadata]\n",
      "                if self.metadata\n",
      "                else [self.request_json, [str(e) for e in self.result]]\n",
      "            )\n",
      "            append_to_jsonl(data, save_filepath)\n",
      "            status_tracker.num_tasks_in_progress -= 1\n",
      "            status_tracker.num_tasks_failed += 1\n",
      "    else:\n",
      "        data = (\n",
      "            [self.request_json, response, self.metadata]\n",
      "            if self.metadata\n",
      "            else [self.request_json, response]\n",
      "        )\n",
      "        append_to_jsonl(data, save_filepath)\n",
      "        status_tracker.num_tasks_in_progress -= 1\n",
      "        status_tracker.num_tasks_succeeded += 1\n",
      "        logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"str\"\n",
      "\t\"str\"\n",
      "\t\"append_to_json…\n",
      "\t\"append_to_json…\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "# functions\n",
      "\n",
      "\n",
      "def api_endpoint_from_url(request_url):\n",
      "    \"\"\"Extract the API endpoint from the request URL.\"\"\"\n",
      "    match = re.search('^https://[^/]+/v\\\\d+/(.+)$', request_url)\n",
      "    return match[1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def append_to_jsonl(data, filename: str) -> None:\n",
      "    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\n",
      "    json_string = json.dumps(data)\n",
      "    with open(filename, \"a\") as f:\n",
      "        f.write(json_string + \"\\n\")\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def num_tokens_consumed_from_request(\n",
      "    request_json: dict,\n",
      "    api_endpoint: str,\n",
      "    token_encoding_name: str,\n",
      "):\n",
      "    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\n",
      "    encoding = tiktoken.get_encoding(token_encoding_name)\n",
      "    # if completions request, tokens = prompt + n * max_tokens\n",
      "    if api_endpoint.endswith(\"completions\"):\n",
      "        max_tokens = request_json.get(\"max_tokens\", 15)\n",
      "        n = request_json.get(\"n\", 1)\n",
      "        completion_tokens = n * max_tokens\n",
      "\n",
      "        # chat completions\n",
      "        if api_endpoint.startswith(\"chat/\"):\n",
      "            num_tokens = 0\n",
      "            for message in request_json[\"messages\"]:\n",
      "                num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
      "                for key, value in message.items():\n",
      "                    num_tokens += len(encoding.encode(value))\n",
      "                    if key == \"name\":  # if there's a name, the role is omitted\n",
      "                        num_tokens -= 1  # role is always required and always 1 token\n",
      "            num_tokens += 2  # every reply is primed with <im_start>assistant\n",
      "            return num_tokens + completion_tokens\n",
      "        # normal completions\n",
      "        else:\n",
      "            prompt = request_json[\"prompt\"]\n",
      "            if isinstance(prompt, str):  # single prompt\n",
      "                prompt_tokens = len(encoding.encode(prompt))\n",
      "                num_tokens = prompt_tokens + completion_tokens\n",
      "                return num_tokens\n",
      "            elif isinstance(prompt, list):  # multiple prompts\n",
      "                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\n",
      "                num_tokens = prompt_tokens + completion_tokens * len(prompt)\n",
      "                return num_tokens\n",
      "            else:\n",
      "                raise TypeError('Expecting either string or list of strings for \"prompt\" field in completion request')\n",
      "    # if embeddings request, tokens = input tokens\n",
      "    elif api_endpoint == \"embeddings\":\n",
      "        input = request_json[\"input\"]\n",
      "        if isinstance(input, str):  # single input\n",
      "            num_tokens = len(encoding.encode(input))\n",
      "            return num_tokens\n",
      "        elif isinstance(input, list):  # multiple inputs\n",
      "            num_tokens = sum([len(encoding.encode(i)) for i in input])\n",
      "            return num_tokens\n",
      "        else:\n",
      "            raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n",
      "    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\n",
      "    else:\n",
      "        raise NotImplementedError(f'API endpoint \"{api_endpoint}\" not implemented in this script')\n",
      "\n",
      "Function calls: shape: (15,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"sum\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"TypeError\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"sum\"\n",
      "\t\"len\"\n",
      "\t\"TypeError\"\n",
      "\t\"NotImplemented…\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def task_id_generator_function():\n",
      "    \"\"\"Generate integers 0, 1, 2, and so on.\"\"\"\n",
      "    task_id = 0\n",
      "    while True:\n",
      "        yield task_id\n",
      "        task_id += 1\n",
      "\n",
      "\n",
      "\n",
      "async def process_chat_requests(\n",
      "    request_data: List[Dict],\n",
      "    save_filepath: str,\n",
      "    request_url: str,\n",
      "    api_key: str,\n",
      "    max_requests_per_minute: float,\n",
      "    max_tokens_per_minute: float,\n",
      "    token_encoding_name: str,\n",
      "    max_attempts: int,\n",
      "    logging_level: int,\n",
      "):\n",
      "    \"\"\"Processes chat requests in parallel, throttling to stay under rate limits.\"\"\"\n",
      "    # constants\n",
      "    seconds_to_pause_after_rate_limit_error = 15\n",
      "    seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n",
      "\n",
      "    # initialize logging\n",
      "    logging.basicConfig(level=logging_level)\n",
      "    logging.debug(f\"Logging initialized at level {logging_level}\")\n",
      "\n",
      "    # infer API endpoint and construct request header\n",
      "    api_endpoint = api_endpoint_from_url(request_url)\n",
      "    request_header = {\"Authorization\": f\"Bearer {api_key}\"}\n",
      "\n",
      "    # initialize trackers\n",
      "    queue_of_requests_to_retry = asyncio.Queue()\n",
      "    task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...\n",
      "    status_tracker = StatusTracker()  # single instance to track a collection of variables\n",
      "    next_request = None  # variable to hold the next request to call\n",
      "\n",
      "    # initialize available capacity counts\n",
      "    available_request_capacity = max_requests_per_minute\n",
      "    available_token_capacity = max_tokens_per_minute\n",
      "    last_update_time = time.time()\n",
      "\n",
      "    # initialize flags\n",
      "    file_not_finished = True  # after file is empty, we'll skip reading it\n",
      "    logging.debug(f\"Initialization complete.\")\n",
      "\n",
      "    # `requests` will provide requests one at a time\n",
      "    requests = iter(request_data)\n",
      "    logging.debug(f\"List initialized. Entering main loop\")\n",
      "\n",
      "    while True:\n",
      "        # get next request (if one is not already waiting for capacity)\n",
      "        if next_request is None:\n",
      "            if not queue_of_requests_to_retry.empty():\n",
      "                next_request = queue_of_requests_to_retry.get_nowait()\n",
      "                logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")\n",
      "            elif file_not_finished:\n",
      "                try:\n",
      "                    # get new request\n",
      "                    request_json = next(requests)\n",
      "                    next_request = APIRequest(\n",
      "                        task_id=next(task_id_generator),\n",
      "                        request_json=request_json,\n",
      "                        token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),\n",
      "                        attempts_left=max_attempts,\n",
      "                        metadata=request_json.pop(\"metadata\", None)\n",
      "                    )\n",
      "                    status_tracker.num_tasks_started += 1\n",
      "                    status_tracker.num_tasks_in_progress += 1\n",
      "                    logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")\n",
      "                except StopIteration:\n",
      "                    # if file runs out, set flag to stop reading it\n",
      "                    logging.debug(\"Read file exhausted\")\n",
      "                    file_not_finished = False\n",
      "\n",
      "        # update available capacity\n",
      "        current_time = time.time()\n",
      "        seconds_since_update = current_time - last_update_time\n",
      "        available_request_capacity = min(\n",
      "            available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,\n",
      "            max_requests_per_minute,\n",
      "        )\n",
      "        available_token_capacity = min(\n",
      "            available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,\n",
      "            max_tokens_per_minute,\n",
      "        )\n",
      "        last_update_time = current_time\n",
      "\n",
      "        # if enough capacity available, call API\n",
      "        if next_request:\n",
      "            next_request_tokens = next_request.token_consumption\n",
      "            if (\n",
      "                available_request_capacity >= 1\n",
      "                and available_token_capacity >= next_request_tokens\n",
      "            ):\n",
      "                # update counters\n",
      "                available_request_capacity -= 1\n",
      "                available_token_capacity -= next_request_tokens\n",
      "                next_request.attempts_left -= 1\n",
      "\n",
      "                # call API\n",
      "                asyncio.create_task(\n",
      "                    next_request.call_api(\n",
      "                        request_url=request_url,\n",
      "                        request_header=request_header,\n",
      "                        retry_queue=queue_of_requests_to_retry,\n",
      "                        save_filepath=save_filepath,\n",
      "                        status_tracker=status_tracker,\n",
      "                    )\n",
      "                )\n",
      "                next_request = None  # reset next_request to empty\n",
      "\n",
      "        # if all tasks are finished, break\n",
      "        if status_tracker.num_tasks_in_progress == 0:\n",
      "            break\n",
      "\n",
      "        # main loop sleeps briefly so concurrent tasks can run\n",
      "        await asyncio.sleep(seconds_to_sleep_each_loop)\n",
      "\n",
      "        # if a rate limit error was hit recently, pause to cool down\n",
      "        seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)\n",
      "        if seconds_since_rate_limit_error < seconds_to_pause_after_rate_limit_error:\n",
      "            remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n",
      "            await asyncio.sleep(remaining_seconds_to_pause)\n",
      "            # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n",
      "            logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")\n",
      "\n",
      "    # after finishing, log final status\n",
      "    logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")\n",
      "    if status_tracker.num_tasks_failed > 0:\n",
      "        logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")\n",
      "    if status_tracker.num_rate_limit_errors > 0:\n",
      "        logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n",
      "\n",
      "Function calls: shape: (10,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"api_endpoint_f…\n",
      "\t\"task_id_genera…\n",
      "\t\"StatusTracker\"\n",
      "\t\"iter\"\n",
      "\t\"next\"\n",
      "\t\"APIRequest\"\n",
      "\t\"next\"\n",
      "\t\"num_tokens_con…\n",
      "\t\"min\"\n",
      "\t\"min\"\n",
      "]\n",
      "\n",
      "\n",
      "class OpenAiEmbedder:\n",
      "\n",
      "    def get_embedding_size(self):\n",
      "        return ADA_EMBEDDING_SIZE\n",
      "\n",
      "    def embed(self, data, verbose=False):\n",
      "        if isinstance(data, list) and len(data) > 1:\n",
      "            logger.info(\"Batch embedding\")\n",
      "            return self.batch_embed(data)\n",
      "        elif isinstance(data, list) and len(data) == 1:\n",
      "            logger.info(\"Serial embedding\")\n",
      "            data = data[0]\n",
      "\n",
      "        if isinstance(data, dict) and \"content\" in data:\n",
      "            if verbose:\n",
      "                logger.info(\"Embedding without mark\", data[\"content\"])\n",
      "            out = openai.Embedding.create(\n",
      "                input=data[\"content\"], engine=\"text-embedding-ada-002\"\n",
      "            )\n",
      "        else:\n",
      "            if len(TOKENIZER.encode(data)) > MAX_CONTEXT_LENGTH:\n",
      "                raise ValueError(f\" The input is too long for OpenAI, num tokens is {len(TOKENIZER.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "            if verbose:\n",
      "                logger.info(f\"Embedding without preprocessing the input {data}\")\n",
      "            out = openai.Embedding.create(\n",
      "                input=str(data), engine=\"text-embedding-ada-002\"\n",
      "            )\n",
      "        return out.data[0].embedding\n",
      "\n",
      "    def batch_embed(self, data: List[str], batch_size: int = 1000):\n",
      "        if isinstance(data, dict) and \"content\" in data:\n",
      "            raise ValueError(\"Batch embedding not supported for dictionaries\")\n",
      "        elif isinstance(data, str):\n",
      "            raise ValueError(\"Batch embedding not supported for strings use embed() instead\")\n",
      "        elif isinstance(data, list):\n",
      "            batch = []\n",
      "            embeddings = []\n",
      "            i = 1\n",
      "            total_number_of_batches = len(data)//batch_size + 1 if len(data) % batch_size > 0 else len(data)//batch_size\n",
      "            for value in data:\n",
      "                batch.append(value)\n",
      "                if len(batch) == batch_size:\n",
      "                    start = time.time()\n",
      "                    out = openai.Embedding.create(\n",
      "                        input=batch, engine=\"text-embedding-ada-002\"\n",
      "                    )\n",
      "                    for embedding in out.data:\n",
      "                        embeddings.append(embedding.embedding)\n",
      "                    logger.info(f\"Batch {i} of {total_number_of_batches}\")\n",
      "                    logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\n",
      "                    i += 1\n",
      "                    batch = []\n",
      "            if len(batch) > 0:\n",
      "                start = time.time()\n",
      "                out = openai.Embedding.create(\n",
      "                    input=batch, engine=\"text-embedding-ada-002\"\n",
      "                )\n",
      "                for embedding in out.data:\n",
      "                    embeddings.append(embedding.embedding)\n",
      "                logger.info(f\"Batch {i} of {total_number_of_batches}\")\n",
      "                logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\n",
      "            logger.info(f'Total number of embeddings {len(embeddings)}')\n",
      "\n",
      "            if len(embeddings) != len(data):\n",
      "                raise ValueError(\"The number of embeddings is different from the number of values an error occured in OpenAI API during the embedding process\")\n",
      "            return embeddings\n",
      "\n",
      "Function calls: shape: (23,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"len\"\n",
      "\t\"str\"\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_embedding_size(self):\n",
      "    return ADA_EMBEDDING_SIZE\n",
      "\n",
      "\n",
      "\n",
      "def embed(self, data, verbose=False):\n",
      "    if isinstance(data, list) and len(data) > 1:\n",
      "        logger.info(\"Batch embedding\")\n",
      "        return self.batch_embed(data)\n",
      "    elif isinstance(data, list) and len(data) == 1:\n",
      "        logger.info(\"Serial embedding\")\n",
      "        data = data[0]\n",
      "\n",
      "    if isinstance(data, dict) and \"content\" in data:\n",
      "        if verbose:\n",
      "            logger.info(\"Embedding without mark\", data[\"content\"])\n",
      "        out = openai.Embedding.create(\n",
      "            input=data[\"content\"], engine=\"text-embedding-ada-002\"\n",
      "        )\n",
      "    else:\n",
      "        if len(TOKENIZER.encode(data)) > MAX_CONTEXT_LENGTH:\n",
      "            raise ValueError(f\" The input is too long for OpenAI, num tokens is {len(TOKENIZER.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "        if verbose:\n",
      "            logger.info(f\"Embedding without preprocessing the input {data}\")\n",
      "        out = openai.Embedding.create(\n",
      "            input=str(data), engine=\"text-embedding-ada-002\"\n",
      "        )\n",
      "    return out.data[0].embedding\n",
      "\n",
      "Function calls: shape: (9,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"len\"\n",
      "\t\"str\"\n",
      "]\n",
      "\n",
      "\n",
      "def batch_embed(self, data: List[str], batch_size: int = 1000):\n",
      "    if isinstance(data, dict) and \"content\" in data:\n",
      "        raise ValueError(\"Batch embedding not supported for dictionaries\")\n",
      "    elif isinstance(data, str):\n",
      "        raise ValueError(\"Batch embedding not supported for strings use embed() instead\")\n",
      "    elif isinstance(data, list):\n",
      "        batch = []\n",
      "        embeddings = []\n",
      "        i = 1\n",
      "        total_number_of_batches = len(data)//batch_size + 1 if len(data) % batch_size > 0 else len(data)//batch_size\n",
      "        for value in data:\n",
      "            batch.append(value)\n",
      "            if len(batch) == batch_size:\n",
      "                start = time.time()\n",
      "                out = openai.Embedding.create(\n",
      "                    input=batch, engine=\"text-embedding-ada-002\"\n",
      "                )\n",
      "                for embedding in out.data:\n",
      "                    embeddings.append(embedding.embedding)\n",
      "                logger.info(f\"Batch {i} of {total_number_of_batches}\")\n",
      "                logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\n",
      "                i += 1\n",
      "                batch = []\n",
      "        if len(batch) > 0:\n",
      "            start = time.time()\n",
      "            out = openai.Embedding.create(\n",
      "                input=batch, engine=\"text-embedding-ada-002\"\n",
      "            )\n",
      "            for embedding in out.data:\n",
      "                embeddings.append(embedding.embedding)\n",
      "            logger.info(f\"Batch {i} of {total_number_of_batches}\")\n",
      "            logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\n",
      "        logger.info(f'Total number of embeddings {len(embeddings)}')\n",
      "\n",
      "        if len(embeddings) != len(data):\n",
      "            raise ValueError(\"The number of embeddings is different from the number of values an error occured in OpenAI API during the embedding process\")\n",
      "        return embeddings\n",
      "\n",
      "Function calls: shape: (14,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"ValueError\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def parse_and_embed_functions(input_str: str) -> List[np.ndarray]:\n",
      "    # Parse the input string with libcst\n",
      "    module = cst.parse_module(input_str)\n",
      "\n",
      "    # Find all the functions in the module and embed them separately\n",
      "    embeddings = []\n",
      "    for node in module.body:\n",
      "\n",
      "        if isinstance(node, cst.FunctionDef) or isinstance(node, cst.ClassDef):\n",
      "            func_str = cst.Module(body=[node]).code\n",
      "            print(\"Function string\", func_str)\n",
      "            embedding = openai.Embedding.create(\n",
      "                input=str(func_str)[:MAX_CONTEXT_LENGTH],\n",
      "                engine=\"text-embedding-ada-002\",\n",
      "            )\n",
      "            if embedding is not None:\n",
      "                embeddings.append(embedding.data[0].embedding)\n",
      "\n",
      "    avg_embedding = avg_embeddings(embeddings)\n",
      "    print(avg_embedding.shape)\n",
      "    return avg_embedding\n",
      "\n",
      "Function calls: shape: (6,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"print\"\n",
      "\t\"str\"\n",
      "\t\"avg_embeddings…\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def avg_embeddings(embeddings: List[np.ndarray]) -> np.ndarray:\n",
      "    print(\"Embeddings len\", len(embeddings))\n",
      "    # convert embeddings to numpy array\n",
      "    embeddings = np.array(embeddings)\n",
      "    print(\"Embedding Matrix Shape\", embeddings.shape)\n",
      "    return np.array([np.sum(embeddings.T, axis=1)]).astype(np.float32)\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "class SBERTEmbedder:\n",
      "    def get_embedding_size(self):\n",
      "        return SBERT_EMBEDDING_SIZE\n",
      "\n",
      "    def embed(self,\n",
      "        data,\n",
      "        key=\"content\",\n",
      "        model_name=\"all-MiniLM-L6-v2\",\n",
      "        batch_size=128,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n",
      "        \"\"\"\n",
      "        print(\"Embedding data\")\n",
      "        model = SentenceTransformer(model_name)\n",
      "        print(\"Model loaded\")\n",
      "        if isinstance(data, dict):\n",
      "            sentences = data[key].tolist()\n",
      "            unique_sentences = data[key].unique()\n",
      "        elif isinstance(data, str):\n",
      "            #breal the string into sentences based on . or ? or !\n",
      "            sentences = re.split('[.!?]', data)\n",
      "            sentences = [s.strip() for s in sentences if s.strip()]  #\n",
      "            #filter empty sentences\n",
      "            sentences = list(filter(lambda x: len(x) > 0, sentences))\n",
      "            unique_sentences = list(set(sentences))\n",
      "        else:\n",
      "            raise ValueError(f\"Data must be a dictionary with attribute {key} or a string, but got {type(data)} instead\")\n",
      "        \n",
      "        print(\"Unique sentences\", len(unique_sentences))\n",
      "        self.unique_sentences = unique_sentences\n",
      "        for sentence in unique_sentences:\n",
      "            tokens = tokenizer.encode(sentence)\n",
      "            if len(tokens) > MAX_CONTEXT_LENGTH:\n",
      "                raise ValueError(f\" The input subsentence is too long for SBERT, num tokens is {len(tokens)}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "\n",
      "       \n",
      "        embeddings = model.encode(\n",
      "                unique_sentences, show_progress_bar=True, batch_size=batch_size\n",
      "            )\n",
      "\n",
      "        print(\"Embeddings computed\")\n",
      "\n",
      "        mapping = {\n",
      "            sentence: embedding\n",
      "            for sentence, embedding in zip(unique_sentences, embeddings)\n",
      "        }\n",
      "        embeddings = np.array([mapping[sentence] for sentence in sentences])\n",
      "\n",
      "        return np.mean(embeddings, axis=0).tolist()\n",
      "\n",
      "Function calls: shape: (19,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"SentenceTransf…\n",
      "\t\"print\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"list\"\n",
      "\t\"filter\"\n",
      "\t\"len\"\n",
      "\t\"list\"\n",
      "\t\"set\"\n",
      "\t\"ValueError\"\n",
      "\t\"type\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"zip\"\n",
      "]\n",
      "\n",
      "def get_embedding_size(self):\n",
      "    return SBERT_EMBEDDING_SIZE\n",
      "\n",
      "\n",
      "\n",
      "def embed(self,\n",
      "    data,\n",
      "    key=\"content\",\n",
      "    model_name=\"all-MiniLM-L6-v2\",\n",
      "    batch_size=128,\n",
      "):\n",
      "    \"\"\"\n",
      "        Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n",
      "        \"\"\"\n",
      "    print(\"Embedding data\")\n",
      "    model = SentenceTransformer(model_name)\n",
      "    print(\"Model loaded\")\n",
      "    if isinstance(data, dict):\n",
      "        sentences = data[key].tolist()\n",
      "        unique_sentences = data[key].unique()\n",
      "    elif isinstance(data, str):\n",
      "        #breal the string into sentences based on . or ? or !\n",
      "        sentences = re.split('[.!?]', data)\n",
      "        sentences = [s.strip() for s in sentences if s.strip()]  #\n",
      "        #filter empty sentences\n",
      "        sentences = list(filter(lambda x: len(x) > 0, sentences))\n",
      "        unique_sentences = list(set(sentences))\n",
      "    else:\n",
      "        raise ValueError(f\"Data must be a dictionary with attribute {key} or a string, but got {type(data)} instead\")\n",
      "    \n",
      "    print(\"Unique sentences\", len(unique_sentences))\n",
      "    self.unique_sentences = unique_sentences\n",
      "    for sentence in unique_sentences:\n",
      "        tokens = tokenizer.encode(sentence)\n",
      "        if len(tokens) > MAX_CONTEXT_LENGTH:\n",
      "            raise ValueError(f\" The input subsentence is too long for SBERT, num tokens is {len(tokens)}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "\n",
      "       \n",
      "    embeddings = model.encode(\n",
      "            unique_sentences, show_progress_bar=True, batch_size=batch_size\n",
      "        )\n",
      "\n",
      "    print(\"Embeddings computed\")\n",
      "\n",
      "    mapping = {\n",
      "        sentence: embedding\n",
      "        for sentence, embedding in zip(unique_sentences, embeddings)\n",
      "    }\n",
      "    embeddings = np.array([mapping[sentence] for sentence in sentences])\n",
      "\n",
      "    return np.mean(embeddings, axis=0).tolist()\n",
      "\n",
      "Function calls: shape: (19,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"SentenceTransf…\n",
      "\t\"print\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"list\"\n",
      "\t\"filter\"\n",
      "\t\"len\"\n",
      "\t\"list\"\n",
      "\t\"set\"\n",
      "\t\"ValueError\"\n",
      "\t\"type\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"zip\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class CohereEmbedder:\n",
      "    def get_embedding_size(self):\n",
      "        return COHERE_EMBEDDING_SIZE\n",
      "\n",
      "    def embed(self, data, verbose=False):\n",
      "        if type(data) is dict and \"content\" in data:\n",
      "            if verbose is True:\n",
      "                print(\"Embedding from dictionary\", data[\"content\"])\n",
      "                response = co.embed(texts= data[\"content\"],model='multilingual-22-12')\n",
      "        else:\n",
      "            if len(tokenizer.encode(data)) > MAX_CONTEXT_LENGTH:\n",
      "                raise ValueError(f\" The input is too long for Cohere, num tokens is {len(tokenizer.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "            if verbose is True:\n",
      "                print(\"Embedding without preprocessing the input\", data)\n",
      "            response = co.embed(texts=[str(data)],model='multilingual-22-12')\n",
      "        return response.embeddings[0]\n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"type\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"str\"\n",
      "]\n",
      "\n",
      "def get_embedding_size(self):\n",
      "    return COHERE_EMBEDDING_SIZE\n",
      "\n",
      "\n",
      "\n",
      "def embed(self, data, verbose=False):\n",
      "    if type(data) is dict and \"content\" in data:\n",
      "        if verbose is True:\n",
      "            print(\"Embedding from dictionary\", data[\"content\"])\n",
      "            response = co.embed(texts= data[\"content\"],model='multilingual-22-12')\n",
      "    else:\n",
      "        if len(tokenizer.encode(data)) > MAX_CONTEXT_LENGTH:\n",
      "            raise ValueError(f\" The input is too long for Cohere, num tokens is {len(tokenizer.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "        if verbose is True:\n",
      "            print(\"Embedding without preprocessing the input\", data)\n",
      "        response = co.embed(texts=[str(data)],model='multilingual-22-12')\n",
      "    return response.embeddings[0]\n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"type\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"str\"\n",
      "]\n",
      "\n",
      "\n",
      "class BatchGenerator:\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        input_df: Union[pl.DataFrame,str] = 'noinput',\n",
      "        task: str = 'chat',\n",
      "        name: str = \"summarizer\",\n",
      "        tokenizer: Optional[Any] = None,\n",
      "        save_path: str = 'batch_generator',\n",
      "        logging_level: int = 10,\n",
      "    ) -> None:\n",
      "\n",
      "        if isinstance(input_df, pl.DataFrame):\n",
      "            self.load_path = f\"{save_path}/{name}.ndjson\" ## pyright: ignore\n",
      "            input_df.write_ndjson(self.load_path)\n",
      "        elif input_df == 'noinput':\n",
      "            raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n",
      "        else:\n",
      "            self.load_path = input_df\n",
      "\n",
      "        self.name = name\n",
      "        self.task = task\n",
      "        self.save_path = save_path\n",
      "        self.logging_level = logging_level    \n",
      "\n",
      "        if tokenizer is None:\n",
      "            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "        else:\n",
      "            self.tokenizer = tokenizer\n",
      "\n",
      "        # debug loads the frame in advance for usefull checks \n",
      "        self.frame = pl.read_ndjson(self.load_path)\n",
      "\n",
      "        # queues\n",
      "        self.requests_queue = asyncio.Queue()\n",
      "        self.retries_queue = asyncio.Queue()\n",
      "        self.errors_queue = asyncio.Queue()\n",
      "\n",
      "        # constants\n",
      "        self.max_attempts = 5\n",
      "        self.seconds_to_pause_after_rate_limit_error = 15\n",
      "        self.seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n",
      "\n",
      "        # initialize logging\n",
      "        logging.basicConfig(level=logging_level)\n",
      "        logging.debug(f\"Logging initialized at level {logging_level}\")\n",
      "        \n",
      "        # initialize available capacity counts\n",
      "        self.available_request_capacity = 1500\n",
      "        self.available_token_capacity = 6250000\n",
      "        self.last_update_time = time.time()\n",
      "\n",
      "        # api authentication\n",
      "        self.api_key =  os.getenv(\"OPENAI_API_KEY\")\n",
      "        self.request_header = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
      "    \n",
      "        @dataclass\n",
      "        class StatusTracker:\n",
      "            num_tasks_started: int = 0\n",
      "            num_tasks_in_progress: int = 0  # script ends when this reaches 0\n",
      "            num_tasks_succeeded: int = 0\n",
      "            num_tasks_failed: int = 0\n",
      "            num_rate_limit_errors: int = 0\n",
      "            num_api_errors: int = 0  # excluding rate limit errors, counted above\n",
      "            num_other_errors: int = 0\n",
      "            time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n",
      "\n",
      "        self.status_tracker = StatusTracker()\n",
      "\n",
      "        self.finished = False  # after file is empty, we'll skip reading it\n",
      "        logging.debug(f\"Initialization complete.\")\n",
      "\n",
      "    def enqueue_objects(self):\n",
      "        with open(self.load_path, 'r') as jsonl_file:\n",
      "            for line in jsonl_file:\n",
      "                line = line.strip()\n",
      "                if not line:\n",
      "                    continue\n",
      "                json_obj = json.loads(line)                \n",
      "                self.requests_queue.put_nowait(json_obj)  \n",
      "\n",
      "\n",
      "    async def process_objects(self, queue):\n",
      "        next_request = None  # variable to hold the next request to call\n",
      "        while True:\n",
      "                    # get next request (if one is not already waiting for capacity)\n",
      "                    if not self.retries_queue.empty():\n",
      "                            next_request = self.retries_queue.get_nowait()\n",
      "                            logging.debug(f\"Retrying request: {next_request}\")\n",
      "                    elif not self.requests_queue.empty():\n",
      "                            try:\n",
      "                                logging.debug(f\"Trying to retrieve next request\")\n",
      "                                next_request = self.requests_queue.get_nowait()     \n",
      "                                logging.info(f'Next request is {next_request}')\n",
      "                                if next_request.respect_token_limit:\n",
      "                                    self.status_tracker.num_tasks_started += 1\n",
      "                                    self.status_tracker.num_tasks_in_progress += 1\n",
      "                                    logging.debug(f\"Reading request: {next_request}\")\n",
      "                                else:\n",
      "                                    self.errors_queue.put_nowait(next_request)\n",
      "                            except StopIteration:\n",
      "                                # if file runs out, set flag to stop reading it\n",
      "                                logging.debug(\"Read file exhausted\")\n",
      "                                self.finished = True\n",
      "\n",
      "                    # update available capacity\n",
      "                    current_time = time.time()\n",
      "                    seconds_since_update = current_time - self.last_update_time\n",
      "                    self.available_request_capacity = min(\n",
      "                    self.available_request_capacity + next_request.max_requests_per_minute * seconds_since_update / 60.0, ## pyright: ignore\n",
      "                        next_request.max_requests_per_minute, ## pyright: ignore\n",
      "                    )\n",
      "                    self.available_token_capacity = min(\n",
      "                        self.available_token_capacity + next_request.max_tokens_per_minute * seconds_since_update / 60.0, ## pyright: ignore\n",
      "                        next_request.max_tokens_per_minute,  ## pyright: ignore\n",
      "                    )\n",
      "                    self.last_update_time = current_time\n",
      "\n",
      "                    # if enough capacity available, call API\n",
      "                    if next_request:\n",
      "                        next_request_tokens = next_request.total_tokens\n",
      "                        if (\n",
      "                            self.available_request_capacity >= 1\n",
      "                            and self.available_token_capacity >= next_request_tokens\n",
      "                        ):\n",
      "                            # update counters\n",
      "                            self.available_request_capacity -= 1\n",
      "                            self.available_token_capacity -= next_request_tokens\n",
      "                            \n",
      "\n",
      "                            # call API\n",
      "                            try:\n",
      "                                logging.info(f\"Calling Api for {next_request.body}\")\n",
      "                                async with aiohttp.ClientSession() as session:\n",
      "                                    async with session.post(\n",
      "                                    url=next_request.url, headers=self.request_header, json=next_request.body\n",
      "                                    ) as response:\n",
      "                                        response = await response.json()\n",
      "                                if \"error\" in response:\n",
      "                                    logging.warning(\n",
      "                                        \"\"\" f\"Request {self.task_id} failed with error {response['error']}\" \"\"\"\n",
      "                                    )\n",
      "                                    self.status_tracker.num_api_errors += 1\n",
      "                                    if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                                        self.status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \n",
      "                                        self.status_tracker.num_rate_limit_errors += 1\n",
      "                                        self.status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n",
      "                                        self.retries_queue.put_nowait(next_request)\n",
      "                                    else:\n",
      "                                        self.errors_queue.put_nowait(next_request)\n",
      "                                else:\n",
      "                                        print(response)\n",
      "                                        \n",
      "\n",
      "                            except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "                                logging.warning(f\"Request {next_request} failed with Exception {e}\")\n",
      "                                self.status_tracker.num_other_errors += 1\n",
      "                            queue.task_done() \n",
      "                            next_request = None  # reset next_request to empty\n",
      "\n",
      "                    # if all tasks are finished, break\n",
      "                    if self.status_tracker.num_tasks_in_progress == 0:\n",
      "                        break\n",
      "\n",
      "                    # main loop sleeps briefly so concurrent tasks can run\n",
      "                    await asyncio.sleep(self.seconds_to_sleep_each_loop)\n",
      "\n",
      "                    # if a rate limit error was hit recently, pause to cool down\n",
      "                    seconds_since_rate_limit_error = (time.time() - self.status_tracker.time_of_last_rate_limit_error)\n",
      "                    if seconds_since_rate_limit_error < self.seconds_to_pause_after_rate_limit_error:\n",
      "                        remaining_seconds_to_pause = (self.seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n",
      "                        await asyncio.sleep(remaining_seconds_to_pause)\n",
      "                        # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n",
      "                        logging.warn(f\"Pausing to cool down until {time.ctime(self.status_tracker.time_of_last_rate_limit_error + self.seconds_to_pause_after_rate_limit_error)}\")\n",
      "\n",
      "        # after finishing, log final status\n",
      "        logging.info(f\"\"\"Parallel processing complete. Results saved to {self.save_path}\"\"\")\n",
      "        if self.status_tracker.num_tasks_failed > 0:\n",
      "            logging.warning(f\"{self.status_tracker.num_tasks_failed} / {self.status_tracker.num_tasks_started} requests failed. Errors logged to {self.save_path}.\")\n",
      "        if self.status_tracker.num_rate_limit_errors > 0:\n",
      "            logging.warning(f\"{self.status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n",
      "\n",
      "\n",
      "\n",
      "    async def main(self):\n",
      "        logging.debug(f\"Entering main loop\")\n",
      "        self.enqueue_objects()\n",
      "        consumers = [asyncio.create_task(self.process_objects(self.requests_queue)) for _ in range(5)]\n",
      "        await self.requests_queue.join()\n",
      "        await self.retries_queue.join()\n",
      "        for consumer in consumers:\n",
      "            consumer.cancel()\n",
      "\n",
      "    def execute(self):\n",
      "        asyncio.run(self.main())\n",
      "\n",
      "Function calls: shape: (8,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"TypeError\"\n",
      "\t\"StatusTracker\"\n",
      "\t\"open\"\n",
      "\t\"min\"\n",
      "\t\"min\"\n",
      "\t\"print\"\n",
      "\t\"range\"\n",
      "]\n",
      "\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    input_df: Union[pl.DataFrame,str] = 'noinput',\n",
      "    task: str = 'chat',\n",
      "    name: str = \"summarizer\",\n",
      "    tokenizer: Optional[Any] = None,\n",
      "    save_path: str = 'batch_generator',\n",
      "    logging_level: int = 10,\n",
      ") -> None:\n",
      "\n",
      "    if isinstance(input_df, pl.DataFrame):\n",
      "        self.load_path = f\"{save_path}/{name}.ndjson\" ## pyright: ignore\n",
      "        input_df.write_ndjson(self.load_path)\n",
      "    elif input_df == 'noinput':\n",
      "        raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n",
      "    else:\n",
      "        self.load_path = input_df\n",
      "\n",
      "    self.name = name\n",
      "    self.task = task\n",
      "    self.save_path = save_path\n",
      "    self.logging_level = logging_level    \n",
      "\n",
      "    if tokenizer is None:\n",
      "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "    else:\n",
      "        self.tokenizer = tokenizer\n",
      "\n",
      "    # debug loads the frame in advance for usefull checks \n",
      "    self.frame = pl.read_ndjson(self.load_path)\n",
      "\n",
      "    # queues\n",
      "    self.requests_queue = asyncio.Queue()\n",
      "    self.retries_queue = asyncio.Queue()\n",
      "    self.errors_queue = asyncio.Queue()\n",
      "\n",
      "    # constants\n",
      "    self.max_attempts = 5\n",
      "    self.seconds_to_pause_after_rate_limit_error = 15\n",
      "    self.seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n",
      "\n",
      "    # initialize logging\n",
      "    logging.basicConfig(level=logging_level)\n",
      "    logging.debug(f\"Logging initialized at level {logging_level}\")\n",
      "    \n",
      "    # initialize available capacity counts\n",
      "    self.available_request_capacity = 1500\n",
      "    self.available_token_capacity = 6250000\n",
      "    self.last_update_time = time.time()\n",
      "\n",
      "    # api authentication\n",
      "    self.api_key =  os.getenv(\"OPENAI_API_KEY\")\n",
      "    self.request_header = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
      "    \n",
      "    @dataclass\n",
      "    class StatusTracker:\n",
      "        num_tasks_started: int = 0\n",
      "        num_tasks_in_progress: int = 0  # script ends when this reaches 0\n",
      "        num_tasks_succeeded: int = 0\n",
      "        num_tasks_failed: int = 0\n",
      "        num_rate_limit_errors: int = 0\n",
      "        num_api_errors: int = 0  # excluding rate limit errors, counted above\n",
      "        num_other_errors: int = 0\n",
      "        time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n",
      "\n",
      "    self.status_tracker = StatusTracker()\n",
      "\n",
      "    self.finished = False  # after file is empty, we'll skip reading it\n",
      "    logging.debug(f\"Initialization complete.\")\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"TypeError\"\n",
      "\t\"StatusTracker\"\n",
      "]\n",
      "\n",
      "    \n",
      "@dataclass\n",
      "class StatusTracker:\n",
      "    num_tasks_started: int = 0\n",
      "    num_tasks_in_progress: int = 0  # script ends when this reaches 0\n",
      "    num_tasks_succeeded: int = 0\n",
      "    num_tasks_failed: int = 0\n",
      "    num_rate_limit_errors: int = 0\n",
      "    num_api_errors: int = 0  # excluding rate limit errors, counted above\n",
      "    num_other_errors: int = 0\n",
      "    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n",
      "\n",
      "\n",
      "\n",
      "def enqueue_objects(self):\n",
      "    with open(self.load_path, 'r') as jsonl_file:\n",
      "        for line in jsonl_file:\n",
      "            line = line.strip()\n",
      "            if not line:\n",
      "                continue\n",
      "            json_obj = json.loads(line)                \n",
      "            self.requests_queue.put_nowait(json_obj)  \n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "async def process_objects(self, queue):\n",
      "    next_request = None  # variable to hold the next request to call\n",
      "    while True:\n",
      "                # get next request (if one is not already waiting for capacity)\n",
      "                if not self.retries_queue.empty():\n",
      "                        next_request = self.retries_queue.get_nowait()\n",
      "                        logging.debug(f\"Retrying request: {next_request}\")\n",
      "                elif not self.requests_queue.empty():\n",
      "                        try:\n",
      "                            logging.debug(f\"Trying to retrieve next request\")\n",
      "                            next_request = self.requests_queue.get_nowait()     \n",
      "                            logging.info(f'Next request is {next_request}')\n",
      "                            if next_request.respect_token_limit:\n",
      "                                self.status_tracker.num_tasks_started += 1\n",
      "                                self.status_tracker.num_tasks_in_progress += 1\n",
      "                                logging.debug(f\"Reading request: {next_request}\")\n",
      "                            else:\n",
      "                                self.errors_queue.put_nowait(next_request)\n",
      "                        except StopIteration:\n",
      "                            # if file runs out, set flag to stop reading it\n",
      "                            logging.debug(\"Read file exhausted\")\n",
      "                            self.finished = True\n",
      "\n",
      "                # update available capacity\n",
      "                current_time = time.time()\n",
      "                seconds_since_update = current_time - self.last_update_time\n",
      "                self.available_request_capacity = min(\n",
      "                self.available_request_capacity + next_request.max_requests_per_minute * seconds_since_update / 60.0, ## pyright: ignore\n",
      "                    next_request.max_requests_per_minute, ## pyright: ignore\n",
      "                )\n",
      "                self.available_token_capacity = min(\n",
      "                    self.available_token_capacity + next_request.max_tokens_per_minute * seconds_since_update / 60.0, ## pyright: ignore\n",
      "                    next_request.max_tokens_per_minute,  ## pyright: ignore\n",
      "                )\n",
      "                self.last_update_time = current_time\n",
      "\n",
      "                # if enough capacity available, call API\n",
      "                if next_request:\n",
      "                    next_request_tokens = next_request.total_tokens\n",
      "                    if (\n",
      "                        self.available_request_capacity >= 1\n",
      "                        and self.available_token_capacity >= next_request_tokens\n",
      "                    ):\n",
      "                        # update counters\n",
      "                        self.available_request_capacity -= 1\n",
      "                        self.available_token_capacity -= next_request_tokens\n",
      "                        \n",
      "\n",
      "                        # call API\n",
      "                        try:\n",
      "                            logging.info(f\"Calling Api for {next_request.body}\")\n",
      "                            async with aiohttp.ClientSession() as session:\n",
      "                                async with session.post(\n",
      "                                url=next_request.url, headers=self.request_header, json=next_request.body\n",
      "                                ) as response:\n",
      "                                    response = await response.json()\n",
      "                            if \"error\" in response:\n",
      "                                logging.warning(\n",
      "                                    \"\"\" f\"Request {self.task_id} failed with error {response['error']}\" \"\"\"\n",
      "                                )\n",
      "                                self.status_tracker.num_api_errors += 1\n",
      "                                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                                    self.status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \n",
      "                                    self.status_tracker.num_rate_limit_errors += 1\n",
      "                                    self.status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n",
      "                                    self.retries_queue.put_nowait(next_request)\n",
      "                                else:\n",
      "                                    self.errors_queue.put_nowait(next_request)\n",
      "                            else:\n",
      "                                    print(response)\n",
      "                                    \n",
      "\n",
      "                        except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "                            logging.warning(f\"Request {next_request} failed with Exception {e}\")\n",
      "                            self.status_tracker.num_other_errors += 1\n",
      "                        queue.task_done() \n",
      "                        next_request = None  # reset next_request to empty\n",
      "\n",
      "                # if all tasks are finished, break\n",
      "                if self.status_tracker.num_tasks_in_progress == 0:\n",
      "                    break\n",
      "\n",
      "                # main loop sleeps briefly so concurrent tasks can run\n",
      "                await asyncio.sleep(self.seconds_to_sleep_each_loop)\n",
      "\n",
      "                # if a rate limit error was hit recently, pause to cool down\n",
      "                seconds_since_rate_limit_error = (time.time() - self.status_tracker.time_of_last_rate_limit_error)\n",
      "                if seconds_since_rate_limit_error < self.seconds_to_pause_after_rate_limit_error:\n",
      "                    remaining_seconds_to_pause = (self.seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n",
      "                    await asyncio.sleep(remaining_seconds_to_pause)\n",
      "                    # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n",
      "                    logging.warn(f\"Pausing to cool down until {time.ctime(self.status_tracker.time_of_last_rate_limit_error + self.seconds_to_pause_after_rate_limit_error)}\")\n",
      "\n",
      "    # after finishing, log final status\n",
      "    logging.info(f\"\"\"Parallel processing complete. Results saved to {self.save_path}\"\"\")\n",
      "    if self.status_tracker.num_tasks_failed > 0:\n",
      "        logging.warning(f\"{self.status_tracker.num_tasks_failed} / {self.status_tracker.num_tasks_started} requests failed. Errors logged to {self.save_path}.\")\n",
      "    if self.status_tracker.num_rate_limit_errors > 0:\n",
      "        logging.warning(f\"{self.status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"min\"\n",
      "\t\"min\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "async def main(self):\n",
      "    logging.debug(f\"Entering main loop\")\n",
      "    self.enqueue_objects()\n",
      "    consumers = [asyncio.create_task(self.process_objects(self.requests_queue)) for _ in range(5)]\n",
      "    await self.requests_queue.join()\n",
      "    await self.retries_queue.join()\n",
      "    for consumer in consumers:\n",
      "        consumer.cancel()\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"range\"\n",
      "]\n",
      "\n",
      "\n",
      "def execute(self):\n",
      "    asyncio.run(self.main())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class ApiRequest:\n",
      "    def __init__(\n",
      "        self, \n",
      "        **parameters\n",
      "        ) -> None:\n",
      "\n",
      "        if ('input' in parameters) or ('input' in parameters.get('body', {})):  \n",
      "            self.request_type = 'embedding'\n",
      "        else:\n",
      "            self.request_type = 'chat'\n",
      "        \n",
      "\n",
      "        if parameters.get('body') is not None:\n",
      "            self.body = parameters.get('body')\n",
      "\n",
      "        else:\n",
      "            self.body = {\n",
      "                \"model\": parameters.get('model'),\n",
      "                \"input\": parameters.get('input'),\n",
      "                \"messages\": parameters.get('messages'),\n",
      "                \"function\": parameters.get('function'),\n",
      "                \"function_call\": parameters.get('function_call'),\n",
      "                \"temperature\": parameters.get('temperature'),\n",
      "                \"top_p\": parameters.get('top_p'),\n",
      "                \"n\": parameters.get('n'),\n",
      "                \"stream\": parameters.get('stream'),\n",
      "                \"stop\": parameters.get('stop'),\n",
      "                \"max_tokens\": parameters.get('max_tokens'),\n",
      "                \"presence_penalty\": parameters.get('presence_penalty'),\n",
      "                \"frequency_penalty\": parameters.get('frequency_penalty'),\n",
      "                \"logit_bias\": parameters.get('logit_bias'),\n",
      "                \"user\": parameters.get('user')\n",
      "            }\n",
      "\n",
      "        # Remove keys with None values\n",
      "        if self.body is not None:\n",
      "            self.body = {k: v for k, v in self.body.items() if v is not None}\n",
      "\n",
      "        \"\"\"Count the number of tokens in the request. Only supports chat and embedding requests.\"\"\"\n",
      "        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "\n",
      "        # if completions request, tokens = prompt + n * max_tokens\n",
      "        if self.request_type == 'chat':\n",
      "            max_tokens = self.body['max_tokens'] # pyright: ignore\n",
      "            n = self.body['n'] # pyright: ignore\n",
      "            completion_tokens = n * max_tokens # pyright: ignore\n",
      "            for message in self.body[\"messages\"]: # pyright: ignore\n",
      "                self.total_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
      "                for key, value in message.items():\n",
      "                    self.total_tokens += len(encoding.encode(value))\n",
      "                    if key == \"name\":  # if there's a name, the role is omitted\n",
      "                        self.total_tokens -= 1  # role is always required and always 1 token\n",
      "                    self.total_tokens += 2  # every reply is primed with <im_start>assistant\n",
      "                self.total_tokens =  self.total_tokens + completion_tokens\n",
      "            \n",
      "        # if embeddings request, tokens = input tokens\n",
      "        elif self.request_type == \"embedding\":\n",
      "            if isinstance(self.body['input'], str):  # single input # pyright: ignore\n",
      "                self.total_tokens = len(encoding.encode(self.body[\"input\"]))  # pyright: ignore\n",
      "            elif isinstance(input, list):  # multiple inputs\n",
      "                self.total_tokens = sum([len(encoding.encode(i)) for i in self.body[\"input\"]]) # pyright: ignore\n",
      "            else:\n",
      "                raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n",
      "\n",
      "        match self.body['model']: # pyright: ignore\n",
      "            case 'gpt-3.5-turbo':\n",
      "                if self.total_tokens < 4000:\n",
      "                    self.respect_token_limit = True\n",
      "                    self.max_requests_per_minute = 3000\n",
      "                    self.max_tokens_per_minute= 1000000\n",
      "                    self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "                else:\n",
      "                    self.respect_token_limit = False\n",
      "            case 'gpt-4':\n",
      "                if self.total_tokens < 16000:\n",
      "                    self.respect_token_limit = True\n",
      "                    self.max_requests_per_minute = 3000\n",
      "                    self.max_tokens_per_minute= 1000000\n",
      "                    self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "                else:\n",
      "                    self.respect_token_limit = False\n",
      "\n",
      "            case 'ext-embedding-ada-002':\n",
      "                if self.total_tokens < 4000:\n",
      "                    self.respect_token_limit = True\n",
      "                    self.max_tokens_per_minute = 1000000\n",
      "                    self.max_requests_per_minute = 3000\n",
      "                    self.url = 'https://api.openai.com/v1/embeddings'\n",
      "                else:\n",
      "                    self.respect_token_limit = False                \n",
      "\n",
      "                \n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"sum\"\n",
      "\t\"len\"\n",
      "\t\"TypeError\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self, \n",
      "    **parameters\n",
      "    ) -> None:\n",
      "\n",
      "    if ('input' in parameters) or ('input' in parameters.get('body', {})):  \n",
      "        self.request_type = 'embedding'\n",
      "    else:\n",
      "        self.request_type = 'chat'\n",
      "    \n",
      "\n",
      "    if parameters.get('body') is not None:\n",
      "        self.body = parameters.get('body')\n",
      "\n",
      "    else:\n",
      "        self.body = {\n",
      "            \"model\": parameters.get('model'),\n",
      "            \"input\": parameters.get('input'),\n",
      "            \"messages\": parameters.get('messages'),\n",
      "            \"function\": parameters.get('function'),\n",
      "            \"function_call\": parameters.get('function_call'),\n",
      "            \"temperature\": parameters.get('temperature'),\n",
      "            \"top_p\": parameters.get('top_p'),\n",
      "            \"n\": parameters.get('n'),\n",
      "            \"stream\": parameters.get('stream'),\n",
      "            \"stop\": parameters.get('stop'),\n",
      "            \"max_tokens\": parameters.get('max_tokens'),\n",
      "            \"presence_penalty\": parameters.get('presence_penalty'),\n",
      "            \"frequency_penalty\": parameters.get('frequency_penalty'),\n",
      "            \"logit_bias\": parameters.get('logit_bias'),\n",
      "            \"user\": parameters.get('user')\n",
      "        }\n",
      "\n",
      "    # Remove keys with None values\n",
      "    if self.body is not None:\n",
      "        self.body = {k: v for k, v in self.body.items() if v is not None}\n",
      "\n",
      "    \"\"\"Count the number of tokens in the request. Only supports chat and embedding requests.\"\"\"\n",
      "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "\n",
      "    # if completions request, tokens = prompt + n * max_tokens\n",
      "    if self.request_type == 'chat':\n",
      "        max_tokens = self.body['max_tokens'] # pyright: ignore\n",
      "        n = self.body['n'] # pyright: ignore\n",
      "        completion_tokens = n * max_tokens # pyright: ignore\n",
      "        for message in self.body[\"messages\"]: # pyright: ignore\n",
      "            self.total_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
      "            for key, value in message.items():\n",
      "                self.total_tokens += len(encoding.encode(value))\n",
      "                if key == \"name\":  # if there's a name, the role is omitted\n",
      "                    self.total_tokens -= 1  # role is always required and always 1 token\n",
      "                self.total_tokens += 2  # every reply is primed with <im_start>assistant\n",
      "            self.total_tokens =  self.total_tokens + completion_tokens\n",
      "        \n",
      "    # if embeddings request, tokens = input tokens\n",
      "    elif self.request_type == \"embedding\":\n",
      "        if isinstance(self.body['input'], str):  # single input # pyright: ignore\n",
      "            self.total_tokens = len(encoding.encode(self.body[\"input\"]))  # pyright: ignore\n",
      "        elif isinstance(input, list):  # multiple inputs\n",
      "            self.total_tokens = sum([len(encoding.encode(i)) for i in self.body[\"input\"]]) # pyright: ignore\n",
      "        else:\n",
      "            raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n",
      "\n",
      "    match self.body['model']: # pyright: ignore\n",
      "        case 'gpt-3.5-turbo':\n",
      "            if self.total_tokens < 4000:\n",
      "                self.respect_token_limit = True\n",
      "                self.max_requests_per_minute = 3000\n",
      "                self.max_tokens_per_minute= 1000000\n",
      "                self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "            else:\n",
      "                self.respect_token_limit = False\n",
      "        case 'gpt-4':\n",
      "            if self.total_tokens < 16000:\n",
      "                self.respect_token_limit = True\n",
      "                self.max_requests_per_minute = 3000\n",
      "                self.max_tokens_per_minute= 1000000\n",
      "                self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "            else:\n",
      "                self.respect_token_limit = False\n",
      "\n",
      "        case 'ext-embedding-ada-002':\n",
      "            if self.total_tokens < 4000:\n",
      "                self.respect_token_limit = True\n",
      "                self.max_tokens_per_minute = 1000000\n",
      "                self.max_requests_per_minute = 3000\n",
      "                self.url = 'https://api.openai.com/v1/embeddings'\n",
      "            else:\n",
      "                self.respect_token_limit = False                \n",
      "\n",
      "            \n",
      "\n",
      "Function calls: shape: (7,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"len\"\n",
      "\t\"isinstance\"\n",
      "\t\"sum\"\n",
      "\t\"len\"\n",
      "\t\"TypeError\"\n",
      "]\n",
      "\n",
      "\n",
      "def chatgpt_response(\n",
      "     \n",
      "    prompt: List[Dict[str, Union[str, Dict[str, str]]]], \n",
      "    model: str = \"gpt-3.5-turbo\", \n",
      "    max_tokens: int = 1000, \n",
      "    stream: bool = False\n",
      ") -> Union[Generator,Tuple]:\n",
      "\n",
      "    try:\n",
      "        print(\"Trying to call OpenAI API...\")\n",
      "        response = openai.ChatCompletion.create(\n",
      "            model=model,\n",
      "            messages=prompt,\n",
      "            max_tokens=max_tokens,\n",
      "            stream=stream\n",
      "        )\n",
      "        return response, True\n",
      "\n",
      "    except openai.APIError as e:\n",
      "        logging.error(f\"Unexpected error in openai call: {e}\") \n",
      "        fail_response = {\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"message\": {\n",
      "                        \"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "        return fail_response, False\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class StatusTrackerModel(BaseModel):\n",
      "    name: str\n",
      "    max_attempts: int = 5\n",
      "    seconds_to_pause_after_rate_limit_error: int = 10\n",
      "    seconds_to_sleep_each_loop: float  = 0.001\n",
      "    available_request_capacity: int = 1500\n",
      "    available_token_capacity: int = 625000\n",
      "    last_update_time: float = now()\n",
      "    num_rate_limit_errors: int = 0\n",
      "    num_overloaded_errors: int = 0\n",
      "    time_of_last_rate_limit_error: float = 0.0\n",
      "    num_tasks_failed: int = 0\n",
      "    num_tasks_started: int = 0\n",
      "    num_api_errors: int = 0\n",
      "    num_other_errors: int = 0 \n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"now\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class OpenaiRequestModel(BaseModel):\n",
      "    model: str\n",
      "    input: Union[str,List,None]\n",
      "    messages: Union[List,None]\n",
      "    function: Union[List,None]\n",
      "    function_call: Union[str,Any,None]\n",
      "    temperature: Union[float,None]\n",
      "    top_p: Union[float,None]\n",
      "    n: Union[int,None]\n",
      "    stream: Union[bool,None]\n",
      "    stop: Union[str,List,None]\n",
      "    max_tokens: Union[int,None]\n",
      "    presence_penalty: Union[float,None]\n",
      "    frequency_penalty: Union[float,None]\n",
      "    logit_bias: Union[Mapping,None]\n",
      "    user: Union[str,None]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def cohere_response(prompt: List[dict],model: str = \"command\", max_tokens: int = 1000\n",
      "    ) -> Tuple[Dict, bool]:\n",
      "    \"\"\"\n",
      "        Call the Cohere API with the given prompt and maximum number of output tokens.\n",
      "        \n",
      "\n",
      "        :param prompt: A list of strings representing the prompt to send to the API.\n",
      "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
      "        :param model: A string representing the model to use. either command or command-nightly\n",
      "        :return: A tuple containing the API response cohere object and a boolean indicating success.\n",
      "        \"\"\"             \n",
      "    try:\n",
      "        prompt= convert_mark_to_str_prompt(prompt)\n",
      "        print(\"Trying to call Cohere API... using model:\", model)\n",
      "        response = co.generate(\n",
      "        model= model,\n",
      "        prompt= prompt,\n",
      "        max_tokens=max_tokens,\n",
      "        #end_sequences=['#SYSTEM:', '#USER:'],\n",
      "        )\n",
      "        return response, True\n",
      "\n",
      "    except:\n",
      "          return None, False\n",
      "    \n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"convert_mark_t…\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def cohere_summarize(prompt: str, model: str = \"summarize-xlarge\", length: str = \"medium\", extractiveness: str = \"medium\", format: str = \"bullets\", additional_command: str = None) -> str:\n",
      "    response = co.summarize( \n",
      "    text=prompt,model=model, \n",
      "    length=length,\n",
      "    extractiveness=extractiveness,\n",
      "    format=format,\n",
      "    additional_command=additional_command\n",
      "    )\n",
      "\n",
      "    summary = response.summary\n",
      "    return summary\n",
      "              \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class OsProcessor:\n",
      "    def __init__(self, directory_path: str):\n",
      "        self.directory_path = directory_path\n",
      "\n",
      "    def get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\n",
      "        \"\"\"Returns a list of all files in a directory\"\"\"\n",
      "        if directory_path is None:\n",
      "            directory_path = self.directory_path\n",
      "\n",
      "        all_files = []\n",
      "        for root, _, files in os.walk(directory_path):\n",
      "            for file in files:\n",
      "                all_files.append(os.path.join(root, file))\n",
      "\n",
      "        return all_files\n",
      "\n",
      "    def get_files_with_extension(\n",
      "        self, extension: str, directory_path: Optional[str] = None\n",
      "    ) -> List[str]:\n",
      "        \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\n",
      "        if directory_path is None:\n",
      "            directory_path = self.directory_path\n",
      "\n",
      "        all_files = self.get_all_files(directory_path)\n",
      "        files_with_extension = [file for file in all_files if file.endswith(extension)]\n",
      "\n",
      "        return files_with_extension\n",
      "\n",
      "    def get_file_extension(self, file_path: str) -> str:\n",
      "        \"\"\"Returns the extension of a file\"\"\"\n",
      "        return Path(file_path).suffix\n",
      "\n",
      "    def get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\n",
      "        \"\"\"Returns a list of all subdirectories in a directory\"\"\"\n",
      "        if directory_path is None:\n",
      "            directory_path = self.directory_path\n",
      "\n",
      "        subdirectories = [\n",
      "            os.path.join(directory_path, d)\n",
      "            for d in os.listdir(directory_path)\n",
      "            if os.path.isdir(os.path.join(directory_path, d))\n",
      "        ]\n",
      "\n",
      "        return subdirectories\n",
      "\n",
      "    def create_directory(self, directory_path: str) -> None:\n",
      "        \"\"\"Creates a directory if it does not exist\"\"\"\n",
      "        if not os.path.exists(directory_path):\n",
      "            os.makedirs(directory_path)\n",
      "\n",
      "    def delete_directory(self, directory_path: str) -> None:\n",
      "        \"\"\"Deletes a directory if it exists\"\"\"\n",
      "        if os.path.exists(directory_path):\n",
      "            shutil.rmtree(directory_path)\n",
      "\n",
      "    def copy_file(self, source_path: str, destination_path: str) -> None:\n",
      "        \"\"\"Copies a file from one location to another\"\"\"\n",
      "        shutil.copy2(source_path, destination_path)\n",
      "\n",
      "    def move_file(self, source_path: str, destination_path: str) -> None:\n",
      "        \"\"\"Moves a file from one location to another\"\"\"\n",
      "        shutil.move(source_path, destination_path)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Path\"\n",
      "]\n",
      "\n",
      "def __init__(self, directory_path: str):\n",
      "    self.directory_path = directory_path\n",
      "\n",
      "\n",
      "\n",
      "def get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\n",
      "    \"\"\"Returns a list of all files in a directory\"\"\"\n",
      "    if directory_path is None:\n",
      "        directory_path = self.directory_path\n",
      "\n",
      "    all_files = []\n",
      "    for root, _, files in os.walk(directory_path):\n",
      "        for file in files:\n",
      "            all_files.append(os.path.join(root, file))\n",
      "\n",
      "    return all_files\n",
      "\n",
      "\n",
      "\n",
      "def get_files_with_extension(\n",
      "    self, extension: str, directory_path: Optional[str] = None\n",
      ") -> List[str]:\n",
      "    \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\n",
      "    if directory_path is None:\n",
      "        directory_path = self.directory_path\n",
      "\n",
      "    all_files = self.get_all_files(directory_path)\n",
      "    files_with_extension = [file for file in all_files if file.endswith(extension)]\n",
      "\n",
      "    return files_with_extension\n",
      "\n",
      "\n",
      "\n",
      "def get_file_extension(self, file_path: str) -> str:\n",
      "    \"\"\"Returns the extension of a file\"\"\"\n",
      "    return Path(file_path).suffix\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Path\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\n",
      "    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\n",
      "    if directory_path is None:\n",
      "        directory_path = self.directory_path\n",
      "\n",
      "    subdirectories = [\n",
      "        os.path.join(directory_path, d)\n",
      "        for d in os.listdir(directory_path)\n",
      "        if os.path.isdir(os.path.join(directory_path, d))\n",
      "    ]\n",
      "\n",
      "    return subdirectories\n",
      "\n",
      "\n",
      "\n",
      "def create_directory(self, directory_path: str) -> None:\n",
      "    \"\"\"Creates a directory if it does not exist\"\"\"\n",
      "    if not os.path.exists(directory_path):\n",
      "        os.makedirs(directory_path)\n",
      "\n",
      "\n",
      "\n",
      "def delete_directory(self, directory_path: str) -> None:\n",
      "    \"\"\"Deletes a directory if it exists\"\"\"\n",
      "    if os.path.exists(directory_path):\n",
      "        shutil.rmtree(directory_path)\n",
      "\n",
      "\n",
      "\n",
      "def copy_file(self, source_path: str, destination_path: str) -> None:\n",
      "    \"\"\"Copies a file from one location to another\"\"\"\n",
      "    shutil.copy2(source_path, destination_path)\n",
      "\n",
      "\n",
      "\n",
      "def move_file(self, source_path: str, destination_path: str) -> None:\n",
      "    \"\"\"Moves a file from one location to another\"\"\"\n",
      "    shutil.move(source_path, destination_path)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class GithubProcessor(OsProcessor):\n",
      "    def __init__(\n",
      "        self,\n",
      "        base_directory: str,\n",
      "        username=None,\n",
      "        repo_name=None,\n",
      "        code_parsers=None,\n",
      "        minify_code: bool = False,\n",
      "        remove_docstrings: bool = False,\n",
      "    ):\n",
      "        self.username = username\n",
      "        self.repo_name = repo_name\n",
      "        self.base_directory = base_directory\n",
      "        self.github = Github()\n",
      "        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\n",
      "        repo_path = self.clone_repo(self.repo.clone_url)\n",
      "\n",
      "        OsProcessor.__init__(self, repo_path)\n",
      "        self.code_parsers = code_parsers or [\n",
      "            PythonParser(\n",
      "                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\n",
      "            )\n",
      "        ]\n",
      "\n",
      "    def get_public_repos(self):\n",
      "        \"\"\"Returns a list of all public repos for the user.\"\"\"\n",
      "        user = self.github.get_user(self.username)\n",
      "        return user.get_repos()\n",
      "\n",
      "    def clone_repo(self, repo_url: str):\n",
      "        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\n",
      "        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
      "        target_directory = os.path.join(self.base_directory, repo_name)\n",
      "\n",
      "        if os.path.exists(target_directory):\n",
      "            shutil.rmtree(target_directory)\n",
      "\n",
      "        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
      "\n",
      "        return target_directory\n",
      "\n",
      "    def process_repo(self, repo_path=None):\n",
      "        \"\"\"Processes the repo at the specified path.\n",
      "        If no path is specified, the repo at self.directory_path is processed.\n",
      "        Returns the list of parsed functions and classes.\"\"\"\n",
      "        if repo_path is None:\n",
      "            repo_path = self.directory_path\n",
      "\n",
      "        for code_parser in self.code_parsers:\n",
      "            code_parser.directory_path = repo_path\n",
      "            code_parser.process_directory(repo_path)\n",
      "\n",
      "    def process_repos(self):\n",
      "        \"\"\"Processes all public repos for the user.\"\"\"\n",
      "        for repo in self.get_public_repos():\n",
      "            if not repo.private:\n",
      "                print(f\"Processing repo: {repo.name}\")\n",
      "                repo_path = self.clone_repo(repo.clone_url)\n",
      "                self.process_repo(repo_path)\n",
      "                shutil.rmtree(repo_path)\n",
      "\n",
      "    def get_repo(self, repo_name):\n",
      "        \"\"\"Returns the repo with the specified name.\"\"\"\n",
      "        user = self.github.get_user(self.username)\n",
      "        return user.get_repo(repo_name)\n",
      "\n",
      "    def process_single_repo(self):\n",
      "\n",
      "        repo = self.get_repo(self.repo_name)\n",
      "        print(f\"Processing repo: {self.repo_name}\")\n",
      "        repo_path = self.clone_repo(repo.clone_url)\n",
      "        self.process_repo(repo_path)\n",
      "        shutil.rmtree(repo_path)\n",
      "\n",
      "    def get_issues(self, state=\"open\"):\n",
      "        \"\"\"\n",
      "        Returns a list of all issues in the repo with the specified state.\n",
      "        \"\"\"\n",
      "        issues = []\n",
      "        for issue in self.repo.get_issues(state=state):\n",
      "            issues.append(issue)\n",
      "        return issues\n",
      "\n",
      "    def parse_issues(self, state=\"open\"):\n",
      "        \"\"\"\n",
      "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
      "        Each dict contains the issue number, title, body, and labels.\n",
      "        \"\"\"\n",
      "        parsed_issues = []\n",
      "        issues = self.get_issues(state=state)\n",
      "        for issue in issues:\n",
      "            parsed_issue = {\n",
      "                \"number\": issue.number,\n",
      "                \"title\": issue.title,\n",
      "                \"body\": issue.body,\n",
      "                \"labels\": [label.name for label in issue.labels],\n",
      "            }\n",
      "            parsed_issues.append(parsed_issue)\n",
      "        return parsed_issues\n",
      "\n",
      "    def get_commits(self):\n",
      "        \"\"\"\n",
      "        Returns a list of all commits in the main branch of the repository.\n",
      "        \"\"\"\n",
      "        commits = []\n",
      "        branch = self.repo.get_branch(\"main\")\n",
      "        for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
      "            commits.append(commit)\n",
      "        return commits\n",
      "\n",
      "    def parse_commits(self):\n",
      "        \"\"\"\n",
      "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
      "        Each dict contains the commit sha, commit message, and author information.\n",
      "        \"\"\"\n",
      "        parsed_commits = []\n",
      "        commits = self.get_commits()\n",
      "        for commit in commits:\n",
      "            parsed_commit = {\n",
      "                \"sha\": commit.sha,\n",
      "                \"message\": commit.commit.message,\n",
      "                \"author\": {\n",
      "                    \"name\": commit.commit.author.name,\n",
      "                    \"email\": commit.commit.author.email,\n",
      "                    \"date\": commit.commit.author.date,\n",
      "                },\n",
      "            }\n",
      "            parsed_commits.append(parsed_commit)\n",
      "        return parsed_commits\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Github\"\n",
      "\t\"PythonParser\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    base_directory: str,\n",
      "    username=None,\n",
      "    repo_name=None,\n",
      "    code_parsers=None,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "):\n",
      "    self.username = username\n",
      "    self.repo_name = repo_name\n",
      "    self.base_directory = base_directory\n",
      "    self.github = Github()\n",
      "    self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\n",
      "    repo_path = self.clone_repo(self.repo.clone_url)\n",
      "\n",
      "    OsProcessor.__init__(self, repo_path)\n",
      "    self.code_parsers = code_parsers or [\n",
      "        PythonParser(\n",
      "            repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\n",
      "        )\n",
      "    ]\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Github\"\n",
      "\t\"PythonParser\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_public_repos(self):\n",
      "    \"\"\"Returns a list of all public repos for the user.\"\"\"\n",
      "    user = self.github.get_user(self.username)\n",
      "    return user.get_repos()\n",
      "\n",
      "\n",
      "\n",
      "def clone_repo(self, repo_url: str):\n",
      "    \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\n",
      "    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
      "    target_directory = os.path.join(self.base_directory, repo_name)\n",
      "\n",
      "    if os.path.exists(target_directory):\n",
      "        shutil.rmtree(target_directory)\n",
      "\n",
      "    subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
      "\n",
      "    return target_directory\n",
      "\n",
      "\n",
      "\n",
      "def process_repo(self, repo_path=None):\n",
      "    \"\"\"Processes the repo at the specified path.\n",
      "        If no path is specified, the repo at self.directory_path is processed.\n",
      "        Returns the list of parsed functions and classes.\"\"\"\n",
      "    if repo_path is None:\n",
      "        repo_path = self.directory_path\n",
      "\n",
      "    for code_parser in self.code_parsers:\n",
      "        code_parser.directory_path = repo_path\n",
      "        code_parser.process_directory(repo_path)\n",
      "\n",
      "\n",
      "\n",
      "def process_repos(self):\n",
      "    \"\"\"Processes all public repos for the user.\"\"\"\n",
      "    for repo in self.get_public_repos():\n",
      "        if not repo.private:\n",
      "            print(f\"Processing repo: {repo.name}\")\n",
      "            repo_path = self.clone_repo(repo.clone_url)\n",
      "            self.process_repo(repo_path)\n",
      "            shutil.rmtree(repo_path)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_repo(self, repo_name):\n",
      "    \"\"\"Returns the repo with the specified name.\"\"\"\n",
      "    user = self.github.get_user(self.username)\n",
      "    return user.get_repo(repo_name)\n",
      "\n",
      "\n",
      "\n",
      "def process_single_repo(self):\n",
      "\n",
      "    repo = self.get_repo(self.repo_name)\n",
      "    print(f\"Processing repo: {self.repo_name}\")\n",
      "    repo_path = self.clone_repo(repo.clone_url)\n",
      "    self.process_repo(repo_path)\n",
      "    shutil.rmtree(repo_path)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_issues(self, state=\"open\"):\n",
      "    \"\"\"\n",
      "        Returns a list of all issues in the repo with the specified state.\n",
      "        \"\"\"\n",
      "    issues = []\n",
      "    for issue in self.repo.get_issues(state=state):\n",
      "        issues.append(issue)\n",
      "    return issues\n",
      "\n",
      "\n",
      "\n",
      "def parse_issues(self, state=\"open\"):\n",
      "    \"\"\"\n",
      "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
      "        Each dict contains the issue number, title, body, and labels.\n",
      "        \"\"\"\n",
      "    parsed_issues = []\n",
      "    issues = self.get_issues(state=state)\n",
      "    for issue in issues:\n",
      "        parsed_issue = {\n",
      "            \"number\": issue.number,\n",
      "            \"title\": issue.title,\n",
      "            \"body\": issue.body,\n",
      "            \"labels\": [label.name for label in issue.labels],\n",
      "        }\n",
      "        parsed_issues.append(parsed_issue)\n",
      "    return parsed_issues\n",
      "\n",
      "\n",
      "\n",
      "def get_commits(self):\n",
      "    \"\"\"\n",
      "        Returns a list of all commits in the main branch of the repository.\n",
      "        \"\"\"\n",
      "    commits = []\n",
      "    branch = self.repo.get_branch(\"main\")\n",
      "    for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
      "        commits.append(commit)\n",
      "    return commits\n",
      "\n",
      "\n",
      "\n",
      "def parse_commits(self):\n",
      "    \"\"\"\n",
      "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
      "        Each dict contains the commit sha, commit message, and author information.\n",
      "        \"\"\"\n",
      "    parsed_commits = []\n",
      "    commits = self.get_commits()\n",
      "    for commit in commits:\n",
      "        parsed_commit = {\n",
      "            \"sha\": commit.sha,\n",
      "            \"message\": commit.commit.message,\n",
      "            \"author\": {\n",
      "                \"name\": commit.commit.author.name,\n",
      "                \"email\": commit.commit.author.email,\n",
      "                \"date\": commit.commit.author.date,\n",
      "            },\n",
      "        }\n",
      "        parsed_commits.append(parsed_commit)\n",
      "    return parsed_commits\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# A custom visitor to find function calls and their arguments\n",
      "class FunctionCallFinder(cst.CSTVisitor):\n",
      "    METADATA_DEPENDENCIES = (PositionProvider,)\n",
      "\n",
      "    def visit_Call(self, node: cst.Call) -> None:\n",
      "        function_name = None\n",
      "        if isinstance(node.func, cst.Name):\n",
      "            function_name = node.func.value\n",
      "\n",
      "        if function_name:\n",
      "            pos = self.get_metadata(PositionProvider, node).start\n",
      "            print(\n",
      "                f\"Function '{function_name}' called at line {pos.line}, column {pos.column} with arguments:\"\n",
      "            )\n",
      "\n",
      "            for arg in node.args:\n",
      "                arg_start_pos = self.get_metadata(PositionProvider, arg).start\n",
      "                arg_value = arg.value\n",
      "                if isinstance(arg_value, cst.SimpleString):\n",
      "                    arg_value = arg_value.evaluated_value\n",
      "                print(\n",
      "                    f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\n",
      "                )\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"print\"\n",
      "\t\"isinstance\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def visit_Call(self, node: cst.Call) -> None:\n",
      "    function_name = None\n",
      "    if isinstance(node.func, cst.Name):\n",
      "        function_name = node.func.value\n",
      "\n",
      "    if function_name:\n",
      "        pos = self.get_metadata(PositionProvider, node).start\n",
      "        print(\n",
      "            f\"Function '{function_name}' called at line {pos.line}, column {pos.column} with arguments:\"\n",
      "        )\n",
      "\n",
      "        for arg in node.args:\n",
      "            arg_start_pos = self.get_metadata(PositionProvider, arg).start\n",
      "            arg_value = arg.value\n",
      "            if isinstance(arg_value, cst.SimpleString):\n",
      "                arg_value = arg_value.evaluated_value\n",
      "            print(\n",
      "                f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\n",
      "            )\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"print\"\n",
      "\t\"isinstance\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class MultiplicationCounterVisitor(cst.CSTVisitor):\n",
      "    def __init__(self):\n",
      "        self.count = 0\n",
      "        self.functions_with_operation_dict = {}\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "        self.current_function = node\n",
      "        self.functions_with_operation_dict[node.name] = []\n",
      "\n",
      "    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "        self.current_function = None\n",
      "\n",
      "    def visit_BinaryOperation(self, node: cst.BinaryOperation) -> None:\n",
      "        if isinstance(node.operator, cst.Multiply) or isinstance(\n",
      "            node.operator, cst.BitAnd\n",
      "        ):\n",
      "            self.count += 1\n",
      "            if self.current_function:\n",
      "                self.functions_with_operation_dict[self.current_function.name].append(\n",
      "                    cst.Module([]).code_for_node(node)\n",
      "                )\n",
      "\n",
      "    def visit_Call(self, node: cst.Call) -> None:\n",
      "        if m.matches(node, m.Call(func=m.Attribute(attr=m.Name(\"dot\")))) or m.matches(\n",
      "            node, m.Call(func=m.Name(\"dot\"), args=[m.Arg(), m.Arg()])\n",
      "        ):\n",
      "            self.count += 1\n",
      "            if self.current_function:\n",
      "                self.functions_with_operation_dict[self.current_function.name].append(\n",
      "                    cst.Module([]).code_for_node(node)\n",
      "                )\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "def __init__(self):\n",
      "    self.count = 0\n",
      "    self.functions_with_operation_dict = {}\n",
      "\n",
      "\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "    self.current_function = node\n",
      "    self.functions_with_operation_dict[node.name] = []\n",
      "\n",
      "\n",
      "\n",
      "def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "    self.current_function = None\n",
      "\n",
      "\n",
      "\n",
      "def visit_BinaryOperation(self, node: cst.BinaryOperation) -> None:\n",
      "    if isinstance(node.operator, cst.Multiply) or isinstance(\n",
      "        node.operator, cst.BitAnd\n",
      "    ):\n",
      "        self.count += 1\n",
      "        if self.current_function:\n",
      "            self.functions_with_operation_dict[self.current_function.name].append(\n",
      "                cst.Module([]).code_for_node(node)\n",
      "            )\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def visit_Call(self, node: cst.Call) -> None:\n",
      "    if m.matches(node, m.Call(func=m.Attribute(attr=m.Name(\"dot\")))) or m.matches(\n",
      "        node, m.Call(func=m.Name(\"dot\"), args=[m.Arg(), m.Arg()])\n",
      "    ):\n",
      "        self.count += 1\n",
      "        if self.current_function:\n",
      "            self.functions_with_operation_dict[self.current_function.name].append(\n",
      "                cst.Module([]).code_for_node(node)\n",
      "            )\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class FunctionAndClassVisitor(cst.CSTVisitor):\n",
      "    def __init__(self):\n",
      "        self.function_source_codes = []\n",
      "        self.function_nodes = []\n",
      "        self.class_source_codes = []\n",
      "        self.class_nodes = []\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "        function_source_code = cst.Module([]).code_for_node(node)\n",
      "        # add in place summary and code mod\n",
      "        self.function_nodes.append(node)\n",
      "        self.function_source_codes.append(function_source_code)\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
      "        class_source_code = cst.Module([]).code_for_node(node)\n",
      "        # add in place summary and code mod\n",
      "        self.class_nodes.append(node)\n",
      "        self.class_source_codes.append(class_source_code)\n",
      "\n",
      "\n",
      "def __init__(self):\n",
      "    self.function_source_codes = []\n",
      "    self.function_nodes = []\n",
      "    self.class_source_codes = []\n",
      "    self.class_nodes = []\n",
      "\n",
      "\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "    function_source_code = cst.Module([]).code_for_node(node)\n",
      "    # add in place summary and code mod\n",
      "    self.function_nodes.append(node)\n",
      "    self.function_source_codes.append(function_source_code)\n",
      "\n",
      "\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
      "    class_source_code = cst.Module([]).code_for_node(node)\n",
      "    # add in place summary and code mod\n",
      "    self.class_nodes.append(node)\n",
      "    self.class_source_codes.append(class_source_code)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class TypingCollector(cst.CSTVisitor):\n",
      "    def __init__(self):\n",
      "        # stack for storing the canonical name of the current function\n",
      "        self.stack: List[Tuple[str, ...]] = []\n",
      "        # store the annotations\n",
      "        self.annotations: Dict[\n",
      "            Tuple[str, ...],  # key: tuple of canonical class/function name\n",
      "            Tuple[cst.Parameters, Optional[cst.Annotation]],  # value: (params, returns)\n",
      "        ] = {}\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef) -> Optional[bool]:\n",
      "        self.stack.append(node.name.value)\n",
      "\n",
      "    def leave_ClassDef(self, node: cst.ClassDef) -> None:\n",
      "        self.stack.pop()\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> Optional[bool]:\n",
      "        self.stack.append(node.name.value)\n",
      "        self.annotations[tuple(self.stack)] = (node.params, node.returns)\n",
      "        return False  # pyi files don't support inner functions, return False to stop the traversal.\n",
      "\n",
      "    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "        self.stack.pop()\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"tuple\"\n",
      "]\n",
      "\n",
      "def __init__(self):\n",
      "    # stack for storing the canonical name of the current function\n",
      "    self.stack: List[Tuple[str, ...]] = []\n",
      "    # store the annotations\n",
      "    self.annotations: Dict[\n",
      "        Tuple[str, ...],  # key: tuple of canonical class/function name\n",
      "        Tuple[cst.Parameters, Optional[cst.Annotation]],  # value: (params, returns)\n",
      "    ] = {}\n",
      "\n",
      "\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef) -> Optional[bool]:\n",
      "    self.stack.append(node.name.value)\n",
      "\n",
      "\n",
      "\n",
      "def leave_ClassDef(self, node: cst.ClassDef) -> None:\n",
      "    self.stack.pop()\n",
      "\n",
      "\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> Optional[bool]:\n",
      "    self.stack.append(node.name.value)\n",
      "    self.annotations[tuple(self.stack)] = (node.params, node.returns)\n",
      "    return False  # pyi files don't support inner functions, return False to stop the traversal.\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"tuple\"\n",
      "]\n",
      "\n",
      "\n",
      "def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "    self.stack.pop()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class PythonMinifier:\n",
      "    def __init__(self, code: str = None):\n",
      "\n",
      "        self.code = code\n",
      "        self.output_code = None\n",
      "\n",
      "    def minify(self):\n",
      "        if self.code:\n",
      "            self.output_code = self.minify_code(self.code)\n",
      "\n",
      "    def get_minified_code(self):\n",
      "        if not self.output_code:\n",
      "            self.minify()\n",
      "        return self.output_code\n",
      "\n",
      "    @staticmethod\n",
      "    def minify_code(code: str) -> str:\n",
      "        return minify(code)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"minify\"\n",
      "]\n",
      "\n",
      "def __init__(self, code: str = None):\n",
      "\n",
      "    self.code = code\n",
      "    self.output_code = None\n",
      "\n",
      "\n",
      "\n",
      "def minify(self):\n",
      "    if self.code:\n",
      "        self.output_code = self.minify_code(self.code)\n",
      "\n",
      "\n",
      "\n",
      "def get_minified_code(self):\n",
      "    if not self.output_code:\n",
      "        self.minify()\n",
      "    return self.output_code\n",
      "\n",
      "\n",
      "\n",
      "@staticmethod\n",
      "def minify_code(code: str) -> str:\n",
      "    return minify(code)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"minify\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class PythonDocstringExtractor:\n",
      "    @staticmethod\n",
      "    def extract_docstring(function_def: cst.FunctionDef) -> str:\n",
      "        docstring = None\n",
      "\n",
      "        for stmt in function_def.body.body:\n",
      "            if isinstance(stmt, cst.SimpleStatementLine):\n",
      "                for expr in stmt.body:\n",
      "                    if isinstance(expr, cst.Expr) and isinstance(\n",
      "                        expr.value, cst.SimpleString\n",
      "                    ):\n",
      "                        docstring = expr.value.value.strip('\"').strip(\"'\")\n",
      "                        break\n",
      "            if docstring is not None:\n",
      "                break\n",
      "\n",
      "        if docstring is not None:\n",
      "            return docstring.strip()\n",
      "        else:\n",
      "            function_name = function_def.name.value\n",
      "            return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "@staticmethod\n",
      "def extract_docstring(function_def: cst.FunctionDef) -> str:\n",
      "    docstring = None\n",
      "\n",
      "    for stmt in function_def.body.body:\n",
      "        if isinstance(stmt, cst.SimpleStatementLine):\n",
      "            for expr in stmt.body:\n",
      "                if isinstance(expr, cst.Expr) and isinstance(\n",
      "                    expr.value, cst.SimpleString\n",
      "                ):\n",
      "                    docstring = expr.value.value.strip('\"').strip(\"'\")\n",
      "                    break\n",
      "        if docstring is not None:\n",
      "            break\n",
      "\n",
      "    if docstring is not None:\n",
      "        return docstring.strip()\n",
      "    else:\n",
      "        function_name = function_def.name.value\n",
      "        return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class FunctionAndClassVisitor(cst.CSTVisitor):\n",
      "    def __init__(self):\n",
      "        self.function_source_codes = []\n",
      "        self.function_nodes = []\n",
      "        self.function_count = 0\n",
      "        self.class_source_codes = []\n",
      "        self.class_nodes = []\n",
      "        self.class_count = 0\n",
      "        self.filename_map = []\n",
      "        self.full_source_list = []\n",
      "        self.full_node_list = []\n",
      "\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "        \"\"\"This method is called for every FunctionDef node in the tree.\n",
      "        and it does the following:\n",
      "        1. Gets the source code for the node\n",
      "        2. Adds the node to the list of function nodes\n",
      "        3. Adds the source code to the list of function source codes\n",
      "        4. Increments the function count\n",
      "        \"\"\"\n",
      "        function_source_code = cst.Module([]).code_for_node(node)\n",
      "        self.function_nodes.append(node)\n",
      "        self.function_source_codes.append(function_source_code)\n",
      "        self.full_node_list.append(node)\n",
      "        self.full_source_list.append(function_source_code)\n",
      "        self.function_count += 1\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
      "        \"\"\"This method is called for every ClassDef node in the tree.\n",
      "        and it does the following:\n",
      "        1. Gets the source code for the node\n",
      "        2. Adds the node to the list of class nodes\n",
      "        3. Adds the source code to the list of class source codes\n",
      "        4. Increments the class count\n",
      "        \"\"\"\n",
      "        class_source_code = cst.Module([]).code_for_node(node)\n",
      "        self.class_nodes.append(node)\n",
      "        self.class_source_codes.append(class_source_code)\n",
      "        self.full_node_list.append(node)\n",
      "        self.full_source_list.append(class_source_code)\n",
      "        self.class_count += 1\n",
      "\n",
      "\n",
      "def __init__(self):\n",
      "    self.function_source_codes = []\n",
      "    self.function_nodes = []\n",
      "    self.function_count = 0\n",
      "    self.class_source_codes = []\n",
      "    self.class_nodes = []\n",
      "    self.class_count = 0\n",
      "    self.filename_map = []\n",
      "    self.full_source_list = []\n",
      "    self.full_node_list = []\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "    \"\"\"This method is called for every FunctionDef node in the tree.\n",
      "        and it does the following:\n",
      "        1. Gets the source code for the node\n",
      "        2. Adds the node to the list of function nodes\n",
      "        3. Adds the source code to the list of function source codes\n",
      "        4. Increments the function count\n",
      "        \"\"\"\n",
      "    function_source_code = cst.Module([]).code_for_node(node)\n",
      "    self.function_nodes.append(node)\n",
      "    self.function_source_codes.append(function_source_code)\n",
      "    self.full_node_list.append(node)\n",
      "    self.full_source_list.append(function_source_code)\n",
      "    self.function_count += 1\n",
      "\n",
      "\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
      "    \"\"\"This method is called for every ClassDef node in the tree.\n",
      "        and it does the following:\n",
      "        1. Gets the source code for the node\n",
      "        2. Adds the node to the list of class nodes\n",
      "        3. Adds the source code to the list of class source codes\n",
      "        4. Increments the class count\n",
      "        \"\"\"\n",
      "    class_source_code = cst.Module([]).code_for_node(node)\n",
      "    self.class_nodes.append(node)\n",
      "    self.class_source_codes.append(class_source_code)\n",
      "    self.full_node_list.append(node)\n",
      "    self.full_source_list.append(class_source_code)\n",
      "    self.class_count += 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class PythonParser(OsProcessor):\n",
      "    def __init__(\n",
      "        self,\n",
      "        directory_path: str,\n",
      "        visitor: Optional[FunctionAndClassVisitor] = None,\n",
      "        minify_code: bool = False,\n",
      "        remove_docstrings: bool = False,\n",
      "    ):\n",
      "        super().__init__(directory_path)\n",
      "        self.visitor = visitor if visitor else FunctionAndClassVisitor()\n",
      "        self.minify_code = minify_code\n",
      "        self.remove_docstrings = remove_docstrings\n",
      "\n",
      "    def remove_docstring(self, tree: cst.Module) -> str:\n",
      "        \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\n",
      "\n",
      "        # Remove docstrings using a transformer\n",
      "        class DocstringRemover(cst.CSTTransformer):\n",
      "            def leave_FunctionDef(\n",
      "                self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
      "            ) -> cst.FunctionDef:\n",
      "                docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
      "                if docstring.startswith(\"No docstring\"):\n",
      "                    return updated_node\n",
      "\n",
      "                return updated_node.with_changes(\n",
      "                    body=updated_node.body.with_changes(\n",
      "                        body=[\n",
      "                            stmt\n",
      "                            for stmt in updated_node.body.body\n",
      "                            if not (\n",
      "                                isinstance(stmt, cst.SimpleStatementLine)\n",
      "                                and any(\n",
      "                                    isinstance(expr, cst.Expr)\n",
      "                                    and isinstance(expr.value, cst.SimpleString)\n",
      "                                    for expr in stmt.body\n",
      "                                )\n",
      "                            )\n",
      "                        ]\n",
      "                    )\n",
      "                )\n",
      "\n",
      "        tree = tree.visit(DocstringRemover())\n",
      "        return tree.code\n",
      "\n",
      "    def _process_file(self, file_path: str):\n",
      "        \"\"\"This method is called for every file in the directory.\n",
      "        It does the following:\n",
      "        1. Reads the file\n",
      "        2. Parses the file\n",
      "        3. Visits the file with the visitor\n",
      "        \"\"\"\n",
      "        #get current number of nodes in visitor\n",
      "        current_node_count = self.visitor.function_count + self.visitor.class_count\n",
      "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
      "            source_code = file.read()\n",
      "\n",
      "        try:\n",
      "            tree = cst.parse_module(source_code)\n",
      "        except cst.ParserSyntaxError:\n",
      "            print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
      "            return\n",
      "\n",
      "        tree.visit(self.visitor)\n",
      "        #calculate how many new nodes were added\n",
      "        new_node_counter = self.visitor.function_count + self.visitor.class_count - current_node_count\n",
      "        self.visitor.filename_map.extend([file_path]*new_node_counter)\n",
      "        # Remove docstrings if specified\n",
      "        if self.remove_docstrings:\n",
      "            source_code = self.remove_docstring(source_code, tree)\n",
      "\n",
      "        # Minify the code if specified\n",
      "        if self.minify_code:\n",
      "            minifier = PythonMinifier(source_code)\n",
      "            source_code = minifier.get_minified_code()\n",
      "\n",
      "        # Add the processed code to the corresponding list in the visitor\n",
      "        self.visitor.function_source_codes.append(source_code)\n",
      "\n",
      "    def process_file(self, file_path: str):\n",
      "        \"\"\"This method is called for every file in the directory.\n",
      "        It does the following:\n",
      "        1. Runs flake8 on the file\n",
      "        if flake8 returns a non-zero exit code, it means the file has a syntax error\n",
      "        2. Reads the file\n",
      "        3. Parses the file\n",
      "        4. Visits the file with the visitor\n",
      "\n",
      "        \"\"\"\n",
      "        result = subprocess.run(\n",
      "            [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
      "        )\n",
      "\n",
      "        if result.returncode != 0:\n",
      "            print(f\"Skipping file with syntax error: {file_path}\")\n",
      "            print(result.stderr.decode(\"utf-8\"))\n",
      "            return\n",
      "\n",
      "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
      "            source_code = f.read()\n",
      "\n",
      "        try:\n",
      "            tree = cst.parse_module(source_code)\n",
      "            tree.visit(self.visitor)\n",
      "        except cst.ParserSyntaxError as e:\n",
      "            print(f\"Syntax error: {e}\")\n",
      "            print(f\"Skipping file with syntax error: {file_path}\")\n",
      "\n",
      "    def process_directory(\n",
      "        self,\n",
      "    ) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\n",
      "        \"\"\"This method is called for every directory.\n",
      "        It does the following:\n",
      "        1. Gets all the python files in the directory\n",
      "        2. Processes each file\n",
      "        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\n",
      "        \"\"\"\n",
      "\n",
      "        python_files = self.get_files_with_extension(\".py\")\n",
      "\n",
      "        for file_path in python_files:\n",
      "            self._process_file(file_path)\n",
      "\n",
      "        result_dict = {\n",
      "            'function_source_codes': self.visitor.function_source_codes,\n",
      "            'function_nodes': self.visitor.function_nodes,\n",
      "            'class_source_codes': self.visitor.class_source_codes,\n",
      "            'class_nodes': self.visitor.class_nodes,\n",
      "            'file_map': self.visitor.filename_map,\n",
      "            'full_nodes': self.visitor.full_node_list,\n",
      "            'full_source': self.visitor.full_source_list\n",
      "        }\n",
      "\n",
      "\n",
      "        return result_dict\n",
      "\n",
      "Function calls: shape: (15,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"FunctionAndCla…\n",
      "\t\"isinstance\"\n",
      "\t\"any\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"DocstringRemov…\n",
      "\t\"open\"\n",
      "\t\"print\"\n",
      "\t\"PythonMinifier…\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"open\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    directory_path: str,\n",
      "    visitor: Optional[FunctionAndClassVisitor] = None,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "):\n",
      "    super().__init__(directory_path)\n",
      "    self.visitor = visitor if visitor else FunctionAndClassVisitor()\n",
      "    self.minify_code = minify_code\n",
      "    self.remove_docstrings = remove_docstrings\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"super\"\n",
      "\t\"FunctionAndCla…\n",
      "]\n",
      "\n",
      "\n",
      "def remove_docstring(self, tree: cst.Module) -> str:\n",
      "    \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\n",
      "\n",
      "    # Remove docstrings using a transformer\n",
      "    class DocstringRemover(cst.CSTTransformer):\n",
      "        def leave_FunctionDef(\n",
      "            self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
      "        ) -> cst.FunctionDef:\n",
      "            docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
      "            if docstring.startswith(\"No docstring\"):\n",
      "                return updated_node\n",
      "\n",
      "            return updated_node.with_changes(\n",
      "                body=updated_node.body.with_changes(\n",
      "                    body=[\n",
      "                        stmt\n",
      "                        for stmt in updated_node.body.body\n",
      "                        if not (\n",
      "                            isinstance(stmt, cst.SimpleStatementLine)\n",
      "                            and any(\n",
      "                                isinstance(expr, cst.Expr)\n",
      "                                and isinstance(expr.value, cst.SimpleString)\n",
      "                                for expr in stmt.body\n",
      "                            )\n",
      "                        )\n",
      "                    ]\n",
      "                )\n",
      "            )\n",
      "\n",
      "    tree = tree.visit(DocstringRemover())\n",
      "    return tree.code\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"any\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "\t\"DocstringRemov…\n",
      "]\n",
      "\n",
      "\n",
      "# Remove docstrings using a transformer\n",
      "class DocstringRemover(cst.CSTTransformer):\n",
      "    def leave_FunctionDef(\n",
      "        self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
      "    ) -> cst.FunctionDef:\n",
      "        docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
      "        if docstring.startswith(\"No docstring\"):\n",
      "            return updated_node\n",
      "\n",
      "        return updated_node.with_changes(\n",
      "            body=updated_node.body.with_changes(\n",
      "                body=[\n",
      "                    stmt\n",
      "                    for stmt in updated_node.body.body\n",
      "                    if not (\n",
      "                        isinstance(stmt, cst.SimpleStatementLine)\n",
      "                        and any(\n",
      "                            isinstance(expr, cst.Expr)\n",
      "                            and isinstance(expr.value, cst.SimpleString)\n",
      "                            for expr in stmt.body\n",
      "                        )\n",
      "                    )\n",
      "                ]\n",
      "            )\n",
      "        )\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"any\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "def leave_FunctionDef(\n",
      "    self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
      ") -> cst.FunctionDef:\n",
      "    docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
      "    if docstring.startswith(\"No docstring\"):\n",
      "        return updated_node\n",
      "\n",
      "    return updated_node.with_changes(\n",
      "        body=updated_node.body.with_changes(\n",
      "            body=[\n",
      "                stmt\n",
      "                for stmt in updated_node.body.body\n",
      "                if not (\n",
      "                    isinstance(stmt, cst.SimpleStatementLine)\n",
      "                    and any(\n",
      "                        isinstance(expr, cst.Expr)\n",
      "                        and isinstance(expr.value, cst.SimpleString)\n",
      "                        for expr in stmt.body\n",
      "                    )\n",
      "                )\n",
      "            ]\n",
      "        )\n",
      "    )\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"isinstance\"\n",
      "\t\"any\"\n",
      "\t\"isinstance\"\n",
      "\t\"isinstance\"\n",
      "]\n",
      "\n",
      "\n",
      "def _process_file(self, file_path: str):\n",
      "    \"\"\"This method is called for every file in the directory.\n",
      "        It does the following:\n",
      "        1. Reads the file\n",
      "        2. Parses the file\n",
      "        3. Visits the file with the visitor\n",
      "        \"\"\"\n",
      "    #get current number of nodes in visitor\n",
      "    current_node_count = self.visitor.function_count + self.visitor.class_count\n",
      "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
      "        source_code = file.read()\n",
      "\n",
      "    try:\n",
      "        tree = cst.parse_module(source_code)\n",
      "    except cst.ParserSyntaxError:\n",
      "        print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
      "        return\n",
      "\n",
      "    tree.visit(self.visitor)\n",
      "    #calculate how many new nodes were added\n",
      "    new_node_counter = self.visitor.function_count + self.visitor.class_count - current_node_count\n",
      "    self.visitor.filename_map.extend([file_path]*new_node_counter)\n",
      "    # Remove docstrings if specified\n",
      "    if self.remove_docstrings:\n",
      "        source_code = self.remove_docstring(source_code, tree)\n",
      "\n",
      "    # Minify the code if specified\n",
      "    if self.minify_code:\n",
      "        minifier = PythonMinifier(source_code)\n",
      "        source_code = minifier.get_minified_code()\n",
      "\n",
      "    # Add the processed code to the corresponding list in the visitor\n",
      "    self.visitor.function_source_codes.append(source_code)\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"print\"\n",
      "\t\"PythonMinifier…\n",
      "]\n",
      "\n",
      "\n",
      "def process_file(self, file_path: str):\n",
      "    \"\"\"This method is called for every file in the directory.\n",
      "        It does the following:\n",
      "        1. Runs flake8 on the file\n",
      "        if flake8 returns a non-zero exit code, it means the file has a syntax error\n",
      "        2. Reads the file\n",
      "        3. Parses the file\n",
      "        4. Visits the file with the visitor\n",
      "\n",
      "        \"\"\"\n",
      "    result = subprocess.run(\n",
      "        [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
      "    )\n",
      "\n",
      "    if result.returncode != 0:\n",
      "        print(f\"Skipping file with syntax error: {file_path}\")\n",
      "        print(result.stderr.decode(\"utf-8\"))\n",
      "        return\n",
      "\n",
      "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
      "        source_code = f.read()\n",
      "\n",
      "    try:\n",
      "        tree = cst.parse_module(source_code)\n",
      "        tree.visit(self.visitor)\n",
      "    except cst.ParserSyntaxError as e:\n",
      "        print(f\"Syntax error: {e}\")\n",
      "        print(f\"Skipping file with syntax error: {file_path}\")\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"open\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def process_directory(\n",
      "    self,\n",
      ") -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\n",
      "    \"\"\"This method is called for every directory.\n",
      "        It does the following:\n",
      "        1. Gets all the python files in the directory\n",
      "        2. Processes each file\n",
      "        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\n",
      "        \"\"\"\n",
      "\n",
      "    python_files = self.get_files_with_extension(\".py\")\n",
      "\n",
      "    for file_path in python_files:\n",
      "        self._process_file(file_path)\n",
      "\n",
      "    result_dict = {\n",
      "        'function_source_codes': self.visitor.function_source_codes,\n",
      "        'function_nodes': self.visitor.function_nodes,\n",
      "        'class_source_codes': self.visitor.class_source_codes,\n",
      "        'class_nodes': self.visitor.class_nodes,\n",
      "        'file_map': self.visitor.filename_map,\n",
      "        'full_nodes': self.visitor.full_node_list,\n",
      "        'full_source': self.visitor.full_source_list\n",
      "    }\n",
      "\n",
      "\n",
      "    return result_dict\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class IssueParser:\n",
      "    def __init__(self, repo_name):\n",
      "        self.g = Github()\n",
      "        self.repo = self.g.get_repo(repo_name)\n",
      "\n",
      "    def get_issues(self, state=\"open\"):\n",
      "        \"\"\"\n",
      "        Returns a list of all issues in the repo with the specified state.\n",
      "        \"\"\"\n",
      "        issues = []\n",
      "        for issue in self.repo.get_issues(state=state):\n",
      "            issues.append(issue)\n",
      "        return issues\n",
      "\n",
      "    def parse_issues(self, state=\"open\"):\n",
      "        \"\"\"\n",
      "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
      "        Each dict contains the issue number, title, body, and labels.\n",
      "        \"\"\"\n",
      "        parsed_issues = []\n",
      "        issues = self.get_issues(state=state)\n",
      "        for issue in issues:\n",
      "            parsed_issue = {\n",
      "                \"number\": issue.number,\n",
      "                \"title\": issue.title,\n",
      "                \"body\": issue.body,\n",
      "                \"labels\": [label.name for label in issue.labels],\n",
      "            }\n",
      "            parsed_issues.append(parsed_issue)\n",
      "        return parsed_issues\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Github\"\n",
      "]\n",
      "\n",
      "def __init__(self, repo_name):\n",
      "    self.g = Github()\n",
      "    self.repo = self.g.get_repo(repo_name)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Github\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_issues(self, state=\"open\"):\n",
      "    \"\"\"\n",
      "        Returns a list of all issues in the repo with the specified state.\n",
      "        \"\"\"\n",
      "    issues = []\n",
      "    for issue in self.repo.get_issues(state=state):\n",
      "        issues.append(issue)\n",
      "    return issues\n",
      "\n",
      "\n",
      "\n",
      "def parse_issues(self, state=\"open\"):\n",
      "    \"\"\"\n",
      "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
      "        Each dict contains the issue number, title, body, and labels.\n",
      "        \"\"\"\n",
      "    parsed_issues = []\n",
      "    issues = self.get_issues(state=state)\n",
      "    for issue in issues:\n",
      "        parsed_issue = {\n",
      "            \"number\": issue.number,\n",
      "            \"title\": issue.title,\n",
      "            \"body\": issue.body,\n",
      "            \"labels\": [label.name for label in issue.labels],\n",
      "        }\n",
      "        parsed_issues.append(parsed_issue)\n",
      "    return parsed_issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class CommitParser:\n",
      "    def __init__(self, repo_name):\n",
      "        self.g = Github()\n",
      "        self.repo = self.g.get_repo(repo_name)\n",
      "\n",
      "    def get_commits(self):\n",
      "        \"\"\"\n",
      "        Returns a list of all commits in the main branch of the repository.\n",
      "        \"\"\"\n",
      "        commits = []\n",
      "        branch = self.repo.get_branch(\"main\")\n",
      "        for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
      "            commits.append(commit)\n",
      "        return commits\n",
      "\n",
      "    def parse_commits(self):\n",
      "        \"\"\"\n",
      "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
      "        Each dict contains the commit sha, commit message, and author information.\n",
      "        \"\"\"\n",
      "        parsed_commits = []\n",
      "        commits = self.get_commits()\n",
      "        for commit in commits:\n",
      "            parsed_commit = {\n",
      "                \"sha\": commit.sha,\n",
      "                \"message\": commit.commit.message,\n",
      "                \"author\": {\n",
      "                    \"name\": commit.commit.author.name,\n",
      "                    \"email\": commit.commit.author.email,\n",
      "                    \"date\": commit.commit.author.date,\n",
      "                },\n",
      "            }\n",
      "            parsed_commits.append(parsed_commit)\n",
      "        return parsed_commits\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Github\"\n",
      "]\n",
      "\n",
      "def __init__(self, repo_name):\n",
      "    self.g = Github()\n",
      "    self.repo = self.g.get_repo(repo_name)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"Github\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_commits(self):\n",
      "    \"\"\"\n",
      "        Returns a list of all commits in the main branch of the repository.\n",
      "        \"\"\"\n",
      "    commits = []\n",
      "    branch = self.repo.get_branch(\"main\")\n",
      "    for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
      "        commits.append(commit)\n",
      "    return commits\n",
      "\n",
      "\n",
      "\n",
      "def parse_commits(self):\n",
      "    \"\"\"\n",
      "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
      "        Each dict contains the commit sha, commit message, and author information.\n",
      "        \"\"\"\n",
      "    parsed_commits = []\n",
      "    commits = self.get_commits()\n",
      "    for commit in commits:\n",
      "        parsed_commit = {\n",
      "            \"sha\": commit.sha,\n",
      "            \"message\": commit.commit.message,\n",
      "            \"author\": {\n",
      "                \"name\": commit.commit.author.name,\n",
      "                \"email\": commit.commit.author.email,\n",
      "                \"date\": commit.commit.author.date,\n",
      "            },\n",
      "        }\n",
      "        parsed_commits.append(parsed_commit)\n",
      "    return parsed_commits\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class DirectoryProcessor:\n",
      "    def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\n",
      "        self.directory_path = directory_path\n",
      "        self.visitor = visitor\n",
      "\n",
      "    def _process_file(self, file_path: str):\n",
      "        with open(file_path, \"r\") as file:\n",
      "            source_code = file.read()\n",
      "\n",
      "        try:\n",
      "            tree = cst.parse_module(source_code)\n",
      "        except cst.ParserSyntaxError:\n",
      "            print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
      "            return\n",
      "\n",
      "        tree.visit(self.visitor)\n",
      "\n",
      "    def process_file(self, file_path: str):\n",
      "        # Run flake8 on the file\n",
      "        result = subprocess.run(\n",
      "            [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
      "        )\n",
      "\n",
      "        if result.returncode != 0:\n",
      "            print(f\"Skipping file with syntax error: {file_path}\")\n",
      "            print(result.stderr.decode(\"utf-8\"))\n",
      "            return\n",
      "\n",
      "        with open(file_path, \"r\") as f:\n",
      "            source_code = f.read()\n",
      "\n",
      "        try:\n",
      "            tree = cst.parse_module(source_code)\n",
      "            tree.visit(self.visitor)\n",
      "        except cst.ParserSyntaxError as e:\n",
      "            print(f\"Syntax error: {e}\")\n",
      "            print(f\"Skipping file with syntax error: {file_path}\")\n",
      "\n",
      "    def process_directory(self) -> List[str]:\n",
      "        function_source_codes = []\n",
      "        class_source_codes = []\n",
      "\n",
      "        for root, _, files in os.walk(self.directory_path):\n",
      "            for file in files:\n",
      "                if file.endswith(\".py\"):\n",
      "                    file_path = os.path.join(root, file)\n",
      "                    self._process_file(file_path)\n",
      "\n",
      "        function_source_codes = self.visitor.function_source_codes\n",
      "        function_nodes = self.visitor.function_nodes\n",
      "        class_source_codes = self.visitor.class_source_codes\n",
      "        class_nodes = self.visitor.class_nodes\n",
      "\n",
      "        return function_source_codes, class_source_codes, function_nodes, class_nodes\n",
      "\n",
      "    def clone_repo(self, repo_url):\n",
      "        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
      "        target_directory = os.path.join(self.directory_path, repo_name)\n",
      "\n",
      "        if os.path.exists(target_directory):\n",
      "            shutil.rmtree(target_directory)\n",
      "\n",
      "        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
      "\n",
      "        return target_directory\n",
      "\n",
      "Function calls: shape: (8,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"FunctionAndCla…\n",
      "\t\"open\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"open\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\n",
      "    self.directory_path = directory_path\n",
      "    self.visitor = visitor\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"FunctionAndCla…\n",
      "]\n",
      "\n",
      "\n",
      "def _process_file(self, file_path: str):\n",
      "    with open(file_path, \"r\") as file:\n",
      "        source_code = file.read()\n",
      "\n",
      "    try:\n",
      "        tree = cst.parse_module(source_code)\n",
      "    except cst.ParserSyntaxError:\n",
      "        print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
      "        return\n",
      "\n",
      "    tree.visit(self.visitor)\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def process_file(self, file_path: str):\n",
      "    # Run flake8 on the file\n",
      "    result = subprocess.run(\n",
      "        [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
      "    )\n",
      "\n",
      "    if result.returncode != 0:\n",
      "        print(f\"Skipping file with syntax error: {file_path}\")\n",
      "        print(result.stderr.decode(\"utf-8\"))\n",
      "        return\n",
      "\n",
      "    with open(file_path, \"r\") as f:\n",
      "        source_code = f.read()\n",
      "\n",
      "    try:\n",
      "        tree = cst.parse_module(source_code)\n",
      "        tree.visit(self.visitor)\n",
      "    except cst.ParserSyntaxError as e:\n",
      "        print(f\"Syntax error: {e}\")\n",
      "        print(f\"Skipping file with syntax error: {file_path}\")\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"open\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def process_directory(self) -> List[str]:\n",
      "    function_source_codes = []\n",
      "    class_source_codes = []\n",
      "\n",
      "    for root, _, files in os.walk(self.directory_path):\n",
      "        for file in files:\n",
      "            if file.endswith(\".py\"):\n",
      "                file_path = os.path.join(root, file)\n",
      "                self._process_file(file_path)\n",
      "\n",
      "    function_source_codes = self.visitor.function_source_codes\n",
      "    function_nodes = self.visitor.function_nodes\n",
      "    class_source_codes = self.visitor.class_source_codes\n",
      "    class_nodes = self.visitor.class_nodes\n",
      "\n",
      "    return function_source_codes, class_source_codes, function_nodes, class_nodes\n",
      "\n",
      "\n",
      "\n",
      "def clone_repo(self, repo_url):\n",
      "    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
      "    target_directory = os.path.join(self.directory_path, repo_name)\n",
      "\n",
      "    if os.path.exists(target_directory):\n",
      "        shutil.rmtree(target_directory)\n",
      "\n",
      "    subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
      "\n",
      "    return target_directory\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class GitHubUserProcessor:\n",
      "    def __init__(\n",
      "        self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
      "    ):\n",
      "        self.username = username\n",
      "        self.repo_name = repo_name\n",
      "        self.github = Github()\n",
      "        self.directory_processor = None\n",
      "        self.function_source_codes = []\n",
      "        self.class_source_codes = []\n",
      "        self.visitor = visitor\n",
      "\n",
      "    def get_public_repos(self):\n",
      "        user = self.github.get_user(self.username)\n",
      "        return user.get_repos()\n",
      "\n",
      "    def process_repos(self, base_directory):\n",
      "        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
      "        for repo in self.get_public_repos():\n",
      "            if not repo.private:\n",
      "                print(f\"Processing repo: {repo.name}\")\n",
      "                repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
      "                (\n",
      "                    function_source_codes,\n",
      "                    class_source_codes,\n",
      "                ) = self.directory_processor.process_directory()\n",
      "                self.function_source_codes.extend(function_source_codes)\n",
      "                self.class_source_codes.extend(class_source_codes)\n",
      "                shutil.rmtree(repo_path)\n",
      "\n",
      "        return self.directory_processor\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"FunctionAndCla…\n",
      "\t\"Github\"\n",
      "\t\"DirectoryProce…\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
      "):\n",
      "    self.username = username\n",
      "    self.repo_name = repo_name\n",
      "    self.github = Github()\n",
      "    self.directory_processor = None\n",
      "    self.function_source_codes = []\n",
      "    self.class_source_codes = []\n",
      "    self.visitor = visitor\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"FunctionAndCla…\n",
      "\t\"Github\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_public_repos(self):\n",
      "    user = self.github.get_user(self.username)\n",
      "    return user.get_repos()\n",
      "\n",
      "\n",
      "\n",
      "def process_repos(self, base_directory):\n",
      "    self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
      "    for repo in self.get_public_repos():\n",
      "        if not repo.private:\n",
      "            print(f\"Processing repo: {repo.name}\")\n",
      "            repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
      "            (\n",
      "                function_source_codes,\n",
      "                class_source_codes,\n",
      "            ) = self.directory_processor.process_directory()\n",
      "            self.function_source_codes.extend(function_source_codes)\n",
      "            self.class_source_codes.extend(class_source_codes)\n",
      "            shutil.rmtree(repo_path)\n",
      "\n",
      "    return self.directory_processor\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"DirectoryProce…\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class GitHubRepoProcessor:\n",
      "    def __init__(\n",
      "        self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
      "    ):\n",
      "        self.username = username\n",
      "        self.repo_name = repo_name\n",
      "        self.github = Github()\n",
      "        self.directory_processor = None\n",
      "        self.function_source_codes = []\n",
      "        self.function_nodes = []\n",
      "        self.class_source_codes = []\n",
      "        self.class_nodes = []\n",
      "        self.visitor = visitor\n",
      "\n",
      "    def get_repo(self, repo_name):\n",
      "        user = self.github.get_user(self.username)\n",
      "        return user.get_repo(repo_name)\n",
      "\n",
      "    def process_repo(self, base_directory):\n",
      "        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
      "        repo = self.get_repo(self.repo_name)\n",
      "        print(f\"Processing repo: {self.repo_name}\")\n",
      "        repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
      "        (\n",
      "            function_source_codes,\n",
      "            class_source_codes,\n",
      "            function_nodes,\n",
      "            class_nodes,\n",
      "        ) = self.directory_processor.process_directory()\n",
      "        self.function_source_codes.extend(function_source_codes)\n",
      "        self.function_nodes.extend(function_nodes)\n",
      "        self.class_source_codes.extend(class_source_codes)\n",
      "        self.class_nodes.extend(class_nodes)\n",
      "        shutil.rmtree(repo_path)\n",
      "        return self.directory_processor\n",
      "\n",
      "    def get_values(self):\n",
      "        # concatenate the function and class source codes\n",
      "        self.function_source_codes.extend(self.class_source_codes)\n",
      "        self.function_nodes.extend(self.class_nodes)\n",
      "        return self.function_source_codes, self.function_nodes\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"FunctionAndCla…\n",
      "\t\"Github\"\n",
      "\t\"DirectoryProce…\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "def __init__(\n",
      "    self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
      "):\n",
      "    self.username = username\n",
      "    self.repo_name = repo_name\n",
      "    self.github = Github()\n",
      "    self.directory_processor = None\n",
      "    self.function_source_codes = []\n",
      "    self.function_nodes = []\n",
      "    self.class_source_codes = []\n",
      "    self.class_nodes = []\n",
      "    self.visitor = visitor\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"FunctionAndCla…\n",
      "\t\"Github\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_repo(self, repo_name):\n",
      "    user = self.github.get_user(self.username)\n",
      "    return user.get_repo(repo_name)\n",
      "\n",
      "\n",
      "\n",
      "def process_repo(self, base_directory):\n",
      "    self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
      "    repo = self.get_repo(self.repo_name)\n",
      "    print(f\"Processing repo: {self.repo_name}\")\n",
      "    repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
      "    (\n",
      "        function_source_codes,\n",
      "        class_source_codes,\n",
      "        function_nodes,\n",
      "        class_nodes,\n",
      "    ) = self.directory_processor.process_directory()\n",
      "    self.function_source_codes.extend(function_source_codes)\n",
      "    self.function_nodes.extend(function_nodes)\n",
      "    self.class_source_codes.extend(class_source_codes)\n",
      "    self.class_nodes.extend(class_nodes)\n",
      "    shutil.rmtree(repo_path)\n",
      "    return self.directory_processor\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"DirectoryProce…\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def get_values(self):\n",
      "    # concatenate the function and class source codes\n",
      "    self.function_source_codes.extend(self.class_source_codes)\n",
      "    self.function_nodes.extend(self.class_nodes)\n",
      "    return self.function_source_codes, self.function_nodes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class PubmedAPI:\n",
      "    def __init__(self):\n",
      "        self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
      "\n",
      "    def search(self, query, max_results=10):\n",
      "        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
      "        record = Entrez.read(handle)\n",
      "        handle.close()\n",
      "        return record[\"IdList\"]\n",
      "\n",
      "    def fetch_abstract(self, pubmed_id):\n",
      "        handle = Entrez.efetch(\n",
      "            db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\"\n",
      "        )\n",
      "        abstract = handle.read()\n",
      "        handle.close()\n",
      "        return abstract\n",
      "\n",
      "    def fetch_pmc_full_text(self, pubmed_id):\n",
      "        # Get the PMC ID for the PubMed ID\n",
      "        handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\n",
      "        record = Entrez.read(handle)\n",
      "        handle.close()\n",
      "        pmc_id = None\n",
      "        for link in record[0][\"LinkSetDb\"]:\n",
      "            if link[\"DbTo\"] == \"pmc\":\n",
      "                pmc_id = link[\"Link\"][0][\"Id\"]\n",
      "                break\n",
      "\n",
      "        if not pmc_id:\n",
      "            return None\n",
      "\n",
      "        # Fetch the PMC article XML\n",
      "        handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\n",
      "        xml_content = handle.read()\n",
      "        handle.close()\n",
      "\n",
      "        # Parse the XML and extract the full text\n",
      "        soup = BeautifulSoup(xml_content, \"xml\")\n",
      "        full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
      "\n",
      "        return full_text\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"BeautifulSoup\"\n",
      "]\n",
      "\n",
      "def __init__(self):\n",
      "    self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
      "\n",
      "\n",
      "\n",
      "def search(self, query, max_results=10):\n",
      "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
      "    record = Entrez.read(handle)\n",
      "    handle.close()\n",
      "    return record[\"IdList\"]\n",
      "\n",
      "\n",
      "\n",
      "def fetch_abstract(self, pubmed_id):\n",
      "    handle = Entrez.efetch(\n",
      "        db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\"\n",
      "    )\n",
      "    abstract = handle.read()\n",
      "    handle.close()\n",
      "    return abstract\n",
      "\n",
      "\n",
      "\n",
      "def fetch_pmc_full_text(self, pubmed_id):\n",
      "    # Get the PMC ID for the PubMed ID\n",
      "    handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\n",
      "    record = Entrez.read(handle)\n",
      "    handle.close()\n",
      "    pmc_id = None\n",
      "    for link in record[0][\"LinkSetDb\"]:\n",
      "        if link[\"DbTo\"] == \"pmc\":\n",
      "            pmc_id = link[\"Link\"][0][\"Id\"]\n",
      "            break\n",
      "\n",
      "    if not pmc_id:\n",
      "        return None\n",
      "\n",
      "    # Fetch the PMC article XML\n",
      "    handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\n",
      "    xml_content = handle.read()\n",
      "    handle.close()\n",
      "\n",
      "    # Parse the XML and extract the full text\n",
      "    soup = BeautifulSoup(xml_content, \"xml\")\n",
      "    full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
      "\n",
      "    return full_text\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"BeautifulSoup\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class PubmedParser:\n",
      "    def __init__(self):\n",
      "        self.api = PubmedAPI()\n",
      "\n",
      "    def parse_papers(self, query, max_results=10):\n",
      "        pubmed_ids = self.api.search(query, max_results)\n",
      "        paper_list = []\n",
      "        for pubmed_id in pubmed_ids:\n",
      "            paper_dict = {}\n",
      "            paper_dict[\"pubmed_id\"] = pubmed_id\n",
      "            paper_dict[\"abstract\"] = self.api.fetch_abstract(pubmed_id)\n",
      "            paper_dict[\"content\"] = self.api.fetch_pmc_full_text(pubmed_id)\n",
      "            paper_list.append(paper_dict)\n",
      "        return paper_list\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"PubmedAPI\"\n",
      "]\n",
      "\n",
      "def __init__(self):\n",
      "    self.api = PubmedAPI()\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"PubmedAPI\"\n",
      "]\n",
      "\n",
      "\n",
      "def parse_papers(self, query, max_results=10):\n",
      "    pubmed_ids = self.api.search(query, max_results)\n",
      "    paper_list = []\n",
      "    for pubmed_id in pubmed_ids:\n",
      "        paper_dict = {}\n",
      "        paper_dict[\"pubmed_id\"] = pubmed_id\n",
      "        paper_dict[\"abstract\"] = self.api.fetch_abstract(pubmed_id)\n",
      "        paper_dict[\"content\"] = self.api.fetch_pmc_full_text(pubmed_id)\n",
      "        paper_list.append(paper_dict)\n",
      "    return paper_list\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class ArxivVanityParser:\n",
      "    def __init__(self):\n",
      "        self.base_url = \"https://www.arxiv-vanity.com/\"\n",
      "\n",
      "    def _get_vanity_url(self, arxiv_id):\n",
      "        return urljoin(self.base_url, \"papers/\" + arxiv_id)\n",
      "\n",
      "    def _fetch_html(self, url):\n",
      "        response = requests.get(url)\n",
      "        if response.status_code == 200:\n",
      "            return response.text\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    def _extract_main_content(self, html):\n",
      "        soup = BeautifulSoup(html, \"html.parser\")\n",
      "        paragraphs = soup.find_all(\"div\", {\"class\": \"ltx_para\"})\n",
      "        content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\n",
      "        return content\n",
      "\n",
      "    def parse_paper(self, arxiv_id):\n",
      "        vanity_url = self._get_vanity_url(arxiv_id)\n",
      "        html = self._fetch_html(vanity_url)\n",
      "        if html is not None:\n",
      "            return self._extract_main_content(html)\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"urljoin\"\n",
      "\t\"BeautifulSoup\"\n",
      "\t\"enumerate\"\n",
      "]\n",
      "\n",
      "def __init__(self):\n",
      "    self.base_url = \"https://www.arxiv-vanity.com/\"\n",
      "\n",
      "\n",
      "\n",
      "def _get_vanity_url(self, arxiv_id):\n",
      "    return urljoin(self.base_url, \"papers/\" + arxiv_id)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"urljoin\"\n",
      "]\n",
      "\n",
      "\n",
      "def _fetch_html(self, url):\n",
      "    response = requests.get(url)\n",
      "    if response.status_code == 200:\n",
      "        return response.text\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "\n",
      "def _extract_main_content(self, html):\n",
      "    soup = BeautifulSoup(html, \"html.parser\")\n",
      "    paragraphs = soup.find_all(\"div\", {\"class\": \"ltx_para\"})\n",
      "    content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\n",
      "    return content\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"BeautifulSoup\"\n",
      "\t\"enumerate\"\n",
      "]\n",
      "\n",
      "\n",
      "def parse_paper(self, arxiv_id):\n",
      "    vanity_url = self._get_vanity_url(arxiv_id)\n",
      "    html = self._fetch_html(vanity_url)\n",
      "    if html is not None:\n",
      "        return self._extract_main_content(html)\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class ArxivAPI:\n",
      "    def __init__(self):\n",
      "        self.base_url = \"http://export.arxiv.org/api/query?\"\n",
      "        self.pdf_download_url = \"https://arxiv.org/pdf/\"\n",
      "\n",
      "    def search(self, query, max_results=10):\n",
      "        url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\n",
      "        response = requests.get(url)\n",
      "        if response.status_code == 200:\n",
      "            return response.text\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    def download_pdf(self, paper_key, save_directory=\"./\"):\n",
      "        pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\n",
      "        response = requests.get(pdf_url)\n",
      "        if response.status_code == 200:\n",
      "            with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\n",
      "                f.write(response.content)\n",
      "            print(f\"PDF for {paper_key} downloaded successfully.\")\n",
      "        else:\n",
      "            print(f\"Error downloading PDF for {paper_key}.\")\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "def __init__(self):\n",
      "    self.base_url = \"http://export.arxiv.org/api/query?\"\n",
      "    self.pdf_download_url = \"https://arxiv.org/pdf/\"\n",
      "\n",
      "\n",
      "\n",
      "def search(self, query, max_results=10):\n",
      "    url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\n",
      "    response = requests.get(url)\n",
      "    if response.status_code == 200:\n",
      "        return response.text\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "\n",
      "def download_pdf(self, paper_key, save_directory=\"./\"):\n",
      "    pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\n",
      "    response = requests.get(pdf_url)\n",
      "    if response.status_code == 200:\n",
      "        with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\n",
      "            f.write(response.content)\n",
      "        print(f\"PDF for {paper_key} downloaded successfully.\")\n",
      "    else:\n",
      "        print(f\"Error downloading PDF for {paper_key}.\")\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class ArxivParser:\n",
      "    def __init__(self):\n",
      "        self.api = ArxivAPI()\n",
      "        self.vanity_parser = ArxivVanityParser()\n",
      "\n",
      "    def _parse_arxiv_id(self, url):\n",
      "        return url.split(\"/\")[-1]\n",
      "\n",
      "    def parse_papers(self, query, max_results=10):\n",
      "        search_results = self.api.search(query, max_results)\n",
      "        if search_results is not None:\n",
      "            soup = BeautifulSoup(search_results, \"html.parser\")\n",
      "            entries = soup.find_all(\"entry\")\n",
      "            paper_list = []\n",
      "            for entry in entries:\n",
      "                paper_dict = {}\n",
      "                arxiv_id = self._parse_arxiv_id(entry.id.string)\n",
      "                paper_dict[\"arxiv_id\"] = arxiv_id\n",
      "                paper_dict[\"title\"] = entry.title.string\n",
      "                paper_dict[\"summary\"] = entry.summary.string\n",
      "                paper_dict[\"content\"] = self.vanity_parser.parse_paper(str(arxiv_id))\n",
      "                if paper_dict[\"content\"] == None:\n",
      "                    continue\n",
      "                paper_list.append(paper_dict)\n",
      "            return paper_list\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"ArxivAPI\"\n",
      "\t\"ArxivVanityPar…\n",
      "\t\"BeautifulSoup\"\n",
      "\t\"str\"\n",
      "]\n",
      "\n",
      "def __init__(self):\n",
      "    self.api = ArxivAPI()\n",
      "    self.vanity_parser = ArxivVanityParser()\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"ArxivAPI\"\n",
      "\t\"ArxivVanityPar…\n",
      "]\n",
      "\n",
      "\n",
      "def _parse_arxiv_id(self, url):\n",
      "    return url.split(\"/\")[-1]\n",
      "\n",
      "\n",
      "\n",
      "def parse_papers(self, query, max_results=10):\n",
      "    search_results = self.api.search(query, max_results)\n",
      "    if search_results is not None:\n",
      "        soup = BeautifulSoup(search_results, \"html.parser\")\n",
      "        entries = soup.find_all(\"entry\")\n",
      "        paper_list = []\n",
      "        for entry in entries:\n",
      "            paper_dict = {}\n",
      "            arxiv_id = self._parse_arxiv_id(entry.id.string)\n",
      "            paper_dict[\"arxiv_id\"] = arxiv_id\n",
      "            paper_dict[\"title\"] = entry.title.string\n",
      "            paper_dict[\"summary\"] = entry.summary.string\n",
      "            paper_dict[\"content\"] = self.vanity_parser.parse_paper(str(arxiv_id))\n",
      "            if paper_dict[\"content\"] == None:\n",
      "                continue\n",
      "            paper_list.append(paper_dict)\n",
      "        return paper_list\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"BeautifulSoup\"\n",
      "\t\"str\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class SubjectPerspectiveAnalyzer:\n",
      "    def __init__(self, chatbot: 'Chat'):\n",
      "        self.chatbot = chatbot\n",
      "\n",
      "    def analyze_subject_perspective(self, user_subject: str, user_perspective: str) -> dict:\n",
      "        prompts = [\n",
      "            f\"Generate ideas and concepts that explore the connection between {user_subject} and {user_perspective}, considering both traditional and unconventional approaches.\",\n",
      "            f\"List key concepts or topics that would help analyze {user_subject} through the lens of {user_perspective}, including relevant principles, theories, or models.\",\n",
      "            f\"Identify potential areas or research topics where {user_subject} and {user_perspective} intersect, highlighting intriguing or innovative perspectives.\"\n",
      "        ]\n",
      "\n",
      "        output = {}\n",
      "\n",
      "        with RateLimitedThreadPoolExecutor(max_workers=3, calls_per_minute=20, verbose = False) as executor:\n",
      "            future_to_prompt = {executor.submit(self._analyze_prompt, prompt): prompt for prompt in prompts}\n",
      "            for future in as_completed(future_to_prompt):\n",
      "                prompt = future_to_prompt[future]\n",
      "                try:\n",
      "                    output[prompt] = future.result()\n",
      "                except Exception as exc:\n",
      "                    counter = 0\n",
      "                    print(f'An exception occurred while analyzing prompt \"{prompt}\": {exc}')\n",
      "                    while counter < 3:\n",
      "                        try:\n",
      "                            output[prompt] = self._analyze_prompt(prompt)\n",
      "                            break\n",
      "                        except Exception as exc:\n",
      "                            counter += 1\n",
      "                            print(f'An exception occurred while analyzing prompt \"{prompt}\": {exc}')\n",
      "                    if counter == 3:\n",
      "                        output[prompt] = []\n",
      "\n",
      "        return output\n",
      "\n",
      "    def _analyze_prompt(self, prompt: str) -> list:\n",
      "        response = self.chatbot.reply(prompt, verbose=False)\n",
      "        return self._format_response(response)\n",
      "\n",
      "    def _format_response(self, response: str) -> list:\n",
      "        formatted_response = response.strip().split('\\n')\n",
      "        return formatted_response\n",
      "    \n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"RateLimitedThr…\n",
      "\t\"as_completed\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "def __init__(self, chatbot: 'Chat'):\n",
      "    self.chatbot = chatbot\n",
      "\n",
      "\n",
      "\n",
      "def analyze_subject_perspective(self, user_subject: str, user_perspective: str) -> dict:\n",
      "    prompts = [\n",
      "        f\"Generate ideas and concepts that explore the connection between {user_subject} and {user_perspective}, considering both traditional and unconventional approaches.\",\n",
      "        f\"List key concepts or topics that would help analyze {user_subject} through the lens of {user_perspective}, including relevant principles, theories, or models.\",\n",
      "        f\"Identify potential areas or research topics where {user_subject} and {user_perspective} intersect, highlighting intriguing or innovative perspectives.\"\n",
      "    ]\n",
      "\n",
      "    output = {}\n",
      "\n",
      "    with RateLimitedThreadPoolExecutor(max_workers=3, calls_per_minute=20, verbose = False) as executor:\n",
      "        future_to_prompt = {executor.submit(self._analyze_prompt, prompt): prompt for prompt in prompts}\n",
      "        for future in as_completed(future_to_prompt):\n",
      "            prompt = future_to_prompt[future]\n",
      "            try:\n",
      "                output[prompt] = future.result()\n",
      "            except Exception as exc:\n",
      "                counter = 0\n",
      "                print(f'An exception occurred while analyzing prompt \"{prompt}\": {exc}')\n",
      "                while counter < 3:\n",
      "                    try:\n",
      "                        output[prompt] = self._analyze_prompt(prompt)\n",
      "                        break\n",
      "                    except Exception as exc:\n",
      "                        counter += 1\n",
      "                        print(f'An exception occurred while analyzing prompt \"{prompt}\": {exc}')\n",
      "                if counter == 3:\n",
      "                    output[prompt] = []\n",
      "\n",
      "    return output\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"RateLimitedThr…\n",
      "\t\"as_completed\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def _analyze_prompt(self, prompt: str) -> list:\n",
      "    response = self.chatbot.reply(prompt, verbose=False)\n",
      "    return self._format_response(response)\n",
      "\n",
      "\n",
      "\n",
      "def _format_response(self, response: str) -> list:\n",
      "    formatted_response = response.strip().split('\\n')\n",
      "    return formatted_response\n",
      "\n",
      "\n",
      "\n",
      "class Ideation:\n",
      "    def __init__(self, memory_index: MemoryIndex):\n",
      "        self.memory_index = memory_index\n",
      "\n",
      "    def retrieve_ideas(self, queries: Dict, k: int = 30, max_tokens: int = 10000):\n",
      "        \"\"\"\n",
      "        Generate ideas based on the given list of queries.\n",
      "\n",
      "        Args:\n",
      "            queries: The list of queries for generating ideas.\n",
      "            k: The number of top search results to consider.\n",
      "            max_tokens: The maximum number of tokens to return.\n",
      "\n",
      "        Returns:\n",
      "            A list of ideas generated based on the queries.\n",
      "        \"\"\"\n",
      "        ideas = []\n",
      "        for key, queries in queries.items():\n",
      "            for query in queries:\n",
      "                if query is None or len(query) < 10:\n",
      "                    continue\n",
      "                top_k_hints, scores, indices = self.memory_index.token_bound_query(\n",
      "                    query, k=k, max_tokens=max_tokens\n",
      "                )\n",
      "                last_query = self.memory_index.query_history[-1]\n",
      "                hints_tokens = last_query[\"hints_tokens\"]\n",
      "                returned_tokens = last_query[\"returned_tokens\"]\n",
      "\n",
      "                ideas.append({\"key_task\": key, \"query\": query, \"hints\": top_k_hints, \"scores\": scores, \"hints_tokens\": hints_tokens, \"returned_tokens\": returned_tokens})\n",
      "\n",
      "        return ideas\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "def __init__(self, memory_index: MemoryIndex):\n",
      "    self.memory_index = memory_index\n",
      "\n",
      "\n",
      "\n",
      "def retrieve_ideas(self, queries: Dict, k: int = 30, max_tokens: int = 10000):\n",
      "    \"\"\"\n",
      "        Generate ideas based on the given list of queries.\n",
      "\n",
      "        Args:\n",
      "            queries: The list of queries for generating ideas.\n",
      "            k: The number of top search results to consider.\n",
      "            max_tokens: The maximum number of tokens to return.\n",
      "\n",
      "        Returns:\n",
      "            A list of ideas generated based on the queries.\n",
      "        \"\"\"\n",
      "    ideas = []\n",
      "    for key, queries in queries.items():\n",
      "        for query in queries:\n",
      "            if query is None or len(query) < 10:\n",
      "                continue\n",
      "            top_k_hints, scores, indices = self.memory_index.token_bound_query(\n",
      "                query, k=k, max_tokens=max_tokens\n",
      "            )\n",
      "            last_query = self.memory_index.query_history[-1]\n",
      "            hints_tokens = last_query[\"hints_tokens\"]\n",
      "            returned_tokens = last_query[\"returned_tokens\"]\n",
      "\n",
      "            ideas.append({\"key_task\": key, \"query\": query, \"hints\": top_k_hints, \"scores\": scores, \"hints_tokens\": hints_tokens, \"returned_tokens\": returned_tokens})\n",
      "\n",
      "    return ideas\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "class IdeaCluster:\n",
      "    def __init__(self, ideas: list, max_tokens_per_cluster: int):\n",
      "        self.ideas = ideas\n",
      "        self.max_tokens_per_cluster = max_tokens_per_cluster\n",
      "        self.idea_index = self.create_idea_index()\n",
      "        self.cluster_labels = None\n",
      "\n",
      "    def create_idea_index(self):\n",
      "        gathered_docs = []\n",
      "        for idea in self.ideas:\n",
      "            for hint in idea[\"hints\"]:\n",
      "                gathered_docs.append(hint)\n",
      "        self.gathered_docs = set(gathered_docs)\n",
      "        idea_index = MemoryIndex(values=self.gathered_docs, is_batched=True, name=\"ideas\")\n",
      "        return idea_index\n",
      "\n",
      "    def cluster_embeddings(self, n_neighbors: int = 10, min_cluster_size: int = 5):\n",
      "        reducer = umap.UMAP(n_neighbors=n_neighbors)\n",
      "        reduced_embeddings = reducer.fit_transform(self.idea_index.get_all_embeddings())\n",
      "\n",
      "        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
      "        labels = clusterer.fit_predict(reduced_embeddings)\n",
      "\n",
      "        token_count_per_cluster = self.count_tokens_per_cluster(labels)\n",
      "        print(token_count_per_cluster)\n",
      "        if max(token_count_per_cluster.values()) <= self.max_tokens_per_cluster:\n",
      "            self.cluster_labels = labels\n",
      "            print(\"Clusters created successfully.\")\n",
      "        else:\n",
      "            print(\"Clusters exceed the maximum token count.\")\n",
      "\n",
      "    def count_tokens_per_cluster(self, labels):\n",
      "        token_count_per_cluster = {}\n",
      "\n",
      "        for label, doc in zip(labels, self.gathered_docs):\n",
      "            if label not in token_count_per_cluster:\n",
      "                token_count_per_cluster[label] = len(tokenizer.encode(doc))\n",
      "            else:\n",
      "                token_count_per_cluster[label] += len(tokenizer.encode(doc))\n",
      "        return token_count_per_cluster\n",
      "\n",
      "    def create_minimum_spanning_paths(self):\n",
      "        if self.cluster_labels is None:\n",
      "            raise ValueError(\"You must run cluster_embeddings() before creating minimum spanning paths.\")\n",
      "\n",
      "        unique_labels = np.unique(self.cluster_labels)\n",
      "        min_span_paths = []\n",
      "\n",
      "        for label in unique_labels:\n",
      "\n",
      "\n",
      "            # Get the indices of the current cluster\n",
      "            cluster_indices = np.where(self.cluster_labels == label)[0]\n",
      "\n",
      "            # Calculate the pairwise distances between embeddings in the cluster\n",
      "            cluster_embeddings = self.idea_index.embeddings[cluster_indices]\n",
      "            dist_matrix = squareform(pdist(cluster_embeddings))\n",
      "\n",
      "            # Create a graph from the distance matrix\n",
      "            graph = nx.from_numpy_array(dist_matrix)\n",
      "\n",
      "            # Compute the minimum spanning tree of the graph\n",
      "            min_span_tree = nx.minimum_spanning_tree(graph)\n",
      "\n",
      "            # Get the minimum spanning paths\n",
      "            min_span_paths_cluster = []\n",
      "            visited = set()\n",
      "            for u, v in min_span_tree.edges():\n",
      "                if u not in visited and v not in visited:\n",
      "                    orig_u = cluster_indices[u]\n",
      "                    orig_v = cluster_indices[v]\n",
      "                    min_span_paths_cluster.append(orig_u)\n",
      "                    visited.add(u)\n",
      "                    visited.add(v)\n",
      "            # Add the last node to complete the path\n",
      "            min_span_paths_cluster.append(orig_v)\n",
      "\n",
      "            min_span_paths.append(min_span_paths_cluster)\n",
      "\n",
      "        self.min_span_paths = min_span_paths\n",
      "\n",
      "    \n",
      "    def plot_embeddings_with_path(self):\n",
      "        paths = self.min_span_paths\n",
      "        embeddings = self.idea_index.embeddings\n",
      "        title = \"Minimum Spanning Paths\"\n",
      "        tsne = TSNE(n_components=2, random_state=42)\n",
      "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
      "\n",
      "        plt.figure(figsize=(10, 8))\n",
      "        colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n",
      "        for i, path in enumerate(paths):\n",
      "            path_embeddings = reduced_embeddings[path]\n",
      "            plt.scatter(\n",
      "                path_embeddings[:, 0],\n",
      "                path_embeddings[:, 1],\n",
      "                color=colors[i],\n",
      "                label=f\"Cluster {i}\",\n",
      "            )\n",
      "            for j in range(len(path) - 1):\n",
      "                plt.plot(\n",
      "                    [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n",
      "                    [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n",
      "                    color=colors[i],\n",
      "                )\n",
      "        plt.title(title)\n",
      "        plt.legend()\n",
      "        plt.show()\n",
      "    def get_clustered_ideas(self):\n",
      "        if self.cluster_labels is None:\n",
      "            raise ValueError(\"You must run cluster_embeddings() before getting clustered ideas.\")\n",
      "        \n",
      "        clustered_ideas = {}\n",
      "        for label, idea in zip(self.cluster_labels, self.idea_index.values):\n",
      "            if label not in clustered_ideas:\n",
      "                clustered_ideas[label] = [idea]\n",
      "            else:\n",
      "                clustered_ideas[label].append(idea)\n",
      "        \n",
      "        # Convert the dictionary to a list of lists (each list corresponds to a cluster)\n",
      "        return list(clustered_ideas.values())\n",
      "\n",
      "Function calls: shape: (21,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"set\"\n",
      "\t\"MemoryIndex\"\n",
      "\t\"print\"\n",
      "\t\"max\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"zip\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"squareform\"\n",
      "\t\"pdist\"\n",
      "\t\"set\"\n",
      "\t\"TSNE\"\n",
      "\t\"len\"\n",
      "\t\"enumerate\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "\t\"ValueError\"\n",
      "\t\"zip\"\n",
      "\t\"list\"\n",
      "]\n",
      "\n",
      "def __init__(self, ideas: list, max_tokens_per_cluster: int):\n",
      "    self.ideas = ideas\n",
      "    self.max_tokens_per_cluster = max_tokens_per_cluster\n",
      "    self.idea_index = self.create_idea_index()\n",
      "    self.cluster_labels = None\n",
      "\n",
      "\n",
      "\n",
      "def create_idea_index(self):\n",
      "    gathered_docs = []\n",
      "    for idea in self.ideas:\n",
      "        for hint in idea[\"hints\"]:\n",
      "            gathered_docs.append(hint)\n",
      "    self.gathered_docs = set(gathered_docs)\n",
      "    idea_index = MemoryIndex(values=self.gathered_docs, is_batched=True, name=\"ideas\")\n",
      "    return idea_index\n",
      "\n",
      "Function calls: shape: (2,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"set\"\n",
      "\t\"MemoryIndex\"\n",
      "]\n",
      "\n",
      "\n",
      "def cluster_embeddings(self, n_neighbors: int = 10, min_cluster_size: int = 5):\n",
      "    reducer = umap.UMAP(n_neighbors=n_neighbors)\n",
      "    reduced_embeddings = reducer.fit_transform(self.idea_index.get_all_embeddings())\n",
      "\n",
      "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
      "    labels = clusterer.fit_predict(reduced_embeddings)\n",
      "\n",
      "    token_count_per_cluster = self.count_tokens_per_cluster(labels)\n",
      "    print(token_count_per_cluster)\n",
      "    if max(token_count_per_cluster.values()) <= self.max_tokens_per_cluster:\n",
      "        self.cluster_labels = labels\n",
      "        print(\"Clusters created successfully.\")\n",
      "    else:\n",
      "        print(\"Clusters exceed the maximum token count.\")\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"print\"\n",
      "\t\"max\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def count_tokens_per_cluster(self, labels):\n",
      "    token_count_per_cluster = {}\n",
      "\n",
      "    for label, doc in zip(labels, self.gathered_docs):\n",
      "        if label not in token_count_per_cluster:\n",
      "            token_count_per_cluster[label] = len(tokenizer.encode(doc))\n",
      "        else:\n",
      "            token_count_per_cluster[label] += len(tokenizer.encode(doc))\n",
      "    return token_count_per_cluster\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"zip\"\n",
      "\t\"len\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "\n",
      "def create_minimum_spanning_paths(self):\n",
      "    if self.cluster_labels is None:\n",
      "        raise ValueError(\"You must run cluster_embeddings() before creating minimum spanning paths.\")\n",
      "\n",
      "    unique_labels = np.unique(self.cluster_labels)\n",
      "    min_span_paths = []\n",
      "\n",
      "    for label in unique_labels:\n",
      "\n",
      "\n",
      "        # Get the indices of the current cluster\n",
      "        cluster_indices = np.where(self.cluster_labels == label)[0]\n",
      "\n",
      "        # Calculate the pairwise distances between embeddings in the cluster\n",
      "        cluster_embeddings = self.idea_index.embeddings[cluster_indices]\n",
      "        dist_matrix = squareform(pdist(cluster_embeddings))\n",
      "\n",
      "        # Create a graph from the distance matrix\n",
      "        graph = nx.from_numpy_array(dist_matrix)\n",
      "\n",
      "        # Compute the minimum spanning tree of the graph\n",
      "        min_span_tree = nx.minimum_spanning_tree(graph)\n",
      "\n",
      "        # Get the minimum spanning paths\n",
      "        min_span_paths_cluster = []\n",
      "        visited = set()\n",
      "        for u, v in min_span_tree.edges():\n",
      "            if u not in visited and v not in visited:\n",
      "                orig_u = cluster_indices[u]\n",
      "                orig_v = cluster_indices[v]\n",
      "                min_span_paths_cluster.append(orig_u)\n",
      "                visited.add(u)\n",
      "                visited.add(v)\n",
      "        # Add the last node to complete the path\n",
      "        min_span_paths_cluster.append(orig_v)\n",
      "\n",
      "        min_span_paths.append(min_span_paths_cluster)\n",
      "\n",
      "    self.min_span_paths = min_span_paths\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"ValueError\"\n",
      "\t\"squareform\"\n",
      "\t\"pdist\"\n",
      "\t\"set\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def plot_embeddings_with_path(self):\n",
      "    paths = self.min_span_paths\n",
      "    embeddings = self.idea_index.embeddings\n",
      "    title = \"Minimum Spanning Paths\"\n",
      "    tsne = TSNE(n_components=2, random_state=42)\n",
      "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
      "\n",
      "    plt.figure(figsize=(10, 8))\n",
      "    colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n",
      "    for i, path in enumerate(paths):\n",
      "        path_embeddings = reduced_embeddings[path]\n",
      "        plt.scatter(\n",
      "            path_embeddings[:, 0],\n",
      "            path_embeddings[:, 1],\n",
      "            color=colors[i],\n",
      "            label=f\"Cluster {i}\",\n",
      "        )\n",
      "        for j in range(len(path) - 1):\n",
      "            plt.plot(\n",
      "                [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n",
      "                [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n",
      "                color=colors[i],\n",
      "            )\n",
      "    plt.title(title)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "Function calls: shape: (5,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"TSNE\"\n",
      "\t\"len\"\n",
      "\t\"enumerate\"\n",
      "\t\"range\"\n",
      "\t\"len\"\n",
      "]\n",
      "\n",
      "def get_clustered_ideas(self):\n",
      "    if self.cluster_labels is None:\n",
      "        raise ValueError(\"You must run cluster_embeddings() before getting clustered ideas.\")\n",
      "    \n",
      "    clustered_ideas = {}\n",
      "    for label, idea in zip(self.cluster_labels, self.idea_index.values):\n",
      "        if label not in clustered_ideas:\n",
      "            clustered_ideas[label] = [idea]\n",
      "        else:\n",
      "            clustered_ideas[label].append(idea)\n",
      "    \n",
      "    # Convert the dictionary to a list of lists (each list corresponds to a cluster)\n",
      "    return list(clustered_ideas.values())\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"ValueError\"\n",
      "\t\"zip\"\n",
      "\t\"list\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "class Summarizer:\n",
      "    def __init__(self, texts: list):\n",
      "        self.texts = texts\n",
      "\n",
      "    def summarize_texts(self) -> dict:\n",
      "        output = {}\n",
      "\n",
      "        with RateLimitedThreadPoolExecutor(max_workers=12, calls_per_minute=200, verbose = False) as executor:\n",
      "            future_to_text = {executor.submit(self._summarize_text, text): text for text in self.texts}\n",
      "            for future in as_completed(future_to_text):\n",
      "                text = future_to_text[future]\n",
      "                try:\n",
      "                    output[text] = future.result()\n",
      "                except Exception as exc:\n",
      "                    print(f'An exception occurred while summarizing text: {exc}')\n",
      "\n",
      "        return output\n",
      "\n",
      "    def _summarize_text(self, text: str) -> str:\n",
      "        summary = cohere_summarize(text, model=\"summarize-xlarge\", length=\"auto\", extractiveness=\"low\", format=\"auto\")\n",
      "        return summary\n",
      "\n",
      "Function calls: shape: (4,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"RateLimitedThr…\n",
      "\t\"as_completed\"\n",
      "\t\"print\"\n",
      "\t\"cohere_summari…\n",
      "]\n",
      "\n",
      "def __init__(self, texts: list):\n",
      "    self.texts = texts\n",
      "\n",
      "\n",
      "\n",
      "def summarize_texts(self) -> dict:\n",
      "    output = {}\n",
      "\n",
      "    with RateLimitedThreadPoolExecutor(max_workers=12, calls_per_minute=200, verbose = False) as executor:\n",
      "        future_to_text = {executor.submit(self._summarize_text, text): text for text in self.texts}\n",
      "        for future in as_completed(future_to_text):\n",
      "            text = future_to_text[future]\n",
      "            try:\n",
      "                output[text] = future.result()\n",
      "            except Exception as exc:\n",
      "                print(f'An exception occurred while summarizing text: {exc}')\n",
      "\n",
      "    return output\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"RateLimitedThr…\n",
      "\t\"as_completed\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "def _summarize_text(self, text: str) -> str:\n",
      "    summary = cohere_summarize(text, model=\"summarize-xlarge\", length=\"auto\", extractiveness=\"low\", format=\"auto\")\n",
      "    return summary\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"cohere_summari…\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def generate_perspective_prompt(user_subject, user_perspective, seed_model = \"gpt-3.5-turbo\"):\n",
      "    start = perf_counter()\n",
      "    chat_instance = Chat(model=seed_model)\n",
      "    analyzer = SubjectPerspectiveAnalyzer(chat_instance)\n",
      "    output = analyzer.analyze_subject_perspective(user_subject, user_perspective)\n",
      "    end = perf_counter()\n",
      "    print(\"Time to analyze_perspective: \", end - start)\n",
      "\n",
      "    dataset_url = \"Cohere/wikipedia-22-12-simple-embeddings\"\n",
      "    start = perf_counter()\n",
      "    index = MemoryIndex(name=\"wiki_index\", load=True, is_batched=True,embedder=CohereEmbedder)\n",
      "    if len(index.values)>0:\n",
      "        loaded = True\n",
      "    else:\n",
      "        loaded = False\n",
      "\n",
      "    if not loaded:\n",
      "        print(\"Index not found, creating new index\")\n",
      "        index = MemoryIndex.from_hf_dataset(dataset_url, [\"title\", \"text\"],embeddings_column= \"emb\", name=\"wiki_index\", is_batched=True,embedder=CohereEmbedder)\n",
      "    end = perf_counter()\n",
      "    print(\"Time to index: \", end - start)\n",
      "\n",
      "    start = perf_counter()\n",
      "    ideation = Ideation(memory_index=index)\n",
      "    ideas = ideation.retrieve_ideas(output, k=40, max_tokens=10000)\n",
      "    token_count = 0\n",
      "    for idea in ideas:\n",
      "        token_count += idea[\"returned_tokens\"]\n",
      "    end = perf_counter()\n",
      "    print(\"Time to retrieve ideas: \", end - start)\n",
      "    print(\"Number of tokens: \", token_count)\n",
      "\n",
      "    start = perf_counter()\n",
      "    max_tokens_per_cluster = 20000\n",
      "    idea_cluster = IdeaCluster(ideas, max_tokens_per_cluster)\n",
      "    idea_cluster.cluster_embeddings()\n",
      "    time.sleep(0.5)\n",
      "    ideas = idea_cluster.get_clustered_ideas()\n",
      "    end = perf_counter()\n",
      "    combined_idea = []\n",
      "    ideas_tokens = []\n",
      "    for idea in ideas:\n",
      "        combined_idea.append(\"\\n\".join(idea))\n",
      "        ideas_tokens.append(tokenizer.encode(combined_idea[-1]))\n",
      "    print(\"Time to cluster ideas: \", end - start)\n",
      "    print(\"Number of clusters: \", len(ideas))\n",
      "    print(\"Number of tokens in each cluster: \", [len(tokens) for tokens in ideas_tokens])\n",
      "    start = perf_counter()\n",
      "    summarizer = Summarizer(combined_idea)\n",
      "    summaries = summarizer.summarize_texts()\n",
      "    end = perf_counter()\n",
      "    print(\"Time to summarize: \", end - start)\n",
      "    print(\"Number of summaries: \", len(summaries))\n",
      "    print(\"Number of tokens in each summary: \", [len(tokenizer.encode(summary)) for summary in summaries.values()])\n",
      "    start = perf_counter()\n",
      "    system_prompt = f\"\"\"With the summarized information from Cohere about the essential ideas, concenpts, priciples, and intersection points between {user_subject} and {user_perspective}, construct an appealing and context-aware chatbot prompt that impels the chatbot to respond with insights and perspectives born from the synergy of these two domains. Use the tips below to help you create an effective chatbot prompt: 1. Begin with a concise introduction: Initiate the chatbot prompt by setting the context, encompassing the user's specified subject and perspective. 2. Accentuate the intersection: Secure that the chatbot prompt underlines the connection between the user_subject and user_perspective, leading to more pertinent and perceptive responses. 3. Foster exploration: Ensure the chatbot prompt provokes the chatbot to delve into the main ideas, principles, and concepts from the summaries with a thoughtful and reflective approach. 4. Pose open-ended questions: Incorporate open-ended queries in the chatbot prompt, stimulating the chatbot to contemplate beyond the summaries and offer comprehensive responses. 5. Prioritize simplicity: Maintain the chatbot prompt's clarity and brevity, assisting the chatbot in comprehending the context and reacting suitably. 6. Steer the conversation: Craft the chatbot prompt in a manner that subtly directs the chatbot's answers, confirming they consistently focus on the user_subject and user_perspective. By leveraging these tips, develop a chatbot prompt that generates responses illustrative of the user's specified subject and perspective, culminating in a customized and significant interaction. Remember to conclude the prompt with 7 content pillars that will help the chatbot use the perspective at the best of its capacity. \"\"\".format(user_subject=user_subject, user_perspective=user_perspective)\n",
      "    for i, summary in enumerate(summaries.values()):\n",
      "        system_prompt += summary + \"\\n\\n\"\n",
      "    prompt_generator = Chat(name= \"prompt generator\",system_prompt=system_prompt, max_output_tokens= 2000, model = \"gpt-4\")\n",
      "    perspective_prompt = prompt_generator.reply(\"\", verbose=False)\n",
      "    additional_prompt =  \"\"\" \\n\\n Bear in mind that while engaging with the user, questions may arise that seem unrelated to the initial prompt. It's crucial to maintain flexibility and creativity, interpreting and addressing these topics through the unique prism provided by the initial prompt.\"\"\"\n",
      "    perspective_prompt += additional_prompt\n",
      "    end = perf_counter()\n",
      "    print(\"Time to generate prompt: \", end - start)\n",
      "    return perspective_prompt\n",
      "\n",
      "Function calls: shape: (37,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"perf_counter\"\n",
      "\t\"Chat\"\n",
      "\t\"SubjectPerspec…\n",
      "\t\"perf_counter\"\n",
      "\t\"print\"\n",
      "\t\"perf_counter\"\n",
      "\t\"MemoryIndex\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"perf_counter\"\n",
      "\t\"print\"\n",
      "\t\"perf_counter\"\n",
      "\t…\n",
      "\t\"perf_counter\"\n",
      "\t\"Summarizer\"\n",
      "\t\"perf_counter\"\n",
      "\t\"print\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"print\"\n",
      "\t\"len\"\n",
      "\t\"perf_counter\"\n",
      "\t\"enumerate\"\n",
      "\t\"Chat\"\n",
      "\t\"perf_counter\"\n",
      "\t\"print\"\n",
      "]\n",
      "\n",
      "\n",
      "class PerspectivePromptGenerator:\n",
      "    def __init__(self, subjects, perspectives, max_workers=10, calls_per_minute=20, base_filename=\"prompt_\"):\n",
      "        self.subjects = subjects\n",
      "        self.perspectives = perspectives\n",
      "        self.executor = RateLimitedThreadPoolExecutor(\n",
      "            max_workers=max_workers, \n",
      "            calls_per_minute=calls_per_minute\n",
      "        )\n",
      "        self.prompts = []\n",
      "        self.base_filename = base_filename\n",
      "        self.lock = threading.Lock()  # create a lock\n",
      "\n",
      "    def handle_future(self, future):\n",
      "        try:\n",
      "            result = future.result()\n",
      "            self.prompts.append(result)\n",
      "            complete_filename = self.base_filename + \"results.json\"\n",
      "            with self.lock:  # acquire the lock before writing to the file\n",
      "                self.save_prompts_to_json(complete_filename)\n",
      "        except Exception as e:\n",
      "            error_report = {\"error\": str(e), \"traceback\": traceback.format_exc()}\n",
      "            self.prompts.append(error_report)\n",
      "            complete_filename = self.base_filename + \"errors.json\"\n",
      "            with self.lock:  # acquire the lock before writing to the file\n",
      "                self.save_prompts_to_json(complete_filename)\n",
      "    \n",
      "    def generate_prompts(self):\n",
      "        for subject in self.subjects:\n",
      "            for perspective in self.perspectives:\n",
      "                future = self.executor.submit(\n",
      "                    generate_perspective_prompt, \n",
      "                    subject, \n",
      "                    perspective\n",
      "                )\n",
      "                future.add_done_callback(self.handle_future)\n",
      "        self.executor.shutdown(wait=True)\n",
      "        return self.prompts\n",
      "\n",
      "    def save_prompts_to_json(self, filename):\n",
      "        with open(filename, 'w') as f:\n",
      "            json.dump(self.prompts, f)\n",
      "\n",
      "Function calls: shape: (3,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"RateLimitedThr…\n",
      "\t\"str\"\n",
      "\t\"open\"\n",
      "]\n",
      "\n",
      "def __init__(self, subjects, perspectives, max_workers=10, calls_per_minute=20, base_filename=\"prompt_\"):\n",
      "    self.subjects = subjects\n",
      "    self.perspectives = perspectives\n",
      "    self.executor = RateLimitedThreadPoolExecutor(\n",
      "        max_workers=max_workers, \n",
      "        calls_per_minute=calls_per_minute\n",
      "    )\n",
      "    self.prompts = []\n",
      "    self.base_filename = base_filename\n",
      "    self.lock = threading.Lock()  # create a lock\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"RateLimitedThr…\n",
      "]\n",
      "\n",
      "\n",
      "def handle_future(self, future):\n",
      "    try:\n",
      "        result = future.result()\n",
      "        self.prompts.append(result)\n",
      "        complete_filename = self.base_filename + \"results.json\"\n",
      "        with self.lock:  # acquire the lock before writing to the file\n",
      "            self.save_prompts_to_json(complete_filename)\n",
      "    except Exception as e:\n",
      "        error_report = {\"error\": str(e), \"traceback\": traceback.format_exc()}\n",
      "        self.prompts.append(error_report)\n",
      "        complete_filename = self.base_filename + \"errors.json\"\n",
      "        with self.lock:  # acquire the lock before writing to the file\n",
      "            self.save_prompts_to_json(complete_filename)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"str\"\n",
      "]\n",
      "\n",
      "\n",
      "def generate_prompts(self):\n",
      "    for subject in self.subjects:\n",
      "        for perspective in self.perspectives:\n",
      "            future = self.executor.submit(\n",
      "                generate_perspective_prompt, \n",
      "                subject, \n",
      "                perspective\n",
      "            )\n",
      "            future.add_done_callback(self.handle_future)\n",
      "    self.executor.shutdown(wait=True)\n",
      "    return self.prompts\n",
      "\n",
      "\n",
      "\n",
      "def save_prompts_to_json(self, filename):\n",
      "    with open(filename, 'w') as f:\n",
      "        json.dump(self.prompts, f)\n",
      "\n",
      "Function calls: shape: (1,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"open\"\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for code, parent_classes in zip(mfp.df['code'], mfp.df['code_function_calls|FunctionCallCollector']):\n",
    "    print(code)\n",
    "    if parent_classes.shape[0] > 0:\n",
    "        print(f\"Function calls: {parent_classes}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "class EmbeddableType(Enum):\n",
      "    TEXT = \"text\"\n",
      "    NUMERIC = \"numeric\"\n",
      "    CATEGORICAL = \"categorical\"\n",
      "    # Add more data types as required\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"Enum\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\n",
      "    # Infer the data type of the column\n",
      "    # This will depend on the type of `column` (whether it's a string, Series, etc.)\n",
      "    # Here we'll assume `column` is a pandas Series for simplicity\n",
      "    column_type = str(column.dtype)\n",
      "    print(column_type)\n",
      "    if column_type == \"Utf8\":\n",
      "        # If it's an object, we'll assume it's text\n",
      "        return EmbeddableType.TEXT, OpenAiEmbedder()\n",
      "    elif np.issubdtype(column.dtype, np.number):\n",
      "        # If it's a number, we'll use a different embedding strategy\n",
      "        return EmbeddableType.NUMERIC, numeric_embedder\n",
      "    else:\n",
      "        # For other types, we could throw an error or have a default strategy\n",
      "        raise ValueError(f\"Cannot infer type for column {column.name}\")\n",
      "\n",
      "Function call: str\n",
      "Function call: print\n",
      "Function call: OpenAiEmbedder\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\nclass OpenAiEmbedder:\\n\\n    def get_embedding_size(self):\\n        return ADA_EMBEDDING_SIZE\\n\\n    def embed(self, data, verbose=False):\\n        if isinstance(data, list) and len(data) > 1:\\n            logger.info(\"Batch embedding\")\\n            return self.batch_embed(data)\\n        elif isinstance(data, list) and len(data) == 1:\\n            logger.info(\"Serial embedding\")\\n            data = data[0]\\n\\n        if isinstance(data, dict) and \"content\" in data:\\n            if verbose:\\n                logger.info(\"Embedding without mark\", data[\"content\"])\\n            out = openai.Embedding.create(\\n                input=data[\"content\"], engine=\"text-embedding-ada-002\"\\n            )\\n        else:\\n            if len(TOKENIZER.encode(data)) > MAX_CONTEXT_LENGTH:\\n                raise ValueError(f\" The input is too long for OpenAI, num tokens is {len(TOKENIZER.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\\n            if verbose:\\n                logger.info(f\"Embedding without preprocessing the input {data}\")\\n            out = openai.Embedding.create(\\n                input=str(data), engine=\"text-embedding-ada-002\"\\n            )\\n        return out.data[0].embedding\\n\\n    def batch_embed(self, data: List[str], batch_size: int = 1000):\\n        if isinstance(data, dict) and \"content\" in data:\\n            raise ValueError(\"Batch embedding not supported for dictionaries\")\\n        elif isinstance(data, str):\\n            raise ValueError(\"Batch embedding not supported for strings use embed() instead\")\\n        elif isinstance(data, list):\\n            batch = []\\n            embeddings = []\\n            i = 1\\n            total_number_of_batches = len(data)//batch_size + 1 if len(data) % batch_size > 0 else len(data)//batch_size\\n            for value in data:\\n                batch.append(value)\\n                if len(batch) == batch_size:\\n                    start = time.time()\\n                    out = openai.Embedding.create(\\n                        input=batch, engine=\"text-embedding-ada-002\"\\n                    )\\n                    for embedding in out.data:\\n                        embeddings.append(embedding.embedding)\\n                    logger.info(f\"Batch {i} of {total_number_of_batches}\")\\n                    logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\\n                    i += 1\\n                    batch = []\\n            if len(batch) > 0:\\n                start = time.time()\\n                out = openai.Embedding.create(\\n                    input=batch, engine=\"text-embedding-ada-002\"\\n                )\\n                for embedding in out.data:\\n                    embeddings.append(embedding.embedding)\\n                logger.info(f\"Batch {i} of {total_number_of_batches}\")\\n                logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\\n            logger.info(f\\'Total number of embeddings {len(embeddings)}\\')\\n\\n            if len(embeddings) != len(data):\\n                raise ValueError(\"The number of embeddings is different from the number of values an error occured in OpenAI API during the embedding process\")\\n            return embeddings\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def numeric_embedder(column):\n",
      "    # Implement the numeric embedding strategy\n",
      "    # This will depend on the type of `column` (whether it's a string, Series, etc.)\n",
      "    # Here we'll assume `column` is a pandas Series for simplicity\n",
      "    return column.values\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class EmbeddingTask(BaseTask):\n",
      "    def __init__(\n",
      "        self,\n",
      "        embedder: OpenAiEmbedder,\n",
      "        values: List[Any],\n",
      "        path: List[List[int]],\n",
      "        max_workers: int = 1,\n",
      "        task_id: str = \"task\",\n",
      "        calls_per_minute: int = 1500,\n",
      "        backup: bool = True,\n",
      "    ):\n",
      "        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup)\n",
      "        self.embedder = embedder\n",
      "        self.values = values\n",
      "\n",
      "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "        # expected to work with a lig of a single element\n",
      "        if len(sub_path) != 1:\n",
      "            raise ValueError(\n",
      "                \"Embedding task expected to work with a list of a single element\"\n",
      "            )\n",
      "        sub_results = {}\n",
      "        for i in sub_path:\n",
      "            embedded_value = self.embedder.embed(self.values[i])\n",
      "            sub_results[i] = embedded_value\n",
      "        return sub_results\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseTask\"]\n",
      "]\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    embedder: OpenAiEmbedder,\n",
      "    values: List[Any],\n",
      "    path: List[List[int]],\n",
      "    max_workers: int = 1,\n",
      "    task_id: str = \"task\",\n",
      "    calls_per_minute: int = 1500,\n",
      "    backup: bool = True,\n",
      "):\n",
      "    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup)\n",
      "    self.embedder = embedder\n",
      "    self.values = values\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "    # expected to work with a lig of a single element\n",
      "    if len(sub_path) != 1:\n",
      "        raise ValueError(\n",
      "            \"Embedding task expected to work with a list of a single element\"\n",
      "        )\n",
      "    sub_results = {}\n",
      "    for i in sub_path:\n",
      "        embedded_value = self.embedder.embed(self.values[i])\n",
      "        sub_results[i] = embedded_value\n",
      "    return sub_results\n",
      "\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def parallel_embeddings(embedder, values, max_workers, backup, name):\n",
      "        # Prepare the paths for the EmbeddingTask\n",
      "        print(\"Embedding {} values\".format(len(values)))\n",
      "        paths = [[i] for i in range(len(values))]\n",
      "\n",
      "        # Initialize the EmbeddingTask and execute it\n",
      "        embedding_task = EmbeddingTask(\n",
      "            embedder,\n",
      "            values,\n",
      "            path=paths,\n",
      "            max_workers=max_workers,\n",
      "            task_id=name + \"_embedding_task\",\n",
      "            backup=backup,\n",
      "        )\n",
      "        embeddings = embedding_task.work()\n",
      "        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\n",
      "        return embeddings\n",
      "\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: EmbeddingTask\n",
      "Function call: sorted\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\nclass EmbeddingTask(BaseTask):\\n    def __init__(\\n        self,\\n        embedder: OpenAiEmbedder,\\n        values: List[Any],\\n        path: List[List[int]],\\n        max_workers: int = 1,\\n        task_id: str = \"task\",\\n        calls_per_minute: int = 1500,\\n        backup: bool = True,\\n    ):\\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup)\\n        self.embedder = embedder\\n        self.values = values\\n\\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\\n        # expected to work with a lig of a single element\\n        if len(sub_path) != 1:\\n            raise ValueError(\\n                \"Embedding task expected to work with a list of a single element\"\\n            )\\n        sub_results = {}\\n        for i in sub_path:\\n            embedded_value = self.embedder.embed(self.values[i])\\n            sub_results[i] = embedded_value\\n        return sub_results\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class TopicTreeTask(BaseTask):\n",
      "    def __init__(\n",
      "        self,\n",
      "        memory_kernel_dict: Dict,\n",
      "        supplement_indexes: Dict,\n",
      "        sim_threshold: float,\n",
      "        chatbot: BaseChat,\n",
      "        parent_kernel_label: str,\n",
      "        child_kernel_label: str,\n",
      "        system_prompt: str,\n",
      "        clustering_method: str,\n",
      "        task_id: str = \"TopicTreeTask\",\n",
      "        max_workers: int = 1,\n",
      "        calls_per_minute: int = 20,\n",
      "    ):\n",
      "        self.clustering_method = clustering_method\n",
      "        self.supplement_indexes = supplement_indexes\n",
      "        self.sim_threshold = sim_threshold\n",
      "        self.parent_kernel_label = parent_kernel_label\n",
      "        self.child_kernel_label = child_kernel_label\n",
      "        self.memory_kernel_dict = memory_kernel_dict\n",
      "        self._setup_memory_kernel_group()\n",
      "        self.generate_task_paths()\n",
      "        self.system_prompt = system_prompt\n",
      "        self.chatbot = chatbot\n",
      "        self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n",
      "        super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n",
      "\n",
      "\n",
      "    def _setup_memory_kernel_group(self):\n",
      "        if self.clustering_method == \"HDBSCAN\":\n",
      "            print(\"Using HDBSCAN\")\n",
      "            self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "        elif self.clustering_method == \"Spectral\":\n",
      "            print(\"Using Spectral\")\n",
      "            self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "        else:\n",
      "            raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n",
      "\n",
      "    def generate_task_paths(self):\n",
      "        print(\"Generating task paths\")\n",
      "\n",
      "        self.memory_kernel_group.generate_path_groups()\n",
      "\n",
      "    def llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n",
      "        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n",
      "        return chatbot.reply(message)\n",
      "\n",
      "    def _execute_sub_task(self, sub_path) -> List[str]:\n",
      "        if self.parallel:\n",
      "            chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "        else:\n",
      "            chatbot_instance = self.chatbot\n",
      "\n",
      "        sub_results = {}\n",
      "        for i in sub_path:\n",
      "            print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n",
      "            try:\n",
      "                current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n",
      "                supplement_values = []\n",
      "                for key, index in self.supplement_indexes.items():\n",
      "                    results, scores, indeces = index.faiss_query(current_val, k=5)\n",
      "                    for result, score in zip(results, scores):\n",
      "                        if score > self.sim_threshold:\n",
      "                            supplement_values.append(result)\n",
      "                topic_tree = self.create_topic_tree(supplement_values)\n",
      "                #response = self.llm_response(chatbot_instance, current_val, id=i)\n",
      "                sub_results[i] = topic_tree\n",
      "            except IndexError:\n",
      "                print(f\"Error: Invalid index {i} in sub_path\")\n",
      "                sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n",
      "            except Exception as e:\n",
      "                print(f\"Error in sub_task for index {i}: {e}\")\n",
      "                sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n",
      "\n",
      "        return sub_results\n",
      "\n",
      "    def execute_task(self) -> None:\n",
      "        BaseTask.execute_task(self)\n",
      "\n",
      "        # Load the results from the JSON file\n",
      "        # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n",
      "        #     task_results = json.load(f)\n",
      "        self._load_results_from_file()\n",
      "        task_results = self.results\n",
      "        new_values = []\n",
      "        #sort task_results by index and add to new_values 0- max values ascending\n",
      "        for task_result in task_results:\n",
      "            if isinstance(task_result, dict):\n",
      "                for key, value in task_result.items():\n",
      "                    new_values.append((int(key), value))\n",
      "            elif isinstance(task_result, str):\n",
      "                print(f\"Error in task_result: {task_result}\")\n",
      "\n",
      "        new_values.sort(key=lambda x: x[0])\n",
      "        values = [x[1] for x in new_values]\n",
      "\n",
      "        task_memory_index = MemoryIndex()\n",
      "        task_memory_index.init_index(values=values)\n",
      "        # Create a new MemoryKernel with the results\n",
      "        new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n",
      "\n",
      "        # Add the new MemoryKernel to the MultiKernel\n",
      "        self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n",
      "        self.generate_task_paths()\n",
      "\n",
      "        #delete the results file\n",
      "        # os.remove(f\"{self.task_id}_results.json\")\n",
      "\n",
      "\n",
      "    def create_topic_tree(self, docs):\n",
      "        return None\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseTask\"]\n",
      "]\n",
      "Function call: super\n",
      "Function call: print\n",
      "Function call: HDBSCANMultiKernel\n",
      "Function call: print\n",
      "Function call: SpectralClusteringMultiKernel\n",
      "Function call: ValueError\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: zip\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: isinstance\n",
      "Function call: int\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "Function call: MemoryIndex\n",
      "Related codes: ['\\n\\nclass HDBSCANMultiKernel(MultiKernel):\\n    def __init__(\\n        self,\\n        memory_kernel_dict: Dict[str, MemoryKernel],\\n        name: str = \"memory_kernel_group\",\\n    ):\\n        super().__init__(memory_kernel_dict, name)\\n        self.cluster_paths = HDBSCANPaths()\\n\\n    def generate_path_groups(self, num_clusters: int = None) -> None:\\n        path_group = {}\\n        for k, v in self.memory_kernel_dict.items():\\n            embeddings = v.node_embeddings\\n            if num_clusters is None:\\n                num_clusters = int(np.sqrt(len(embeddings)))\\n            paths = self.cluster_paths.create_paths(embeddings, num_clusters)\\n            path_group[k] = paths\\n        self.path_group = path_group\\n', '\\n\\nclass SpectralClusteringMultiKernel(MultiKernel):\\n    def __init__(\\n        self,\\n        memory_kernel_dict: Dict[str, MemoryKernel],\\n        name: str = \"memory_kernel_group\",\\n    ):\\n        super().__init__(memory_kernel_dict, name)\\n        self.cluster_paths = SpectralClusteringPaths()\\n\\n    def generate_path_groups(self, num_clusters: int = None) -> None:\\n        path_group = {}\\n        for k, v in self.memory_kernel_dict.items():\\n            A_k = v.A_k\\n            if num_clusters is None:\\n                num_clusters = int(np.sqrt(len(A_k)))\\n            paths = self.cluster_paths.create_paths(A_k, num_clusters)\\n            path_group[k] = paths\\n        self.path_group = path_group\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\n\\n\\nclass MemoryIndex(NpIndex):\\n    \"\"\"\\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \\n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\\n    \"\"\"\\n    @staticmethod\\n    def check_uniform_context_type(context: List[Any]) -> None:\\n        \"\"\"Check if all context elements are of the same type.\"\"\"\\n        if not all(isinstance(x, type(context[0])) for x in context):\\n            raise ValueError(\"All context elements must be of the same type.\")\\n\\n    def __init__(\\n        self,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        context: Optional[List[Any]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ):\\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\\n        if context is not None and len(context) != len(values):\\n            raise ValueError(\"The context must have the same length as the values\")\\n\\n\\n        self.markdown = markdown\\n        if context is not None and values is not None:\\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\\n\\n        if context is not None:\\n            self.check_uniform_context_type(context)\\n            self.context_type = type(context[0])\\n\\n\\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\\n        if isinstance(identifier, list):\\n            return [self.get_context(value) for value in identifier]\\n        else:\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n            return self.context[value]\\n\\n    def clean_context(self):\\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\\n        self.context = {value: self.context[value] for value in self.values}\\n\\n    def add_to_context(self, value: str, context: Any):\\n        \"\"\" add a context to a value \"\"\"\\n        if not isinstance(context, self.context_type):\\n            raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if value in self.values:\\n            if value not in self.context:\\n                self.context[value] = []\\n            self.context[value].append(context)\\n\\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\\n        if isinstance(values, str):\\n            values = [values]\\n        NpIndex.add(self, values, embedding)\\n        if context is not None:\\n            for value, cont in zip(values, context):\\n                self.add_to_context(value, cont)\\n\\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\\n        if not isinstance(identifier, list):\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n        NpIndex.remove(self, identifier)\\n        if not isinstance(identifier, list):\\n            self.context.pop(value)\\n\\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\\n        #recover value from old_identifier\\n        if new_context is not None:\\n            if not isinstance(new_context, self.context_type):\\n                raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if not isinstance(old_identifier, list):\\n            old_id = self.identify_input(old_identifier)\\n            old_value = self.values[old_id]\\n            # Only perform the update if the old_value is not the same as the new_value.\\n            if old_value != new_value:\\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\\n\\n            if new_context is not None:\\n                self.context[new_value] = [new_context]\\n            else:\\n                self.context[new_value] = self.context.pop(old_value)\\n        else:\\n            self.context[new_value] = self.context.pop(old_value)\\n\\n    @classmethod\\n    def from_pandas(\\n        cls,\\n        data_frame: Union[pd.DataFrame, str],\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        if (\\n            isinstance(data_frame, str)\\n            and data_frame.endswith(\".csv\")\\n            and os.path.isfile(data_frame)\\n        ):\\n            logger.info(\"Loading the CSV file\")\\n            data_frame = pd.read_csv(data_frame)\\n            name = os.path.basename(data_frame).split(\".\")[0]\\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\\n            logger.info(\"Loading the pandas DataFrame\")\\n        else:\\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\\n\\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_pandas(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_hf_dataset(\\n        cls,\\n        dataset_url: str,\\n        value_column: str,\\n        data_split: str = \"train\",\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        dataset = load_dataset(dataset_url)[data_split]\\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_hf(dataset, context_columns)\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_polars(\\n        cls,\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        print(\"Loading the Polars DataFrame\")\\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_polars(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n\\n    @classmethod\\n    def from_python(\\n        cls,\\n        directory_path: str,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        markdown: str = \"python/markdown\",\\n        resolution: str = \"both\",\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    memory_kernel_dict: Dict,\n",
      "    supplement_indexes: Dict,\n",
      "    sim_threshold: float,\n",
      "    chatbot: BaseChat,\n",
      "    parent_kernel_label: str,\n",
      "    child_kernel_label: str,\n",
      "    system_prompt: str,\n",
      "    clustering_method: str,\n",
      "    task_id: str = \"TopicTreeTask\",\n",
      "    max_workers: int = 1,\n",
      "    calls_per_minute: int = 20,\n",
      "):\n",
      "    self.clustering_method = clustering_method\n",
      "    self.supplement_indexes = supplement_indexes\n",
      "    self.sim_threshold = sim_threshold\n",
      "    self.parent_kernel_label = parent_kernel_label\n",
      "    self.child_kernel_label = child_kernel_label\n",
      "    self.memory_kernel_dict = memory_kernel_dict\n",
      "    self._setup_memory_kernel_group()\n",
      "    self.generate_task_paths()\n",
      "    self.system_prompt = system_prompt\n",
      "    self.chatbot = chatbot\n",
      "    self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n",
      "    super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n",
      "\n",
      "Function call: super\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def _setup_memory_kernel_group(self):\n",
      "    if self.clustering_method == \"HDBSCAN\":\n",
      "        print(\"Using HDBSCAN\")\n",
      "        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "    elif self.clustering_method == \"Spectral\":\n",
      "        print(\"Using Spectral\")\n",
      "        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "    else:\n",
      "        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n",
      "\n",
      "Function call: print\n",
      "Function call: HDBSCANMultiKernel\n",
      "Function call: print\n",
      "Function call: SpectralClusteringMultiKernel\n",
      "Function call: ValueError\n",
      "Related codes: ['\\n\\nclass HDBSCANMultiKernel(MultiKernel):\\n    def __init__(\\n        self,\\n        memory_kernel_dict: Dict[str, MemoryKernel],\\n        name: str = \"memory_kernel_group\",\\n    ):\\n        super().__init__(memory_kernel_dict, name)\\n        self.cluster_paths = HDBSCANPaths()\\n\\n    def generate_path_groups(self, num_clusters: int = None) -> None:\\n        path_group = {}\\n        for k, v in self.memory_kernel_dict.items():\\n            embeddings = v.node_embeddings\\n            if num_clusters is None:\\n                num_clusters = int(np.sqrt(len(embeddings)))\\n            paths = self.cluster_paths.create_paths(embeddings, num_clusters)\\n            path_group[k] = paths\\n        self.path_group = path_group\\n', '\\n\\nclass SpectralClusteringMultiKernel(MultiKernel):\\n    def __init__(\\n        self,\\n        memory_kernel_dict: Dict[str, MemoryKernel],\\n        name: str = \"memory_kernel_group\",\\n    ):\\n        super().__init__(memory_kernel_dict, name)\\n        self.cluster_paths = SpectralClusteringPaths()\\n\\n    def generate_path_groups(self, num_clusters: int = None) -> None:\\n        path_group = {}\\n        for k, v in self.memory_kernel_dict.items():\\n            A_k = v.A_k\\n            if num_clusters is None:\\n                num_clusters = int(np.sqrt(len(A_k)))\\n            paths = self.cluster_paths.create_paths(A_k, num_clusters)\\n            path_group[k] = paths\\n        self.path_group = path_group\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def generate_task_paths(self):\n",
      "    print(\"Generating task paths\")\n",
      "\n",
      "    self.memory_kernel_group.generate_path_groups()\n",
      "\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n",
      "    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n",
      "    return chatbot.reply(message)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _execute_sub_task(self, sub_path) -> List[str]:\n",
      "    if self.parallel:\n",
      "        chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "    else:\n",
      "        chatbot_instance = self.chatbot\n",
      "\n",
      "    sub_results = {}\n",
      "    for i in sub_path:\n",
      "        print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n",
      "        try:\n",
      "            current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n",
      "            supplement_values = []\n",
      "            for key, index in self.supplement_indexes.items():\n",
      "                results, scores, indeces = index.faiss_query(current_val, k=5)\n",
      "                for result, score in zip(results, scores):\n",
      "                    if score > self.sim_threshold:\n",
      "                        supplement_values.append(result)\n",
      "            topic_tree = self.create_topic_tree(supplement_values)\n",
      "            #response = self.llm_response(chatbot_instance, current_val, id=i)\n",
      "            sub_results[i] = topic_tree\n",
      "        except IndexError:\n",
      "            print(f\"Error: Invalid index {i} in sub_path\")\n",
      "            sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n",
      "        except Exception as e:\n",
      "            print(f\"Error in sub_task for index {i}: {e}\")\n",
      "            sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n",
      "\n",
      "    return sub_results\n",
      "\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: zip\n",
      "Function call: print\n",
      "Function call: print\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def execute_task(self) -> None:\n",
      "    BaseTask.execute_task(self)\n",
      "\n",
      "    # Load the results from the JSON file\n",
      "    # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n",
      "    #     task_results = json.load(f)\n",
      "    self._load_results_from_file()\n",
      "    task_results = self.results\n",
      "    new_values = []\n",
      "    #sort task_results by index and add to new_values 0- max values ascending\n",
      "    for task_result in task_results:\n",
      "        if isinstance(task_result, dict):\n",
      "            for key, value in task_result.items():\n",
      "                new_values.append((int(key), value))\n",
      "        elif isinstance(task_result, str):\n",
      "            print(f\"Error in task_result: {task_result}\")\n",
      "\n",
      "    new_values.sort(key=lambda x: x[0])\n",
      "    values = [x[1] for x in new_values]\n",
      "\n",
      "    task_memory_index = MemoryIndex()\n",
      "    task_memory_index.init_index(values=values)\n",
      "    # Create a new MemoryKernel with the results\n",
      "    new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n",
      "\n",
      "    # Add the new MemoryKernel to the MultiKernel\n",
      "    self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n",
      "    self.generate_task_paths()\n",
      "\n",
      "    #delete the results file\n",
      "    # os.remove(f\"{self.task_id}_results.json\")\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: int\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "Function call: MemoryIndex\n",
      "Related codes: ['\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\n\\n\\nclass MemoryIndex(NpIndex):\\n    \"\"\"\\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \\n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\\n    \"\"\"\\n    @staticmethod\\n    def check_uniform_context_type(context: List[Any]) -> None:\\n        \"\"\"Check if all context elements are of the same type.\"\"\"\\n        if not all(isinstance(x, type(context[0])) for x in context):\\n            raise ValueError(\"All context elements must be of the same type.\")\\n\\n    def __init__(\\n        self,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        context: Optional[List[Any]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ):\\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\\n        if context is not None and len(context) != len(values):\\n            raise ValueError(\"The context must have the same length as the values\")\\n\\n\\n        self.markdown = markdown\\n        if context is not None and values is not None:\\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\\n\\n        if context is not None:\\n            self.check_uniform_context_type(context)\\n            self.context_type = type(context[0])\\n\\n\\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\\n        if isinstance(identifier, list):\\n            return [self.get_context(value) for value in identifier]\\n        else:\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n            return self.context[value]\\n\\n    def clean_context(self):\\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\\n        self.context = {value: self.context[value] for value in self.values}\\n\\n    def add_to_context(self, value: str, context: Any):\\n        \"\"\" add a context to a value \"\"\"\\n        if not isinstance(context, self.context_type):\\n            raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if value in self.values:\\n            if value not in self.context:\\n                self.context[value] = []\\n            self.context[value].append(context)\\n\\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\\n        if isinstance(values, str):\\n            values = [values]\\n        NpIndex.add(self, values, embedding)\\n        if context is not None:\\n            for value, cont in zip(values, context):\\n                self.add_to_context(value, cont)\\n\\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\\n        if not isinstance(identifier, list):\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n        NpIndex.remove(self, identifier)\\n        if not isinstance(identifier, list):\\n            self.context.pop(value)\\n\\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\\n        #recover value from old_identifier\\n        if new_context is not None:\\n            if not isinstance(new_context, self.context_type):\\n                raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if not isinstance(old_identifier, list):\\n            old_id = self.identify_input(old_identifier)\\n            old_value = self.values[old_id]\\n            # Only perform the update if the old_value is not the same as the new_value.\\n            if old_value != new_value:\\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\\n\\n            if new_context is not None:\\n                self.context[new_value] = [new_context]\\n            else:\\n                self.context[new_value] = self.context.pop(old_value)\\n        else:\\n            self.context[new_value] = self.context.pop(old_value)\\n\\n    @classmethod\\n    def from_pandas(\\n        cls,\\n        data_frame: Union[pd.DataFrame, str],\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        if (\\n            isinstance(data_frame, str)\\n            and data_frame.endswith(\".csv\")\\n            and os.path.isfile(data_frame)\\n        ):\\n            logger.info(\"Loading the CSV file\")\\n            data_frame = pd.read_csv(data_frame)\\n            name = os.path.basename(data_frame).split(\".\")[0]\\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\\n            logger.info(\"Loading the pandas DataFrame\")\\n        else:\\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\\n\\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_pandas(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_hf_dataset(\\n        cls,\\n        dataset_url: str,\\n        value_column: str,\\n        data_split: str = \"train\",\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        dataset = load_dataset(dataset_url)[data_split]\\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_hf(dataset, context_columns)\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_polars(\\n        cls,\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        print(\"Loading the Polars DataFrame\")\\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_polars(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n\\n    @classmethod\\n    def from_python(\\n        cls,\\n        directory_path: str,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        markdown: str = \"python/markdown\",\\n        resolution: str = \"both\",\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def create_topic_tree(self, docs):\n",
      "    return None\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class MultiKernelTask(BaseTask):\n",
      "    def __init__(\n",
      "        self,\n",
      "        memory_kernel_dict: Dict,\n",
      "        chatbot: BaseChat,\n",
      "        parent_kernel_label: str,\n",
      "        child_kernel_label: str,\n",
      "        system_prompt: str,\n",
      "        clustering_method: str,\n",
      "        path_group: Dict[str, List[List[int]]],\n",
      "        task_id: str = \"MultiKernelTask\",\n",
      "        max_workers: int = 1,\n",
      "        calls_per_minute: int = 20,\n",
      "    ):\n",
      "        self.clustering_method = clustering_method\n",
      "        self.parent_kernel_label = parent_kernel_label\n",
      "        self.child_kernel_label = child_kernel_label\n",
      "        self.memory_kernel_dict = memory_kernel_dict\n",
      "        \n",
      "        self._setup_memory_kernel_group()\n",
      "        if path_group:\n",
      "            self.memory_kernel_group.path_group = path_group\n",
      "        else:\n",
      "            self.generate_task_paths()\n",
      "        self.system_prompt = system_prompt\n",
      "        self.chatbot = chatbot\n",
      "        self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n",
      "        super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n",
      "\n",
      "\n",
      "    def _setup_memory_kernel_group(self):\n",
      "        if self.clustering_method == \"HDBSCAN\":\n",
      "            print(\"Using HDBSCAN\")\n",
      "            self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "        elif self.clustering_method == \"Spectral\":\n",
      "            print(\"Using Spectral\")\n",
      "            self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "        else:\n",
      "            raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n",
      "\n",
      "    def generate_task_paths(self):\n",
      "        print(\"Generating task paths\")\n",
      "\n",
      "        self.memory_kernel_group.generate_path_groups()\n",
      "\n",
      "    def llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n",
      "        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n",
      "        return chatbot.reply(message)\n",
      "\n",
      "    def _execute_sub_task(self, sub_path) -> List[str]:\n",
      "        if self.parallel:\n",
      "            chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "        else:\n",
      "            chatbot_instance = self.chatbot\n",
      "\n",
      "        sub_results = {}\n",
      "        for i in sub_path:\n",
      "            print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n",
      "            try:\n",
      "                current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n",
      "                response = self.llm_response(chatbot_instance, current_val, id=i)\n",
      "                sub_results[i] = response\n",
      "            except IndexError:\n",
      "                print(f\"Error: Invalid index {i} in sub_path\")\n",
      "                sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n",
      "            except Exception as e:\n",
      "                print(f\"Error in sub_task for index {i}: {e}\")\n",
      "                sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n",
      "\n",
      "        return sub_results\n",
      "\n",
      "    def execute_task(self) -> None:\n",
      "        BaseTask.execute_task(self)\n",
      "\n",
      "        # Load the results from the JSON file\n",
      "        # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n",
      "        #     task_results = json.load(f)\n",
      "        self._load_results_from_file()\n",
      "        task_results = self.results\n",
      "        new_values = []\n",
      "        #sort task_results by index and add to new_values 0- max values ascending\n",
      "        for task_result in task_results:\n",
      "            if isinstance(task_result, dict):\n",
      "                for key, value in task_result.items():\n",
      "                    new_values.append((int(key), value))\n",
      "            elif isinstance(task_result, str):\n",
      "                print(f\"Error in task_result: {task_result}\")\n",
      "\n",
      "        new_values.sort(key=lambda x: x[0])\n",
      "        values = [x[1] for x in new_values]\n",
      "\n",
      "        task_memory_index = MemoryIndex()\n",
      "        task_memory_index.init_index(values=values)\n",
      "        # Create a new MemoryKernel with the results\n",
      "        new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n",
      "\n",
      "        # Add the new MemoryKernel to the MultiKernel\n",
      "        self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n",
      "        self.generate_task_paths()\n",
      "\n",
      "        #delete the results file\n",
      "        # os.remove(f\"{self.task_id}_results.json\")\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseTask\"]\n",
      "]\n",
      "Function call: super\n",
      "Function call: print\n",
      "Function call: HDBSCANMultiKernel\n",
      "Function call: print\n",
      "Function call: SpectralClusteringMultiKernel\n",
      "Function call: ValueError\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: isinstance\n",
      "Function call: int\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "Function call: MemoryIndex\n",
      "Related codes: ['\\n\\nclass HDBSCANMultiKernel(MultiKernel):\\n    def __init__(\\n        self,\\n        memory_kernel_dict: Dict[str, MemoryKernel],\\n        name: str = \"memory_kernel_group\",\\n    ):\\n        super().__init__(memory_kernel_dict, name)\\n        self.cluster_paths = HDBSCANPaths()\\n\\n    def generate_path_groups(self, num_clusters: int = None) -> None:\\n        path_group = {}\\n        for k, v in self.memory_kernel_dict.items():\\n            embeddings = v.node_embeddings\\n            if num_clusters is None:\\n                num_clusters = int(np.sqrt(len(embeddings)))\\n            paths = self.cluster_paths.create_paths(embeddings, num_clusters)\\n            path_group[k] = paths\\n        self.path_group = path_group\\n', '\\n\\nclass SpectralClusteringMultiKernel(MultiKernel):\\n    def __init__(\\n        self,\\n        memory_kernel_dict: Dict[str, MemoryKernel],\\n        name: str = \"memory_kernel_group\",\\n    ):\\n        super().__init__(memory_kernel_dict, name)\\n        self.cluster_paths = SpectralClusteringPaths()\\n\\n    def generate_path_groups(self, num_clusters: int = None) -> None:\\n        path_group = {}\\n        for k, v in self.memory_kernel_dict.items():\\n            A_k = v.A_k\\n            if num_clusters is None:\\n                num_clusters = int(np.sqrt(len(A_k)))\\n            paths = self.cluster_paths.create_paths(A_k, num_clusters)\\n            path_group[k] = paths\\n        self.path_group = path_group\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\n\\n\\nclass MemoryIndex(NpIndex):\\n    \"\"\"\\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \\n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\\n    \"\"\"\\n    @staticmethod\\n    def check_uniform_context_type(context: List[Any]) -> None:\\n        \"\"\"Check if all context elements are of the same type.\"\"\"\\n        if not all(isinstance(x, type(context[0])) for x in context):\\n            raise ValueError(\"All context elements must be of the same type.\")\\n\\n    def __init__(\\n        self,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        context: Optional[List[Any]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ):\\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\\n        if context is not None and len(context) != len(values):\\n            raise ValueError(\"The context must have the same length as the values\")\\n\\n\\n        self.markdown = markdown\\n        if context is not None and values is not None:\\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\\n\\n        if context is not None:\\n            self.check_uniform_context_type(context)\\n            self.context_type = type(context[0])\\n\\n\\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\\n        if isinstance(identifier, list):\\n            return [self.get_context(value) for value in identifier]\\n        else:\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n            return self.context[value]\\n\\n    def clean_context(self):\\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\\n        self.context = {value: self.context[value] for value in self.values}\\n\\n    def add_to_context(self, value: str, context: Any):\\n        \"\"\" add a context to a value \"\"\"\\n        if not isinstance(context, self.context_type):\\n            raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if value in self.values:\\n            if value not in self.context:\\n                self.context[value] = []\\n            self.context[value].append(context)\\n\\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\\n        if isinstance(values, str):\\n            values = [values]\\n        NpIndex.add(self, values, embedding)\\n        if context is not None:\\n            for value, cont in zip(values, context):\\n                self.add_to_context(value, cont)\\n\\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\\n        if not isinstance(identifier, list):\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n        NpIndex.remove(self, identifier)\\n        if not isinstance(identifier, list):\\n            self.context.pop(value)\\n\\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\\n        #recover value from old_identifier\\n        if new_context is not None:\\n            if not isinstance(new_context, self.context_type):\\n                raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if not isinstance(old_identifier, list):\\n            old_id = self.identify_input(old_identifier)\\n            old_value = self.values[old_id]\\n            # Only perform the update if the old_value is not the same as the new_value.\\n            if old_value != new_value:\\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\\n\\n            if new_context is not None:\\n                self.context[new_value] = [new_context]\\n            else:\\n                self.context[new_value] = self.context.pop(old_value)\\n        else:\\n            self.context[new_value] = self.context.pop(old_value)\\n\\n    @classmethod\\n    def from_pandas(\\n        cls,\\n        data_frame: Union[pd.DataFrame, str],\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        if (\\n            isinstance(data_frame, str)\\n            and data_frame.endswith(\".csv\")\\n            and os.path.isfile(data_frame)\\n        ):\\n            logger.info(\"Loading the CSV file\")\\n            data_frame = pd.read_csv(data_frame)\\n            name = os.path.basename(data_frame).split(\".\")[0]\\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\\n            logger.info(\"Loading the pandas DataFrame\")\\n        else:\\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\\n\\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_pandas(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_hf_dataset(\\n        cls,\\n        dataset_url: str,\\n        value_column: str,\\n        data_split: str = \"train\",\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        dataset = load_dataset(dataset_url)[data_split]\\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_hf(dataset, context_columns)\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_polars(\\n        cls,\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        print(\"Loading the Polars DataFrame\")\\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_polars(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n\\n    @classmethod\\n    def from_python(\\n        cls,\\n        directory_path: str,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        markdown: str = \"python/markdown\",\\n        resolution: str = \"both\",\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    memory_kernel_dict: Dict,\n",
      "    chatbot: BaseChat,\n",
      "    parent_kernel_label: str,\n",
      "    child_kernel_label: str,\n",
      "    system_prompt: str,\n",
      "    clustering_method: str,\n",
      "    path_group: Dict[str, List[List[int]]],\n",
      "    task_id: str = \"MultiKernelTask\",\n",
      "    max_workers: int = 1,\n",
      "    calls_per_minute: int = 20,\n",
      "):\n",
      "    self.clustering_method = clustering_method\n",
      "    self.parent_kernel_label = parent_kernel_label\n",
      "    self.child_kernel_label = child_kernel_label\n",
      "    self.memory_kernel_dict = memory_kernel_dict\n",
      "    \n",
      "    self._setup_memory_kernel_group()\n",
      "    if path_group:\n",
      "        self.memory_kernel_group.path_group = path_group\n",
      "    else:\n",
      "        self.generate_task_paths()\n",
      "    self.system_prompt = system_prompt\n",
      "    self.chatbot = chatbot\n",
      "    self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\n",
      "    super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\n",
      "\n",
      "Function call: super\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def _setup_memory_kernel_group(self):\n",
      "    if self.clustering_method == \"HDBSCAN\":\n",
      "        print(\"Using HDBSCAN\")\n",
      "        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "    elif self.clustering_method == \"Spectral\":\n",
      "        print(\"Using Spectral\")\n",
      "        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\n",
      "    else:\n",
      "        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\n",
      "\n",
      "Function call: print\n",
      "Function call: HDBSCANMultiKernel\n",
      "Function call: print\n",
      "Function call: SpectralClusteringMultiKernel\n",
      "Function call: ValueError\n",
      "Related codes: ['\\n\\nclass HDBSCANMultiKernel(MultiKernel):\\n    def __init__(\\n        self,\\n        memory_kernel_dict: Dict[str, MemoryKernel],\\n        name: str = \"memory_kernel_group\",\\n    ):\\n        super().__init__(memory_kernel_dict, name)\\n        self.cluster_paths = HDBSCANPaths()\\n\\n    def generate_path_groups(self, num_clusters: int = None) -> None:\\n        path_group = {}\\n        for k, v in self.memory_kernel_dict.items():\\n            embeddings = v.node_embeddings\\n            if num_clusters is None:\\n                num_clusters = int(np.sqrt(len(embeddings)))\\n            paths = self.cluster_paths.create_paths(embeddings, num_clusters)\\n            path_group[k] = paths\\n        self.path_group = path_group\\n', '\\n\\nclass SpectralClusteringMultiKernel(MultiKernel):\\n    def __init__(\\n        self,\\n        memory_kernel_dict: Dict[str, MemoryKernel],\\n        name: str = \"memory_kernel_group\",\\n    ):\\n        super().__init__(memory_kernel_dict, name)\\n        self.cluster_paths = SpectralClusteringPaths()\\n\\n    def generate_path_groups(self, num_clusters: int = None) -> None:\\n        path_group = {}\\n        for k, v in self.memory_kernel_dict.items():\\n            A_k = v.A_k\\n            if num_clusters is None:\\n                num_clusters = int(np.sqrt(len(A_k)))\\n            paths = self.cluster_paths.create_paths(A_k, num_clusters)\\n            path_group[k] = paths\\n        self.path_group = path_group\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def generate_task_paths(self):\n",
      "    print(\"Generating task paths\")\n",
      "\n",
      "    self.memory_kernel_group.generate_path_groups()\n",
      "\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\n",
      "    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n",
      "    return chatbot.reply(message)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _execute_sub_task(self, sub_path) -> List[str]:\n",
      "    if self.parallel:\n",
      "        chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "    else:\n",
      "        chatbot_instance = self.chatbot\n",
      "\n",
      "    sub_results = {}\n",
      "    for i in sub_path:\n",
      "        print(f'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}')\n",
      "        try:\n",
      "            current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\n",
      "            response = self.llm_response(chatbot_instance, current_val, id=i)\n",
      "            sub_results[i] = response\n",
      "        except IndexError:\n",
      "            print(f\"Error: Invalid index {i} in sub_path\")\n",
      "            sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\n",
      "        except Exception as e:\n",
      "            print(f\"Error in sub_task for index {i}: {e}\")\n",
      "            sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\n",
      "\n",
      "    return sub_results\n",
      "\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def execute_task(self) -> None:\n",
      "    BaseTask.execute_task(self)\n",
      "\n",
      "    # Load the results from the JSON file\n",
      "    # with open(f\"{self.task_id}_results.json\", \"r\") as f:\n",
      "    #     task_results = json.load(f)\n",
      "    self._load_results_from_file()\n",
      "    task_results = self.results\n",
      "    new_values = []\n",
      "    #sort task_results by index and add to new_values 0- max values ascending\n",
      "    for task_result in task_results:\n",
      "        if isinstance(task_result, dict):\n",
      "            for key, value in task_result.items():\n",
      "                new_values.append((int(key), value))\n",
      "        elif isinstance(task_result, str):\n",
      "            print(f\"Error in task_result: {task_result}\")\n",
      "\n",
      "    new_values.sort(key=lambda x: x[0])\n",
      "    values = [x[1] for x in new_values]\n",
      "\n",
      "    task_memory_index = MemoryIndex()\n",
      "    task_memory_index.init_index(values=values)\n",
      "    # Create a new MemoryKernel with the results\n",
      "    new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\n",
      "\n",
      "    # Add the new MemoryKernel to the MultiKernel\n",
      "    self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\n",
      "    self.generate_task_paths()\n",
      "\n",
      "    #delete the results file\n",
      "    # os.remove(f\"{self.task_id}_results.json\")\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: int\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "Function call: MemoryIndex\n",
      "Related codes: ['\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\n\\n\\nclass MemoryIndex(NpIndex):\\n    \"\"\"\\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \\n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\\n    \"\"\"\\n    @staticmethod\\n    def check_uniform_context_type(context: List[Any]) -> None:\\n        \"\"\"Check if all context elements are of the same type.\"\"\"\\n        if not all(isinstance(x, type(context[0])) for x in context):\\n            raise ValueError(\"All context elements must be of the same type.\")\\n\\n    def __init__(\\n        self,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        context: Optional[List[Any]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ):\\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\\n        if context is not None and len(context) != len(values):\\n            raise ValueError(\"The context must have the same length as the values\")\\n\\n\\n        self.markdown = markdown\\n        if context is not None and values is not None:\\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\\n\\n        if context is not None:\\n            self.check_uniform_context_type(context)\\n            self.context_type = type(context[0])\\n\\n\\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\\n        if isinstance(identifier, list):\\n            return [self.get_context(value) for value in identifier]\\n        else:\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n            return self.context[value]\\n\\n    def clean_context(self):\\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\\n        self.context = {value: self.context[value] for value in self.values}\\n\\n    def add_to_context(self, value: str, context: Any):\\n        \"\"\" add a context to a value \"\"\"\\n        if not isinstance(context, self.context_type):\\n            raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if value in self.values:\\n            if value not in self.context:\\n                self.context[value] = []\\n            self.context[value].append(context)\\n\\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\\n        if isinstance(values, str):\\n            values = [values]\\n        NpIndex.add(self, values, embedding)\\n        if context is not None:\\n            for value, cont in zip(values, context):\\n                self.add_to_context(value, cont)\\n\\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\\n        if not isinstance(identifier, list):\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n        NpIndex.remove(self, identifier)\\n        if not isinstance(identifier, list):\\n            self.context.pop(value)\\n\\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\\n        #recover value from old_identifier\\n        if new_context is not None:\\n            if not isinstance(new_context, self.context_type):\\n                raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if not isinstance(old_identifier, list):\\n            old_id = self.identify_input(old_identifier)\\n            old_value = self.values[old_id]\\n            # Only perform the update if the old_value is not the same as the new_value.\\n            if old_value != new_value:\\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\\n\\n            if new_context is not None:\\n                self.context[new_value] = [new_context]\\n            else:\\n                self.context[new_value] = self.context.pop(old_value)\\n        else:\\n            self.context[new_value] = self.context.pop(old_value)\\n\\n    @classmethod\\n    def from_pandas(\\n        cls,\\n        data_frame: Union[pd.DataFrame, str],\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        if (\\n            isinstance(data_frame, str)\\n            and data_frame.endswith(\".csv\")\\n            and os.path.isfile(data_frame)\\n        ):\\n            logger.info(\"Loading the CSV file\")\\n            data_frame = pd.read_csv(data_frame)\\n            name = os.path.basename(data_frame).split(\".\")[0]\\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\\n            logger.info(\"Loading the pandas DataFrame\")\\n        else:\\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\\n\\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_pandas(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_hf_dataset(\\n        cls,\\n        dataset_url: str,\\n        value_column: str,\\n        data_split: str = \"train\",\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        dataset = load_dataset(dataset_url)[data_split]\\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_hf(dataset, context_columns)\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_polars(\\n        cls,\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        print(\"Loading the Polars DataFrame\")\\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_polars(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n\\n    @classmethod\\n    def from_python(\\n        cls,\\n        directory_path: str,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        markdown: str = \"python/markdown\",\\n        resolution: str = \"both\",\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class BaseTask:\n",
      "    def __init__(\n",
      "        self,\n",
      "        path: List[List[int]],\n",
      "        max_workers: int = 1,\n",
      "        task_id: str = \"task\",\n",
      "        calls_per_minute: int = 20,\n",
      "        backup: bool = True,\n",
      "        save_path: str = None,\n",
      "    ):\n",
      "        self.task_id = task_id\n",
      "        self.path = path\n",
      "        self.results = []\n",
      "        self.max_workers = max_workers\n",
      "        self.parallel = True if max_workers > 1 else False\n",
      "        self.rate_limiter = RateLimiter(calls_per_minute)\n",
      "        self.failed_sub_tasks = []\n",
      "        self.backup = backup\n",
      "        print(\"setting up savepath\")\n",
      "        self.save_path = save_path if save_path is not None else os.path.join(\"storage\", \"tasks\")\n",
      "\n",
      "    def _save_results_to_file(self) -> None:\n",
      "        os.makedirs(self.save_path, exist_ok=True)\n",
      "        with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"w\") as f:\n",
      "            json.dump(self.results, f)\n",
      "\n",
      "    def _load_results_from_file(self) -> None:\n",
      "        if os.path.exists(os.path.join(self.save_path, f\"{self.task_id}_results.json\")):\n",
      "            try:\n",
      "                with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"r\") as f:\n",
      "                    self.results = json.load(f)\n",
      "                    print(f\"Loaded {len(self.results)} results from file.\")\n",
      "            except Exception as e:\n",
      "                print(f\"Error loading results from file: {e}\")\n",
      "                print(\"Starting from scratch.\")\n",
      "        else:\n",
      "            print(\"No results file found, starting from scratch.\")\n",
      "\n",
      "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "        sub_results = []\n",
      "        for i in sub_path:\n",
      "            response = \"Implement the response function in the subclass\"\n",
      "            sub_results.append(response)\n",
      "        return sub_results\n",
      "\n",
      "    def execute_task(self) -> None:\n",
      "        if self.backup:\n",
      "            self._load_results_from_file()\n",
      "\n",
      "        with RateLimitedThreadPoolExecutor(\n",
      "            max_workers=self.max_workers,\n",
      "            calls_per_minute=self.rate_limiter.calls_per_minute,\n",
      "        ) as executor:\n",
      "            futures = []\n",
      "            print(f\"Executing task {self.task_id} using {self.max_workers} workers.\")\n",
      "\n",
      "            for i, sub_path in enumerate(self.path):\n",
      "                if i < len(self.results):\n",
      "                    pass\n",
      "                else:\n",
      "                    future = executor.submit(self._execute_sub_task, sub_path)\n",
      "                    futures.append((i, future))\n",
      "\n",
      "            for i, future in futures:\n",
      "                try:\n",
      "                    execution_start_time = time.time()\n",
      "                    sub_task_result = future.result()\n",
      "                    execution_end_time = time.time()\n",
      "                    print(\n",
      "                        f\"Sub-task {i} executed in {execution_end_time - execution_start_time:.2f} seconds.\"\n",
      "                    )\n",
      "\n",
      "                    save_start_time = time.time()\n",
      "                    self.results.append(sub_task_result)\n",
      "                    # self.results.append(sub_task_result)\n",
      "                    if self.backup:\n",
      "                        self._save_results_to_file()\n",
      "                    save_end_time = time.time()\n",
      "                    print(\n",
      "                        f\"Sub-task {i} results saved in {save_end_time - save_start_time:.2f} seconds.\"\n",
      "                    )\n",
      "                except Exception as e:\n",
      "                    print(f\"Error in sub-task {i}: {e}\")\n",
      "                    # default_result = f\"Error in sub-task {i}: {e}\"\n",
      "                    default_result =  {i:f\"Error in sub-task {i}: {e}\"} \n",
      "                    self.results.append(default_result)\n",
      "                    if self.backup:\n",
      "                        self._save_results_to_file()\n",
      "                    self.failed_sub_tasks.append((self.path[i], str(e)))\n",
      "\n",
      "                except KeyboardInterrupt:\n",
      "                    print(\"Keyboard interrupt detected, stopping task execution.\")\n",
      "                    executor.shutdown(wait=False)\n",
      "                    break\n",
      "\n",
      "        print(\"Task execution completed.\")\n",
      "\n",
      "    def work(self) -> List[Any]:\n",
      "        self.execute_task()\n",
      "        if not self.backup:\n",
      "            self._save_results_to_file()\n",
      "        work = []\n",
      "        for sub_result in self.results:\n",
      "            for index_id, response in sub_result.items():\n",
      "                work.append((index_id, response))\n",
      "        # sort the content to write by index_id\n",
      "        work.sort(key=lambda x: int(x[0]))\n",
      "        return work\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: RateLimiter\n",
      "Function call: print\n",
      "Function call: open\n",
      "Function call: open\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: RateLimitedThreadPoolExecutor\n",
      "Function call: print\n",
      "Function call: enumerate\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: str\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: int\n",
      "Related codes: ['\\n\\nclass RateLimiter:\\n    def __init__(self, calls_per_minute: int, verbose: bool = False):\\n        self.calls_per_minute = calls_per_minute\\n        self.interval = 60 / calls_per_minute\\n        self.lock = Lock()\\n        self.last_call_time = None\\n        self.verbose = verbose\\n\\n    def __call__(self, func):\\n        @functools.wraps(func)\\n        def wrapper(*args, **kwargs):\\n            with self.lock:\\n                if self.last_call_time is not None:\\n                    time_since_last_call = time.time() - self.last_call_time\\n                    if time_since_last_call < self.interval:\\n                        time_to_wait = self.interval - time_since_last_call\\n                        if self.verbose:\\n                            print(\\n                                f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\\n                            )\\n                        time.sleep(time_to_wait)\\n                    elif self.verbose:\\n                        print(\\n                            f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\\n                        )\\n                else:\\n                    if self.verbose:\\n                        print(\"RateLimiter: This is the first call, no wait required.\")\\n                self.last_call_time = time.time()\\n            return func(*args, **kwargs)\\n\\n        return wrapper\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\nclass RateLimitedThreadPoolExecutor(ThreadPoolExecutor):\\n    def __init__(self, max_workers=None, *args, **kwargs):\\n        super().__init__(max_workers)\\n        self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\\n\\n    def submit(self, fn, *args, **kwargs):\\n        rate_limited_fn = self.rate_limiter(fn)\\n        return super().submit(rate_limited_fn, *args, **kwargs)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    path: List[List[int]],\n",
      "    max_workers: int = 1,\n",
      "    task_id: str = \"task\",\n",
      "    calls_per_minute: int = 20,\n",
      "    backup: bool = True,\n",
      "    save_path: str = None,\n",
      "):\n",
      "    self.task_id = task_id\n",
      "    self.path = path\n",
      "    self.results = []\n",
      "    self.max_workers = max_workers\n",
      "    self.parallel = True if max_workers > 1 else False\n",
      "    self.rate_limiter = RateLimiter(calls_per_minute)\n",
      "    self.failed_sub_tasks = []\n",
      "    self.backup = backup\n",
      "    print(\"setting up savepath\")\n",
      "    self.save_path = save_path if save_path is not None else os.path.join(\"storage\", \"tasks\")\n",
      "\n",
      "Function call: RateLimiter\n",
      "Function call: print\n",
      "Related codes: ['\\n\\nclass RateLimiter:\\n    def __init__(self, calls_per_minute: int, verbose: bool = False):\\n        self.calls_per_minute = calls_per_minute\\n        self.interval = 60 / calls_per_minute\\n        self.lock = Lock()\\n        self.last_call_time = None\\n        self.verbose = verbose\\n\\n    def __call__(self, func):\\n        @functools.wraps(func)\\n        def wrapper(*args, **kwargs):\\n            with self.lock:\\n                if self.last_call_time is not None:\\n                    time_since_last_call = time.time() - self.last_call_time\\n                    if time_since_last_call < self.interval:\\n                        time_to_wait = self.interval - time_since_last_call\\n                        if self.verbose:\\n                            print(\\n                                f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\\n                            )\\n                        time.sleep(time_to_wait)\\n                    elif self.verbose:\\n                        print(\\n                            f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\\n                        )\\n                else:\\n                    if self.verbose:\\n                        print(\"RateLimiter: This is the first call, no wait required.\")\\n                self.last_call_time = time.time()\\n            return func(*args, **kwargs)\\n\\n        return wrapper\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _save_results_to_file(self) -> None:\n",
      "    os.makedirs(self.save_path, exist_ok=True)\n",
      "    with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"w\") as f:\n",
      "        json.dump(self.results, f)\n",
      "\n",
      "Function call: open\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _load_results_from_file(self) -> None:\n",
      "    if os.path.exists(os.path.join(self.save_path, f\"{self.task_id}_results.json\")):\n",
      "        try:\n",
      "            with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"r\") as f:\n",
      "                self.results = json.load(f)\n",
      "                print(f\"Loaded {len(self.results)} results from file.\")\n",
      "        except Exception as e:\n",
      "            print(f\"Error loading results from file: {e}\")\n",
      "            print(\"Starting from scratch.\")\n",
      "    else:\n",
      "        print(\"No results file found, starting from scratch.\")\n",
      "\n",
      "Function call: open\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "    sub_results = []\n",
      "    for i in sub_path:\n",
      "        response = \"Implement the response function in the subclass\"\n",
      "        sub_results.append(response)\n",
      "    return sub_results\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def execute_task(self) -> None:\n",
      "    if self.backup:\n",
      "        self._load_results_from_file()\n",
      "\n",
      "    with RateLimitedThreadPoolExecutor(\n",
      "        max_workers=self.max_workers,\n",
      "        calls_per_minute=self.rate_limiter.calls_per_minute,\n",
      "    ) as executor:\n",
      "        futures = []\n",
      "        print(f\"Executing task {self.task_id} using {self.max_workers} workers.\")\n",
      "\n",
      "        for i, sub_path in enumerate(self.path):\n",
      "            if i < len(self.results):\n",
      "                pass\n",
      "            else:\n",
      "                future = executor.submit(self._execute_sub_task, sub_path)\n",
      "                futures.append((i, future))\n",
      "\n",
      "        for i, future in futures:\n",
      "            try:\n",
      "                execution_start_time = time.time()\n",
      "                sub_task_result = future.result()\n",
      "                execution_end_time = time.time()\n",
      "                print(\n",
      "                    f\"Sub-task {i} executed in {execution_end_time - execution_start_time:.2f} seconds.\"\n",
      "                )\n",
      "\n",
      "                save_start_time = time.time()\n",
      "                self.results.append(sub_task_result)\n",
      "                # self.results.append(sub_task_result)\n",
      "                if self.backup:\n",
      "                    self._save_results_to_file()\n",
      "                save_end_time = time.time()\n",
      "                print(\n",
      "                    f\"Sub-task {i} results saved in {save_end_time - save_start_time:.2f} seconds.\"\n",
      "                )\n",
      "            except Exception as e:\n",
      "                print(f\"Error in sub-task {i}: {e}\")\n",
      "                # default_result = f\"Error in sub-task {i}: {e}\"\n",
      "                default_result =  {i:f\"Error in sub-task {i}: {e}\"} \n",
      "                self.results.append(default_result)\n",
      "                if self.backup:\n",
      "                    self._save_results_to_file()\n",
      "                self.failed_sub_tasks.append((self.path[i], str(e)))\n",
      "\n",
      "            except KeyboardInterrupt:\n",
      "                print(\"Keyboard interrupt detected, stopping task execution.\")\n",
      "                executor.shutdown(wait=False)\n",
      "                break\n",
      "\n",
      "    print(\"Task execution completed.\")\n",
      "\n",
      "Function call: RateLimitedThreadPoolExecutor\n",
      "Function call: print\n",
      "Function call: enumerate\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: str\n",
      "Function call: print\n",
      "Function call: print\n",
      "Related codes: ['\\n\\nclass RateLimitedThreadPoolExecutor(ThreadPoolExecutor):\\n    def __init__(self, max_workers=None, *args, **kwargs):\\n        super().__init__(max_workers)\\n        self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\\n\\n    def submit(self, fn, *args, **kwargs):\\n        rate_limited_fn = self.rate_limiter(fn)\\n        return super().submit(rate_limited_fn, *args, **kwargs)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def work(self) -> List[Any]:\n",
      "    self.execute_task()\n",
      "    if not self.backup:\n",
      "        self._save_results_to_file()\n",
      "    work = []\n",
      "    for sub_result in self.results:\n",
      "        for index_id, response in sub_result.items():\n",
      "            work.append((index_id, response))\n",
      "    # sort the content to write by index_id\n",
      "    work.sort(key=lambda x: int(x[0]))\n",
      "    return work\n",
      "\n",
      "Function call: int\n",
      "Related codes: ['\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class LLMReader(BaseTask):\n",
      "    def __init__(\n",
      "        self,\n",
      "        index: MemoryIndex,\n",
      "        path: List[List[int]],\n",
      "        chatbot: Chat,\n",
      "        read_func=None,\n",
      "        max_workers: int = 1,\n",
      "        task_id: str = \"LLMReadTask\",\n",
      "        calls_per_minute: int = 20,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize a LLMReadTask instance.\n",
      "\n",
      "        :param index: List of strings representing the queries.\n",
      "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
      "        :param chatbot: Chatbot instance used for executing queries.\n",
      "        :param max_workers: Maximum number of worker threads (default is 4).\n",
      "        \"\"\"\n",
      "        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute)\n",
      "        self.index = index\n",
      "        self.chatbot = chatbot\n",
      "        self.read_func = read_func if read_func else self.llm_response\n",
      "\n",
      "    def llm_response(chatbot: Chat, message: str, string_out=False):\n",
      "        if string_out:\n",
      "            return chatbot.reply(message)\n",
      "        return chatbot.query(message)\n",
      "\n",
      "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "        \"\"\"\n",
      "        Execute a sub-task using a separate copy of the chatbot instance. each sub-stasks uses a\n",
      "        a clean memory instance.\n",
      "\n",
      "        :param sub_path: List of indices representing the sub-task's sequence.\n",
      "        :return: List of strings representing the responses for each query in the sub-task.\n",
      "        \"\"\"\n",
      "        if self.parallel:\n",
      "            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
      "            chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "        else:\n",
      "            chatbot_instance = self.chatbot\n",
      "        if isinstance(self.chatbot, BaseThread):\n",
      "            chatbot_instance.reset_memory()\n",
      "\n",
      "        sub_results = []\n",
      "        for i in sub_path:\n",
      "            response = self.read_func(chatbot_instance, self.index.values[i])\n",
      "            sub_results.append(response)\n",
      "        return sub_results\n",
      "\n",
      "    def read(self):\n",
      "        self.execute_task()\n",
      "        return self.results\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseTask\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    index: MemoryIndex,\n",
      "    path: List[List[int]],\n",
      "    chatbot: Chat,\n",
      "    read_func=None,\n",
      "    max_workers: int = 1,\n",
      "    task_id: str = \"LLMReadTask\",\n",
      "    calls_per_minute: int = 20,\n",
      "):\n",
      "    \"\"\"\n",
      "        Initialize a LLMReadTask instance.\n",
      "\n",
      "        :param index: List of strings representing the queries.\n",
      "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
      "        :param chatbot: Chatbot instance used for executing queries.\n",
      "        :param max_workers: Maximum number of worker threads (default is 4).\n",
      "        \"\"\"\n",
      "    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute)\n",
      "    self.index = index\n",
      "    self.chatbot = chatbot\n",
      "    self.read_func = read_func if read_func else self.llm_response\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def llm_response(chatbot: Chat, message: str, string_out=False):\n",
      "    if string_out:\n",
      "        return chatbot.reply(message)\n",
      "    return chatbot.query(message)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "    \"\"\"\n",
      "        Execute a sub-task using a separate copy of the chatbot instance. each sub-stasks uses a\n",
      "        a clean memory instance.\n",
      "\n",
      "        :param sub_path: List of indices representing the sub-task's sequence.\n",
      "        :return: List of strings representing the responses for each query in the sub-task.\n",
      "        \"\"\"\n",
      "    if self.parallel:\n",
      "        # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
      "        chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "    else:\n",
      "        chatbot_instance = self.chatbot\n",
      "    if isinstance(self.chatbot, BaseThread):\n",
      "        chatbot_instance.reset_memory()\n",
      "\n",
      "    sub_results = []\n",
      "    for i in sub_path:\n",
      "        response = self.read_func(chatbot_instance, self.index.values[i])\n",
      "        sub_results.append(response)\n",
      "    return sub_results\n",
      "\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def read(self):\n",
      "    self.execute_task()\n",
      "    return self.results\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class LLMWriter(BaseTask):\n",
      "    def __init__(\n",
      "        self,\n",
      "        index: MemoryIndex,\n",
      "        path: List[List[int]],\n",
      "        chatbot: Chat,\n",
      "        write_func=None,\n",
      "        context=None,\n",
      "        task_name=\"summary\",\n",
      "        max_workers: int = 1,\n",
      "        task_id: str = \"LLMWriteTask\",\n",
      "        calls_per_minute: int = 20,\n",
      "        backup: bool = True,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize a LLMWriteTask instance.\n",
      "\n",
      "        :param index: List of strings representing the queries.\n",
      "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
      "        :param chatbot: Chatbot instance used for executing queries.\n",
      "        :param max_workers: Maximum number of worker threads (default is 4).\n",
      "        \"\"\"\n",
      "        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\n",
      "        self.index = index\n",
      "        self.chatbot = chatbot\n",
      "        self.write_func = write_func if write_func else self.llm_response\n",
      "        self.new_index_name = self.index.name + f\"_{task_name}\"\n",
      "        self.context = context\n",
      "\n",
      "    def llm_response(self, chatbot: Chat, message: str, context=None, id=None):\n",
      "        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n",
      "        # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\n",
      "        #     return \"the message is too long to be processed\"\n",
      "        # moved the error catching to multi-threading but custom method could report the error here\n",
      "        return chatbot.reply(message)\n",
      "\n",
      "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "        \"\"\"\n",
      "        Execute a sub-task using a separate copy of the chatbot instance.\n",
      "\n",
      "        :param sub_path: List of indices representing the sub-task's sequence.\n",
      "        :return: List of strings representing the responses for each query in the sub-task.\n",
      "        \"\"\"\n",
      "        if self.parallel:\n",
      "            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
      "            chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "        else:\n",
      "            chatbot_instance = self.chatbot\n",
      "        if isinstance(self.chatbot, BaseThread):\n",
      "            chatbot_instance.reset_memory()\n",
      "\n",
      "        sub_results = {}\n",
      "        for i in sub_path:\n",
      "            current_val = self.index.values[i]\n",
      "            response = self.write_func(\n",
      "                chatbot_instance, current_val, self.context, id=i\n",
      "            )\n",
      "            sub_results[i] = response\n",
      "        return sub_results\n",
      "\n",
      "    def write(self):\n",
      "        content_to_write = self.work()\n",
      "        self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\n",
      "        self.new_index.save()\n",
      "        return self.new_index\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseTask\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: MemoryIndex\n",
      "Related codes: ['\\n\\n\\nclass MemoryIndex(NpIndex):\\n    \"\"\"\\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \\n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\\n    \"\"\"\\n    @staticmethod\\n    def check_uniform_context_type(context: List[Any]) -> None:\\n        \"\"\"Check if all context elements are of the same type.\"\"\"\\n        if not all(isinstance(x, type(context[0])) for x in context):\\n            raise ValueError(\"All context elements must be of the same type.\")\\n\\n    def __init__(\\n        self,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        context: Optional[List[Any]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ):\\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\\n        if context is not None and len(context) != len(values):\\n            raise ValueError(\"The context must have the same length as the values\")\\n\\n\\n        self.markdown = markdown\\n        if context is not None and values is not None:\\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\\n\\n        if context is not None:\\n            self.check_uniform_context_type(context)\\n            self.context_type = type(context[0])\\n\\n\\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\\n        if isinstance(identifier, list):\\n            return [self.get_context(value) for value in identifier]\\n        else:\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n            return self.context[value]\\n\\n    def clean_context(self):\\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\\n        self.context = {value: self.context[value] for value in self.values}\\n\\n    def add_to_context(self, value: str, context: Any):\\n        \"\"\" add a context to a value \"\"\"\\n        if not isinstance(context, self.context_type):\\n            raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if value in self.values:\\n            if value not in self.context:\\n                self.context[value] = []\\n            self.context[value].append(context)\\n\\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\\n        if isinstance(values, str):\\n            values = [values]\\n        NpIndex.add(self, values, embedding)\\n        if context is not None:\\n            for value, cont in zip(values, context):\\n                self.add_to_context(value, cont)\\n\\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\\n        if not isinstance(identifier, list):\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n        NpIndex.remove(self, identifier)\\n        if not isinstance(identifier, list):\\n            self.context.pop(value)\\n\\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\\n        #recover value from old_identifier\\n        if new_context is not None:\\n            if not isinstance(new_context, self.context_type):\\n                raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if not isinstance(old_identifier, list):\\n            old_id = self.identify_input(old_identifier)\\n            old_value = self.values[old_id]\\n            # Only perform the update if the old_value is not the same as the new_value.\\n            if old_value != new_value:\\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\\n\\n            if new_context is not None:\\n                self.context[new_value] = [new_context]\\n            else:\\n                self.context[new_value] = self.context.pop(old_value)\\n        else:\\n            self.context[new_value] = self.context.pop(old_value)\\n\\n    @classmethod\\n    def from_pandas(\\n        cls,\\n        data_frame: Union[pd.DataFrame, str],\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        if (\\n            isinstance(data_frame, str)\\n            and data_frame.endswith(\".csv\")\\n            and os.path.isfile(data_frame)\\n        ):\\n            logger.info(\"Loading the CSV file\")\\n            data_frame = pd.read_csv(data_frame)\\n            name = os.path.basename(data_frame).split(\".\")[0]\\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\\n            logger.info(\"Loading the pandas DataFrame\")\\n        else:\\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\\n\\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_pandas(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_hf_dataset(\\n        cls,\\n        dataset_url: str,\\n        value_column: str,\\n        data_split: str = \"train\",\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        dataset = load_dataset(dataset_url)[data_split]\\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_hf(dataset, context_columns)\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_polars(\\n        cls,\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        print(\"Loading the Polars DataFrame\")\\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_polars(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n\\n    @classmethod\\n    def from_python(\\n        cls,\\n        directory_path: str,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        markdown: str = \"python/markdown\",\\n        resolution: str = \"both\",\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    index: MemoryIndex,\n",
      "    path: List[List[int]],\n",
      "    chatbot: Chat,\n",
      "    write_func=None,\n",
      "    context=None,\n",
      "    task_name=\"summary\",\n",
      "    max_workers: int = 1,\n",
      "    task_id: str = \"LLMWriteTask\",\n",
      "    calls_per_minute: int = 20,\n",
      "    backup: bool = True,\n",
      "):\n",
      "    \"\"\"\n",
      "        Initialize a LLMWriteTask instance.\n",
      "\n",
      "        :param index: List of strings representing the queries.\n",
      "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
      "        :param chatbot: Chatbot instance used for executing queries.\n",
      "        :param max_workers: Maximum number of worker threads (default is 4).\n",
      "        \"\"\"\n",
      "    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\n",
      "    self.index = index\n",
      "    self.chatbot = chatbot\n",
      "    self.write_func = write_func if write_func else self.llm_response\n",
      "    self.new_index_name = self.index.name + f\"_{task_name}\"\n",
      "    self.context = context\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def llm_response(self, chatbot: Chat, message: str, context=None, id=None):\n",
      "    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\n",
      "    # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\n",
      "    #     return \"the message is too long to be processed\"\n",
      "    # moved the error catching to multi-threading but custom method could report the error here\n",
      "    return chatbot.reply(message)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
      "    \"\"\"\n",
      "        Execute a sub-task using a separate copy of the chatbot instance.\n",
      "\n",
      "        :param sub_path: List of indices representing the sub-task's sequence.\n",
      "        :return: List of strings representing the responses for each query in the sub-task.\n",
      "        \"\"\"\n",
      "    if self.parallel:\n",
      "        # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
      "        chatbot_instance = copy.deepcopy(self.chatbot)\n",
      "    else:\n",
      "        chatbot_instance = self.chatbot\n",
      "    if isinstance(self.chatbot, BaseThread):\n",
      "        chatbot_instance.reset_memory()\n",
      "\n",
      "    sub_results = {}\n",
      "    for i in sub_path:\n",
      "        current_val = self.index.values[i]\n",
      "        response = self.write_func(\n",
      "            chatbot_instance, current_val, self.context, id=i\n",
      "        )\n",
      "        sub_results[i] = response\n",
      "    return sub_results\n",
      "\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def write(self):\n",
      "    content_to_write = self.work()\n",
      "    self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\n",
      "    self.new_index.save()\n",
      "    return self.new_index\n",
      "\n",
      "Function call: MemoryIndex\n",
      "Related codes: ['\\n\\n\\nclass MemoryIndex(NpIndex):\\n    \"\"\"\\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \\n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\\n    \"\"\"\\n    @staticmethod\\n    def check_uniform_context_type(context: List[Any]) -> None:\\n        \"\"\"Check if all context elements are of the same type.\"\"\"\\n        if not all(isinstance(x, type(context[0])) for x in context):\\n            raise ValueError(\"All context elements must be of the same type.\")\\n\\n    def __init__(\\n        self,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        context: Optional[List[Any]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ):\\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\\n        if context is not None and len(context) != len(values):\\n            raise ValueError(\"The context must have the same length as the values\")\\n\\n\\n        self.markdown = markdown\\n        if context is not None and values is not None:\\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\\n\\n        if context is not None:\\n            self.check_uniform_context_type(context)\\n            self.context_type = type(context[0])\\n\\n\\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\\n        if isinstance(identifier, list):\\n            return [self.get_context(value) for value in identifier]\\n        else:\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n            return self.context[value]\\n\\n    def clean_context(self):\\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\\n        self.context = {value: self.context[value] for value in self.values}\\n\\n    def add_to_context(self, value: str, context: Any):\\n        \"\"\" add a context to a value \"\"\"\\n        if not isinstance(context, self.context_type):\\n            raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if value in self.values:\\n            if value not in self.context:\\n                self.context[value] = []\\n            self.context[value].append(context)\\n\\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\\n        if isinstance(values, str):\\n            values = [values]\\n        NpIndex.add(self, values, embedding)\\n        if context is not None:\\n            for value, cont in zip(values, context):\\n                self.add_to_context(value, cont)\\n\\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\\n        if not isinstance(identifier, list):\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n        NpIndex.remove(self, identifier)\\n        if not isinstance(identifier, list):\\n            self.context.pop(value)\\n\\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\\n        #recover value from old_identifier\\n        if new_context is not None:\\n            if not isinstance(new_context, self.context_type):\\n                raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if not isinstance(old_identifier, list):\\n            old_id = self.identify_input(old_identifier)\\n            old_value = self.values[old_id]\\n            # Only perform the update if the old_value is not the same as the new_value.\\n            if old_value != new_value:\\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\\n\\n            if new_context is not None:\\n                self.context[new_value] = [new_context]\\n            else:\\n                self.context[new_value] = self.context.pop(old_value)\\n        else:\\n            self.context[new_value] = self.context.pop(old_value)\\n\\n    @classmethod\\n    def from_pandas(\\n        cls,\\n        data_frame: Union[pd.DataFrame, str],\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        if (\\n            isinstance(data_frame, str)\\n            and data_frame.endswith(\".csv\")\\n            and os.path.isfile(data_frame)\\n        ):\\n            logger.info(\"Loading the CSV file\")\\n            data_frame = pd.read_csv(data_frame)\\n            name = os.path.basename(data_frame).split(\".\")[0]\\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\\n            logger.info(\"Loading the pandas DataFrame\")\\n        else:\\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\\n\\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_pandas(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_hf_dataset(\\n        cls,\\n        dataset_url: str,\\n        value_column: str,\\n        data_split: str = \"train\",\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        dataset = load_dataset(dataset_url)[data_split]\\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_hf(dataset, context_columns)\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_polars(\\n        cls,\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        print(\"Loading the Polars DataFrame\")\\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_polars(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n\\n    @classmethod\\n    def from_python(\\n        cls,\\n        directory_path: str,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        markdown: str = \"python/markdown\",\\n        resolution: str = \"both\",\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class DiscreteDataInt(BDType):\n",
      "    alphabet: Optional[Set[int]] = Field(None, description=\"Set of allowed discrete variables. All elements should be integers.\")\n",
      "    value: int = Field(..., description=\"The discrete data value. It should be an integer.\")\n",
      "\n",
      "    @field_validator('alphabet')\n",
      "    def check_alphabet(cls, v):\n",
      "        if not all(isinstance(item, int) for item in v):\n",
      "            raise ValueError(\"All elements in 'alphabet' should be integers.\")\n",
      "        return v\n",
      "\n",
      "    @field_validator('value')\n",
      "    def check_value(cls, v, info: FieldValidationInfo):\n",
      "        alphabet = info.data.get('alphabet')\n",
      "        if alphabet is not None and v not in alphabet:\n",
      "            raise ValueError(\"Value must be in the alphabet.\")\n",
      "        return v\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BDType\"]\n",
      "]\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: field_validator\n",
      "Function call: all\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: field_validator\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator('alphabet')\n",
      "def check_alphabet(cls, v):\n",
      "    if not all(isinstance(item, int) for item in v):\n",
      "        raise ValueError(\"All elements in 'alphabet' should be integers.\")\n",
      "    return v\n",
      "\n",
      "Function call: field_validator\n",
      "Function call: all\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator('value')\n",
      "def check_value(cls, v, info: FieldValidationInfo):\n",
      "    alphabet = info.data.get('alphabet')\n",
      "    if alphabet is not None and v not in alphabet:\n",
      "        raise ValueError(\"Value must be in the alphabet.\")\n",
      "    return v\n",
      "\n",
      "Function call: field_validator\n",
      "Function call: ValueError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class DiscreteDataStr(BDType):\n",
      "    alphabet: Optional[Set[str]] = Field(None, description=\"Set of allowed discrete variables. All elements should be strings.\")\n",
      "    value: str = Field(..., description=\"The discrete data value. It should be a string.\")\n",
      "\n",
      "    @field_validator('alphabet')\n",
      "    def check_alphabet(cls, v):\n",
      "        if not all(isinstance(item, str) for item in v):\n",
      "            raise ValueError(\"All elements in 'alphabet' should be strings.\")\n",
      "        return v\n",
      "\n",
      "    @field_validator('value')\n",
      "    def check_value(cls, v, info: FieldValidationInfo):\n",
      "        alphabet = info.data.get('alphabet')\n",
      "        if alphabet is not None and v not in alphabet:\n",
      "            raise ValueError(\"Value must be in the alphabet.\")\n",
      "        return v\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BDType\"]\n",
      "]\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: field_validator\n",
      "Function call: all\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: field_validator\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator('alphabet')\n",
      "def check_alphabet(cls, v):\n",
      "    if not all(isinstance(item, str) for item in v):\n",
      "        raise ValueError(\"All elements in 'alphabet' should be strings.\")\n",
      "    return v\n",
      "\n",
      "Function call: field_validator\n",
      "Function call: all\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator('value')\n",
      "def check_value(cls, v, info: FieldValidationInfo):\n",
      "    alphabet = info.data.get('alphabet')\n",
      "    if alphabet is not None and v not in alphabet:\n",
      "        raise ValueError(\"Value must be in the alphabet.\")\n",
      "    return v\n",
      "\n",
      "Function call: field_validator\n",
      "Function call: ValueError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# The DiscreteDataInt and DiscreteDataStr models are defined as before\n",
      "\n",
      "class DiscreteDataList(BaseModel):\n",
      "    alphabet: Optional[Set[Union[int, str]]] = Field(None, description=\"Set of allowed discrete variables. All elements should be of the same type (either integers or strings).\")\n",
      "    value: List[Union[DiscreteDataInt, DiscreteDataStr]] = Field(..., description=\"The list of discrete data values. All elements should be either DiscreteDataInt or DiscreteDataStr, not a mix.\")\n",
      "\n",
      "    @field_validator('value')\n",
      "    def check_alphabets(cls, value, info: FieldValidationInfo):\n",
      "        list_alphabet = info.data.get('alphabet')\n",
      "        if list_alphabet is not None:\n",
      "            for item in value:\n",
      "                item_alphabet = item.alphabet\n",
      "                if item_alphabet is not None and not set(item_alphabet).issubset(list_alphabet):\n",
      "                    raise ValueError(f\"Item alphabet {item_alphabet} is not a subset of the list alphabet {list_alphabet}.\")\n",
      "        return value\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseModel\"]\n",
      "]\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: field_validator\n",
      "Function call: set\n",
      "Function call: ValueError\n",
      "Related codes: ['\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator('value')\n",
      "def check_alphabets(cls, value, info: FieldValidationInfo):\n",
      "    list_alphabet = info.data.get('alphabet')\n",
      "    if list_alphabet is not None:\n",
      "        for item in value:\n",
      "            item_alphabet = item.alphabet\n",
      "            if item_alphabet is not None and not set(item_alphabet).issubset(list_alphabet):\n",
      "                raise ValueError(f\"Item alphabet {item_alphabet} is not a subset of the list alphabet {list_alphabet}.\")\n",
      "    return value\n",
      "\n",
      "Function call: field_validator\n",
      "Function call: set\n",
      "Function call: ValueError\n",
      "Related codes: ['\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "class MultiDimensionalDiscrete(BDType):\n",
      "    value: List[Union[DiscreteDataInt, DiscreteDataStr]] = Field(..., description=\"The multidimensional discrete data value. It should be a list of either DiscreteDataInt or DiscreteDataStr.\")\n",
      "    type_dictionary: Dict[int, str] = Field(default_factory=dict, description=\"The helper dictionary containing the type of each dimension of the value list.\")\n",
      "    def __init__(self, **data):\n",
      "        super().__init__(**data)\n",
      "        self.type_dictionary = {i: item.__class__.__name__ for i, item in enumerate(self.value)}\n",
      "        \n",
      "    @field_validator('value')\n",
      "    def check_value(cls, value):\n",
      "        if len(value) < 2:\n",
      "            raise ValueError(\"For multidimensional discrete data, size of the list should be at least 2. For less than 2, use DiscreteDataInt or DiscreteDataStr.\")\n",
      "        return value\n",
      "    \n",
      "    from pydantic import BaseModel, Field, field_validator\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BDType\"]\n",
      "]\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: super\n",
      "Function call: enumerate\n",
      "Function call: field_validator\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, **data):\n",
      "    super().__init__(**data)\n",
      "    self.type_dictionary = {i: item.__class__.__name__ for i, item in enumerate(self.value)}\n",
      "    \n",
      "\n",
      "Function call: super\n",
      "Function call: enumerate\n",
      "\n",
      "********************************************************************************\n",
      "@field_validator('value')\n",
      "def check_value(cls, value):\n",
      "    if len(value) < 2:\n",
      "        raise ValueError(\"For multidimensional discrete data, size of the list should be at least 2. For less than 2, use DiscreteDataInt or DiscreteDataStr.\")\n",
      "    return value\n",
      "\n",
      "Function call: field_validator\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MultiDimensionalDiscreteList(BDType):\n",
      "    values: List[MultiDimensionalDiscrete] = Field(..., description=\"The list of multidimensional discrete data values. All elements should be instances of MultiDimensionalDiscrete.\")\n",
      "    joint_alphabet: Optional[Set[Tuple[Any, ...]]] = Field(None, description=\"Set of tuples representing allowed discrete variable combinations. All elements should be tuples of the same length as the number of dimensions in each joint discrete variable.\")\n",
      "\n",
      "    @field_validator('values')\n",
      "    def check_type_dictionaries(cls, values):\n",
      "        first_type_dictionary = values[0].type_dictionary\n",
      "        for value in values[1:]:\n",
      "            if value.type_dictionary != first_type_dictionary:\n",
      "                raise ValueError(\"All elements in 'values' should have the same 'type_dictionary'.\")\n",
      "        return values\n",
      "\n",
      "    @field_validator('joint_alphabet')\n",
      "    def check_joint_alphabet(cls, v, info):\n",
      "        if v is not None and \"values\" in info.data:\n",
      "            expected_tuple_length = len(info.data[\"values\"][0].value)\n",
      "            for item in v:\n",
      "                if not isinstance(item, tuple) or len(item) != expected_tuple_length:\n",
      "                    raise ValueError(f\"Each element in 'joint_alphabet' should be a tuple of length {expected_tuple_length}.\")\n",
      "                for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\n",
      "                    if dim_alphabet is not None and dim_value not in dim_alphabet:\n",
      "                        raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\n",
      "        return v\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BDType\"]\n",
      "]\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: field_validator\n",
      "Function call: ValueError\n",
      "Function call: field_validator\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: zip\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator('values')\n",
      "def check_type_dictionaries(cls, values):\n",
      "    first_type_dictionary = values[0].type_dictionary\n",
      "    for value in values[1:]:\n",
      "        if value.type_dictionary != first_type_dictionary:\n",
      "            raise ValueError(\"All elements in 'values' should have the same 'type_dictionary'.\")\n",
      "    return values\n",
      "\n",
      "Function call: field_validator\n",
      "Function call: ValueError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator('joint_alphabet')\n",
      "def check_joint_alphabet(cls, v, info):\n",
      "    if v is not None and \"values\" in info.data:\n",
      "        expected_tuple_length = len(info.data[\"values\"][0].value)\n",
      "        for item in v:\n",
      "            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\n",
      "                raise ValueError(f\"Each element in 'joint_alphabet' should be a tuple of length {expected_tuple_length}.\")\n",
      "            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\n",
      "                if dim_alphabet is not None and dim_value not in dim_alphabet:\n",
      "                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\n",
      "    return v\n",
      "\n",
      "Function call: field_validator\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: zip\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class RealData(BDType):\n",
      "    range: Optional[Tuple[Union[float, int], Union[float, int]]] = Field(None, description=\"An optional inclusive range (min, max) for the value.\")\n",
      "    value: Union[float, int] = Field(..., description=\"The real value data. It should be a float or an integer.\")\n",
      "\n",
      "    @field_validator(\"value\")\n",
      "    def validate_value(cls, v, values):\n",
      "        value_range = values.data[\"range\"]\n",
      "        if value_range is not None:\n",
      "            min_value, max_value = value_range\n",
      "            if not min_value <= v <= max_value:\n",
      "                raise ValueError(f\"Value {v} is not within the specified range {value_range}.\")\n",
      "        return v\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BDType\"]\n",
      "]\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: field_validator\n",
      "Function call: ValueError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator(\"value\")\n",
      "def validate_value(cls, v, values):\n",
      "    value_range = values.data[\"range\"]\n",
      "    if value_range is not None:\n",
      "        min_value, max_value = value_range\n",
      "        if not min_value <= v <= max_value:\n",
      "            raise ValueError(f\"Value {v} is not within the specified range {value_range}.\")\n",
      "    return v\n",
      "\n",
      "Function call: field_validator\n",
      "Function call: ValueError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class RealDataList(BDType):\n",
      "    range: Optional[Tuple[Union[float, int], Union[float, int]]] = Field(None, description=\"An optional inclusive range (min, max) for the values.\")\n",
      "    values: List[RealData] = Field(..., description=\"The list of real value data. Each should be a RealeData object.\")\n",
      "\n",
      "    @field_validator(\"values\")\n",
      "    def validate_values(cls, values, values_dict):\n",
      "        list_range = values_dict.data.get(\"range\")\n",
      "        if list_range is not None:\n",
      "            min_value, max_value = list_range\n",
      "            for value in values:\n",
      "                if not min_value <= value.value <= max_value:\n",
      "                    raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {list_range}.\")\n",
      "        return values\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BDType\"]\n",
      "]\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: field_validator\n",
      "Function call: ValueError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator(\"values\")\n",
      "def validate_values(cls, values, values_dict):\n",
      "    list_range = values_dict.data.get(\"range\")\n",
      "    if list_range is not None:\n",
      "        min_value, max_value = list_range\n",
      "        for value in values:\n",
      "            if not min_value <= value.value <= max_value:\n",
      "                raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {list_range}.\")\n",
      "    return values\n",
      "\n",
      "Function call: field_validator\n",
      "Function call: ValueError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MultiDimensionalReal(BDType):\n",
      "    range: Optional[Union[Tuple[Union[float, int], Union[float, int]], List[Tuple[Union[float, int], Union[float, int]]]]] = Field(\n",
      "        None, \n",
      "        description=\"An optional inclusive range (min, max) for the values. If a tuple, applies to all dimensions. If a list, it must match the dimension length.\"\n",
      "    )\n",
      "    values: List[RealData] = Field(\n",
      "        ..., \n",
      "        description=\"The list of real data for each dimension. Each should be a RealData object.\"\n",
      "    )\n",
      "\n",
      "    @field_validator(\"values\")\n",
      "    def validate_values(cls, values, values_dict):\n",
      "        range_values = values_dict.data.get(\"range\")\n",
      "        if range_values is not None:\n",
      "            # If range is a tuple, apply it to all dimensions\n",
      "            if isinstance(range_values, tuple):\n",
      "                min_value, max_value = range_values\n",
      "                for value in values:\n",
      "                    if not min_value <= value.value <= max_value:\n",
      "                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n",
      "            # If range is a list, it must have the same length as values\n",
      "            elif isinstance(range_values, list):\n",
      "                if len(values) != len(range_values):\n",
      "                    raise ValueError(\"If range is a list, it must have the same length as values.\")\n",
      "                for value, (min_value, max_value) in zip(values, range_values):\n",
      "                    if not min_value <= value.value <= max_value:\n",
      "                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n",
      "        return values\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BDType\"]\n",
      "]\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: field_validator\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: zip\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator(\"values\")\n",
      "def validate_values(cls, values, values_dict):\n",
      "    range_values = values_dict.data.get(\"range\")\n",
      "    if range_values is not None:\n",
      "        # If range is a tuple, apply it to all dimensions\n",
      "        if isinstance(range_values, tuple):\n",
      "            min_value, max_value = range_values\n",
      "            for value in values:\n",
      "                if not min_value <= value.value <= max_value:\n",
      "                    raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n",
      "        # If range is a list, it must have the same length as values\n",
      "        elif isinstance(range_values, list):\n",
      "            if len(values) != len(range_values):\n",
      "                raise ValueError(\"If range is a list, it must have the same length as values.\")\n",
      "            for value, (min_value, max_value) in zip(values, range_values):\n",
      "                if not min_value <= value.value <= max_value:\n",
      "                    raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n",
      "    return values\n",
      "\n",
      "Function call: field_validator\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: zip\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MultiDimensionalRealList(BDType):\n",
      "    range: Optional[Union[Tuple[Union[float, int], Union[float, int]], List[Tuple[Union[float, int], Union[float, int]]]]] = Field(\n",
      "        None, \n",
      "        description=\"An optional inclusive range (min, max) for the values in all dimensions. If a tuple, applies to all dimensions. If a list, it must match the dimension length.\"\n",
      "    )\n",
      "    values: List[MultiDimensionalReal] = Field(\n",
      "        ..., \n",
      "        description=\"The list of multi-dimensional real data. Each should be a MultiDimensionalReal object.\"\n",
      "    )\n",
      "\n",
      "    @field_validator(\"values\")\n",
      "    def validate_values(cls, values, values_dict):\n",
      "        range_values = values_dict.data.get(\"range\")\n",
      "        dimension_length = len(values[0].values) if values else 0\n",
      "        if range_values is not None:\n",
      "            # If range is a tuple, apply it to all dimensions\n",
      "            if isinstance(range_values, tuple):\n",
      "                min_value, max_value = range_values\n",
      "                for multi_real in values:\n",
      "                    if len(multi_real.values) != dimension_length:\n",
      "                        raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n",
      "                    for value in multi_real.values:\n",
      "                        if not min_value <= value.value <= max_value:\n",
      "                            raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n",
      "            # If range is a list, it must have the same length as values in each dimension\n",
      "            elif isinstance(range_values, list):\n",
      "                if len(range_values) != dimension_length:\n",
      "                    raise ValueError(\"If range is a list, it must have the same length as values in each dimension.\")\n",
      "                for multi_real in values:\n",
      "                    if len(multi_real.values) != dimension_length:\n",
      "                        raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n",
      "                    for value, (min_value, max_value) in zip(multi_real.values, range_values):\n",
      "                        if not min_value <= value.value <= max_value:\n",
      "                            raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n",
      "        return values\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BDType\"]\n",
      "]\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: field_validator\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: zip\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator(\"values\")\n",
      "def validate_values(cls, values, values_dict):\n",
      "    range_values = values_dict.data.get(\"range\")\n",
      "    dimension_length = len(values[0].values) if values else 0\n",
      "    if range_values is not None:\n",
      "        # If range is a tuple, apply it to all dimensions\n",
      "        if isinstance(range_values, tuple):\n",
      "            min_value, max_value = range_values\n",
      "            for multi_real in values:\n",
      "                if len(multi_real.values) != dimension_length:\n",
      "                    raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n",
      "                for value in multi_real.values:\n",
      "                    if not min_value <= value.value <= max_value:\n",
      "                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range {range_values}.\")\n",
      "        # If range is a list, it must have the same length as values in each dimension\n",
      "        elif isinstance(range_values, list):\n",
      "            if len(range_values) != dimension_length:\n",
      "                raise ValueError(\"If range is a list, it must have the same length as values in each dimension.\")\n",
      "            for multi_real in values:\n",
      "                if len(multi_real.values) != dimension_length:\n",
      "                    raise ValueError(\"All MultiDimensionalReal in the list must have the same length.\")\n",
      "                for value, (min_value, max_value) in zip(multi_real.values, range_values):\n",
      "                    if not min_value <= value.value <= max_value:\n",
      "                        raise ValueError(f\"Value {value.value} of RealData object is not within the specified range ({min_value}, {max_value}).\")\n",
      "    return values\n",
      "\n",
      "Function call: field_validator\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: zip\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class NaturalLanguageSingle(BDType):\n",
      "    text: str = Field(..., description=\"The natural language text. It should be less than or equal to `max_tokens` in length when tokenized.\")\n",
      "    max_tokens: int = Field(8000, description=\"The maximum allowed length of the text in tokens. The default value is 8000.\")\n",
      "    \n",
      "    @field_validator(\"text\")\n",
      "    def validate_text(cls, v, info):\n",
      "        try:\n",
      "            # Tokenize the text and get the token count\n",
      "            token_count = len(tokenizer.encode(v))\n",
      "        except Exception as e:\n",
      "            raise ValueError(\"Failed to tokenize text.\") from e\n",
      "\n",
      "        # Get max_tokens from info.data, if not available, default to 8000\n",
      "        max_tokens = info.data.get(\"max_tokens\", 8000)\n",
      "\n",
      "        if token_count > max_tokens:\n",
      "            raise ValueError(f\"Text is longer than {max_tokens} tokens.\")\n",
      "\n",
      "        return v\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BDType\"]\n",
      "]\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: field_validator\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator(\"text\")\n",
      "def validate_text(cls, v, info):\n",
      "    try:\n",
      "        # Tokenize the text and get the token count\n",
      "        token_count = len(tokenizer.encode(v))\n",
      "    except Exception as e:\n",
      "        raise ValueError(\"Failed to tokenize text.\") from e\n",
      "\n",
      "    # Get max_tokens from info.data, if not available, default to 8000\n",
      "    max_tokens = info.data.get(\"max_tokens\", 8000)\n",
      "\n",
      "    if token_count > max_tokens:\n",
      "        raise ValueError(f\"Text is longer than {max_tokens} tokens.\")\n",
      "\n",
      "    return v\n",
      "\n",
      "Function call: field_validator\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class NaturalLanguageList(BDType):\n",
      "    texts: List[NaturalLanguageSingle] = Field(..., description=\"A list of `NaturalLanguageSingle` objects. Each object should pass the validation requirements of the `NaturalLanguageSingle` class.\")\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BDType\"]\n",
      "]\n",
      "Function call: Field\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# Base class for all bd_types\n",
      "class BDType(BaseModel):\n",
      "    source: str = Field(\"babydragon\", description=\"The source of the data.\")\n",
      "    timestamp: Optional[datetime.datetime] = Field(None, description=\"When the data was collected or created. If not provided, the current time is used.\")\n",
      "    id: uuid.UUID = Field(default_factory=uuid.uuid4, description=\"Unique identifier of the data.\")\n",
      "    data_name: Optional[str] = Field(None, description=\"Name of the data.\")\n",
      "    elements_name: Optional[List[str]] = Field(None, description=\"Names of the elements if the data is a list.\")\n",
      "\n",
      "    @field_validator(\"timestamp\")\n",
      "    def set_timestamp(cls, v):\n",
      "        return v or datetime.datetime.now()\n",
      "\n",
      "    @field_validator(\"id\")\n",
      "    def set_id(cls, values, **kwargs):\n",
      "        if \"id\" not in values:\n",
      "            values[\"id\"] = uuid.uuid4()\n",
      "        return values\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseModel\"]\n",
      "]\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: Field\n",
      "Function call: field_validator\n",
      "Function call: field_validator\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator(\"timestamp\")\n",
      "def set_timestamp(cls, v):\n",
      "    return v or datetime.datetime.now()\n",
      "\n",
      "Function call: field_validator\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@field_validator(\"id\")\n",
      "def set_id(cls, values, **kwargs):\n",
      "    if \"id\" not in values:\n",
      "        values[\"id\"] = uuid.uuid4()\n",
      "    return values\n",
      "\n",
      "Function call: field_validator\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class MemoryKernel(MemoryIndex):\n",
      "    def __init__(\n",
      "        self,\n",
      "        mem_index: MemoryIndex,\n",
      "        name: str = \"memory_kernel\",\n",
      "        k: int = 2,\n",
      "        save_path: str = None,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\n",
      "\n",
      "        Args:\n",
      "            mem_index (MemoryIndex): A MemoryIndex instance.\n",
      "            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\n",
      "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
      "            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\n",
      "        \"\"\"\n",
      "        super().__init__(\n",
      "            index=mem_index.index,\n",
      "            values=mem_index.values,\n",
      "            embeddings=mem_index.embeddings,\n",
      "            name=name,\n",
      "            save_path=save_path,\n",
      "        )\n",
      "        self.k = k\n",
      "        if len(self.values) > 0: \n",
      "            self.create_k_hop_index(k=k)\n",
      "        else:\n",
      "            raise ValueError(\"The input MemoryIndex is empty. Please check the input MemoryIndex.\")\n",
      "\n",
      "    def cos_sim(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
      "        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
      "        \"\"\"\n",
      "        if not isinstance(a, np.ndarray):\n",
      "            a = np.array(a)\n",
      "\n",
      "        if not isinstance(b, np.ndarray):\n",
      "            b = np.array(b)\n",
      "\n",
      "        if len(a.shape) == 1:\n",
      "            a = a[np.newaxis, :]\n",
      "\n",
      "        if len(b.shape) == 1:\n",
      "            b = b[np.newaxis, :]\n",
      "\n",
      "        a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\n",
      "        b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\n",
      "        return np.dot(a_norm, b_norm.T)\n",
      "\n",
      "    def compute_kernel(\n",
      "        self,\n",
      "        embedding_set: np.ndarray,\n",
      "        threshold: float = 0.65,\n",
      "        use_softmax: bool = False,\n",
      "    ) -> np.ndarray:\n",
      "\n",
      "        \"\"\"\n",
      "        Compute the adjacency matrix of the graph.\n",
      "\n",
      "        Parameters:\n",
      "        embedding_set (numpy array): The embedding matrix of the nodes.\n",
      "        threshold (float): The threshold for the adjacency matrix.\n",
      "        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\n",
      "        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\n",
      "\n",
      "        Returns:\n",
      "        adj_matrix (numpy array): The adjacency matrix of the graph.\n",
      "        \"\"\"\n",
      "\n",
      "        A = self.cos_sim(embedding_set, embedding_set)\n",
      "        if use_softmax:\n",
      "            # softmax\n",
      "            A = np.exp(A)\n",
      "            A = A / np.sum(A, axis=1)[:, np.newaxis]\n",
      "        adj_matrix = np.zeros_like(A)\n",
      "        adj_matrix[A > threshold] = 1\n",
      "        adj_matrix[A <= threshold] = 0\n",
      "        adj_matrix = adj_matrix.astype(np.float32)\n",
      "        return adj_matrix\n",
      "\n",
      "    def k_hop_message_passing(\n",
      "        self, A: np.ndarray, node_features: np.ndarray, k: int\n",
      "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
      "        \"\"\"\n",
      "        Compute the k-hop adjacency matrix and aggregated features using message passing.\n",
      "\n",
      "        Parameters:\n",
      "        A (numpy array): The adjacency matrix of the graph.\n",
      "        node_features (numpy array): The feature matrix of the nodes.\n",
      "        k (int): The number of hops for message passing.\n",
      "\n",
      "        Returns:\n",
      "        A_k (numpy array): The k-hop adjacency matrix.\n",
      "        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n",
      "        \"\"\"\n",
      "\n",
      "        print(\"Compute the k-hop adjacency matrix\")\n",
      "        A_k = np.linalg.matrix_power(A, k)\n",
      "\n",
      "        print(\"Aggregate the messages from the k-hop neighborhood:\")\n",
      "        agg_features = node_features.copy()\n",
      "\n",
      "        for i in tqdm(range(k)):\n",
      "            agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n",
      "\n",
      "        return A_k, agg_features\n",
      "\n",
      "    def graph_sylvester_embedding(self, G: Tuple, m: int, ts: np.ndarray) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\n",
      "\n",
      "        Args:\n",
      "            G (Tuple): A tuple containing the graph's vertices (V) and weights (W).\n",
      "            m (int): The number of singular values to consider.\n",
      "            ts (np.ndarray): The spectral scales.\n",
      "\n",
      "        Returns:\n",
      "            np.ndarray: The node_embeddings matrix.\n",
      "        \"\"\"\n",
      "        V, W = G\n",
      "        n = len(V)\n",
      "        D_BE = np.diag(W.sum(axis=1))\n",
      "        L_BE = np.identity(n) - np.dot(\n",
      "            np.diag(1 / np.sqrt(D_BE.diagonal())),\n",
      "            np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n",
      "        )\n",
      "\n",
      "        A = W\n",
      "        B = L_BE\n",
      "        C = np.identity(n)\n",
      "        X = solve_sylvester(A, B, C)\n",
      "\n",
      "        U, S, _ = svd(X, full_matrices=False)\n",
      "        U_m = U[:, :m]\n",
      "        S_m = S[:m]\n",
      "\n",
      "        node_embeddings = np.zeros((n, m))\n",
      "\n",
      "        for i in range(n):\n",
      "            for s in range(m):\n",
      "                # Spectral kernel descriptor\n",
      "                node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n",
      "\n",
      "        return node_embeddings\n",
      "\n",
      "    def gen_gse_embeddings(\n",
      "        self, A: np.ndarray, embeddings: np.ndarray, m: int = 7\n",
      "    ) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Generate Graph Sylvester Embeddings.\n",
      "\n",
      "        Args:\n",
      "            A (np.ndarray): The adjacency matrix of the graph.\n",
      "            embeddings (np.ndarray): The original node embeddings.\n",
      "            m (int, optional): The number of spectral scales. Defaults to 7.\n",
      "\n",
      "        Returns:\n",
      "            np.ndarray: The generated Graph Sylvester Embeddings.\n",
      "        \"\"\"\n",
      "        V = list(range(len(embeddings)))\n",
      "        W = A\n",
      "\n",
      "        G = (V, W)\n",
      "        ts = np.linspace(0, 1, m)  # equally spaced scales\n",
      "\n",
      "        gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\n",
      "        return gse_embeddings\n",
      "\n",
      "    def create_k_hop_index(self, k: int = 2):\n",
      "        \"\"\"\n",
      "        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\n",
      "        aggregated features, and updating the memory index.\n",
      "\n",
      "        Args:\n",
      "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
      "        \"\"\"\n",
      "        self.k = k\n",
      "        print(\"Computing the adjacency matrix\")\n",
      "        print(\"Embeddings shape: \", self.embeddings.shape)\n",
      "        self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n",
      "        print(\"Computing the k-hop adjacency matrix and aggregated features\")\n",
      "        self.A_k, self.node_embeddings = self.k_hop_message_passing(\n",
      "            self.A, self.embeddings, k\n",
      "        )\n",
      "        print(\"Updating the memory index\")\n",
      "        self.k_hop_index = MemoryIndex(name=self.name)\n",
      "        self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\n",
      "\n",
      "    @classmethod\n",
      "    def from_task_results(cls, task_memory_index):\n",
      "        new_memory_kernel = cls(mem_index=task_memory_index)\n",
      "\n",
      "        # Create a new index for the new MemoryKernel\n",
      "        new_memory_kernel.create_k_hop_index()\n",
      "\n",
      "        return new_memory_kernel\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"MemoryIndex\"]\n",
      "]\n",
      "Function call: super\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: tqdm\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: solve_sylvester\n",
      "Function call: svd\n",
      "Function call: range\n",
      "Function call: range\n",
      "Function call: list\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: MemoryIndex\n",
      "Function call: cls\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\n\\nclass MemoryIndex(NpIndex):\\n    \"\"\"\\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \\n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\\n    \"\"\"\\n    @staticmethod\\n    def check_uniform_context_type(context: List[Any]) -> None:\\n        \"\"\"Check if all context elements are of the same type.\"\"\"\\n        if not all(isinstance(x, type(context[0])) for x in context):\\n            raise ValueError(\"All context elements must be of the same type.\")\\n\\n    def __init__(\\n        self,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        context: Optional[List[Any]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ):\\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\\n        if context is not None and len(context) != len(values):\\n            raise ValueError(\"The context must have the same length as the values\")\\n\\n\\n        self.markdown = markdown\\n        if context is not None and values is not None:\\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\\n\\n        if context is not None:\\n            self.check_uniform_context_type(context)\\n            self.context_type = type(context[0])\\n\\n\\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\\n        if isinstance(identifier, list):\\n            return [self.get_context(value) for value in identifier]\\n        else:\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n            return self.context[value]\\n\\n    def clean_context(self):\\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\\n        self.context = {value: self.context[value] for value in self.values}\\n\\n    def add_to_context(self, value: str, context: Any):\\n        \"\"\" add a context to a value \"\"\"\\n        if not isinstance(context, self.context_type):\\n            raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if value in self.values:\\n            if value not in self.context:\\n                self.context[value] = []\\n            self.context[value].append(context)\\n\\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\\n        if isinstance(values, str):\\n            values = [values]\\n        NpIndex.add(self, values, embedding)\\n        if context is not None:\\n            for value, cont in zip(values, context):\\n                self.add_to_context(value, cont)\\n\\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\\n        if not isinstance(identifier, list):\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n        NpIndex.remove(self, identifier)\\n        if not isinstance(identifier, list):\\n            self.context.pop(value)\\n\\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\\n        #recover value from old_identifier\\n        if new_context is not None:\\n            if not isinstance(new_context, self.context_type):\\n                raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if not isinstance(old_identifier, list):\\n            old_id = self.identify_input(old_identifier)\\n            old_value = self.values[old_id]\\n            # Only perform the update if the old_value is not the same as the new_value.\\n            if old_value != new_value:\\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\\n\\n            if new_context is not None:\\n                self.context[new_value] = [new_context]\\n            else:\\n                self.context[new_value] = self.context.pop(old_value)\\n        else:\\n            self.context[new_value] = self.context.pop(old_value)\\n\\n    @classmethod\\n    def from_pandas(\\n        cls,\\n        data_frame: Union[pd.DataFrame, str],\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        if (\\n            isinstance(data_frame, str)\\n            and data_frame.endswith(\".csv\")\\n            and os.path.isfile(data_frame)\\n        ):\\n            logger.info(\"Loading the CSV file\")\\n            data_frame = pd.read_csv(data_frame)\\n            name = os.path.basename(data_frame).split(\".\")[0]\\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\\n            logger.info(\"Loading the pandas DataFrame\")\\n        else:\\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\\n\\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_pandas(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_hf_dataset(\\n        cls,\\n        dataset_url: str,\\n        value_column: str,\\n        data_split: str = \"train\",\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        dataset = load_dataset(dataset_url)[data_split]\\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_hf(dataset, context_columns)\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_polars(\\n        cls,\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        print(\"Loading the Polars DataFrame\")\\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_polars(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n\\n    @classmethod\\n    def from_python(\\n        cls,\\n        directory_path: str,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        markdown: str = \"python/markdown\",\\n        resolution: str = \"both\",\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    mem_index: MemoryIndex,\n",
      "    name: str = \"memory_kernel\",\n",
      "    k: int = 2,\n",
      "    save_path: str = None,\n",
      "):\n",
      "    \"\"\"\n",
      "        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\n",
      "\n",
      "        Args:\n",
      "            mem_index (MemoryIndex): A MemoryIndex instance.\n",
      "            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\n",
      "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
      "            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\n",
      "        \"\"\"\n",
      "    super().__init__(\n",
      "        index=mem_index.index,\n",
      "        values=mem_index.values,\n",
      "        embeddings=mem_index.embeddings,\n",
      "        name=name,\n",
      "        save_path=save_path,\n",
      "    )\n",
      "    self.k = k\n",
      "    if len(self.values) > 0: \n",
      "        self.create_k_hop_index(k=k)\n",
      "    else:\n",
      "        raise ValueError(\"The input MemoryIndex is empty. Please check the input MemoryIndex.\")\n",
      "\n",
      "Function call: super\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def cos_sim(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
      "    \"\"\"\n",
      "        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
      "        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
      "        \"\"\"\n",
      "    if not isinstance(a, np.ndarray):\n",
      "        a = np.array(a)\n",
      "\n",
      "    if not isinstance(b, np.ndarray):\n",
      "        b = np.array(b)\n",
      "\n",
      "    if len(a.shape) == 1:\n",
      "        a = a[np.newaxis, :]\n",
      "\n",
      "    if len(b.shape) == 1:\n",
      "        b = b[np.newaxis, :]\n",
      "\n",
      "    a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\n",
      "    b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\n",
      "    return np.dot(a_norm, b_norm.T)\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def compute_kernel(\n",
      "    self,\n",
      "    embedding_set: np.ndarray,\n",
      "    threshold: float = 0.65,\n",
      "    use_softmax: bool = False,\n",
      ") -> np.ndarray:\n",
      "\n",
      "    \"\"\"\n",
      "        Compute the adjacency matrix of the graph.\n",
      "\n",
      "        Parameters:\n",
      "        embedding_set (numpy array): The embedding matrix of the nodes.\n",
      "        threshold (float): The threshold for the adjacency matrix.\n",
      "        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\n",
      "        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\n",
      "\n",
      "        Returns:\n",
      "        adj_matrix (numpy array): The adjacency matrix of the graph.\n",
      "        \"\"\"\n",
      "\n",
      "    A = self.cos_sim(embedding_set, embedding_set)\n",
      "    if use_softmax:\n",
      "        # softmax\n",
      "        A = np.exp(A)\n",
      "        A = A / np.sum(A, axis=1)[:, np.newaxis]\n",
      "    adj_matrix = np.zeros_like(A)\n",
      "    adj_matrix[A > threshold] = 1\n",
      "    adj_matrix[A <= threshold] = 0\n",
      "    adj_matrix = adj_matrix.astype(np.float32)\n",
      "    return adj_matrix\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def k_hop_message_passing(\n",
      "    self, A: np.ndarray, node_features: np.ndarray, k: int\n",
      ") -> Tuple[np.ndarray, np.ndarray]:\n",
      "    \"\"\"\n",
      "        Compute the k-hop adjacency matrix and aggregated features using message passing.\n",
      "\n",
      "        Parameters:\n",
      "        A (numpy array): The adjacency matrix of the graph.\n",
      "        node_features (numpy array): The feature matrix of the nodes.\n",
      "        k (int): The number of hops for message passing.\n",
      "\n",
      "        Returns:\n",
      "        A_k (numpy array): The k-hop adjacency matrix.\n",
      "        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n",
      "        \"\"\"\n",
      "\n",
      "    print(\"Compute the k-hop adjacency matrix\")\n",
      "    A_k = np.linalg.matrix_power(A, k)\n",
      "\n",
      "    print(\"Aggregate the messages from the k-hop neighborhood:\")\n",
      "    agg_features = node_features.copy()\n",
      "\n",
      "    for i in tqdm(range(k)):\n",
      "        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n",
      "\n",
      "    return A_k, agg_features\n",
      "\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: tqdm\n",
      "Function call: range\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def graph_sylvester_embedding(self, G: Tuple, m: int, ts: np.ndarray) -> np.ndarray:\n",
      "    \"\"\"\n",
      "        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\n",
      "\n",
      "        Args:\n",
      "            G (Tuple): A tuple containing the graph's vertices (V) and weights (W).\n",
      "            m (int): The number of singular values to consider.\n",
      "            ts (np.ndarray): The spectral scales.\n",
      "\n",
      "        Returns:\n",
      "            np.ndarray: The node_embeddings matrix.\n",
      "        \"\"\"\n",
      "    V, W = G\n",
      "    n = len(V)\n",
      "    D_BE = np.diag(W.sum(axis=1))\n",
      "    L_BE = np.identity(n) - np.dot(\n",
      "        np.diag(1 / np.sqrt(D_BE.diagonal())),\n",
      "        np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n",
      "    )\n",
      "\n",
      "    A = W\n",
      "    B = L_BE\n",
      "    C = np.identity(n)\n",
      "    X = solve_sylvester(A, B, C)\n",
      "\n",
      "    U, S, _ = svd(X, full_matrices=False)\n",
      "    U_m = U[:, :m]\n",
      "    S_m = S[:m]\n",
      "\n",
      "    node_embeddings = np.zeros((n, m))\n",
      "\n",
      "    for i in range(n):\n",
      "        for s in range(m):\n",
      "            # Spectral kernel descriptor\n",
      "            node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n",
      "\n",
      "    return node_embeddings\n",
      "\n",
      "Function call: len\n",
      "Function call: solve_sylvester\n",
      "Function call: svd\n",
      "Function call: range\n",
      "Function call: range\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def gen_gse_embeddings(\n",
      "    self, A: np.ndarray, embeddings: np.ndarray, m: int = 7\n",
      ") -> np.ndarray:\n",
      "    \"\"\"\n",
      "        Generate Graph Sylvester Embeddings.\n",
      "\n",
      "        Args:\n",
      "            A (np.ndarray): The adjacency matrix of the graph.\n",
      "            embeddings (np.ndarray): The original node embeddings.\n",
      "            m (int, optional): The number of spectral scales. Defaults to 7.\n",
      "\n",
      "        Returns:\n",
      "            np.ndarray: The generated Graph Sylvester Embeddings.\n",
      "        \"\"\"\n",
      "    V = list(range(len(embeddings)))\n",
      "    W = A\n",
      "\n",
      "    G = (V, W)\n",
      "    ts = np.linspace(0, 1, m)  # equally spaced scales\n",
      "\n",
      "    gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\n",
      "    return gse_embeddings\n",
      "\n",
      "Function call: list\n",
      "Function call: range\n",
      "Function call: len\n",
      "Related codes: [\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def create_k_hop_index(self, k: int = 2):\n",
      "    \"\"\"\n",
      "        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\n",
      "        aggregated features, and updating the memory index.\n",
      "\n",
      "        Args:\n",
      "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
      "        \"\"\"\n",
      "    self.k = k\n",
      "    print(\"Computing the adjacency matrix\")\n",
      "    print(\"Embeddings shape: \", self.embeddings.shape)\n",
      "    self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n",
      "    print(\"Computing the k-hop adjacency matrix and aggregated features\")\n",
      "    self.A_k, self.node_embeddings = self.k_hop_message_passing(\n",
      "        self.A, self.embeddings, k\n",
      "    )\n",
      "    print(\"Updating the memory index\")\n",
      "    self.k_hop_index = MemoryIndex(name=self.name)\n",
      "    self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\n",
      "\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: MemoryIndex\n",
      "Related codes: ['\\n\\n\\nclass MemoryIndex(NpIndex):\\n    \"\"\"\\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \\n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\\n    \"\"\"\\n    @staticmethod\\n    def check_uniform_context_type(context: List[Any]) -> None:\\n        \"\"\"Check if all context elements are of the same type.\"\"\"\\n        if not all(isinstance(x, type(context[0])) for x in context):\\n            raise ValueError(\"All context elements must be of the same type.\")\\n\\n    def __init__(\\n        self,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        context: Optional[List[Any]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ):\\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\\n        if context is not None and len(context) != len(values):\\n            raise ValueError(\"The context must have the same length as the values\")\\n\\n\\n        self.markdown = markdown\\n        if context is not None and values is not None:\\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\\n\\n        if context is not None:\\n            self.check_uniform_context_type(context)\\n            self.context_type = type(context[0])\\n\\n\\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\\n        if isinstance(identifier, list):\\n            return [self.get_context(value) for value in identifier]\\n        else:\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n            return self.context[value]\\n\\n    def clean_context(self):\\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\\n        self.context = {value: self.context[value] for value in self.values}\\n\\n    def add_to_context(self, value: str, context: Any):\\n        \"\"\" add a context to a value \"\"\"\\n        if not isinstance(context, self.context_type):\\n            raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if value in self.values:\\n            if value not in self.context:\\n                self.context[value] = []\\n            self.context[value].append(context)\\n\\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\\n        if isinstance(values, str):\\n            values = [values]\\n        NpIndex.add(self, values, embedding)\\n        if context is not None:\\n            for value, cont in zip(values, context):\\n                self.add_to_context(value, cont)\\n\\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\\n        if not isinstance(identifier, list):\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n        NpIndex.remove(self, identifier)\\n        if not isinstance(identifier, list):\\n            self.context.pop(value)\\n\\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\\n        #recover value from old_identifier\\n        if new_context is not None:\\n            if not isinstance(new_context, self.context_type):\\n                raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if not isinstance(old_identifier, list):\\n            old_id = self.identify_input(old_identifier)\\n            old_value = self.values[old_id]\\n            # Only perform the update if the old_value is not the same as the new_value.\\n            if old_value != new_value:\\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\\n\\n            if new_context is not None:\\n                self.context[new_value] = [new_context]\\n            else:\\n                self.context[new_value] = self.context.pop(old_value)\\n        else:\\n            self.context[new_value] = self.context.pop(old_value)\\n\\n    @classmethod\\n    def from_pandas(\\n        cls,\\n        data_frame: Union[pd.DataFrame, str],\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        if (\\n            isinstance(data_frame, str)\\n            and data_frame.endswith(\".csv\")\\n            and os.path.isfile(data_frame)\\n        ):\\n            logger.info(\"Loading the CSV file\")\\n            data_frame = pd.read_csv(data_frame)\\n            name = os.path.basename(data_frame).split(\".\")[0]\\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\\n            logger.info(\"Loading the pandas DataFrame\")\\n        else:\\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\\n\\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_pandas(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_hf_dataset(\\n        cls,\\n        dataset_url: str,\\n        value_column: str,\\n        data_split: str = \"train\",\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        dataset = load_dataset(dataset_url)[data_split]\\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_hf(dataset, context_columns)\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_polars(\\n        cls,\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        print(\"Loading the Polars DataFrame\")\\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_polars(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n\\n    @classmethod\\n    def from_python(\\n        cls,\\n        directory_path: str,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        markdown: str = \"python/markdown\",\\n        resolution: str = \"both\",\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@classmethod\n",
      "def from_task_results(cls, task_memory_index):\n",
      "    new_memory_kernel = cls(mem_index=task_memory_index)\n",
      "\n",
      "    # Create a new index for the new MemoryKernel\n",
      "    new_memory_kernel.create_k_hop_index()\n",
      "\n",
      "    return new_memory_kernel\n",
      "\n",
      "Function call: cls\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def calc_shgo_mode(scores: List[float]) -> float:\n",
      "    def objective(x):\n",
      "        return -estimate_pdf(scores)(x)\n",
      "\n",
      "    bounds = [(min(scores), max(scores))]\n",
      "    result = scipy.optimize.shgo(objective, bounds)\n",
      "    return result.x\n",
      "\n",
      "Function call: estimate_pdf\n",
      "Function call: min\n",
      "Function call: max\n",
      "Related codes: ['\\n\\ndef estimate_pdf(scores: List[float]) -> callable:\\n    pdf = scipy.stats.gaussian_kde(scores)\\n    return pdf\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def objective(x):\n",
      "    return -estimate_pdf(scores)(x)\n",
      "\n",
      "Function call: estimate_pdf\n",
      "Related codes: ['\\n\\ndef estimate_pdf(scores: List[float]) -> callable:\\n    pdf = scipy.stats.gaussian_kde(scores)\\n    return pdf\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def estimate_pdf(scores: List[float]) -> callable:\n",
      "    pdf = scipy.stats.gaussian_kde(scores)\n",
      "    return pdf\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def sort_paths_by_mode_distance(\n",
      "    paths, memory_kernel, distance_metric: str = \"cosine\"\n",
      ") -> List[List[int]]:\n",
      "    sorted_paths = []\n",
      "    for i, path in enumerate(paths):\n",
      "        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
      "        cluster_embeddings = np.array(cluster_embeddings)\n",
      "        cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
      "        if distance_metric == \"cosine\" or distance_metric == \"guassian\":\n",
      "            scores = [\n",
      "                (i, cosine(cluster_mean, emb))\n",
      "                for i, emb in zip(path, cluster_embeddings)\n",
      "            ]\n",
      "        elif distance_metric == \"euclidean\":\n",
      "            scores = [\n",
      "                (i, np.linalg.norm(cluster_mean - emb))\n",
      "                for i, emb in zip(path, cluster_embeddings)\n",
      "            ]\n",
      "        score_values = [score for _, score in scores]  # Extract score values\n",
      "        mu = calc_shgo_mode(score_values)\n",
      "        sigma = np.std(score_values)\n",
      "        if distance_metric == \"guassian\":\n",
      "            scores = [\n",
      "                (i, np.exp(-((x - mu) ** 2) / (2 * sigma**2))) for i, x in scores\n",
      "            ]\n",
      "        # Sort path by score\n",
      "        sorted_path_and_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
      "        sorted_path = [x[0] for x in sorted_path_and_scores]\n",
      "        sorted_paths.append(sorted_path)\n",
      "    return sorted_paths\n",
      "\n",
      "Function call: enumerate\n",
      "Function call: cosine\n",
      "Function call: zip\n",
      "Function call: zip\n",
      "Function call: calc_shgo_mode\n",
      "Function call: sorted\n",
      "Related codes: ['\\n@staticmethod\\n@abstractmethod\\ndef batched_cosine_similarity(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\\n    pass\\n', '\\n\\ndef calc_shgo_mode(scores: List[float]) -> float:\\n    def objective(x):\\n        return -estimate_pdf(scores)(x)\\n\\n    bounds = [(min(scores), max(scores))]\\n    result = scipy.optimize.shgo(objective, bounds)\\n    return result.x\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def sort_paths_by_kernel_density(\n",
      "    paths, memory_kernel, distance_metric: str = \"cosine\"\n",
      ") -> List[List[int]]:\n",
      "    sorted_paths = []\n",
      "    for i, path in enumerate(paths):\n",
      "        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
      "        cluster_embeddings = np.array(cluster_embeddings)\n",
      "        cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
      "        if distance_metric == \"cosine\":\n",
      "            scores = [\n",
      "                (i, cosine(cluster_mean, emb))\n",
      "                for i, emb in zip(path, cluster_embeddings)\n",
      "            ]\n",
      "        elif distance_metric == \"euclidean\":\n",
      "            scores = [\n",
      "                (i, np.linalg.norm(cluster_mean - emb))\n",
      "                for i, emb in zip(path, cluster_embeddings)\n",
      "            ]\n",
      "        score_values = [score for _, score in scores]  # Extract score values\n",
      "\n",
      "        # Estimate PDF using Kernel Density Estimation\n",
      "        kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(\n",
      "            np.array(score_values).reshape(-1, 1)\n",
      "        )\n",
      "        kde_scores = [kde.score_samples([[x]])[0] for _, x in scores]\n",
      "\n",
      "        # Sort path by score\n",
      "        sorted_path_and_scores = sorted(\n",
      "            zip(path, kde_scores), key=lambda x: x[1], reverse=True\n",
      "        )\n",
      "        sorted_path = [x[0] for x in sorted_path_and_scores]\n",
      "        sorted_paths.append(sorted_path)\n",
      "    return sorted_paths\n",
      "\n",
      "Function call: enumerate\n",
      "Function call: cosine\n",
      "Function call: zip\n",
      "Function call: zip\n",
      "Function call: KernelDensity\n",
      "Function call: sorted\n",
      "Function call: zip\n",
      "Related codes: ['\\n@staticmethod\\n@abstractmethod\\ndef batched_cosine_similarity(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\\n    pass\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class ClusterPaths:\n",
      "    def create_paths(\n",
      "        self, embeddings: np.ndarray, num_clusters: int\n",
      "    ) -> List[List[int]]:\n",
      "        raise NotImplementedError\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def create_paths(\n",
      "    self, embeddings: np.ndarray, num_clusters: int\n",
      ") -> List[List[int]]:\n",
      "    raise NotImplementedError\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class HDBSCANPaths(ClusterPaths):\n",
      "    def create_paths(\n",
      "        self, embeddings: np.ndarray, num_clusters: int\n",
      "    ) -> List[List[int]]:\n",
      "        clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n",
      "        cluster_assignments = clusterer.fit_predict(embeddings)\n",
      "        paths = [[] for _ in range(num_clusters)]\n",
      "        for i, cluster in enumerate(cluster_assignments):\n",
      "            paths[cluster].append(i)\n",
      "        paths = [path for path in paths if path]\n",
      "        return paths\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"ClusterPaths\"]\n",
      "]\n",
      "Function call: range\n",
      "Function call: enumerate\n",
      "\n",
      "********************************************************************************\n",
      "def create_paths(\n",
      "    self, embeddings: np.ndarray, num_clusters: int\n",
      ") -> List[List[int]]:\n",
      "    clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n",
      "    cluster_assignments = clusterer.fit_predict(embeddings)\n",
      "    paths = [[] for _ in range(num_clusters)]\n",
      "    for i, cluster in enumerate(cluster_assignments):\n",
      "        paths[cluster].append(i)\n",
      "    paths = [path for path in paths if path]\n",
      "    return paths\n",
      "\n",
      "Function call: range\n",
      "Function call: enumerate\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class SpectralClusteringPaths(ClusterPaths):\n",
      "    def create_paths(\n",
      "        self, A: np.ndarray, num_clusters: int\n",
      "    ) -> List[List[int]]:\n",
      "        n_samples = A.shape[0]\n",
      "        n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\n",
      "        spectral_clustering = SpectralClustering(\n",
      "            n_clusters=num_clusters,\n",
      "            affinity=\"precomputed\",\n",
      "            n_neighbors=n_neighbors,\n",
      "            random_state=42,\n",
      "        )\n",
      "        cluster_assignments = spectral_clustering.fit_predict(A)\n",
      "        paths = [[] for _ in range(num_clusters)]\n",
      "        for i, cluster in enumerate(cluster_assignments):\n",
      "            paths[cluster].append(i)\n",
      "        paths = [path for path in paths if path]\n",
      "        return paths\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"ClusterPaths\"]\n",
      "]\n",
      "Function call: min\n",
      "Function call: SpectralClustering\n",
      "Function call: range\n",
      "Function call: enumerate\n",
      "Related codes: ['\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\n\\nclass SpectralClusteringPaths(ClusterPaths):\\n    def create_paths(\\n        self, A: np.ndarray, num_clusters: int\\n    ) -> List[List[int]]:\\n        n_samples = A.shape[0]\\n        n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\\n        spectral_clustering = SpectralClustering(\\n            n_clusters=num_clusters,\\n            affinity=\"precomputed\",\\n            n_neighbors=n_neighbors,\\n            random_state=42,\\n        )\\n        cluster_assignments = spectral_clustering.fit_predict(A)\\n        paths = [[] for _ in range(num_clusters)]\\n        for i, cluster in enumerate(cluster_assignments):\\n            paths[cluster].append(i)\\n        paths = [path for path in paths if path]\\n        return paths\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def create_paths(\n",
      "    self, A: np.ndarray, num_clusters: int\n",
      ") -> List[List[int]]:\n",
      "    n_samples = A.shape[0]\n",
      "    n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\n",
      "    spectral_clustering = SpectralClustering(\n",
      "        n_clusters=num_clusters,\n",
      "        affinity=\"precomputed\",\n",
      "        n_neighbors=n_neighbors,\n",
      "        random_state=42,\n",
      "    )\n",
      "    cluster_assignments = spectral_clustering.fit_predict(A)\n",
      "    paths = [[] for _ in range(num_clusters)]\n",
      "    for i, cluster in enumerate(cluster_assignments):\n",
      "        paths[cluster].append(i)\n",
      "    paths = [path for path in paths if path]\n",
      "    return paths\n",
      "\n",
      "Function call: min\n",
      "Function call: SpectralClustering\n",
      "Function call: range\n",
      "Function call: enumerate\n",
      "Related codes: ['\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\n\\nclass SpectralClusteringPaths(ClusterPaths):\\n    def create_paths(\\n        self, A: np.ndarray, num_clusters: int\\n    ) -> List[List[int]]:\\n        n_samples = A.shape[0]\\n        n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\\n        spectral_clustering = SpectralClustering(\\n            n_clusters=num_clusters,\\n            affinity=\"precomputed\",\\n            n_neighbors=n_neighbors,\\n            random_state=42,\\n        )\\n        cluster_assignments = spectral_clustering.fit_predict(A)\\n        paths = [[] for _ in range(num_clusters)]\\n        for i, cluster in enumerate(cluster_assignments):\\n            paths[cluster].append(i)\\n        paths = [path for path in paths if path]\\n        return paths\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MultiKernelVisualization:\n",
      "    def __init__(self, memory_kernel_group: MultiKernel):\n",
      "        self.memory_kernel_group = memory_kernel_group\n",
      "        self.memory_kernel_dict = memory_kernel_group.memory_kernel_dict\n",
      "        self.memory_kernel_group.generate_path_groups()\n",
      "\n",
      "    def plot_embeddings_with_path(self, embeddings, title, paths):\n",
      "        tsne = TSNE(n_components=2, random_state=42)\n",
      "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
      "\n",
      "        plt.figure(figsize=(10, 8))\n",
      "        colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n",
      "        for i, path in enumerate(paths):\n",
      "            path_embeddings = reduced_embeddings[path]\n",
      "            plt.scatter(\n",
      "                path_embeddings[:, 0],\n",
      "                path_embeddings[:, 1],\n",
      "                color=colors[i],\n",
      "                label=f\"Cluster {i}\",\n",
      "            )\n",
      "            for j in range(len(path) - 1):\n",
      "                plt.plot(\n",
      "                    [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n",
      "                    [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n",
      "                    color=colors[i],\n",
      "                )\n",
      "        plt.title(title)\n",
      "        plt.legend()\n",
      "        plt.show()\n",
      "\n",
      "    def visualize_paths(self):\n",
      "        #loop through memory kernels and print path_group\n",
      "        for key, kernel in self.memory_kernel_dict.items():\n",
      "            print(f\"Kernel: {key}\")\n",
      "            paths = self.memory_kernel_group.path_group[key]\n",
      "            print(f\"Path Group: {paths}\")\n",
      "            node_embeddings = kernel.node_embeddings\n",
      "            self.plot_embeddings_with_path(\n",
      "                node_embeddings, f\"Node Embeddings for {key}\", paths\n",
      "            )\n",
      "    def plot_singular_values(self):\n",
      "        #loop through memory kernels and print path_group\n",
      "        for key, kernel in self.memory_kernel_dict.items():\n",
      "            print(f\"Kernel: {key}\")\n",
      "            A_k = kernel.A_k\n",
      "            U, S, V = np.linalg.svd(A_k)\n",
      "            plt.plot(np.log(S))\n",
      "            plt.show()\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: TSNE\n",
      "Function call: len\n",
      "Function call: enumerate\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, memory_kernel_group: MultiKernel):\n",
      "    self.memory_kernel_group = memory_kernel_group\n",
      "    self.memory_kernel_dict = memory_kernel_group.memory_kernel_dict\n",
      "    self.memory_kernel_group.generate_path_groups()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def plot_embeddings_with_path(self, embeddings, title, paths):\n",
      "    tsne = TSNE(n_components=2, random_state=42)\n",
      "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
      "\n",
      "    plt.figure(figsize=(10, 8))\n",
      "    colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n",
      "    for i, path in enumerate(paths):\n",
      "        path_embeddings = reduced_embeddings[path]\n",
      "        plt.scatter(\n",
      "            path_embeddings[:, 0],\n",
      "            path_embeddings[:, 1],\n",
      "            color=colors[i],\n",
      "            label=f\"Cluster {i}\",\n",
      "        )\n",
      "        for j in range(len(path) - 1):\n",
      "            plt.plot(\n",
      "                [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n",
      "                [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n",
      "                color=colors[i],\n",
      "            )\n",
      "    plt.title(title)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "Function call: TSNE\n",
      "Function call: len\n",
      "Function call: enumerate\n",
      "Function call: range\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visualize_paths(self):\n",
      "    #loop through memory kernels and print path_group\n",
      "    for key, kernel in self.memory_kernel_dict.items():\n",
      "        print(f\"Kernel: {key}\")\n",
      "        paths = self.memory_kernel_group.path_group[key]\n",
      "        print(f\"Path Group: {paths}\")\n",
      "        node_embeddings = kernel.node_embeddings\n",
      "        self.plot_embeddings_with_path(\n",
      "            node_embeddings, f\"Node Embeddings for {key}\", paths\n",
      "        )\n",
      "\n",
      "Function call: print\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "def plot_singular_values(self):\n",
      "    #loop through memory kernels and print path_group\n",
      "    for key, kernel in self.memory_kernel_dict.items():\n",
      "        print(f\"Kernel: {key}\")\n",
      "        A_k = kernel.A_k\n",
      "        U, S, V = np.linalg.svd(A_k)\n",
      "        plt.plot(np.log(S))\n",
      "        plt.show()\n",
      "\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MultiKernelStabilityAnalysis:\n",
      "    def __init__(self, memory_kernel_group: MultiKernel):\n",
      "        self.memory_kernel_group = memory_kernel_group\n",
      "\n",
      "    def get_cluster_labels(self, kernel_label: str) -> Tuple[np.ndarray, int]:\n",
      "        paths = self.memory_kernel_group.path_group[kernel_label]\n",
      "        num_clusters = len(paths)\n",
      "        cluster_labels = np.empty(len(self.memory_kernel_group.memory_kernel_dict[kernel_label].node_embeddings), dtype=int)\n",
      "\n",
      "        for cluster_index, path in enumerate(paths):\n",
      "            cluster_labels[path] = cluster_index\n",
      "\n",
      "        return cluster_labels, num_clusters\n",
      "\n",
      "    def compute_nmi(self, kernel_label1: str, kernel_label2: str) -> float:\n",
      "        cluster_labels1, _ = self.get_cluster_labels(kernel_label1)\n",
      "        cluster_labels2, _ = self.get_cluster_labels(kernel_label2)\n",
      "        nmi = normalized_mutual_info_score(cluster_labels1, cluster_labels2)\n",
      "        return nmi\n",
      "\n",
      "    def evaluate_stability(self) -> float:\n",
      "        kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\n",
      "        pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\n",
      "        nmi_sum = 0\n",
      "\n",
      "        for kernel_label1, kernel_label2 in pairwise_combinations:\n",
      "            nmi = self.compute_nmi(kernel_label1, kernel_label2)\n",
      "            nmi_sum += nmi\n",
      "\n",
      "        stability_score = nmi_sum / len(pairwise_combinations)\n",
      "        return stability_score\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: enumerate\n",
      "Function call: normalized_mutual_info_score\n",
      "Function call: list\n",
      "Function call: list\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, memory_kernel_group: MultiKernel):\n",
      "    self.memory_kernel_group = memory_kernel_group\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_cluster_labels(self, kernel_label: str) -> Tuple[np.ndarray, int]:\n",
      "    paths = self.memory_kernel_group.path_group[kernel_label]\n",
      "    num_clusters = len(paths)\n",
      "    cluster_labels = np.empty(len(self.memory_kernel_group.memory_kernel_dict[kernel_label].node_embeddings), dtype=int)\n",
      "\n",
      "    for cluster_index, path in enumerate(paths):\n",
      "        cluster_labels[path] = cluster_index\n",
      "\n",
      "    return cluster_labels, num_clusters\n",
      "\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: enumerate\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def compute_nmi(self, kernel_label1: str, kernel_label2: str) -> float:\n",
      "    cluster_labels1, _ = self.get_cluster_labels(kernel_label1)\n",
      "    cluster_labels2, _ = self.get_cluster_labels(kernel_label2)\n",
      "    nmi = normalized_mutual_info_score(cluster_labels1, cluster_labels2)\n",
      "    return nmi\n",
      "\n",
      "Function call: normalized_mutual_info_score\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def evaluate_stability(self) -> float:\n",
      "    kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\n",
      "    pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\n",
      "    nmi_sum = 0\n",
      "\n",
      "    for kernel_label1, kernel_label2 in pairwise_combinations:\n",
      "        nmi = self.compute_nmi(kernel_label1, kernel_label2)\n",
      "        nmi_sum += nmi\n",
      "\n",
      "    stability_score = nmi_sum / len(pairwise_combinations)\n",
      "    return stability_score\n",
      "\n",
      "Function call: list\n",
      "Function call: list\n",
      "Function call: len\n",
      "Related codes: [\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class MultiKernel(MemoryKernel):\n",
      "    def __init__(\n",
      "        self,\n",
      "        memory_kernel_dict: Dict[str, MemoryKernel],\n",
      "        name: str = \"memory_kernel_group\",\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize the MultiKernel with a dictionary of MemoryKernel instances.\n",
      "\n",
      "        Args:\n",
      "            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\n",
      "            name (str, optional): The name of the MultiKernel. Defaults to \"memory_kernel_group\".\n",
      "        \"\"\"\n",
      "        self.memory_kernel_dict = memory_kernel_dict\n",
      "        self.path_group = {}\n",
      "        self.name = name\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"MemoryKernel\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    memory_kernel_dict: Dict[str, MemoryKernel],\n",
      "    name: str = \"memory_kernel_group\",\n",
      "):\n",
      "    \"\"\"\n",
      "        Initialize the MultiKernel with a dictionary of MemoryKernel instances.\n",
      "\n",
      "        Args:\n",
      "            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\n",
      "            name (str, optional): The name of the MultiKernel. Defaults to \"memory_kernel_group\".\n",
      "        \"\"\"\n",
      "    self.memory_kernel_dict = memory_kernel_dict\n",
      "    self.path_group = {}\n",
      "    self.name = name\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class HDBSCANMultiKernel(MultiKernel):\n",
      "    def __init__(\n",
      "        self,\n",
      "        memory_kernel_dict: Dict[str, MemoryKernel],\n",
      "        name: str = \"memory_kernel_group\",\n",
      "    ):\n",
      "        super().__init__(memory_kernel_dict, name)\n",
      "        self.cluster_paths = HDBSCANPaths()\n",
      "\n",
      "    def generate_path_groups(self, num_clusters: int = None) -> None:\n",
      "        path_group = {}\n",
      "        for k, v in self.memory_kernel_dict.items():\n",
      "            embeddings = v.node_embeddings\n",
      "            if num_clusters is None:\n",
      "                num_clusters = int(np.sqrt(len(embeddings)))\n",
      "            paths = self.cluster_paths.create_paths(embeddings, num_clusters)\n",
      "            path_group[k] = paths\n",
      "        self.path_group = path_group\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"MultiKernel\"]\n",
      "]\n",
      "Function call: super\n",
      "Function call: HDBSCANPaths\n",
      "Function call: int\n",
      "Function call: len\n",
      "Related codes: ['\\n\\nclass HDBSCANPaths(ClusterPaths):\\n    def create_paths(\\n        self, embeddings: np.ndarray, num_clusters: int\\n    ) -> List[List[int]]:\\n        clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\\n        cluster_assignments = clusterer.fit_predict(embeddings)\\n        paths = [[] for _ in range(num_clusters)]\\n        for i, cluster in enumerate(cluster_assignments):\\n            paths[cluster].append(i)\\n        paths = [path for path in paths if path]\\n        return paths\\n', '\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    memory_kernel_dict: Dict[str, MemoryKernel],\n",
      "    name: str = \"memory_kernel_group\",\n",
      "):\n",
      "    super().__init__(memory_kernel_dict, name)\n",
      "    self.cluster_paths = HDBSCANPaths()\n",
      "\n",
      "Function call: super\n",
      "Function call: HDBSCANPaths\n",
      "Related codes: ['\\n\\nclass HDBSCANPaths(ClusterPaths):\\n    def create_paths(\\n        self, embeddings: np.ndarray, num_clusters: int\\n    ) -> List[List[int]]:\\n        clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\\n        cluster_assignments = clusterer.fit_predict(embeddings)\\n        paths = [[] for _ in range(num_clusters)]\\n        for i, cluster in enumerate(cluster_assignments):\\n            paths[cluster].append(i)\\n        paths = [path for path in paths if path]\\n        return paths\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def generate_path_groups(self, num_clusters: int = None) -> None:\n",
      "    path_group = {}\n",
      "    for k, v in self.memory_kernel_dict.items():\n",
      "        embeddings = v.node_embeddings\n",
      "        if num_clusters is None:\n",
      "            num_clusters = int(np.sqrt(len(embeddings)))\n",
      "        paths = self.cluster_paths.create_paths(embeddings, num_clusters)\n",
      "        path_group[k] = paths\n",
      "    self.path_group = path_group\n",
      "\n",
      "Function call: int\n",
      "Function call: len\n",
      "Related codes: ['\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class SpectralClusteringMultiKernel(MultiKernel):\n",
      "    def __init__(\n",
      "        self,\n",
      "        memory_kernel_dict: Dict[str, MemoryKernel],\n",
      "        name: str = \"memory_kernel_group\",\n",
      "    ):\n",
      "        super().__init__(memory_kernel_dict, name)\n",
      "        self.cluster_paths = SpectralClusteringPaths()\n",
      "\n",
      "    def generate_path_groups(self, num_clusters: int = None) -> None:\n",
      "        path_group = {}\n",
      "        for k, v in self.memory_kernel_dict.items():\n",
      "            A_k = v.A_k\n",
      "            if num_clusters is None:\n",
      "                num_clusters = int(np.sqrt(len(A_k)))\n",
      "            paths = self.cluster_paths.create_paths(A_k, num_clusters)\n",
      "            path_group[k] = paths\n",
      "        self.path_group = path_group\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"MultiKernel\"]\n",
      "]\n",
      "Function call: super\n",
      "Function call: SpectralClusteringPaths\n",
      "Function call: int\n",
      "Function call: len\n",
      "Related codes: ['\\n\\nclass SpectralClusteringPaths(ClusterPaths):\\n    def create_paths(\\n        self, A: np.ndarray, num_clusters: int\\n    ) -> List[List[int]]:\\n        n_samples = A.shape[0]\\n        n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\\n        spectral_clustering = SpectralClustering(\\n            n_clusters=num_clusters,\\n            affinity=\"precomputed\",\\n            n_neighbors=n_neighbors,\\n            random_state=42,\\n        )\\n        cluster_assignments = spectral_clustering.fit_predict(A)\\n        paths = [[] for _ in range(num_clusters)]\\n        for i, cluster in enumerate(cluster_assignments):\\n            paths[cluster].append(i)\\n        paths = [path for path in paths if path]\\n        return paths\\n', '\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    memory_kernel_dict: Dict[str, MemoryKernel],\n",
      "    name: str = \"memory_kernel_group\",\n",
      "):\n",
      "    super().__init__(memory_kernel_dict, name)\n",
      "    self.cluster_paths = SpectralClusteringPaths()\n",
      "\n",
      "Function call: super\n",
      "Function call: SpectralClusteringPaths\n",
      "Related codes: ['\\n\\nclass SpectralClusteringPaths(ClusterPaths):\\n    def create_paths(\\n        self, A: np.ndarray, num_clusters: int\\n    ) -> List[List[int]]:\\n        n_samples = A.shape[0]\\n        n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\\n        spectral_clustering = SpectralClustering(\\n            n_clusters=num_clusters,\\n            affinity=\"precomputed\",\\n            n_neighbors=n_neighbors,\\n            random_state=42,\\n        )\\n        cluster_assignments = spectral_clustering.fit_predict(A)\\n        paths = [[] for _ in range(num_clusters)]\\n        for i, cluster in enumerate(cluster_assignments):\\n            paths[cluster].append(i)\\n        paths = [path for path in paths if path]\\n        return paths\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def generate_path_groups(self, num_clusters: int = None) -> None:\n",
      "    path_group = {}\n",
      "    for k, v in self.memory_kernel_dict.items():\n",
      "        A_k = v.A_k\n",
      "        if num_clusters is None:\n",
      "            num_clusters = int(np.sqrt(len(A_k)))\n",
      "        paths = self.cluster_paths.create_paths(A_k, num_clusters)\n",
      "        path_group[k] = paths\n",
      "    self.path_group = path_group\n",
      "\n",
      "Function call: int\n",
      "Function call: len\n",
      "Related codes: ['\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "class BaseFrame(ABC):\n",
      "    def __init__(self,\n",
      "                context_columns: List = [],\n",
      "                embeddable_columns: List = [],\n",
      "                embedding_columns: List = [],\n",
      "                name: str = \"base_frame\",\n",
      "                save_path: Optional[str] = \"/storage\",\n",
      "                text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "                markdown: str = \"text/markdown\",):\n",
      "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "        self.meta_columns = ['ID', 'Name', 'Source', 'Author', 'Created At', 'Last Modified At']\n",
      "        self.context_columns = context_columns\n",
      "        self.embeddable_columns = embeddable_columns\n",
      "        self.embedding_columns = embedding_columns\n",
      "        self.name = name\n",
      "        self.save_path = save_path\n",
      "        self.save_dir = f'{self.save_path}/{self.name}'\n",
      "        self.text_embedder = text_embedder\n",
      "        self.markdown = markdown\n",
      "\n",
      "    @abstractmethod\n",
      "    def __getattr__(self, name: str):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def get_overwritten_attr(self):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def embed_columns(self, embeddable_columns: List):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def _embed_column(self, column, embedder):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def search_column_polar(self, query, embeddable_column_name, top_k):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def save(self):\n",
      "        pass\n",
      "\n",
      "    @classmethod\n",
      "    @abstractmethod\n",
      "    def load(cls, frame_path, name):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def generate_column(self, row_generator, new_column_name):\n",
      "        pass\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"ABC\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self,\n",
      "            context_columns: List = [],\n",
      "            embeddable_columns: List = [],\n",
      "            embedding_columns: List = [],\n",
      "            name: str = \"base_frame\",\n",
      "            save_path: Optional[str] = \"/storage\",\n",
      "            text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "            markdown: str = \"text/markdown\",):\n",
      "    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "    self.meta_columns = ['ID', 'Name', 'Source', 'Author', 'Created At', 'Last Modified At']\n",
      "    self.context_columns = context_columns\n",
      "    self.embeddable_columns = embeddable_columns\n",
      "    self.embedding_columns = embedding_columns\n",
      "    self.name = name\n",
      "    self.save_path = save_path\n",
      "    self.save_dir = f'{self.save_path}/{self.name}'\n",
      "    self.text_embedder = text_embedder\n",
      "    self.markdown = markdown\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def __getattr__(self, name: str):\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def get_overwritten_attr(self):\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def embed_columns(self, embeddable_columns: List):\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def _embed_column(self, column, embedder):\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def search_column_polar(self, query, embeddable_column_name, top_k):\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def save(self):\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@classmethod\n",
      "@abstractmethod\n",
      "def load(cls, frame_path, name):\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def generate_column(self, row_generator, new_column_name):\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "class CodeFramePydantic(BaseModel):\n",
      "    df_path: str\n",
      "    context_columns: List\n",
      "    embeddable_columns: List\n",
      "    embedding_columns: List\n",
      "    name: str\n",
      "    save_path: Optional[str]\n",
      "    save_dir: str\n",
      "    text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder\n",
      "    markdown: str\n",
      "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseModel\"]\n",
      "]\n",
      "Function call: ConfigDict\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class CodeFrame(BaseFrame):\n",
      "    def __init__(self, df: pl.DataFrame, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.df = df\n",
      "        self.frame_template = CodeFramePydantic(df_path=f'{self.save_dir}/{self.name}.parquet', context_columns=self.context_columns, embeddable_columns=self.embeddable_columns, embedding_columns=self.embedding_columns, name=self.name, save_path=self.save_path, save_dir=self.save_dir, load=True, text_embedder=self.text_embedder, markdown=self.markdown)\n",
      "\n",
      "    def __getattr__(self, name: str):\n",
      "        if \"df\" in self.__dict__:\n",
      "            return getattr(self.df.lazy(), name)\n",
      "        raise AttributeError(f\"{self.__class__.__name__} object has no attribute {name}\")\n",
      "\n",
      "    def get_overwritten_attr(self):\n",
      "        df_methods = [method for method in dir(self.df) if callable(getattr(self.df, method))]\n",
      "        memory_frame_methods = [method for method in dir(CodeFrame) if callable(getattr(CodeFrame, method))]\n",
      "        common_methods = list(set(df_methods) & set(memory_frame_methods))\n",
      "        return common_methods\n",
      "\n",
      "    def tokenize_column(self, column_name: str):\n",
      "        new_values = self.tokenizer.encode_batch(self.df[column_name].to_list())\n",
      "        new_series = pl.Series(f'tokens|{column_name}', new_values)\n",
      "        len_values = [len(x) for x in new_values]\n",
      "        new_series_len = pl.Series(f'tokens_len|{column_name}', len_values)\n",
      "        self.df = self.df.with_columns(new_series)\n",
      "        self.df = self.df.with_columns(new_series_len)\n",
      "        return self\n",
      "\n",
      "    def embed_columns(self, embeddable_columns: List):\n",
      "        for column_name in embeddable_columns:\n",
      "            column = self.df[column_name]\n",
      "            _, embedder = infer_embeddable_type(column)\n",
      "            self._embed_column(column, embedder)\n",
      "\n",
      "    def _embed_column(self, column, embedder):\n",
      "        # Add the embeddings as a new column\n",
      "        # Generate new values\n",
      "        new_values = embedder.embed(self.df[column.name].to_list())\n",
      "        # Add new column to DataFrame\n",
      "        new_column_name = f'embedding|{column.name}'\n",
      "        new_series = pl.Series(new_column_name, new_values)\n",
      "        self.df = self.df.with_columns(new_series)\n",
      "        self.embedding_columns.append(new_column_name)\n",
      "\n",
      "    def apply_validator_to_column(self, column_name: str, validator: type):\n",
      "        # Ensure the validator is a subclass of BaseModel from Pydantic\n",
      "        if not issubclass(validator, BaseModel):\n",
      "            raise TypeError('validator must be a subclass of BaseModel from Pydantic')\n",
      "        if column_name not in self.df.columns:\n",
      "            raise ValueError(f\"Column '{column_name}' does not exist.\")\n",
      "        if column_name not in self.embeddable_columns:\n",
      "            raise ValueError(f\"Column '{column_name}' is not set to embeddable.\")\n",
      "        # Iterate over the specified column\n",
      "        for text in self.df[column_name]:\n",
      "            # Create a validator instance and validate the text\n",
      "            try:\n",
      "                _ = validator(text=text).text\n",
      "            except Exception as e:\n",
      "                raise ValueError(f\"Failed to validate text in column '{column_name}'.\") from e\n",
      "\n",
      "        return self\n",
      "\n",
      "    def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n",
      "        df = self.df.filter(sql_query)\n",
      "        embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "        query_as_series = pl.Series(query)\n",
      "        dot_product_frame = df.with_columns(df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "        # Sort by dot product and select top_k rows\n",
      "        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "        return result\n",
      "\n",
      "    def search_column_polar(self, query, embeddable_column_name, top_k):\n",
      "        embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "        query_as_series = pl.Series(query)\n",
      "        dot_product_frame = self.df.with_columns(self.df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "        # Sort by dot product and select top_k rows\n",
      "        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "        return result\n",
      "\n",
      "    def save(self):\n",
      "        #create dir in storage if not exists\n",
      "        if not os.path.exists(self.save_dir):\n",
      "            os.makedirs(self.save_dir)\n",
      "        self.full_save_path = f'{self.save_path}/{self.name}/{self.name}.parquet'\n",
      "        self.df.write_parquet(self.full_save_path)\n",
      "        frame_template_json = self.frame_template.json()\n",
      "        with open(f'{self.save_dir}/{self.name}.json', 'w') as f:\n",
      "            f.write(frame_template_json)\n",
      "\n",
      "    @classmethod\n",
      "    def load(cls, frame_path, name):\n",
      "        df = pl.read_parquet(f'{frame_path}/{name}.parquet')\n",
      "        with open(f'{frame_path}/{name}.json', 'r') as f:\n",
      "            frame_template = CodeFramePydantic.parse_raw(f.read())\n",
      "        return cls(df=df, context_columns=frame_template.context_columns, embeddable_columns=frame_template.embeddable_columns, embedding_columns=frame_template.embedding_columns, name=frame_template.name, save_path=frame_template.save_path, text_embedder=frame_template.text_embedder, markdown=frame_template.markdown)\n",
      "\n",
      "    def generate_column(self, row_generator, new_column_name):\n",
      "        # Generate new values\n",
      "        new_values = row_generator.generate(self.df)\n",
      "        # Add new column to DataFrame\n",
      "        new_df = pl.DataFrame({ new_column_name: new_values })\n",
      "        # Concatenate horizontally\n",
      "        self.df = self.df.hstack([new_df])\n",
      "\n",
      "    def apply_visitor_to_column(self, column_name: str, visitor_class: type, new_column_prefix: Optional[str] = None):\n",
      "        # Ensure the visitor_class is a subclass of PythonCodeVisitor\n",
      "        if not issubclass(visitor_class, cst.CSTVisitor):\n",
      "            raise TypeError('visitor_class must be a subclass of PythonCodeVisitor')\n",
      "\n",
      "        # Iterate over the specified column\n",
      "        new_values = []\n",
      "        for code in self.df[column_name]:\n",
      "            # Create a visitor and apply it to the code\n",
      "            visitor = visitor_class(code)\n",
      "            new_value = visitor.collect()\n",
      "            new_values.append(new_value)\n",
      "        # Generate new column\n",
      "        new_column_name = f'{column_name}_{new_column_prefix}|{visitor_class.__name__}'\n",
      "        new_series = pl.Series(new_column_name, new_values)\n",
      "        self.df = self.df.with_columns(new_series)\n",
      "\n",
      "        return self\n",
      "\n",
      "    def count_node_types(self, column_name: str, new_column_prefix: str = 'node_count'):\n",
      "        for node_type_counter in NODETYPE_COUNTERS:\n",
      "            self.apply_visitor_to_column(column_name, globals()[node_type_counter], new_column_prefix)\n",
      "        return self\n",
      "\n",
      "    def count_operators(self, column_name: str, new_column_prefix: str = 'operator_count'):\n",
      "        for operator_counter in OPERATOR_COUNTERS:\n",
      "            self.apply_visitor_to_column(column_name, globals()[operator_counter], new_column_prefix)\n",
      "        return self\n",
      "\n",
      "\n",
      "    def replace_code_in_files(self, filename_column: str, original_code_column: str, replacing_code_column: str):\n",
      "        visitor = CodeReplacerVisitor(filename_column, original_code_column, replacing_code_column)\n",
      "        for row in self.df.rows():\n",
      "            filename = row[filename_column]\n",
      "            original_code = row[original_code_column]\n",
      "            replacing_code = row[replacing_code_column]\n",
      "\n",
      "            if filename and original_code and replacing_code and os.path.isfile(filename):\n",
      "                node = cst.parse_module(original_code)\n",
      "                node.metadata[original_code_column] = original_code\n",
      "                node.metadata[replacing_code_column] = replacing_code\n",
      "                node.metadata[filename_column] = filename\n",
      "\n",
      "                modified_node = node.visit(visitor)\n",
      "                modified_code = cst.Module(body=modified_node.body).code\n",
      "                row[original_code_column] = modified_code\n",
      "\n",
      "        return self\n",
      "\n",
      "    @classmethod\n",
      "    def from_python(\n",
      "        cls,\n",
      "        directory_path: str,\n",
      "        value_column: str,\n",
      "        minify_code: bool = False,\n",
      "        remove_docstrings: bool = False,\n",
      "        resolution: str = \"both\",\n",
      "        embeddings_column: List = [],\n",
      "        embeddable_columns: List = [],\n",
      "        context_columns: Optional[List[str]] = None,\n",
      "        name: str = \"code_frame\",\n",
      "        save_path: Optional[str] = \"./storage\",\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= None,\n",
      "        markdown: str = \"text/markdown\",\n",
      "    ) -> \"CodeFrame\":\n",
      "        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n",
      "        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n",
      "        #convert retrieved data to polars dataframe\n",
      "        df = pl.DataFrame({value_column: values})\n",
      "        context_df = pl.DataFrame(context)\n",
      "        #merge context columns with dataframe\n",
      "        df = pl.concat([df, context_df], how='horizontal')\n",
      "        if value_column not in embeddable_columns:\n",
      "            embeddable_columns.append(value_column)\n",
      "        print(type(embedder))\n",
      "        kwargs = {\n",
      "            \"context_columns\": context_columns,\n",
      "            \"embeddable_columns\": embeddable_columns,\n",
      "            \"embedding_columns\": embeddings_column,\n",
      "            \"name\": name,\n",
      "            \"save_path\": save_path,\n",
      "            \"text_embedder\": embedder,\n",
      "            \"markdown\": markdown\n",
      "        }\n",
      "        return cls(df, **kwargs)\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseFrame\"]\n",
      "]\n",
      "Function call: super\n",
      "Function call: CodeFramePydantic\n",
      "Function call: getattr\n",
      "Function call: AttributeError\n",
      "Function call: dir\n",
      "Function call: callable\n",
      "Function call: getattr\n",
      "Function call: dir\n",
      "Function call: callable\n",
      "Function call: getattr\n",
      "Function call: list\n",
      "Function call: set\n",
      "Function call: set\n",
      "Function call: len\n",
      "Function call: infer_embeddable_type\n",
      "Function call: issubclass\n",
      "Function call: TypeError\n",
      "Function call: ValueError\n",
      "Function call: ValueError\n",
      "Function call: validator\n",
      "Function call: ValueError\n",
      "Function call: open\n",
      "Function call: open\n",
      "Function call: cls\n",
      "Function call: issubclass\n",
      "Function call: TypeError\n",
      "Function call: visitor_class\n",
      "Function call: globals\n",
      "Function call: globals\n",
      "Function call: CodeReplacerVisitor\n",
      "Function call: extract_values_and_embeddings_python\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: cls\n",
      "Related codes: ['\\n\\n\\nclass CodeFramePydantic(BaseModel):\\n    df_path: str\\n    context_columns: List\\n    embeddable_columns: List\\n    embedding_columns: List\\n    name: str\\n    save_path: Optional[str]\\n    save_dir: str\\n    text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder\\n    markdown: str\\n    model_config = ConfigDict(arbitrary_types_allowed=True)\\n', '\\n@abstractmethod\\ndef __getattr__(self, name: str):\\n    pass\\n', '\\ndef get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\\n    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    subdirectories = [\\n        os.path.join(directory_path, d)\\n        for d in os.listdir(directory_path)\\n        if os.path.isdir(os.path.join(directory_path, d))\\n    ]\\n\\n    return subdirectories\\n', '\\n@abstractmethod\\ndef __getattr__(self, name: str):\\n    pass\\n', '\\ndef get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\\n    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    subdirectories = [\\n        os.path.join(directory_path, d)\\n        for d in os.listdir(directory_path)\\n        if os.path.isdir(os.path.join(directory_path, d))\\n    ]\\n\\n    return subdirectories\\n', '\\n@abstractmethod\\ndef __getattr__(self, name: str):\\n    pass\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef apply_validator_to_column(self, column_name: str, validator: type):\\n    # Ensure the validator is a subclass of BaseModel from Pydantic\\n    if not issubclass(validator, BaseModel):\\n        raise TypeError(\\'validator must be a subclass of BaseModel from Pydantic\\')\\n    if column_name not in self.df.columns:\\n        raise ValueError(f\"Column \\'{column_name}\\' does not exist.\")\\n    if column_name not in self.embeddable_columns:\\n        raise ValueError(f\"Column \\'{column_name}\\' is not set to embeddable.\")\\n    # Iterate over the specified column\\n    for text in self.df[column_name]:\\n        # Create a validator instance and validate the text\\n        try:\\n            _ = validator(text=text).text\\n        except Exception as e:\\n            raise ValueError(f\"Failed to validate text in column \\'{column_name}\\'.\") from e\\n\\n    return self\\n', '\\nclass CodeReplacerVisitor(cst.CSTTransformer):\\n    def __init__(self, filename_column: str, original_code_column: str, replacing_code_column: str):\\n        self.filename_column = filename_column\\n        self.original_code_column = original_code_column\\n        self.replacing_code_column = replacing_code_column\\n\\n    def visit_Module(self, node: cst.Module) -> cst.Module:\\n        # Get the filename from the node metadata\\n        filename = node.metadata.get(self.filename_column)\\n        if filename is None:\\n            return node\\n\\n        # Load the content of the file\\n        with open(filename, \"r\") as f:\\n            file_content = f.read()\\n\\n        # Get the original code and replacing code from the node metadata\\n        original_code = node.metadata.get(self.original_code_column)\\n        replacing_code = node.metadata.get(self.replacing_code_column)\\n\\n        if original_code is None or replacing_code is None:\\n            return node\\n\\n        # Replace the original code with the replacing code in the file content\\n        modified_content = file_content.replace(original_code, replacing_code)\\n\\n        # Save the modified content back to the file\\n        with open(filename, \"w\") as f:\\n            f.write(modified_content)\\n\\n        # Parse the modified content to update the node\\n        return cst.parse_module(modified_content)\\n\\n    def leave_Module(self, original_node: cst.Module, updated_node: cst.Module) -> cst.Module:\\n        # Copy the metadata from the original node to the updated node\\n        updated_node.metadata = original_node.metadata\\n        return updated_node\\n', '\\ndef extract_values_and_embeddings_python(\\n    directory_path: str,\\n    minify_code: bool = False,\\n    remove_docstrings: bool = False,\\n    resolution: str = \"function\"\\n) -> Tuple[List[str], List[Dict[str, str]]]:\\n    values = []\\n    context = []\\n\\n    parser = PythonParser(\\n        directory_path=directory_path,\\n        minify_code=minify_code,\\n        remove_docstrings=remove_docstrings\\n    )\\n\\n    results_dict = parser.process_directory()\\n\\n    if resolution == \"function\":\\n        source_codes = results_dict[\\'function_source_codes\\']\\n        nodes = results_dict[\\'function_nodes\\']\\n    elif resolution == \"class\":\\n        source_codes = results_dict[\\'class_source_codes\\']\\n        nodes = results_dict[\\'class_nodes\\']\\n    elif resolution == \"both\":\\n        source_codes = results_dict[\\'full_source\\']\\n        nodes = results_dict[\\'full_nodes\\']\\n    else:\\n        raise ValueError(f\"Invalid resolution: {resolution}\")\\n    if resolution in [\\'function\\', \\'class\\']:\\n        for source_code, node in zip(source_codes, nodes):\\n            values.append(source_code)\\n            context.append({\"libcst tree\": str(node)})\\n    elif resolution == \"both\":\\n        for source_code, node, filename in zip(source_codes, nodes, results_dict[\\'file_map\\']):\\n            values.append(source_code)\\n            context.append({\"libcst tree\": str(node), \"filename\": filename})\\n    else:\\n        raise ValueError(f\"Invalid resolution: {resolution}\")\\n    return values, context\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, df: pl.DataFrame, **kwargs):\n",
      "    super().__init__(**kwargs)\n",
      "    self.df = df\n",
      "    self.frame_template = CodeFramePydantic(df_path=f'{self.save_dir}/{self.name}.parquet', context_columns=self.context_columns, embeddable_columns=self.embeddable_columns, embedding_columns=self.embedding_columns, name=self.name, save_path=self.save_path, save_dir=self.save_dir, load=True, text_embedder=self.text_embedder, markdown=self.markdown)\n",
      "\n",
      "Function call: super\n",
      "Function call: CodeFramePydantic\n",
      "Related codes: ['\\n\\n\\nclass CodeFramePydantic(BaseModel):\\n    df_path: str\\n    context_columns: List\\n    embeddable_columns: List\\n    embedding_columns: List\\n    name: str\\n    save_path: Optional[str]\\n    save_dir: str\\n    text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder\\n    markdown: str\\n    model_config = ConfigDict(arbitrary_types_allowed=True)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __getattr__(self, name: str):\n",
      "    if \"df\" in self.__dict__:\n",
      "        return getattr(self.df.lazy(), name)\n",
      "    raise AttributeError(f\"{self.__class__.__name__} object has no attribute {name}\")\n",
      "\n",
      "Function call: getattr\n",
      "Function call: AttributeError\n",
      "Related codes: ['\\n@abstractmethod\\ndef __getattr__(self, name: str):\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_overwritten_attr(self):\n",
      "    df_methods = [method for method in dir(self.df) if callable(getattr(self.df, method))]\n",
      "    memory_frame_methods = [method for method in dir(CodeFrame) if callable(getattr(CodeFrame, method))]\n",
      "    common_methods = list(set(df_methods) & set(memory_frame_methods))\n",
      "    return common_methods\n",
      "\n",
      "Function call: dir\n",
      "Function call: callable\n",
      "Function call: getattr\n",
      "Function call: dir\n",
      "Function call: callable\n",
      "Function call: getattr\n",
      "Function call: list\n",
      "Function call: set\n",
      "Function call: set\n",
      "Related codes: ['\\ndef get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\\n    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    subdirectories = [\\n        os.path.join(directory_path, d)\\n        for d in os.listdir(directory_path)\\n        if os.path.isdir(os.path.join(directory_path, d))\\n    ]\\n\\n    return subdirectories\\n', '\\n@abstractmethod\\ndef __getattr__(self, name: str):\\n    pass\\n', '\\ndef get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\\n    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    subdirectories = [\\n        os.path.join(directory_path, d)\\n        for d in os.listdir(directory_path)\\n        if os.path.isdir(os.path.join(directory_path, d))\\n    ]\\n\\n    return subdirectories\\n', '\\n@abstractmethod\\ndef __getattr__(self, name: str):\\n    pass\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def tokenize_column(self, column_name: str):\n",
      "    new_values = self.tokenizer.encode_batch(self.df[column_name].to_list())\n",
      "    new_series = pl.Series(f'tokens|{column_name}', new_values)\n",
      "    len_values = [len(x) for x in new_values]\n",
      "    new_series_len = pl.Series(f'tokens_len|{column_name}', len_values)\n",
      "    self.df = self.df.with_columns(new_series)\n",
      "    self.df = self.df.with_columns(new_series_len)\n",
      "    return self\n",
      "\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def embed_columns(self, embeddable_columns: List):\n",
      "    for column_name in embeddable_columns:\n",
      "        column = self.df[column_name]\n",
      "        _, embedder = infer_embeddable_type(column)\n",
      "        self._embed_column(column, embedder)\n",
      "\n",
      "Function call: infer_embeddable_type\n",
      "Related codes: ['\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _embed_column(self, column, embedder):\n",
      "    # Add the embeddings as a new column\n",
      "    # Generate new values\n",
      "    new_values = embedder.embed(self.df[column.name].to_list())\n",
      "    # Add new column to DataFrame\n",
      "    new_column_name = f'embedding|{column.name}'\n",
      "    new_series = pl.Series(new_column_name, new_values)\n",
      "    self.df = self.df.with_columns(new_series)\n",
      "    self.embedding_columns.append(new_column_name)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def apply_validator_to_column(self, column_name: str, validator: type):\n",
      "    # Ensure the validator is a subclass of BaseModel from Pydantic\n",
      "    if not issubclass(validator, BaseModel):\n",
      "        raise TypeError('validator must be a subclass of BaseModel from Pydantic')\n",
      "    if column_name not in self.df.columns:\n",
      "        raise ValueError(f\"Column '{column_name}' does not exist.\")\n",
      "    if column_name not in self.embeddable_columns:\n",
      "        raise ValueError(f\"Column '{column_name}' is not set to embeddable.\")\n",
      "    # Iterate over the specified column\n",
      "    for text in self.df[column_name]:\n",
      "        # Create a validator instance and validate the text\n",
      "        try:\n",
      "            _ = validator(text=text).text\n",
      "        except Exception as e:\n",
      "            raise ValueError(f\"Failed to validate text in column '{column_name}'.\") from e\n",
      "\n",
      "    return self\n",
      "\n",
      "Function call: issubclass\n",
      "Function call: TypeError\n",
      "Function call: ValueError\n",
      "Function call: ValueError\n",
      "Function call: validator\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef apply_validator_to_column(self, column_name: str, validator: type):\\n    # Ensure the validator is a subclass of BaseModel from Pydantic\\n    if not issubclass(validator, BaseModel):\\n        raise TypeError(\\'validator must be a subclass of BaseModel from Pydantic\\')\\n    if column_name not in self.df.columns:\\n        raise ValueError(f\"Column \\'{column_name}\\' does not exist.\")\\n    if column_name not in self.embeddable_columns:\\n        raise ValueError(f\"Column \\'{column_name}\\' is not set to embeddable.\")\\n    # Iterate over the specified column\\n    for text in self.df[column_name]:\\n        # Create a validator instance and validate the text\\n        try:\\n            _ = validator(text=text).text\\n        except Exception as e:\\n            raise ValueError(f\"Failed to validate text in column \\'{column_name}\\'.\") from e\\n\\n    return self\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n",
      "    df = self.df.filter(sql_query)\n",
      "    embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "    query_as_series = pl.Series(query)\n",
      "    dot_product_frame = df.with_columns(df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "    # Sort by dot product and select top_k rows\n",
      "    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "    return result\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def search_column_polar(self, query, embeddable_column_name, top_k):\n",
      "    embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "    query_as_series = pl.Series(query)\n",
      "    dot_product_frame = self.df.with_columns(self.df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "    # Sort by dot product and select top_k rows\n",
      "    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "    return result\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def save(self):\n",
      "    #create dir in storage if not exists\n",
      "    if not os.path.exists(self.save_dir):\n",
      "        os.makedirs(self.save_dir)\n",
      "    self.full_save_path = f'{self.save_path}/{self.name}/{self.name}.parquet'\n",
      "    self.df.write_parquet(self.full_save_path)\n",
      "    frame_template_json = self.frame_template.json()\n",
      "    with open(f'{self.save_dir}/{self.name}.json', 'w') as f:\n",
      "        f.write(frame_template_json)\n",
      "\n",
      "Function call: open\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@classmethod\n",
      "def load(cls, frame_path, name):\n",
      "    df = pl.read_parquet(f'{frame_path}/{name}.parquet')\n",
      "    with open(f'{frame_path}/{name}.json', 'r') as f:\n",
      "        frame_template = CodeFramePydantic.parse_raw(f.read())\n",
      "    return cls(df=df, context_columns=frame_template.context_columns, embeddable_columns=frame_template.embeddable_columns, embedding_columns=frame_template.embedding_columns, name=frame_template.name, save_path=frame_template.save_path, text_embedder=frame_template.text_embedder, markdown=frame_template.markdown)\n",
      "\n",
      "Function call: open\n",
      "Function call: cls\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def generate_column(self, row_generator, new_column_name):\n",
      "    # Generate new values\n",
      "    new_values = row_generator.generate(self.df)\n",
      "    # Add new column to DataFrame\n",
      "    new_df = pl.DataFrame({ new_column_name: new_values })\n",
      "    # Concatenate horizontally\n",
      "    self.df = self.df.hstack([new_df])\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def apply_visitor_to_column(self, column_name: str, visitor_class: type, new_column_prefix: Optional[str] = None):\n",
      "    # Ensure the visitor_class is a subclass of PythonCodeVisitor\n",
      "    if not issubclass(visitor_class, cst.CSTVisitor):\n",
      "        raise TypeError('visitor_class must be a subclass of PythonCodeVisitor')\n",
      "\n",
      "    # Iterate over the specified column\n",
      "    new_values = []\n",
      "    for code in self.df[column_name]:\n",
      "        # Create a visitor and apply it to the code\n",
      "        visitor = visitor_class(code)\n",
      "        new_value = visitor.collect()\n",
      "        new_values.append(new_value)\n",
      "    # Generate new column\n",
      "    new_column_name = f'{column_name}_{new_column_prefix}|{visitor_class.__name__}'\n",
      "    new_series = pl.Series(new_column_name, new_values)\n",
      "    self.df = self.df.with_columns(new_series)\n",
      "\n",
      "    return self\n",
      "\n",
      "Function call: issubclass\n",
      "Function call: TypeError\n",
      "Function call: visitor_class\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def count_node_types(self, column_name: str, new_column_prefix: str = 'node_count'):\n",
      "    for node_type_counter in NODETYPE_COUNTERS:\n",
      "        self.apply_visitor_to_column(column_name, globals()[node_type_counter], new_column_prefix)\n",
      "    return self\n",
      "\n",
      "Function call: globals\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def count_operators(self, column_name: str, new_column_prefix: str = 'operator_count'):\n",
      "    for operator_counter in OPERATOR_COUNTERS:\n",
      "        self.apply_visitor_to_column(column_name, globals()[operator_counter], new_column_prefix)\n",
      "    return self\n",
      "\n",
      "Function call: globals\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def replace_code_in_files(self, filename_column: str, original_code_column: str, replacing_code_column: str):\n",
      "    visitor = CodeReplacerVisitor(filename_column, original_code_column, replacing_code_column)\n",
      "    for row in self.df.rows():\n",
      "        filename = row[filename_column]\n",
      "        original_code = row[original_code_column]\n",
      "        replacing_code = row[replacing_code_column]\n",
      "\n",
      "        if filename and original_code and replacing_code and os.path.isfile(filename):\n",
      "            node = cst.parse_module(original_code)\n",
      "            node.metadata[original_code_column] = original_code\n",
      "            node.metadata[replacing_code_column] = replacing_code\n",
      "            node.metadata[filename_column] = filename\n",
      "\n",
      "            modified_node = node.visit(visitor)\n",
      "            modified_code = cst.Module(body=modified_node.body).code\n",
      "            row[original_code_column] = modified_code\n",
      "\n",
      "    return self\n",
      "\n",
      "Function call: CodeReplacerVisitor\n",
      "Related codes: ['\\nclass CodeReplacerVisitor(cst.CSTTransformer):\\n    def __init__(self, filename_column: str, original_code_column: str, replacing_code_column: str):\\n        self.filename_column = filename_column\\n        self.original_code_column = original_code_column\\n        self.replacing_code_column = replacing_code_column\\n\\n    def visit_Module(self, node: cst.Module) -> cst.Module:\\n        # Get the filename from the node metadata\\n        filename = node.metadata.get(self.filename_column)\\n        if filename is None:\\n            return node\\n\\n        # Load the content of the file\\n        with open(filename, \"r\") as f:\\n            file_content = f.read()\\n\\n        # Get the original code and replacing code from the node metadata\\n        original_code = node.metadata.get(self.original_code_column)\\n        replacing_code = node.metadata.get(self.replacing_code_column)\\n\\n        if original_code is None or replacing_code is None:\\n            return node\\n\\n        # Replace the original code with the replacing code in the file content\\n        modified_content = file_content.replace(original_code, replacing_code)\\n\\n        # Save the modified content back to the file\\n        with open(filename, \"w\") as f:\\n            f.write(modified_content)\\n\\n        # Parse the modified content to update the node\\n        return cst.parse_module(modified_content)\\n\\n    def leave_Module(self, original_node: cst.Module, updated_node: cst.Module) -> cst.Module:\\n        # Copy the metadata from the original node to the updated node\\n        updated_node.metadata = original_node.metadata\\n        return updated_node\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@classmethod\n",
      "def from_python(\n",
      "    cls,\n",
      "    directory_path: str,\n",
      "    value_column: str,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "    resolution: str = \"both\",\n",
      "    embeddings_column: List = [],\n",
      "    embeddable_columns: List = [],\n",
      "    context_columns: Optional[List[str]] = None,\n",
      "    name: str = \"code_frame\",\n",
      "    save_path: Optional[str] = \"./storage\",\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= None,\n",
      "    markdown: str = \"text/markdown\",\n",
      ") -> \"CodeFrame\":\n",
      "    values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n",
      "    logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n",
      "    #convert retrieved data to polars dataframe\n",
      "    df = pl.DataFrame({value_column: values})\n",
      "    context_df = pl.DataFrame(context)\n",
      "    #merge context columns with dataframe\n",
      "    df = pl.concat([df, context_df], how='horizontal')\n",
      "    if value_column not in embeddable_columns:\n",
      "        embeddable_columns.append(value_column)\n",
      "    print(type(embedder))\n",
      "    kwargs = {\n",
      "        \"context_columns\": context_columns,\n",
      "        \"embeddable_columns\": embeddable_columns,\n",
      "        \"embedding_columns\": embeddings_column,\n",
      "        \"name\": name,\n",
      "        \"save_path\": save_path,\n",
      "        \"text_embedder\": embedder,\n",
      "        \"markdown\": markdown\n",
      "    }\n",
      "    return cls(df, **kwargs)\n",
      "\n",
      "Function call: extract_values_and_embeddings_python\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: cls\n",
      "Related codes: ['\\ndef extract_values_and_embeddings_python(\\n    directory_path: str,\\n    minify_code: bool = False,\\n    remove_docstrings: bool = False,\\n    resolution: str = \"function\"\\n) -> Tuple[List[str], List[Dict[str, str]]]:\\n    values = []\\n    context = []\\n\\n    parser = PythonParser(\\n        directory_path=directory_path,\\n        minify_code=minify_code,\\n        remove_docstrings=remove_docstrings\\n    )\\n\\n    results_dict = parser.process_directory()\\n\\n    if resolution == \"function\":\\n        source_codes = results_dict[\\'function_source_codes\\']\\n        nodes = results_dict[\\'function_nodes\\']\\n    elif resolution == \"class\":\\n        source_codes = results_dict[\\'class_source_codes\\']\\n        nodes = results_dict[\\'class_nodes\\']\\n    elif resolution == \"both\":\\n        source_codes = results_dict[\\'full_source\\']\\n        nodes = results_dict[\\'full_nodes\\']\\n    else:\\n        raise ValueError(f\"Invalid resolution: {resolution}\")\\n    if resolution in [\\'function\\', \\'class\\']:\\n        for source_code, node in zip(source_codes, nodes):\\n            values.append(source_code)\\n            context.append({\"libcst tree\": str(node)})\\n    elif resolution == \"both\":\\n        for source_code, node, filename in zip(source_codes, nodes, results_dict[\\'file_map\\']):\\n            values.append(source_code)\\n            context.append({\"libcst tree\": str(node), \"filename\": filename})\\n    else:\\n        raise ValueError(f\"Invalid resolution: {resolution}\")\\n    return values, context\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "class MemoryFrame:\n",
      "    def __init__(self, df: pl.DataFrame,\n",
      "                context_columns: List = [],\n",
      "                embeddable_columns: List = [],\n",
      "                time_series_columns: List = [],\n",
      "                name: str = \"memory_frame\",\n",
      "                save_path: Optional[str] = None,\n",
      "                load: bool = False,\n",
      "                text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "                markdown: str = \"text/markdown\",):\n",
      "        self.df = df\n",
      "        self.context_columns = context_columns\n",
      "        self.time_series_columns = time_series_columns\n",
      "        self.embeddable_columns = embeddable_columns\n",
      "        self.meta_columns = ['ID', 'Name', 'Source', 'Author', 'Created At', 'Last Modified At']\n",
      "        self.embedding_columns = []\n",
      "        self.name = name\n",
      "        self.save_path = save_path\n",
      "        self.load = load\n",
      "        self.text_embedder = text_embedder\n",
      "        self.markdown = markdown\n",
      "\n",
      "\n",
      "    def __getattr__(self, name: str):\n",
      "        # delegate to the self.df object\n",
      "        return getattr(self.df, name)\n",
      "\n",
      "    def get_overwritten_attr(self):\n",
      "        df_methods = [method for method in dir(self.df) if callable(getattr(self.df, method))]\n",
      "        memory_frame_methods = [method for method in dir(MemoryFrame) if callable(getattr(MemoryFrame, method))]\n",
      "        common_methods = list(set(df_methods) & set(memory_frame_methods))\n",
      "        return common_methods\n",
      "\n",
      "    def embed_columns(self, embeddable_columns: List):\n",
      "        for column_name in embeddable_columns:\n",
      "            column = self.df[column_name]\n",
      "            _, embedder = infer_embeddable_type(column)\n",
      "            self._embed_column(column, embedder)\n",
      "\n",
      "    def _embed_column(self, column, embedder):\n",
      "        # Add the embeddings as a new column\n",
      "        # Generate new values\n",
      "        new_values = embedder.embed(self.df[column.name].to_list())\n",
      "        # Add new column to DataFrame\n",
      "        new_column_name = f'embedding|{column.name}'\n",
      "        new_series = pl.Series(new_column_name, new_values)\n",
      "        self.df = self.df.with_columns(new_series)\n",
      "        self.embedding_columns.append(new_column_name)\n",
      "\n",
      "\n",
      "    def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n",
      "        df = self.df.filter(sql_query)\n",
      "        embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "        query_as_series = pl.Series(query)\n",
      "        dot_product_frame = df.with_columns(df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "        # Sort by dot product and select top_k rows\n",
      "        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "        return result\n",
      "\n",
      "\n",
      "    def search_column_polar(self, query, embeddable_column_name, top_k):\n",
      "        embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "        query_as_series = pl.Series(query)\n",
      "        dot_product_frame = self.df.with_columns(self.df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "        # Sort by dot product and select top_k rows\n",
      "        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "        return result\n",
      "\n",
      "    def search_column_numpy(self, query, embeddable_column_name, top_k):\n",
      "        embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "        #convert query and column to numpy arrays\n",
      "        column_np = self.df[embedding_column_name].to_numpy()\n",
      "        #calculate dot product\n",
      "        dot_product = np.dot(column_np, query)\n",
      "        #add dot products as column to dataframe\n",
      "        dot_product_frame = self.df.with_columns(dot_product)\n",
      "        # Sort by dot product and select top_k rows\n",
      "        result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "        return result\n",
      "\n",
      "    def save_parquet(self):\n",
      "        #save to arrow\n",
      "        self.full_save_path = self.save_path + self.name + '.parquet'\n",
      "        self.df.write_parquet(self.full_save_path)\n",
      "\n",
      "    def load_parquet(self):\n",
      "        self.full_save_path = self.save_path + self.name + '.parquet'\n",
      "        self.df = pl.read_parquet(self.full_save_path)\n",
      "\n",
      "\n",
      "    def search_time_series_column(self, query, embeddable_column_name, top_k):\n",
      "        ## uses dtw to match any sub-sequence of the query to the time series in the column\n",
      "        ## time series column have a date o time or delta time column associated with them \n",
      "        ## each row-value is a list of across rows variable length for both the time series and the date or time column\n",
      "        pass\n",
      "\n",
      "    def generate_column(self, row_generator, new_column_name):\n",
      "        # Generate new values\n",
      "        new_values = row_generator.generate(self.df)\n",
      "        # Add new column to DataFrame\n",
      "        new_df = pl.DataFrame({ new_column_name: new_values })\n",
      "\n",
      "        # Concatenate horizontally\n",
      "        self.df = self.df.hstack([new_df])\n",
      "    \n",
      "    def create_stratas(self):\n",
      "        \"\"\"\n",
      "        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def _create_strata(self, column_name: str):\n",
      "        \"\"\"\n",
      "        Determine the correct strata creation function to call based on the column's data type,\n",
      "        and then calls the corresponding function.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def _create_strata_from_categorical(self, column_name: str):\n",
      "        \"\"\"\n",
      "        Create strata for a categorical column.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def _create_strata_from_real(self, column_name: str):\n",
      "        \"\"\"\n",
      "        Create strata for a real valued column.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def _create_strata_from_embeddings(self, column_name: str):\n",
      "        \"\"\"\n",
      "        Create strata for a column with embeddings.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def _create_strata_from_episodic_time_series(self, column_name: str):\n",
      "        \"\"\"\n",
      "        Create strata for a column with episodic time series.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def create_joint_strata(self, column_names: list):\n",
      "        \"\"\"\n",
      "        Create strata based on the unique combinations of values across given columns.\n",
      "        \n",
      "        Args:\n",
      "            column_names (list): The names of the columns.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def stratified_sampling(self, strata_columns: list, n_samples: int):\n",
      "        \"\"\"\n",
      "        Perform stratified sampling on given stratum columns.\n",
      "        \n",
      "        Args:\n",
      "            strata_columns (list): The names of the stratum columns.\n",
      "            n_samples (int): The number of samples to draw.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    def stratified_cross_validation(self, strata_columns: list, n_folds: int):\n",
      "        \"\"\"\n",
      "        Perform stratified cross-validation on given stratum columns.\n",
      "        \n",
      "        Args:\n",
      "            strata_columns (list): The names of the stratum columns.\n",
      "            n_folds (int): The number of folds for the cross-validation.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    @classmethod\n",
      "    def from_hf_dataset(\n",
      "        cls,\n",
      "        dataset_url: str,\n",
      "        value_column: str,\n",
      "        data_split: str = \"train\",\n",
      "        embeddings_column: Optional[str] = None,\n",
      "        embeddable_columns: List = [],\n",
      "        context_columns: Optional[List[str]] = None,\n",
      "        time_series_columns: List = [],\n",
      "        name: str = \"memory_frame\",\n",
      "        save_path: Optional[str] = None,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "        markdown: str = \"text/markdown\",\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ) -> \"MemoryFrame\":\n",
      "        dataset = load_dataset(dataset_url)[data_split]\n",
      "        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\n",
      "        if context_columns is not None:\n",
      "            context = get_context_from_hf(dataset, context_columns)\n",
      "        else:\n",
      "            context = None\n",
      "        #convert retrieved data to polars dataframe\n",
      "        if embeddings is not None:\n",
      "            df = pl.DataFrame({value_column: values, embeddings_column: embeddings})\n",
      "        else:\n",
      "            df = pl.DataFrame({value_column: values})\n",
      "        context_df = pl.DataFrame(context)\n",
      "        #merge context columns with dataframe\n",
      "        df = pl.concat([df, context_df], how='horizontal')\n",
      "        if value_column not in embeddable_columns:\n",
      "            embeddable_columns.append(value_column)\n",
      "        return cls(df, context_columns, embeddable_columns, time_series_columns, name, save_path, embedder, markdown, token_overflow_strategy)\n",
      "\n",
      "    @classmethod\n",
      "    def from_python(\n",
      "        cls,\n",
      "        directory_path: str,\n",
      "        value_column: str,\n",
      "        minify_code: bool = False,\n",
      "        remove_docstrings: bool = False,\n",
      "        resolution: str = \"both\",\n",
      "        embeddings_column: Optional[str] = None,\n",
      "        embeddable_columns: List = [],\n",
      "        context_columns: Optional[List[str]] = None,\n",
      "        time_series_columns: List = [],\n",
      "        name: str = \"memory_frame\",\n",
      "        save_path: Optional[str] = None,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "        markdown: str = \"text/markdown\",\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ) -> \"MemoryFrame\":\n",
      "        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n",
      "        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n",
      "        #convert retrieved data to polars dataframe\n",
      "        df = pl.DataFrame({value_column: values})\n",
      "        context_df = pl.DataFrame(context)\n",
      "        #merge context columns with dataframe\n",
      "        df = pl.concat([df, context_df], how='horizontal')\n",
      "        if value_column not in embeddable_columns:\n",
      "            embeddable_columns.append(value_column)\n",
      "        return cls(df, context_columns, embeddable_columns, time_series_columns, name, save_path, embedder, markdown, token_overflow_strategy)\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: getattr\n",
      "Function call: dir\n",
      "Function call: callable\n",
      "Function call: getattr\n",
      "Function call: dir\n",
      "Function call: callable\n",
      "Function call: getattr\n",
      "Function call: list\n",
      "Function call: set\n",
      "Function call: set\n",
      "Function call: infer_embeddable_type\n",
      "Function call: load_dataset\n",
      "Function call: extract_values_and_embeddings_hf\n",
      "Function call: get_context_from_hf\n",
      "Function call: cls\n",
      "Function call: extract_values_and_embeddings_python\n",
      "Function call: len\n",
      "Function call: cls\n",
      "Related codes: ['\\n@abstractmethod\\ndef __getattr__(self, name: str):\\n    pass\\n', '\\ndef get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\\n    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    subdirectories = [\\n        os.path.join(directory_path, d)\\n        for d in os.listdir(directory_path)\\n        if os.path.isdir(os.path.join(directory_path, d))\\n    ]\\n\\n    return subdirectories\\n', '\\n@abstractmethod\\ndef __getattr__(self, name: str):\\n    pass\\n', '\\ndef get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\\n    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    subdirectories = [\\n        os.path.join(directory_path, d)\\n        for d in os.listdir(directory_path)\\n        if os.path.isdir(os.path.join(directory_path, d))\\n    ]\\n\\n    return subdirectories\\n', '\\n@abstractmethod\\ndef __getattr__(self, name: str):\\n    pass\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef extract_values_and_embeddings_hf(\\n    dataset: datasets.Dataset,\\n    value_column: Union[str, List[str]],\\n    embeddings_column: Optional[str],\\n) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n    Extract values and embeddings from a Hugging Face dataset.\\n\\n    Args:\\n        dataset: The Hugging Face dataset to extract values and embeddings from.\\n        value_column: The column(s) of the dataset to use as values.\\n        embeddings_column: The column name containing the embeddings.\\n\\n    Returns:\\n        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n    \"\"\"\\n    if isinstance(value_column, str):\\n        value_column = [value_column]\\n    print(\"Merging values: Start\")\\n    merged_docs = dataset.map(concat_columns, with_indices=True)\\n    print(\"Merging values: Done\")\\n    values = merged_docs[\\'merged_column\\']\\n    embeddings = dataset[embeddings_column]\\n    return values, embeddings if embeddings_column is not None else None\\n', '\\ndef get_context_from_hf(\\n          data_frame: datasets.Dataset,\\n          context_columns: List[str]):\\n    \"\"\"return a list dictionaries with the keys the column name and value the context columns values\"\"\"\\n    context_data = {column: data_frame[column] for column in context_columns}\\n    context = []\\n    data_frame_len = len(data_frame)\\n    for row in range(data_frame_len):\\n         context.append({column: context_data[column][row] for column in context_columns})\\n    return context\\n', '\\ndef extract_values_and_embeddings_python(\\n    directory_path: str,\\n    minify_code: bool = False,\\n    remove_docstrings: bool = False,\\n    resolution: str = \"function\"\\n) -> Tuple[List[str], List[Dict[str, str]]]:\\n    values = []\\n    context = []\\n\\n    parser = PythonParser(\\n        directory_path=directory_path,\\n        minify_code=minify_code,\\n        remove_docstrings=remove_docstrings\\n    )\\n\\n    results_dict = parser.process_directory()\\n\\n    if resolution == \"function\":\\n        source_codes = results_dict[\\'function_source_codes\\']\\n        nodes = results_dict[\\'function_nodes\\']\\n    elif resolution == \"class\":\\n        source_codes = results_dict[\\'class_source_codes\\']\\n        nodes = results_dict[\\'class_nodes\\']\\n    elif resolution == \"both\":\\n        source_codes = results_dict[\\'full_source\\']\\n        nodes = results_dict[\\'full_nodes\\']\\n    else:\\n        raise ValueError(f\"Invalid resolution: {resolution}\")\\n    if resolution in [\\'function\\', \\'class\\']:\\n        for source_code, node in zip(source_codes, nodes):\\n            values.append(source_code)\\n            context.append({\"libcst tree\": str(node)})\\n    elif resolution == \"both\":\\n        for source_code, node, filename in zip(source_codes, nodes, results_dict[\\'file_map\\']):\\n            values.append(source_code)\\n            context.append({\"libcst tree\": str(node), \"filename\": filename})\\n    else:\\n        raise ValueError(f\"Invalid resolution: {resolution}\")\\n    return values, context\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, df: pl.DataFrame,\n",
      "            context_columns: List = [],\n",
      "            embeddable_columns: List = [],\n",
      "            time_series_columns: List = [],\n",
      "            name: str = \"memory_frame\",\n",
      "            save_path: Optional[str] = None,\n",
      "            load: bool = False,\n",
      "            text_embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "            markdown: str = \"text/markdown\",):\n",
      "    self.df = df\n",
      "    self.context_columns = context_columns\n",
      "    self.time_series_columns = time_series_columns\n",
      "    self.embeddable_columns = embeddable_columns\n",
      "    self.meta_columns = ['ID', 'Name', 'Source', 'Author', 'Created At', 'Last Modified At']\n",
      "    self.embedding_columns = []\n",
      "    self.name = name\n",
      "    self.save_path = save_path\n",
      "    self.load = load\n",
      "    self.text_embedder = text_embedder\n",
      "    self.markdown = markdown\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def __getattr__(self, name: str):\n",
      "    # delegate to the self.df object\n",
      "    return getattr(self.df, name)\n",
      "\n",
      "Function call: getattr\n",
      "Related codes: ['\\n@abstractmethod\\ndef __getattr__(self, name: str):\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_overwritten_attr(self):\n",
      "    df_methods = [method for method in dir(self.df) if callable(getattr(self.df, method))]\n",
      "    memory_frame_methods = [method for method in dir(MemoryFrame) if callable(getattr(MemoryFrame, method))]\n",
      "    common_methods = list(set(df_methods) & set(memory_frame_methods))\n",
      "    return common_methods\n",
      "\n",
      "Function call: dir\n",
      "Function call: callable\n",
      "Function call: getattr\n",
      "Function call: dir\n",
      "Function call: callable\n",
      "Function call: getattr\n",
      "Function call: list\n",
      "Function call: set\n",
      "Function call: set\n",
      "Related codes: ['\\ndef get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\\n    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    subdirectories = [\\n        os.path.join(directory_path, d)\\n        for d in os.listdir(directory_path)\\n        if os.path.isdir(os.path.join(directory_path, d))\\n    ]\\n\\n    return subdirectories\\n', '\\n@abstractmethod\\ndef __getattr__(self, name: str):\\n    pass\\n', '\\ndef get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\\n    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    subdirectories = [\\n        os.path.join(directory_path, d)\\n        for d in os.listdir(directory_path)\\n        if os.path.isdir(os.path.join(directory_path, d))\\n    ]\\n\\n    return subdirectories\\n', '\\n@abstractmethod\\ndef __getattr__(self, name: str):\\n    pass\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def embed_columns(self, embeddable_columns: List):\n",
      "    for column_name in embeddable_columns:\n",
      "        column = self.df[column_name]\n",
      "        _, embedder = infer_embeddable_type(column)\n",
      "        self._embed_column(column, embedder)\n",
      "\n",
      "Function call: infer_embeddable_type\n",
      "Related codes: ['\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _embed_column(self, column, embedder):\n",
      "    # Add the embeddings as a new column\n",
      "    # Generate new values\n",
      "    new_values = embedder.embed(self.df[column.name].to_list())\n",
      "    # Add new column to DataFrame\n",
      "    new_column_name = f'embedding|{column.name}'\n",
      "    new_series = pl.Series(new_column_name, new_values)\n",
      "    self.df = self.df.with_columns(new_series)\n",
      "    self.embedding_columns.append(new_column_name)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def search_column_with_sql_polar(self, sql_query, query, embeddable_column_name, top_k):\n",
      "    df = self.df.filter(sql_query)\n",
      "    embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "    query_as_series = pl.Series(query)\n",
      "    dot_product_frame = df.with_columns(df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "    # Sort by dot product and select top_k rows\n",
      "    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "    return result\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def search_column_polar(self, query, embeddable_column_name, top_k):\n",
      "    embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "\n",
      "    query_as_series = pl.Series(query)\n",
      "    dot_product_frame = self.df.with_columns(self.df[embedding_column_name].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
      "    # Sort by dot product and select top_k rows\n",
      "    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "    return result\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def search_column_numpy(self, query, embeddable_column_name, top_k):\n",
      "    embedding_column_name = 'embedding|' + embeddable_column_name\n",
      "    #convert query and column to numpy arrays\n",
      "    column_np = self.df[embedding_column_name].to_numpy()\n",
      "    #calculate dot product\n",
      "    dot_product = np.dot(column_np, query)\n",
      "    #add dot products as column to dataframe\n",
      "    dot_product_frame = self.df.with_columns(dot_product)\n",
      "    # Sort by dot product and select top_k rows\n",
      "    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
      "    return result\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def save_parquet(self):\n",
      "    #save to arrow\n",
      "    self.full_save_path = self.save_path + self.name + '.parquet'\n",
      "    self.df.write_parquet(self.full_save_path)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def load_parquet(self):\n",
      "    self.full_save_path = self.save_path + self.name + '.parquet'\n",
      "    self.df = pl.read_parquet(self.full_save_path)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def search_time_series_column(self, query, embeddable_column_name, top_k):\n",
      "    ## uses dtw to match any sub-sequence of the query to the time series in the column\n",
      "    ## time series column have a date o time or delta time column associated with them \n",
      "    ## each row-value is a list of across rows variable length for both the time series and the date or time column\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def generate_column(self, row_generator, new_column_name):\n",
      "    # Generate new values\n",
      "    new_values = row_generator.generate(self.df)\n",
      "    # Add new column to DataFrame\n",
      "    new_df = pl.DataFrame({ new_column_name: new_values })\n",
      "\n",
      "    # Concatenate horizontally\n",
      "    self.df = self.df.hstack([new_df])\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def create_stratas(self):\n",
      "    \"\"\"\n",
      "        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _create_strata(self, column_name: str):\n",
      "    \"\"\"\n",
      "        Determine the correct strata creation function to call based on the column's data type,\n",
      "        and then calls the corresponding function.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _create_strata_from_categorical(self, column_name: str):\n",
      "    \"\"\"\n",
      "        Create strata for a categorical column.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _create_strata_from_real(self, column_name: str):\n",
      "    \"\"\"\n",
      "        Create strata for a real valued column.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _create_strata_from_embeddings(self, column_name: str):\n",
      "    \"\"\"\n",
      "        Create strata for a column with embeddings.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _create_strata_from_episodic_time_series(self, column_name: str):\n",
      "    \"\"\"\n",
      "        Create strata for a column with episodic time series.\n",
      "\n",
      "        Args:\n",
      "            column_name (str): The name of the column.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def create_joint_strata(self, column_names: list):\n",
      "    \"\"\"\n",
      "        Create strata based on the unique combinations of values across given columns.\n",
      "        \n",
      "        Args:\n",
      "            column_names (list): The names of the columns.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def stratified_sampling(self, strata_columns: list, n_samples: int):\n",
      "    \"\"\"\n",
      "        Perform stratified sampling on given stratum columns.\n",
      "        \n",
      "        Args:\n",
      "            strata_columns (list): The names of the stratum columns.\n",
      "            n_samples (int): The number of samples to draw.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def stratified_cross_validation(self, strata_columns: list, n_folds: int):\n",
      "    \"\"\"\n",
      "        Perform stratified cross-validation on given stratum columns.\n",
      "        \n",
      "        Args:\n",
      "            strata_columns (list): The names of the stratum columns.\n",
      "            n_folds (int): The number of folds for the cross-validation.\n",
      "        \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@classmethod\n",
      "def from_hf_dataset(\n",
      "    cls,\n",
      "    dataset_url: str,\n",
      "    value_column: str,\n",
      "    data_split: str = \"train\",\n",
      "    embeddings_column: Optional[str] = None,\n",
      "    embeddable_columns: List = [],\n",
      "    context_columns: Optional[List[str]] = None,\n",
      "    time_series_columns: List = [],\n",
      "    name: str = \"memory_frame\",\n",
      "    save_path: Optional[str] = None,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "    markdown: str = \"text/markdown\",\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      ") -> \"MemoryFrame\":\n",
      "    dataset = load_dataset(dataset_url)[data_split]\n",
      "    values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\n",
      "    if context_columns is not None:\n",
      "        context = get_context_from_hf(dataset, context_columns)\n",
      "    else:\n",
      "        context = None\n",
      "    #convert retrieved data to polars dataframe\n",
      "    if embeddings is not None:\n",
      "        df = pl.DataFrame({value_column: values, embeddings_column: embeddings})\n",
      "    else:\n",
      "        df = pl.DataFrame({value_column: values})\n",
      "    context_df = pl.DataFrame(context)\n",
      "    #merge context columns with dataframe\n",
      "    df = pl.concat([df, context_df], how='horizontal')\n",
      "    if value_column not in embeddable_columns:\n",
      "        embeddable_columns.append(value_column)\n",
      "    return cls(df, context_columns, embeddable_columns, time_series_columns, name, save_path, embedder, markdown, token_overflow_strategy)\n",
      "\n",
      "Function call: load_dataset\n",
      "Function call: extract_values_and_embeddings_hf\n",
      "Function call: get_context_from_hf\n",
      "Function call: cls\n",
      "Related codes: ['\\ndef extract_values_and_embeddings_hf(\\n    dataset: datasets.Dataset,\\n    value_column: Union[str, List[str]],\\n    embeddings_column: Optional[str],\\n) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n    Extract values and embeddings from a Hugging Face dataset.\\n\\n    Args:\\n        dataset: The Hugging Face dataset to extract values and embeddings from.\\n        value_column: The column(s) of the dataset to use as values.\\n        embeddings_column: The column name containing the embeddings.\\n\\n    Returns:\\n        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n    \"\"\"\\n    if isinstance(value_column, str):\\n        value_column = [value_column]\\n    print(\"Merging values: Start\")\\n    merged_docs = dataset.map(concat_columns, with_indices=True)\\n    print(\"Merging values: Done\")\\n    values = merged_docs[\\'merged_column\\']\\n    embeddings = dataset[embeddings_column]\\n    return values, embeddings if embeddings_column is not None else None\\n', '\\ndef get_context_from_hf(\\n          data_frame: datasets.Dataset,\\n          context_columns: List[str]):\\n    \"\"\"return a list dictionaries with the keys the column name and value the context columns values\"\"\"\\n    context_data = {column: data_frame[column] for column in context_columns}\\n    context = []\\n    data_frame_len = len(data_frame)\\n    for row in range(data_frame_len):\\n         context.append({column: context_data[column][row] for column in context_columns})\\n    return context\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@classmethod\n",
      "def from_python(\n",
      "    cls,\n",
      "    directory_path: str,\n",
      "    value_column: str,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "    resolution: str = \"both\",\n",
      "    embeddings_column: Optional[str] = None,\n",
      "    embeddable_columns: List = [],\n",
      "    context_columns: Optional[List[str]] = None,\n",
      "    time_series_columns: List = [],\n",
      "    name: str = \"memory_frame\",\n",
      "    save_path: Optional[str] = None,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "    markdown: str = \"text/markdown\",\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      ") -> \"MemoryFrame\":\n",
      "    values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n",
      "    logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n",
      "    #convert retrieved data to polars dataframe\n",
      "    df = pl.DataFrame({value_column: values})\n",
      "    context_df = pl.DataFrame(context)\n",
      "    #merge context columns with dataframe\n",
      "    df = pl.concat([df, context_df], how='horizontal')\n",
      "    if value_column not in embeddable_columns:\n",
      "        embeddable_columns.append(value_column)\n",
      "    return cls(df, context_columns, embeddable_columns, time_series_columns, name, save_path, embedder, markdown, token_overflow_strategy)\n",
      "\n",
      "Function call: extract_values_and_embeddings_python\n",
      "Function call: len\n",
      "Function call: cls\n",
      "Related codes: ['\\ndef extract_values_and_embeddings_python(\\n    directory_path: str,\\n    minify_code: bool = False,\\n    remove_docstrings: bool = False,\\n    resolution: str = \"function\"\\n) -> Tuple[List[str], List[Dict[str, str]]]:\\n    values = []\\n    context = []\\n\\n    parser = PythonParser(\\n        directory_path=directory_path,\\n        minify_code=minify_code,\\n        remove_docstrings=remove_docstrings\\n    )\\n\\n    results_dict = parser.process_directory()\\n\\n    if resolution == \"function\":\\n        source_codes = results_dict[\\'function_source_codes\\']\\n        nodes = results_dict[\\'function_nodes\\']\\n    elif resolution == \"class\":\\n        source_codes = results_dict[\\'class_source_codes\\']\\n        nodes = results_dict[\\'class_nodes\\']\\n    elif resolution == \"both\":\\n        source_codes = results_dict[\\'full_source\\']\\n        nodes = results_dict[\\'full_nodes\\']\\n    else:\\n        raise ValueError(f\"Invalid resolution: {resolution}\")\\n    if resolution in [\\'function\\', \\'class\\']:\\n        for source_code, node in zip(source_codes, nodes):\\n            values.append(source_code)\\n            context.append({\"libcst tree\": str(node)})\\n    elif resolution == \"both\":\\n        for source_code, node, filename in zip(source_codes, nodes, results_dict[\\'file_map\\']):\\n            values.append(source_code)\\n            context.append({\"libcst tree\": str(node), \"filename\": filename})\\n    else:\\n        raise ValueError(f\"Invalid resolution: {resolution}\")\\n    return values, context\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class FunctionCallCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.function_call_count = 0\n",
      "\n",
      "    def visit_Call(self, node: cst.Call) -> bool:\n",
      "        self.function_call_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.function_call_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.function_call_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Call(self, node: cst.Call) -> bool:\n",
      "    self.function_call_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.function_call_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Argument Type Counter\n",
      "class ArgumentTypeCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.argument_type_count = 0\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n",
      "        self.argument_type_count += len(node.params.params)\n",
      "        return True\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n",
      "        for stmt in node.body.body:\n",
      "            if isinstance(stmt, cst.FunctionDef):\n",
      "                self.argument_type_count += len(stmt.params.params)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.argument_type_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.argument_type_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n",
      "    self.argument_type_count += len(node.params.params)\n",
      "    return True\n",
      "\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n",
      "    for stmt in node.body.body:\n",
      "        if isinstance(stmt, cst.FunctionDef):\n",
      "            self.argument_type_count += len(stmt.params.params)\n",
      "    return True\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.argument_type_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Import Counter\n",
      "class ImportCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.import_count = 0\n",
      "\n",
      "    def visit_Import(self, node: cst.Import) -> bool:\n",
      "        self.import_count += 1\n",
      "        return True\n",
      "\n",
      "    def visit_ImportFrom(self, node: cst.ImportFrom) -> bool:\n",
      "        self.import_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.import_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.import_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Import(self, node: cst.Import) -> bool:\n",
      "    self.import_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ImportFrom(self, node: cst.ImportFrom) -> bool:\n",
      "    self.import_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.import_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# If Statement Counter\n",
      "class IfStatementCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.if_statement_count = 0\n",
      "\n",
      "    def visit_If(self, node: cst.If) -> bool:\n",
      "        self.if_statement_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.if_statement_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.if_statement_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_If(self, node: cst.If) -> bool:\n",
      "    self.if_statement_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.if_statement_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Base Compound Statement Counter\n",
      "class BaseCompoundStatementCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.compound_statement_count = 0\n",
      "\n",
      "    def visit_BaseCompoundStatement(self, node: cst.BaseCompoundStatement) -> bool:\n",
      "        self.compound_statement_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.compound_statement_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.compound_statement_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BaseCompoundStatement(self, node: cst.BaseCompoundStatement) -> bool:\n",
      "    self.compound_statement_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.compound_statement_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# For Loop Counter\n",
      "class ForLoopCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.for_loop_count = 0\n",
      "\n",
      "    def visit_For(self, node: cst.For) -> bool:\n",
      "        self.for_loop_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.for_loop_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.for_loop_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_For(self, node: cst.For) -> bool:\n",
      "    self.for_loop_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.for_loop_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# While Loop Counter\n",
      "class WhileLoopCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.while_loop_count = 0\n",
      "\n",
      "    def visit_While(self, node: cst.While) -> bool:\n",
      "        self.while_loop_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.while_loop_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.while_loop_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_While(self, node: cst.While) -> bool:\n",
      "    self.while_loop_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.while_loop_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Try Except Counter\n",
      "class TryExceptCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.try_except_count = 0\n",
      "\n",
      "    def visit_Try(self, node: cst.Try) -> bool:\n",
      "        self.try_except_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.try_except_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.try_except_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Try(self, node: cst.Try) -> bool:\n",
      "    self.try_except_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.try_except_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# With Statement Counter\n",
      "class WithStatementCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.with_statement_count = 0\n",
      "\n",
      "    def visit_With(self, node: cst.With) -> bool:\n",
      "        self.with_statement_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.with_statement_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.with_statement_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_With(self, node: cst.With) -> bool:\n",
      "    self.with_statement_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.with_statement_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Lambda Function Counter\n",
      "class LambdaFunctionCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.lambda_function_count = 0\n",
      "\n",
      "    def visit_Lambda(self, node: cst.Lambda) -> bool:\n",
      "        self.lambda_function_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.lambda_function_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.lambda_function_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Lambda(self, node: cst.Lambda) -> bool:\n",
      "    self.lambda_function_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.lambda_function_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Global Statement Counter\n",
      "class GlobalStatementCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.global_statement_count = 0\n",
      "\n",
      "    def visit_Global(self, node: cst.Global) -> bool:\n",
      "        self.global_statement_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.global_statement_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.global_statement_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Global(self, node: cst.Global) -> bool:\n",
      "    self.global_statement_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.global_statement_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Nonlocal Statement Counter\n",
      "class NonlocalStatementCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.nonlocal_statement_count = 0\n",
      "\n",
      "    def visit_Nonlocal(self, node: cst.Nonlocal) -> bool:\n",
      "        self.nonlocal_statement_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.nonlocal_statement_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.nonlocal_statement_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Nonlocal(self, node: cst.Nonlocal) -> bool:\n",
      "    self.nonlocal_statement_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.nonlocal_statement_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ListComprehensionCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.list_comprehension_count = 0\n",
      "\n",
      "    def visit_ListComp(self, node: cst.ListComp) -> bool:\n",
      "        self.list_comprehension_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.list_comprehension_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.list_comprehension_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ListComp(self, node: cst.ListComp) -> bool:\n",
      "    self.list_comprehension_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.list_comprehension_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class DictComprehensionCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.dict_comprehension_count = 0\n",
      "\n",
      "    def visit_DictComp(self, node: cst.DictComp) -> bool:\n",
      "        self.dict_comprehension_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.dict_comprehension_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.dict_comprehension_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_DictComp(self, node: cst.DictComp) -> bool:\n",
      "    self.dict_comprehension_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.dict_comprehension_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class SetComprehensionCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.set_comprehension_count = 0\n",
      "\n",
      "    def visit_SetComp(self, node: cst.SetComp) -> bool:\n",
      "        self.set_comprehension_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.set_comprehension_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.set_comprehension_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_SetComp(self, node: cst.SetComp) -> bool:\n",
      "    self.set_comprehension_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.set_comprehension_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class GeneratorExpressionCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.generator_expression_count = 0\n",
      "\n",
      "    def visit_GeneratorExp(self, node: cst.GeneratorExp) -> bool:\n",
      "        self.generator_expression_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.generator_expression_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.generator_expression_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_GeneratorExp(self, node: cst.GeneratorExp) -> bool:\n",
      "    self.generator_expression_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.generator_expression_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class YieldCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.yield_count = 0\n",
      "\n",
      "    def visit_Yield(self, node: cst.Yield) -> bool:\n",
      "        self.yield_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.yield_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.yield_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Yield(self, node: cst.Yield) -> bool:\n",
      "    self.yield_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.yield_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class AwaitCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.await_count = 0\n",
      "\n",
      "    def visit_Await(self, node: cst.Await) -> bool:\n",
      "        self.await_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.await_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.await_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Await(self, node: cst.Await) -> bool:\n",
      "    self.await_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.await_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ReturnCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.return_count = 0\n",
      "\n",
      "    def visit_Return(self, node: cst.Return) -> bool:\n",
      "        self.return_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.return_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.return_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Return(self, node: cst.Return) -> bool:\n",
      "    self.return_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.return_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BreakCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.break_count = 0\n",
      "\n",
      "    def visit_Break(self, node: cst.Break) -> bool:\n",
      "        self.break_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.break_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.break_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Break(self, node: cst.Break) -> bool:\n",
      "    self.break_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.break_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ContinueCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.continue_count = 0\n",
      "\n",
      "    def visit_Continue(self, node: cst.Continue) -> bool:\n",
      "        self.continue_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.continue_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.continue_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Continue(self, node: cst.Continue) -> bool:\n",
      "    self.continue_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.continue_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class RaiseCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.raise_count = 0\n",
      "\n",
      "    def visit_Raise(self, node: cst.Raise) -> bool:\n",
      "        self.raise_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.raise_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.raise_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Raise(self, node: cst.Raise) -> bool:\n",
      "    self.raise_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.raise_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class AssertCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.assert_count = 0\n",
      "\n",
      "    def visit_Assert(self, node: cst.Assert) -> bool:\n",
      "        self.assert_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.assert_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.assert_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Assert(self, node: cst.Assert) -> bool:\n",
      "    self.assert_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.assert_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class PassCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.pass_count = 0\n",
      "\n",
      "    def visit_Pass(self, node: cst.Pass) -> bool:\n",
      "        self.pass_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.pass_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.pass_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Pass(self, node: cst.Pass) -> bool:\n",
      "    self.pass_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.pass_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Unary Operators\n",
      "class UnaryOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.unary_operators = []\n",
      "\n",
      "    def visit_UnaryOperation(self, node: cst.UnaryOperation) -> bool:\n",
      "        if isinstance(node.operator, cst.BitInvert) or isinstance(node.operator, cst.Minus) or \\\n",
      "                isinstance(node.operator, cst.Not) or isinstance(node.operator, cst.Plus):\n",
      "            self.unary_operators.append(node.operator.__class__.__name__)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.unary_operators\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.unary_operators = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_UnaryOperation(self, node: cst.UnaryOperation) -> bool:\n",
      "    if isinstance(node.operator, cst.BitInvert) or isinstance(node.operator, cst.Minus) or \\\n",
      "                isinstance(node.operator, cst.Not) or isinstance(node.operator, cst.Plus):\n",
      "        self.unary_operators.append(node.operator.__class__.__name__)\n",
      "    return True\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.unary_operators\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Boolean Operators\n",
      "class BooleanOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.boolean_operators = []\n",
      "\n",
      "    def visit_BooleanOperation(self, node: cst.BooleanOperation) -> bool:\n",
      "        if isinstance(node.operator, cst.And) or isinstance(node.operator, cst.Or):\n",
      "            self.boolean_operators.append(node.operator.__class__.__name__)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.boolean_operators\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.boolean_operators = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BooleanOperation(self, node: cst.BooleanOperation) -> bool:\n",
      "    if isinstance(node.operator, cst.And) or isinstance(node.operator, cst.Or):\n",
      "        self.boolean_operators.append(node.operator.__class__.__name__)\n",
      "    return True\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.boolean_operators\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Binary Operators\n",
      "class BinaryOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.binary_operators = []\n",
      "\n",
      "    def visit_BinaryOperation(self, node: cst.BinaryOperation) -> bool:\n",
      "        if isinstance(node.operator, cst.Add) or isinstance(node.operator, cst.BitAnd) or \\\n",
      "                isinstance(node.operator, cst.BitOr) or isinstance(node.operator, cst.BitXor) or \\\n",
      "                isinstance(node.operator, cst.Divide) or isinstance(node.operator, cst.FloorDivide) or \\\n",
      "                isinstance(node.operator, cst.LeftShift) or isinstance(node.operator, cst.MatrixMultiply) or \\\n",
      "                isinstance(node.operator, cst.Modulo) or isinstance(node.operator, cst.Multiply) or \\\n",
      "                isinstance(node.operator, cst.Power) or isinstance(node.operator, cst.RightShift) or \\\n",
      "                isinstance(node.operator, cst.Subtract):\n",
      "            self.binary_operators.append(node.operator.__class__.__name__)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.binary_operators\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.binary_operators = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BinaryOperation(self, node: cst.BinaryOperation) -> bool:\n",
      "    if isinstance(node.operator, cst.Add) or isinstance(node.operator, cst.BitAnd) or \\\n",
      "                isinstance(node.operator, cst.BitOr) or isinstance(node.operator, cst.BitXor) or \\\n",
      "                isinstance(node.operator, cst.Divide) or isinstance(node.operator, cst.FloorDivide) or \\\n",
      "                isinstance(node.operator, cst.LeftShift) or isinstance(node.operator, cst.MatrixMultiply) or \\\n",
      "                isinstance(node.operator, cst.Modulo) or isinstance(node.operator, cst.Multiply) or \\\n",
      "                isinstance(node.operator, cst.Power) or isinstance(node.operator, cst.RightShift) or \\\n",
      "                isinstance(node.operator, cst.Subtract):\n",
      "        self.binary_operators.append(node.operator.__class__.__name__)\n",
      "    return True\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.binary_operators\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Comparison Operators\n",
      "class ComparisonOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.comparison_operators = []\n",
      "\n",
      "    def visit_Comparison(self, node: cst.Comparison) -> bool:\n",
      "        for operator in node.operators:\n",
      "            if isinstance(operator, cst.Equal) or isinstance(operator, cst.GreaterThan) or \\\n",
      "                    isinstance(operator, cst.GreaterThanEqual) or isinstance(operator, cst.In) or \\\n",
      "                    isinstance(operator, cst.Is) or isinstance(operator, cst.LessThan) or \\\n",
      "                    isinstance(operator, cst.LessThanEqual) or isinstance(operator, cst.NotEqual) or \\\n",
      "                    isinstance(operator, cst.IsNot) or isinstance(operator, cst.NotIn):\n",
      "                self.comparison_operators.append(operator.__class__.__name__)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.comparison_operators\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.comparison_operators = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Comparison(self, node: cst.Comparison) -> bool:\n",
      "    for operator in node.operators:\n",
      "        if isinstance(operator, cst.Equal) or isinstance(operator, cst.GreaterThan) or \\\n",
      "                    isinstance(operator, cst.GreaterThanEqual) or isinstance(operator, cst.In) or \\\n",
      "                    isinstance(operator, cst.Is) or isinstance(operator, cst.LessThan) or \\\n",
      "                    isinstance(operator, cst.LessThanEqual) or isinstance(operator, cst.NotEqual) or \\\n",
      "                    isinstance(operator, cst.IsNot) or isinstance(operator, cst.NotIn):\n",
      "            self.comparison_operators.append(operator.__class__.__name__)\n",
      "    return True\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.comparison_operators\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Augmented Assignment Operators\n",
      "class AugmentedAssignmentOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.augmented_assignment_operators = []\n",
      "\n",
      "    def visit_AugAssign(self, node: cst.AugAssign) -> bool:\n",
      "        if isinstance(node.operator, cst.AddAssign) or isinstance(node.operator, cst.BitAndAssign) or \\\n",
      "                isinstance(node.operator, cst.BitOrAssign) or isinstance(node.operator, cst.BitXorAssign) or \\\n",
      "                isinstance(node.operator, cst.DivideAssign) or isinstance(node.operator, cst.FloorDivideAssign) or \\\n",
      "                isinstance(node.operator, cst.LeftShiftAssign) or isinstance(node.operator, cst.MatrixMultiplyAssign) or \\\n",
      "                isinstance(node.operator, cst.ModuloAssign) or isinstance(node.operator, cst.MultiplyAssign) or \\\n",
      "                isinstance(node.operator, cst.PowerAssign) or isinstance(node.operator, cst.RightShiftAssign) or \\\n",
      "                isinstance(node.operator, cst.SubtractAssign):\n",
      "            self.augmented_assignment_operators.append(node.operator.__class__.__name__)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.augmented_assignment_operators\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.augmented_assignment_operators = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_AugAssign(self, node: cst.AugAssign) -> bool:\n",
      "    if isinstance(node.operator, cst.AddAssign) or isinstance(node.operator, cst.BitAndAssign) or \\\n",
      "                isinstance(node.operator, cst.BitOrAssign) or isinstance(node.operator, cst.BitXorAssign) or \\\n",
      "                isinstance(node.operator, cst.DivideAssign) or isinstance(node.operator, cst.FloorDivideAssign) or \\\n",
      "                isinstance(node.operator, cst.LeftShiftAssign) or isinstance(node.operator, cst.MatrixMultiplyAssign) or \\\n",
      "                isinstance(node.operator, cst.ModuloAssign) or isinstance(node.operator, cst.MultiplyAssign) or \\\n",
      "                isinstance(node.operator, cst.PowerAssign) or isinstance(node.operator, cst.RightShiftAssign) or \\\n",
      "                isinstance(node.operator, cst.SubtractAssign):\n",
      "        self.augmented_assignment_operators.append(node.operator.__class__.__name__)\n",
      "    return True\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.augmented_assignment_operators\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Miscellaneous Operators\n",
      "class MiscellaneousOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.miscellaneous_operators = []\n",
      "\n",
      "    def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "        self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def visit_Colon(self, node: cst.Colon) -> bool:\n",
      "        self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def visit_Comma(self, node: cst.Comma) -> bool:\n",
      "        self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def visit_Dot(self, node: cst.Dot) -> bool:\n",
      "        self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n",
      "        self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n",
      "        self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.miscellaneous_operators\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.miscellaneous_operators = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "    self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Colon(self, node: cst.Colon) -> bool:\n",
      "    self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Comma(self, node: cst.Comma) -> bool:\n",
      "    self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Dot(self, node: cst.Dot) -> bool:\n",
      "    self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n",
      "    self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n",
      "    self.miscellaneous_operators.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.miscellaneous_operators\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# Unary Operators\n",
      "class BitInvertOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_invert_count = []\n",
      "\n",
      "    def visit_BitInvert(self, node: cst.BitInvert) -> bool:\n",
      "        self.bit_invert_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_invert_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_invert_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitInvert(self, node: cst.BitInvert) -> bool:\n",
      "    self.bit_invert_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_invert_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MinusOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.minus_count = []\n",
      "\n",
      "    def visit_Minus(self, node: cst.Minus) -> bool:\n",
      "        self.minus_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.minus_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.minus_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Minus(self, node: cst.Minus) -> bool:\n",
      "    self.minus_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.minus_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class NotOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.not_count = []\n",
      "\n",
      "    def visit_Not(self, node: cst.Not) -> bool:\n",
      "        self.not_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.not_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.not_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Not(self, node: cst.Not) -> bool:\n",
      "    self.not_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.not_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class PlusOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.plus_count = []\n",
      "\n",
      "    def visit_Plus(self, node: cst.Plus) -> bool:\n",
      "        self.plus_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.plus_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.plus_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Plus(self, node: cst.Plus) -> bool:\n",
      "    self.plus_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.plus_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# Boolean Operators\n",
      "class AndOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.and_count = []\n",
      "\n",
      "    def visit_And(self, node: cst.And) -> bool:\n",
      "        self.and_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.and_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.and_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_And(self, node: cst.And) -> bool:\n",
      "    self.and_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.and_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class OrOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.or_count = []\n",
      "\n",
      "    def visit_Or(self, node: cst.Or) -> bool:\n",
      "        self.or_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.or_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.or_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Or(self, node: cst.Or) -> bool:\n",
      "    self.or_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.or_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# Binary Operators\n",
      "class AddOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.add_count = []\n",
      "\n",
      "    def visit_Add(self, node: cst.Add) -> bool:\n",
      "        self.add_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.add_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.add_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Add(self, node: cst.Add) -> bool:\n",
      "    self.add_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.add_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BitAndOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_and_count = []\n",
      "\n",
      "    def visit_BitAnd(self, node: cst.BitAnd) -> bool:\n",
      "        self.bit_and_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_and_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_and_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitAnd(self, node: cst.BitAnd) -> bool:\n",
      "    self.bit_and_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_and_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BitOrOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_or_count = []\n",
      "\n",
      "    def visit_BitOr(self, node: cst.BitOr) -> bool:\n",
      "        self.bit_or_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_or_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_or_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitOr(self, node: cst.BitOr) -> bool:\n",
      "    self.bit_or_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_or_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BitXorOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_xor_count = []\n",
      "\n",
      "    def visit_BitXor(self, node: cst.BitXor) -> bool:\n",
      "        self.bit_xor_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_xor_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_xor_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitXor(self, node: cst.BitXor) -> bool:\n",
      "    self.bit_xor_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_xor_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class DivideOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.divide_count = []\n",
      "\n",
      "    def visit_Divide(self, node: cst.Divide) -> bool:\n",
      "        self.divide_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.divide_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.divide_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Divide(self, node: cst.Divide) -> bool:\n",
      "    self.divide_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.divide_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class FloorDivideOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.floor_divide_count = []\n",
      "\n",
      "    def visit_FloorDivide(self, node: cst.FloorDivide) -> bool:\n",
      "        self.floor_divide_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.floor_divide_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.floor_divide_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_FloorDivide(self, node: cst.FloorDivide) -> bool:\n",
      "    self.floor_divide_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.floor_divide_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class LeftShiftOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.left_shift_count = []\n",
      "\n",
      "    def visit_LeftShift(self, node: cst.LeftShift) -> bool:\n",
      "        self.left_shift_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.left_shift_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.left_shift_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_LeftShift(self, node: cst.LeftShift) -> bool:\n",
      "    self.left_shift_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.left_shift_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MatrixMultiplyOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.matrix_multiply_count = []\n",
      "\n",
      "    def visit_MatrixMultiply(self, node: cst.MatrixMultiply) -> bool:\n",
      "        self.matrix_multiply_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.matrix_multiply_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.matrix_multiply_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_MatrixMultiply(self, node: cst.MatrixMultiply) -> bool:\n",
      "    self.matrix_multiply_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.matrix_multiply_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ModuloOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.modulo_count = []\n",
      "\n",
      "    def visit_Modulo(self, node: cst.Modulo) -> bool:\n",
      "        self.modulo_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.modulo_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.modulo_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Modulo(self, node: cst.Modulo) -> bool:\n",
      "    self.modulo_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.modulo_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MultiplyOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.multiply_count = []\n",
      "\n",
      "    def visit_Multiply(self, node: cst.Multiply) -> bool:\n",
      "        self.multiply_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.multiply_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.multiply_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Multiply(self, node: cst.Multiply) -> bool:\n",
      "    self.multiply_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.multiply_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class PowerOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.power_count = []\n",
      "\n",
      "    def visit_Power(self, node: cst.Power) -> bool:\n",
      "        self.power_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.power_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.power_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Power(self, node: cst.Power) -> bool:\n",
      "    self.power_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.power_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class RightShiftOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.right_shift_count = []\n",
      "\n",
      "    def visit_RightShift(self, node: cst.RightShift) -> bool:\n",
      "        self.right_shift_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.right_shift_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.right_shift_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_RightShift(self, node: cst.RightShift) -> bool:\n",
      "    self.right_shift_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.right_shift_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class SubtractOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.subtract_count = []\n",
      "\n",
      "    def visit_Subtract(self, node: cst.Subtract) -> bool:\n",
      "        self.subtract_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.subtract_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.subtract_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Subtract(self, node: cst.Subtract) -> bool:\n",
      "    self.subtract_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.subtract_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# Comparison Operators\n",
      "class EqualOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.equal_count = []\n",
      "\n",
      "    def visit_Equal(self, node: cst.Equal) -> bool:\n",
      "        self.equal_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.equal_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.equal_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Equal(self, node: cst.Equal) -> bool:\n",
      "    self.equal_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.equal_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class GreaterThanOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.greater_than_count = []\n",
      "\n",
      "    def visit_GreaterThan(self, node: cst.GreaterThan) -> bool:\n",
      "        self.greater_than_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.greater_than_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.greater_than_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_GreaterThan(self, node: cst.GreaterThan) -> bool:\n",
      "    self.greater_than_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.greater_than_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# ... continue with other comparison operators\n",
      "\n",
      "\n",
      "# Augmented Assignment Operators\n",
      "class AddAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.add_assign_count = []\n",
      "\n",
      "    def visit_AddAssign(self, node: cst.AddAssign) -> bool:\n",
      "        self.add_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.add_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.add_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_AddAssign(self, node: cst.AddAssign) -> bool:\n",
      "    self.add_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.add_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BitAndAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_and_assign_count = []\n",
      "\n",
      "    def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n",
      "        self.bit_and_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_and_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_and_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n",
      "    self.bit_and_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_and_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "class BitAndAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_and_assign_count = []\n",
      "\n",
      "    def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n",
      "        self.bit_and_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_and_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_and_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n",
      "    self.bit_and_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_and_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BitOrAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_or_assign_count = []\n",
      "\n",
      "    def visit_BitOrAssign(self, node: cst.BitOrAssign) -> bool:\n",
      "        self.bit_or_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_or_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_or_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitOrAssign(self, node: cst.BitOrAssign) -> bool:\n",
      "    self.bit_or_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_or_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BitXorAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_xor_assign_count = []\n",
      "\n",
      "    def visit_BitXorAssign(self, node: cst.BitXorAssign) -> bool:\n",
      "        self.bit_xor_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_xor_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_xor_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitXorAssign(self, node: cst.BitXorAssign) -> bool:\n",
      "    self.bit_xor_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_xor_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class DivideAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.divide_assign_count = []\n",
      "\n",
      "    def visit_DivideAssign(self, node: cst.DivideAssign) -> bool:\n",
      "        self.divide_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.divide_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.divide_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_DivideAssign(self, node: cst.DivideAssign) -> bool:\n",
      "    self.divide_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.divide_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class FloorDivideAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.floor_divide_assign_count = []\n",
      "\n",
      "    def visit_FloorDivideAssign(self, node: cst.FloorDivideAssign) -> bool:\n",
      "        self.floor_divide_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.floor_divide_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.floor_divide_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_FloorDivideAssign(self, node: cst.FloorDivideAssign) -> bool:\n",
      "    self.floor_divide_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.floor_divide_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class LeftShiftAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.left_shift_assign_count = []\n",
      "\n",
      "    def visit_LeftShiftAssign(self, node: cst.LeftShiftAssign) -> bool:\n",
      "        self.left_shift_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.left_shift_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.left_shift_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_LeftShiftAssign(self, node: cst.LeftShiftAssign) -> bool:\n",
      "    self.left_shift_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.left_shift_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MatrixMultiplyAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.matrix_multiply_assign_count = []\n",
      "\n",
      "    def visit_MatrixMultiplyAssign(self, node: cst.MatrixMultiplyAssign) -> bool:\n",
      "        self.matrix_multiply_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.matrix_multiply_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.matrix_multiply_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_MatrixMultiplyAssign(self, node: cst.MatrixMultiplyAssign) -> bool:\n",
      "    self.matrix_multiply_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.matrix_multiply_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ModuloAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.modulo_assign_count = []\n",
      "\n",
      "    def visit_ModuloAssign(self, node: cst.ModuloAssign) -> bool:\n",
      "        self.modulo_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.modulo_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.modulo_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ModuloAssign(self, node: cst.ModuloAssign) -> bool:\n",
      "    self.modulo_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.modulo_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MultiplyAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.multiply_assign_count = []\n",
      "\n",
      "    def visit_MultiplyAssign(self, node: cst.MultiplyAssign) -> bool:\n",
      "        self.multiply_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.multiply_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.multiply_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_MultiplyAssign(self, node: cst.MultiplyAssign) -> bool:\n",
      "    self.multiply_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.multiply_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class PowerAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.power_assign_count = []\n",
      "\n",
      "    def visit_PowerAssign(self, node: cst.PowerAssign) -> bool:\n",
      "        self.power_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.power_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.power_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_PowerAssign(self, node: cst.PowerAssign) -> bool:\n",
      "    self.power_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.power_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class RightShiftAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.right_shift_assign_count = []\n",
      "\n",
      "    def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n",
      "        self.right_shift_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.right_shift_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.right_shift_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n",
      "    self.right_shift_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.right_shift_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class RightShiftAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.right_shift_assign_count = []\n",
      "\n",
      "    def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n",
      "        self.right_shift_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.right_shift_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.right_shift_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n",
      "    self.right_shift_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.right_shift_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class SubtractAssignOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.subtract_assign_count = []\n",
      "\n",
      "    def visit_SubtractAssign(self, node: cst.SubtractAssign) -> bool:\n",
      "        self.subtract_assign_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.subtract_assign_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.subtract_assign_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_SubtractAssign(self, node: cst.SubtractAssign) -> bool:\n",
      "    self.subtract_assign_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.subtract_assign_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Miscellaneous Operators\n",
      "class AssignEqualOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.assign_equal_count = []\n",
      "\n",
      "    def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "        self.assign_equal_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.assign_equal_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.assign_equal_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "    self.assign_equal_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.assign_equal_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ColonOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.colon_count = []\n",
      "\n",
      "    def visit_Colon(self, node: cst.Colon) -> bool:\n",
      "        self.colon_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.colon_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.colon_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Colon(self, node: cst.Colon) -> bool:\n",
      "    self.colon_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.colon_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class CommaOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.comma_count = []\n",
      "\n",
      "    def visit_Comma(self, node: cst.Comma) -> bool:\n",
      "        self.comma_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.comma_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.comma_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Comma(self, node: cst.Comma) -> bool:\n",
      "    self.comma_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.comma_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class DotOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.dot_count = []\n",
      "\n",
      "    def visit_Dot(self, node: cst.Dot) -> bool:\n",
      "        self.dot_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.dot_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.dot_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Dot(self, node: cst.Dot) -> bool:\n",
      "    self.dot_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.dot_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ImportStarOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.import_star_count = []\n",
      "\n",
      "    def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n",
      "        self.import_star_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.import_star_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.import_star_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n",
      "    self.import_star_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.import_star_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class SemicolonOperatorCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.semicolon_count = []\n",
      "\n",
      "    def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n",
      "        self.semicolon_count.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.semicolon_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.semicolon_count = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n",
      "    self.semicolon_count.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.semicolon_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class DocstringCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.docstrings = []\n",
      "\n",
      "    def visit_Module(self, node: cst.Module) -> bool:\n",
      "        if node.body and isinstance(node.body[0].body, cst.SimpleStatementLine):\n",
      "            for stmt in node.body[0].body.body:\n",
      "                if isinstance(stmt, cst.Expr) and isinstance(stmt.value, cst.SimpleString):\n",
      "                    self.docstrings.append(stmt.value.value)\n",
      "        return True\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n",
      "        docstring = node.get_docstring()\n",
      "        if docstring is not None:\n",
      "            self.docstrings.append(docstring)\n",
      "        return True\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n",
      "        docstring = node.get_docstring()\n",
      "        if docstring is not None:\n",
      "            self.docstrings.append(docstring)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.docstrings\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.docstrings = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Module(self, node: cst.Module) -> bool:\n",
      "    if node.body and isinstance(node.body[0].body, cst.SimpleStatementLine):\n",
      "        for stmt in node.body[0].body.body:\n",
      "            if isinstance(stmt, cst.Expr) and isinstance(stmt.value, cst.SimpleString):\n",
      "                self.docstrings.append(stmt.value.value)\n",
      "    return True\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n",
      "    docstring = node.get_docstring()\n",
      "    if docstring is not None:\n",
      "        self.docstrings.append(docstring)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n",
      "    docstring = node.get_docstring()\n",
      "    if docstring is not None:\n",
      "        self.docstrings.append(docstring)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.docstrings\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class FunctionCallCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.function_calls = []\n",
      "\n",
      "    def visit_Call(self, node: cst.Call) -> bool:\n",
      "        if isinstance(node.func, cst.Name):\n",
      "            # Add the function name to the list\n",
      "            self.function_calls.append(node.func.value)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.function_calls\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.function_calls = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Call(self, node: cst.Call) -> bool:\n",
      "    if isinstance(node.func, cst.Name):\n",
      "        # Add the function name to the list\n",
      "        self.function_calls.append(node.func.value)\n",
      "    return True\n",
      "\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.function_calls\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ArgumentTypeCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.argument_types = []\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n",
      "        # Collect argument types for functions\n",
      "        arg_types = []\n",
      "        for param in node.params.params:\n",
      "            if isinstance(param.annotation, cst.Annotation):\n",
      "                arg_types.append(param.annotation.annotation.value)\n",
      "            else:\n",
      "                arg_types.append(None)\n",
      "\n",
      "        self.argument_types.append(arg_types)\n",
      "        return True\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n",
      "        # Collect argument types for methods\n",
      "        for stmt in node.body.body:\n",
      "            if isinstance(stmt, cst.FunctionDef):\n",
      "                arg_types = []\n",
      "                for param in stmt.params.params:\n",
      "                    if isinstance(param.annotation, cst.Annotation):\n",
      "                        arg_types.append(param.annotation.annotation.value)\n",
      "                    else:\n",
      "                        arg_types.append(None)\n",
      "\n",
      "                self.argument_types.append(arg_types)\n",
      "\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.argument_types\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.argument_types = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> bool:\n",
      "    # Collect argument types for functions\n",
      "    arg_types = []\n",
      "    for param in node.params.params:\n",
      "        if isinstance(param.annotation, cst.Annotation):\n",
      "            arg_types.append(param.annotation.annotation.value)\n",
      "        else:\n",
      "            arg_types.append(None)\n",
      "\n",
      "    self.argument_types.append(arg_types)\n",
      "    return True\n",
      "\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef) -> bool:\n",
      "    # Collect argument types for methods\n",
      "    for stmt in node.body.body:\n",
      "        if isinstance(stmt, cst.FunctionDef):\n",
      "            arg_types = []\n",
      "            for param in stmt.params.params:\n",
      "                if isinstance(param.annotation, cst.Annotation):\n",
      "                    arg_types.append(param.annotation.annotation.value)\n",
      "                else:\n",
      "                    arg_types.append(None)\n",
      "\n",
      "            self.argument_types.append(arg_types)\n",
      "\n",
      "    return True\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.argument_types\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ImportCollector(cst.CSTVisitor):\n",
      "    def __init__(self, filename: str):\n",
      "        with open(filename, \"r\") as file:\n",
      "            self.module = cst.parse_module(file.read())\n",
      "        self.imports = []\n",
      "\n",
      "    def visit_Import(self, node: cst.Import) -> bool:\n",
      "        for name in node.names:\n",
      "            self.imports.append(cst.Module([node]))\n",
      "        return True\n",
      "\n",
      "    def visit_ImportFrom(self, node: cst.ImportFrom) -> bool:\n",
      "        module = node.module.value if node.module else \"\"\n",
      "        for name in node.names:\n",
      "            self.imports.append(cst.Module([node]))\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        import_code = [import_statement.code for import_statement in self.imports]\n",
      "        return import_code\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "Function call: open\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, filename: str):\n",
      "    with open(filename, \"r\") as file:\n",
      "        self.module = cst.parse_module(file.read())\n",
      "    self.imports = []\n",
      "\n",
      "Function call: open\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Import(self, node: cst.Import) -> bool:\n",
      "    for name in node.names:\n",
      "        self.imports.append(cst.Module([node]))\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ImportFrom(self, node: cst.ImportFrom) -> bool:\n",
      "    module = node.module.value if node.module else \"\"\n",
      "    for name in node.names:\n",
      "        self.imports.append(cst.Module([node]))\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    import_code = [import_statement.code for import_statement in self.imports]\n",
      "    return import_code\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class IfStatementCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.if_statements = []\n",
      "\n",
      "    def visit_If(self, node: cst.If) -> bool:\n",
      "        self.if_statements.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.if_statements\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.if_statements = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_If(self, node: cst.If) -> bool:\n",
      "    self.if_statements.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.if_statements\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BaseCompoundStatementCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.compound_statements = []\n",
      "\n",
      "    def visit_BaseCompoundStatement(self, node: cst.BaseCompoundStatement) -> bool:\n",
      "        self.compound_statements.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.compound_statements\n",
      "    \n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.compound_statements = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BaseCompoundStatement(self, node: cst.BaseCompoundStatement) -> bool:\n",
      "    self.compound_statements.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.compound_statements\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "class ForLoopCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.for_loops = []\n",
      "\n",
      "    def visit_For(self, node: cst.For) -> bool:\n",
      "        self.for_loops.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.for_loops\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.for_loops = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_For(self, node: cst.For) -> bool:\n",
      "    self.for_loops.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.for_loops\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class WhileLoopCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.while_loops = []\n",
      "\n",
      "    def visit_While(self, node: cst.While) -> bool:\n",
      "        self.while_loops.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.while_loops\n",
      "    \n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.while_loops = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_While(self, node: cst.While) -> bool:\n",
      "    self.while_loops.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.while_loops\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "class TryExceptCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.try_excepts = []\n",
      "\n",
      "    def visit_Try(self, node: cst.Try) -> bool:\n",
      "        self.try_excepts.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.try_excepts\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.try_excepts = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Try(self, node: cst.Try) -> bool:\n",
      "    self.try_excepts.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.try_excepts\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class WithCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.withs = []\n",
      "\n",
      "    def visit_With(self, node: cst.With) -> bool:\n",
      "        self.withs.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.withs\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.withs = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_With(self, node: cst.With) -> bool:\n",
      "    self.withs.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.withs\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class VariableDeclarationCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.variable_declarations = []\n",
      "\n",
      "    def visit_Assign(self, node: cst.Assign) -> bool:\n",
      "        self.variable_declarations.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.variable_declarations\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.variable_declarations = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Assign(self, node: cst.Assign) -> bool:\n",
      "    self.variable_declarations.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.variable_declarations\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ListComprehensionCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.list_comprehensions = []\n",
      "\n",
      "    def visit_ListComp(self, node: cst.ListComp) -> bool:\n",
      "        self.list_comprehensions.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.list_comprehensions\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.list_comprehensions = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ListComp(self, node: cst.ListComp) -> bool:\n",
      "    self.list_comprehensions.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.list_comprehensions\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class DictComprehensionCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.dict_comprehensions = []\n",
      "\n",
      "    def visit_DictComp(self, node: cst.DictComp) -> bool:\n",
      "        self.dict_comprehensions.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.dict_comprehensions\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.dict_comprehensions = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_DictComp(self, node: cst.DictComp) -> bool:\n",
      "    self.dict_comprehensions.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.dict_comprehensions\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# Set Comprehension Collector\n",
      "class SetComprehensionCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.set_comprehensions = []\n",
      "\n",
      "    def visit_SetComp(self, node: cst.SetComp) -> bool:\n",
      "        self.set_comprehensions.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.set_comprehensions\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.set_comprehensions = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_SetComp(self, node: cst.SetComp) -> bool:\n",
      "    self.set_comprehensions.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.set_comprehensions\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Generator Expression Collector\n",
      "class GeneratorExpressionCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.generator_expressions = []\n",
      "\n",
      "    def visit_GeneratorExp(self, node: cst.GeneratorExp) -> bool:\n",
      "        self.generator_expressions.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.generator_expressions\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.generator_expressions = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_GeneratorExp(self, node: cst.GeneratorExp) -> bool:\n",
      "    self.generator_expressions.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.generator_expressions\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Yield Statement Collector\n",
      "class YieldCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.yields = []\n",
      "\n",
      "    def visit_Yield(self, node: cst.Yield) -> bool:\n",
      "        self.yields.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.yields\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.yields = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Yield(self, node: cst.Yield) -> bool:\n",
      "    self.yields.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.yields\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# Return Statement Collector\n",
      "class ReturnCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.returns = []\n",
      "\n",
      "    def visit_Return(self, node: cst.Return) -> bool:\n",
      "        self.returns.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.returns\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.returns = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Return(self, node: cst.Return) -> bool:\n",
      "    self.returns.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.returns\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Raise Statement Collector\n",
      "class RaiseCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.raises = []\n",
      "\n",
      "    def visit_Raise(self, node: cst.Raise) -> bool:\n",
      "        self.raises.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.raises\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.raises = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Raise(self, node: cst.Raise) -> bool:\n",
      "    self.raises.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.raises\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Assert Statement Collector\n",
      "class AssertCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.asserts = []\n",
      "\n",
      "    def visit_Assert(self, node: cst.Assert) -> bool:\n",
      "        self.asserts.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.asserts\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.asserts = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Assert(self, node: cst.Assert) -> bool:\n",
      "    self.asserts.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.asserts\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Break Statement Collector\n",
      "class BreakCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.breaks = []\n",
      "\n",
      "    def visit_Break(self, node: cst.Break) -> bool:\n",
      "        self.breaks.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.breaks\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.breaks = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Break(self, node: cst.Break) -> bool:\n",
      "    self.breaks.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.breaks\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Continue Statement Collector\n",
      "class ContinueCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.continues = []\n",
      "\n",
      "    def visit_Continue(self, node: cst.Continue) -> bool:\n",
      "        self.continues.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.continues\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.continues = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Continue(self, node: cst.Continue) -> bool:\n",
      "    self.continues.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.continues\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Pass Statement Collector\n",
      "class PassCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.passes = []\n",
      "\n",
      "    def visit_Pass(self, node: cst.Pass) -> bool:\n",
      "        self.passes.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.passes\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.passes = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Pass(self, node: cst.Pass) -> bool:\n",
      "    self.passes.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.passes\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "# With Statement Collector\n",
      "class WithStatementCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.with_statements = []\n",
      "\n",
      "    def visit_With(self, node: cst.With) -> bool:\n",
      "        self.with_statements.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.with_statements\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.with_statements = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_With(self, node: cst.With) -> bool:\n",
      "    self.with_statements.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.with_statements\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Try Statement Collector\n",
      "class TryStatementCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.try_statements = []\n",
      "\n",
      "    def visit_Try(self, node: cst.Try) -> bool:\n",
      "        self.try_statements.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.try_statements\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.try_statements = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Try(self, node: cst.Try) -> bool:\n",
      "    self.try_statements.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.try_statements\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Except Clause Collector\n",
      "class ExceptClauseCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.except_clauses = []\n",
      "\n",
      "    def visit_ExceptHandler(self, node: cst.ExceptHandler) -> bool:\n",
      "        self.except_clauses.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.except_clauses\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.except_clauses = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ExceptHandler(self, node: cst.ExceptHandler) -> bool:\n",
      "    self.except_clauses.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.except_clauses\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Lambda Function Collector\n",
      "class LambdaFunctionCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.lambda_functions = []\n",
      "\n",
      "    def visit_Lambda(self, node: cst.Lambda) -> bool:\n",
      "        self.lambda_functions.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.lambda_functions\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.lambda_functions = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Lambda(self, node: cst.Lambda) -> bool:\n",
      "    self.lambda_functions.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.lambda_functions\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Global Statement Collector\n",
      "class GlobalStatementCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.global_statements = []\n",
      "\n",
      "    def visit_Global(self, node: cst.Global) -> bool:\n",
      "        self.global_statements.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.global_statements\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.global_statements = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Global(self, node: cst.Global) -> bool:\n",
      "    self.global_statements.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.global_statements\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Nonlocal Statement Collector\n",
      "class NonlocalStatementCollector(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.nonlocal_statements = []\n",
      "\n",
      "    def visit_Nonlocal(self, node: cst.Nonlocal) -> bool:\n",
      "        self.nonlocal_statements.append(cst.Module([node]).code)\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.nonlocal_statements\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.nonlocal_statements = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Nonlocal(self, node: cst.Nonlocal) -> bool:\n",
      "    self.nonlocal_statements.append(cst.Module([node]).code)\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.nonlocal_statements\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class PassInserter(cst.CSTTransformer):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.inside_class = False\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef):\n",
      "        self.inside_class = True\n",
      "        return super().visit_ClassDef(node)\n",
      "\n",
      "    def leave_ClassDef(self, original_node: cst.ClassDef, updated_node: cst.ClassDef) -> cst.ClassDef:\n",
      "        self.inside_class = False\n",
      "        return updated_node\n",
      "\n",
      "    def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\n",
      "        body = updated_node.body\n",
      "        body_elements = body.body\n",
      "\n",
      "        # Check if the first element is a docstring\n",
      "\n",
      "        #if body_elements and matchers.matches(body_elements[0], matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\n",
      "            #docstring = body_elements[0]\n",
      "        docstrings = []\n",
      "        for element in body_elements:\n",
      "            if matchers.matches(element, matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\n",
      "                docstrings.append(element)\n",
      "\n",
      "        # Prepare new body\n",
      "        new_body = [cst.SimpleStatementLine(body=(cst.Pass(),))]\n",
      "        if docstrings[0] is not None:\n",
      "            new_body.insert(0, docstrings[0])\n",
      "\n",
      "        return updated_node.with_changes(\n",
      "            body=cst.IndentedBlock(body=tuple(new_body))\n",
      "        )\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTTransformer\"]\n",
      "]\n",
      "Function call: super\n",
      "Function call: super\n",
      "Function call: tuple\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self):\n",
      "    super().__init__()\n",
      "    self.inside_class = False\n",
      "\n",
      "Function call: super\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef):\n",
      "    self.inside_class = True\n",
      "    return super().visit_ClassDef(node)\n",
      "\n",
      "Function call: super\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def leave_ClassDef(self, original_node: cst.ClassDef, updated_node: cst.ClassDef) -> cst.ClassDef:\n",
      "    self.inside_class = False\n",
      "    return updated_node\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\n",
      "    body = updated_node.body\n",
      "    body_elements = body.body\n",
      "\n",
      "    # Check if the first element is a docstring\n",
      "\n",
      "    #if body_elements and matchers.matches(body_elements[0], matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\n",
      "        #docstring = body_elements[0]\n",
      "    docstrings = []\n",
      "    for element in body_elements:\n",
      "        if matchers.matches(element, matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\n",
      "            docstrings.append(element)\n",
      "\n",
      "    # Prepare new body\n",
      "    new_body = [cst.SimpleStatementLine(body=(cst.Pass(),))]\n",
      "    if docstrings[0] is not None:\n",
      "        new_body.insert(0, docstrings[0])\n",
      "\n",
      "    return updated_node.with_changes(\n",
      "        body=cst.IndentedBlock(body=tuple(new_body))\n",
      "    )\n",
      "\n",
      "Function call: tuple\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def generate_skeleton(code: str) -> str:\n",
      "    \"\"\"\n",
      "    Generate a skeleton for the given code, replacing function and class bodies with a pass statement.\n",
      "    \"\"\"\n",
      "    # Parse the code into a CST\n",
      "    module = cst.parse_module(code)\n",
      "\n",
      "    # Apply the transformer\n",
      "    transformer = PassInserter()\n",
      "    transformed_module = module.visit(transformer)\n",
      "\n",
      "    # Convert the transformed CST back to code\n",
      "    skeleton_code = transformed_module.code\n",
      "\n",
      "    return skeleton_code\n",
      "\n",
      "Function call: PassInserter\n",
      "Related codes: ['\\nclass PassInserter(cst.CSTTransformer):\\n    def __init__(self):\\n        super().__init__()\\n        self.inside_class = False\\n\\n    def visit_ClassDef(self, node: cst.ClassDef):\\n        self.inside_class = True\\n        return super().visit_ClassDef(node)\\n\\n    def leave_ClassDef(self, original_node: cst.ClassDef, updated_node: cst.ClassDef) -> cst.ClassDef:\\n        self.inside_class = False\\n        return updated_node\\n\\n    def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\\n        body = updated_node.body\\n        body_elements = body.body\\n\\n        # Check if the first element is a docstring\\n\\n        #if body_elements and matchers.matches(body_elements[0], matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\\n            #docstring = body_elements[0]\\n        docstrings = []\\n        for element in body_elements:\\n            if matchers.matches(element, matchers.Expr(matchers.SimpleString() | matchers.ConcatenatedString())):\\n                docstrings.append(element)\\n\\n        # Prepare new body\\n        new_body = [cst.SimpleStatementLine(body=(cst.Pass(),))]\\n        if docstrings[0] is not None:\\n            new_body.insert(0, docstrings[0])\\n\\n        return updated_node.with_changes(\\n            body=cst.IndentedBlock(body=tuple(new_body))\\n        )\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class CodeReplacerVisitor(cst.CSTTransformer):\n",
      "    def __init__(self, filename_column: str, original_code_column: str, replacing_code_column: str):\n",
      "        self.filename_column = filename_column\n",
      "        self.original_code_column = original_code_column\n",
      "        self.replacing_code_column = replacing_code_column\n",
      "\n",
      "    def visit_Module(self, node: cst.Module) -> cst.Module:\n",
      "        # Get the filename from the node metadata\n",
      "        filename = node.metadata.get(self.filename_column)\n",
      "        if filename is None:\n",
      "            return node\n",
      "\n",
      "        # Load the content of the file\n",
      "        with open(filename, \"r\") as f:\n",
      "            file_content = f.read()\n",
      "\n",
      "        # Get the original code and replacing code from the node metadata\n",
      "        original_code = node.metadata.get(self.original_code_column)\n",
      "        replacing_code = node.metadata.get(self.replacing_code_column)\n",
      "\n",
      "        if original_code is None or replacing_code is None:\n",
      "            return node\n",
      "\n",
      "        # Replace the original code with the replacing code in the file content\n",
      "        modified_content = file_content.replace(original_code, replacing_code)\n",
      "\n",
      "        # Save the modified content back to the file\n",
      "        with open(filename, \"w\") as f:\n",
      "            f.write(modified_content)\n",
      "\n",
      "        # Parse the modified content to update the node\n",
      "        return cst.parse_module(modified_content)\n",
      "\n",
      "    def leave_Module(self, original_node: cst.Module, updated_node: cst.Module) -> cst.Module:\n",
      "        # Copy the metadata from the original node to the updated node\n",
      "        updated_node.metadata = original_node.metadata\n",
      "        return updated_node\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTTransformer\"]\n",
      "]\n",
      "Function call: open\n",
      "Function call: open\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, filename_column: str, original_code_column: str, replacing_code_column: str):\n",
      "    self.filename_column = filename_column\n",
      "    self.original_code_column = original_code_column\n",
      "    self.replacing_code_column = replacing_code_column\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Module(self, node: cst.Module) -> cst.Module:\n",
      "    # Get the filename from the node metadata\n",
      "    filename = node.metadata.get(self.filename_column)\n",
      "    if filename is None:\n",
      "        return node\n",
      "\n",
      "    # Load the content of the file\n",
      "    with open(filename, \"r\") as f:\n",
      "        file_content = f.read()\n",
      "\n",
      "    # Get the original code and replacing code from the node metadata\n",
      "    original_code = node.metadata.get(self.original_code_column)\n",
      "    replacing_code = node.metadata.get(self.replacing_code_column)\n",
      "\n",
      "    if original_code is None or replacing_code is None:\n",
      "        return node\n",
      "\n",
      "    # Replace the original code with the replacing code in the file content\n",
      "    modified_content = file_content.replace(original_code, replacing_code)\n",
      "\n",
      "    # Save the modified content back to the file\n",
      "    with open(filename, \"w\") as f:\n",
      "        f.write(modified_content)\n",
      "\n",
      "    # Parse the modified content to update the node\n",
      "    return cst.parse_module(modified_content)\n",
      "\n",
      "Function call: open\n",
      "Function call: open\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def leave_Module(self, original_node: cst.Module, updated_node: cst.Module) -> cst.Module:\n",
      "    # Copy the metadata from the original node to the updated node\n",
      "    updated_node.metadata = original_node.metadata\n",
      "    return updated_node\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# Unary Operators\n",
      "class BitInvertOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_invert_operator_count = 0\n",
      "\n",
      "    def visit_BitInvert(self, node: cst.BitInvert) -> bool:\n",
      "        self.bit_invert_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_invert_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_invert_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitInvert(self, node: cst.BitInvert) -> bool:\n",
      "    self.bit_invert_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_invert_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MinusOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.minus_operator_count = 0\n",
      "\n",
      "    def visit_Minus(self, node: cst.Minus) -> bool:\n",
      "        self.minus_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.minus_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.minus_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Minus(self, node: cst.Minus) -> bool:\n",
      "    self.minus_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.minus_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class NotOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.not_operator_count = 0\n",
      "\n",
      "    def visit_Not(self, node: cst.Not) -> bool:\n",
      "        self.not_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.not_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.not_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Not(self, node: cst.Not) -> bool:\n",
      "    self.not_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.not_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class PlusOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.plus_operator_count = 0\n",
      "\n",
      "    def visit_Plus(self, node: cst.Plus) -> bool:\n",
      "        self.plus_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.plus_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.plus_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Plus(self, node: cst.Plus) -> bool:\n",
      "    self.plus_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.plus_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Boolean Operators\n",
      "class AndOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.and_operator_count = 0\n",
      "\n",
      "    def visit_And(self, node: cst.And) -> bool:\n",
      "        self.and_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.and_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.and_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_And(self, node: cst.And) -> bool:\n",
      "    self.and_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.and_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class OrOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.or_operator_count = 0\n",
      "\n",
      "    def visit_Or(self, node: cst.Or) -> bool:\n",
      "        self.or_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.or_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.or_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Or(self, node: cst.Or) -> bool:\n",
      "    self.or_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.or_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Binary Operators\n",
      "class AddOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.add_operator_count = 0\n",
      "\n",
      "    def visit_Add(self, node: cst.Add) -> bool:\n",
      "        self.add_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.add_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.add_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Add(self, node: cst.Add) -> bool:\n",
      "    self.add_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.add_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Binary Operators\n",
      "class BitAndOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_and_operator_count = 0\n",
      "\n",
      "    def visit_BitAnd(self, node: cst.BitAnd) -> bool:\n",
      "        self.bit_and_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_and_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_and_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitAnd(self, node: cst.BitAnd) -> bool:\n",
      "    self.bit_and_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_and_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BitOrOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_or_operator_count = 0\n",
      "\n",
      "    def visit_BitOr(self, node: cst.BitOr) -> bool:\n",
      "        self.bit_or_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_or_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_or_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitOr(self, node: cst.BitOr) -> bool:\n",
      "    self.bit_or_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_or_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BitXorOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_xor_operator_count = 0\n",
      "\n",
      "    def visit_BitXor(self, node: cst.BitXor) -> bool:\n",
      "        self.bit_xor_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_xor_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_xor_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitXor(self, node: cst.BitXor) -> bool:\n",
      "    self.bit_xor_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_xor_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class DivideOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.divide_operator_count = 0\n",
      "\n",
      "    def visit_Divide(self, node: cst.Divide) -> bool:\n",
      "        self.divide_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.divide_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.divide_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Divide(self, node: cst.Divide) -> bool:\n",
      "    self.divide_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.divide_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class FloorDivideOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.floor_divide_operator_count = 0\n",
      "\n",
      "    def visit_FloorDivide(self, node: cst.FloorDivide) -> bool:\n",
      "        self.floor_divide_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.floor_divide_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.floor_divide_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_FloorDivide(self, node: cst.FloorDivide) -> bool:\n",
      "    self.floor_divide_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.floor_divide_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class LeftShiftOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.left_shift_operator_count = 0\n",
      "\n",
      "    def visit_LeftShift(self, node: cst.LeftShift) -> bool:\n",
      "        self.left_shift_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.left_shift_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.left_shift_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_LeftShift(self, node: cst.LeftShift) -> bool:\n",
      "    self.left_shift_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.left_shift_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MatrixMultiplyOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.matrix_multiply_operator_count = 0\n",
      "\n",
      "    def visit_MatrixMultiply(self, node: cst.MatrixMultiply) -> bool:\n",
      "        self.matrix_multiply_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.matrix_multiply_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.matrix_multiply_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_MatrixMultiply(self, node: cst.MatrixMultiply) -> bool:\n",
      "    self.matrix_multiply_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.matrix_multiply_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ModuloOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.modulo_operator_count = 0\n",
      "\n",
      "    def visit_Modulo(self, node: cst.Modulo) -> bool:\n",
      "        self.modulo_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.modulo_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.modulo_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Modulo(self, node: cst.Modulo) -> bool:\n",
      "    self.modulo_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.modulo_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MultiplyOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.multiply_operator_count = 0\n",
      "\n",
      "    def visit_Multiply(self, node: cst.Multiply) -> bool:\n",
      "        self.multiply_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.multiply_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.multiply_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Multiply(self, node: cst.Multiply) -> bool:\n",
      "    self.multiply_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.multiply_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class PowerOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.power_operator_count = 0\n",
      "\n",
      "    def visit_Power(self, node: cst.Power) -> bool:\n",
      "        self.power_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.power_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.power_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Power(self, node: cst.Power) -> bool:\n",
      "    self.power_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.power_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class RightShiftOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.right_shift_operator_count = 0\n",
      "\n",
      "    def visit_RightShift(self, node: cst.RightShift) -> bool:\n",
      "        self.right_shift_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.right_shift_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.right_shift_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_RightShift(self, node: cst.RightShift) -> bool:\n",
      "    self.right_shift_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.right_shift_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class SubtractOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.subtract_operator_count = 0\n",
      "\n",
      "    def visit_Subtract(self, node: cst.Subtract) -> bool:\n",
      "        self.subtract_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.subtract_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.subtract_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Subtract(self, node: cst.Subtract) -> bool:\n",
      "    self.subtract_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.subtract_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# Comparison Operators\n",
      "class EqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.equal_operator_count = 0\n",
      "\n",
      "    def visit_Equal(self, node: cst.Equal) -> bool:\n",
      "        self.equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.equal_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.equal_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Equal(self, node: cst.Equal) -> bool:\n",
      "    self.equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.equal_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Comparison Operators\n",
      "class EqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.equal_operator_count = 0\n",
      "\n",
      "    def visit_Equal(self, node: cst.Equal) -> bool:\n",
      "        self.equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.equal_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.equal_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Equal(self, node: cst.Equal) -> bool:\n",
      "    self.equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.equal_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class GreaterThanOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.greater_than_operator_count = 0\n",
      "\n",
      "    def visit_GreaterThan(self, node: cst.GreaterThan) -> bool:\n",
      "        self.greater_than_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.greater_than_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.greater_than_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_GreaterThan(self, node: cst.GreaterThan) -> bool:\n",
      "    self.greater_than_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.greater_than_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class GreaterThanEqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.greater_than_equal_operator_count = 0\n",
      "\n",
      "    def visit_GreaterThanEqual(self, node: cst.GreaterThanEqual) -> bool:\n",
      "        self.greater_than_equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.greater_than_equal_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.greater_than_equal_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_GreaterThanEqual(self, node: cst.GreaterThanEqual) -> bool:\n",
      "    self.greater_than_equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.greater_than_equal_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class InOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.in_operator_count = 0\n",
      "\n",
      "    def visit_In(self, node: cst.In) -> bool:\n",
      "        self.in_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.in_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.in_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_In(self, node: cst.In) -> bool:\n",
      "    self.in_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.in_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class IsOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.is_operator_count = 0\n",
      "\n",
      "    def visit_Is(self, node: cst.Is) -> bool:\n",
      "        self.is_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.is_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.is_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Is(self, node: cst.Is) -> bool:\n",
      "    self.is_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.is_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class LessThanOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.less_than_operator_count = 0\n",
      "\n",
      "    def visit_LessThan(self, node: cst.LessThan) -> bool:\n",
      "        self.less_than_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.less_than_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.less_than_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_LessThan(self, node: cst.LessThan) -> bool:\n",
      "    self.less_than_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.less_than_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class LessThanEqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.less_than_equal_operator_count = 0\n",
      "\n",
      "    def visit_LessThanEqual(self, node: cst.LessThanEqual) -> bool:\n",
      "        self.less_than_equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.less_than_equal_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.less_than_equal_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_LessThanEqual(self, node: cst.LessThanEqual) -> bool:\n",
      "    self.less_than_equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.less_than_equal_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class NotEqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.not_equal_operator_count = 0\n",
      "\n",
      "    def visit_NotEqual(self, node: cst.NotEqual) -> bool:\n",
      "        self.not_equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.not_equal_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.not_equal_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_NotEqual(self, node: cst.NotEqual) -> bool:\n",
      "    self.not_equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.not_equal_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class IsNotOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.is_not_operator_count = 0\n",
      "\n",
      "    def visit_IsNot(self, node: cst.IsNot) -> bool:\n",
      "        self.is_not_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.is_not_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.is_not_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_IsNot(self, node: cst.IsNot) -> bool:\n",
      "    self.is_not_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.is_not_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class NotInOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.not_in_operator_count = 0\n",
      "\n",
      "    def visit_NotIn(self, node: cst.NotIn) -> bool:\n",
      "        self.not_in_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.not_in_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.not_in_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_NotIn(self, node: cst.NotIn) -> bool:\n",
      "    self.not_in_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.not_in_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# Augmented Assignment Operators\n",
      "class AddAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.add_assign_operator_count = 0\n",
      "\n",
      "    def visit_AddAssign(self, node: cst.AddAssign) -> bool:\n",
      "        self.add_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.add_assign_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.add_assign_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_AddAssign(self, node: cst.AddAssign) -> bool:\n",
      "    self.add_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.add_assign_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BitAndAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_and_assign_operator_count = 0\n",
      "\n",
      "    def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n",
      "        self.bit_and_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_and_assign_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_and_assign_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitAndAssign(self, node: cst.BitAndAssign) -> bool:\n",
      "    self.bit_and_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_and_assign_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BitOrAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_or_assign_operator_count = 0\n",
      "\n",
      "    def visit_BitOrAssign(self, node: cst.BitOrAssign) -> bool:\n",
      "        self.bit_or_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_or_assign_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_or_assign_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitOrAssign(self, node: cst.BitOrAssign) -> bool:\n",
      "    self.bit_or_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_or_assign_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BitXorAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.bit_xor_assign_operator_count = 0\n",
      "\n",
      "    def visit_BitXorAssign(self, node: cst.BitXorAssign) -> bool:\n",
      "        self.bit_xor_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.bit_xor_assign_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.bit_xor_assign_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BitXorAssign(self, node: cst.BitXorAssign) -> bool:\n",
      "    self.bit_xor_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.bit_xor_assign_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class DivideAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.divide_assign_operator_count = 0\n",
      "\n",
      "    def visit_DivideAssign(self, node: cst.DivideAssign) -> bool:\n",
      "        self.divide_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.divide_assign_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.divide_assign_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_DivideAssign(self, node: cst.DivideAssign) -> bool:\n",
      "    self.divide_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.divide_assign_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class FloorDivideAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.floor_divide_assign_operator_count = 0\n",
      "\n",
      "    def visit_FloorDivideAssign(self, node: cst.FloorDivideAssign) -> bool:\n",
      "        self.floor_divide_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.floor_divide_assign_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.floor_divide_assign_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_FloorDivideAssign(self, node: cst.FloorDivideAssign) -> bool:\n",
      "    self.floor_divide_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.floor_divide_assign_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class LeftShiftAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.left_shift_assign_operator_count = 0\n",
      "\n",
      "    def visit_LeftShiftAssign(self, node: cst.LeftShiftAssign) -> bool:\n",
      "        self.left_shift_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.left_shift_assign_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.left_shift_assign_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_LeftShiftAssign(self, node: cst.LeftShiftAssign) -> bool:\n",
      "    self.left_shift_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.left_shift_assign_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MatrixMultiplyAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.matrix_multiply_assign_operator_count = 0\n",
      "\n",
      "    def visit_MatrixMultiplyAssign(self, node: cst.MatrixMultiplyAssign) -> bool:\n",
      "        self.matrix_multiply_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.matrix_multiply_assign_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.matrix_multiply_assign_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_MatrixMultiplyAssign(self, node: cst.MatrixMultiplyAssign) -> bool:\n",
      "    self.matrix_multiply_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.matrix_multiply_assign_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ModuloAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.modulo_assign_operator_count = 0\n",
      "\n",
      "    def visit_ModuloAssign(self, node: cst.ModuloAssign) -> bool:\n",
      "        self.modulo_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.modulo_assign_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.modulo_assign_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ModuloAssign(self, node: cst.ModuloAssign) -> bool:\n",
      "    self.modulo_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.modulo_assign_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MultiplyAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.multiply_assign_operator_count = 0\n",
      "\n",
      "    def visit_MultiplyAssign(self, node: cst.MultiplyAssign) -> bool:\n",
      "        self.multiply_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.multiply_assign_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.multiply_assign_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_MultiplyAssign(self, node: cst.MultiplyAssign) -> bool:\n",
      "    self.multiply_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.multiply_assign_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class PowerAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.power_assign_operator_count = 0\n",
      "\n",
      "    def visit_PowerAssign(self, node: cst.PowerAssign) -> bool:\n",
      "        self.power_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.power_assign_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.power_assign_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_PowerAssign(self, node: cst.PowerAssign) -> bool:\n",
      "    self.power_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.power_assign_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class RightShiftAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.right_shift_assign_operator_count = 0\n",
      "\n",
      "    def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n",
      "        self.right_shift_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.right_shift_assign_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.right_shift_assign_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_RightShiftAssign(self, node: cst.RightShiftAssign) -> bool:\n",
      "    self.right_shift_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.right_shift_assign_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class SubtractAssignOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.subtract_assign_operator_count = 0\n",
      "\n",
      "    def visit_SubtractAssign(self, node: cst.SubtractAssign) -> bool:\n",
      "        self.subtract_assign_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.subtract_assign_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.subtract_assign_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_SubtractAssign(self, node: cst.SubtractAssign) -> bool:\n",
      "    self.subtract_assign_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.subtract_assign_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# Miscellaneous Operators\n",
      "class AssignEqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.assign_equal_operator_count = 0\n",
      "\n",
      "    def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "        self.assign_equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.assign_equal_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.assign_equal_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "    self.assign_equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.assign_equal_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Miscellaneous Operators\n",
      "class AssignEqualOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.assign_equal_operator_count = 0\n",
      "\n",
      "    def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "        self.assign_equal_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.assign_equal_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.assign_equal_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_AssignEqual(self, node: cst.AssignEqual) -> bool:\n",
      "    self.assign_equal_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.assign_equal_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ColonOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.colon_operator_count = 0\n",
      "\n",
      "    def visit_Colon(self, node: cst.Colon) -> bool:\n",
      "        self.colon_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.colon_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.colon_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Colon(self, node: cst.Colon) -> bool:\n",
      "    self.colon_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.colon_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class CommaOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.comma_operator_count = 0\n",
      "\n",
      "    def visit_Comma(self, node: cst.Comma) -> bool:\n",
      "        self.comma_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.comma_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.comma_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Comma(self, node: cst.Comma) -> bool:\n",
      "    self.comma_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.comma_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class DotOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.dot_operator_count = 0\n",
      "\n",
      "    def visit_Dot(self, node: cst.Dot) -> bool:\n",
      "        self.dot_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.dot_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.dot_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Dot(self, node: cst.Dot) -> bool:\n",
      "    self.dot_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.dot_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ImportStarOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.import_star_operator_count = 0\n",
      "\n",
      "    def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n",
      "        self.import_star_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.import_star_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.import_star_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ImportStar(self, node: cst.ImportStar) -> bool:\n",
      "    self.import_star_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.import_star_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class SemicolonOperatorCounter(cst.CSTVisitor):\n",
      "    def __init__(self, code: str):\n",
      "        self.module = cst.parse_module(code)\n",
      "        self.semicolon_operator_count = 0\n",
      "\n",
      "    def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n",
      "        self.semicolon_operator_count += 1\n",
      "        return True\n",
      "\n",
      "    def collect(self):\n",
      "        self.module.visit(self)\n",
      "        return self.semicolon_operator_count\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str):\n",
      "    self.module = cst.parse_module(code)\n",
      "    self.semicolon_operator_count = 0\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Semicolon(self, node: cst.Semicolon) -> bool:\n",
      "    self.semicolon_operator_count += 1\n",
      "    return True\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def collect(self):\n",
      "    self.module.visit(self)\n",
      "    return self.semicolon_operator_count\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class VectorThread(BaseThread, MemoryIndex):\n",
      "    \"\"\"vector BaseThread, creates a faiss index with the messages and allows to search for similar messages, memory BaseThread can return messages in either similarity or chronological order\n",
      "    add a parameter to choose the order of the messages\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\n",
      "        BaseThread.__init__(self, name=name, max_memory=None)\n",
      "        MemoryIndex.__init__(self, index=None, name=name)\n",
      "        self.max_context = max_context\n",
      "        self.use_mark = use_mark\n",
      "        self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "\n",
      "    def index_message(self, message: str, verbose: bool = False):\n",
      "        \"\"\"index a message in the faiss index, the message is embedded and added to the index\n",
      "        self.values and self.embeddings and self.index are updated\n",
      "        \"\"\"\n",
      "\n",
      "        self.add_to_index(value=message, verbose=verbose)\n",
      "\n",
      "    def add_message(self, message_dict: dict, verbose: bool = False):\n",
      "        \"\"\"add a message to the memory thread, the message is embedded and added to the index\n",
      "        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\n",
      "        \"\"\"\n",
      "        # print(\"checking the dict\")\n",
      "        message_dict = check_dict(message_dict)\n",
      "        # print(\"trying to add the message\")\n",
      "        BaseThread.add_message(self, message_dict)\n",
      "        # print(message_dict)\n",
      "        message = message_dict[\"content\"]\n",
      "        self.index_message(message, verbose=verbose)\n",
      "        return True\n",
      "\n",
      "    def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\n",
      "        \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\n",
      "        if self.use_mark:\n",
      "            query = mark_question(query)\n",
      "        return MemoryIndex.token_bound_query(self, query, k, max_tokens)\n",
      "\n",
      "    def sorted_query(\n",
      "        self,\n",
      "        query,\n",
      "        k: int = 10,\n",
      "        max_tokens: int = 4000,\n",
      "        reverse: bool = False,\n",
      "        return_from_thread=True,\n",
      "    ) -> Tuple[List[str], List[float], List[int]]:\n",
      "        \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\n",
      "        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\n",
      "        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\n",
      "        \"\"\"\n",
      "        unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\n",
      "\n",
      "        num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\n",
      "        # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\n",
      "        unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\n",
      "\n",
      "        # Sort the indices\n",
      "        sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\n",
      "        \n",
      "        print(sorted_indices)\n",
      "        print(type(sorted_indices))\n",
      "\n",
      "        if reverse:\n",
      "            sorted_indices.reverse()\n",
      "\n",
      "        # Fetch the sorted messages, scores, and indices based on sorted_indices\n",
      "        sorted_messages = [unsorted_messages[i] for i in sorted_indices]\n",
      "        sorted_scores = [unsorted_scores[i] for i in sorted_indices]\n",
      "        sorted_indices = [unsorted_indices[i] for i in sorted_indices]\n",
      "\n",
      "        if return_from_thread:\n",
      "            sorted_messages = [self.memory_thread[i] for i in sorted_indices]\n",
      "\n",
      "        return sorted_messages, sorted_scores, sorted_indices\n",
      "    def weighted_query(\n",
      "        self,\n",
      "        query,\n",
      "        k: int = 10,\n",
      "        max_tokens: int = 4000,\n",
      "        decay_factor: float = 0.1,\n",
      "        temporal_weight: float = 0.5,\n",
      "        order_by: str = \"chronological\",\n",
      "        reverse: bool = False,\n",
      "    ) -> list:\n",
      "        \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\n",
      "        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\n",
      "        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\n",
      "        The order_by parameter controls the order of the results. If it is set to 'similarity', the results are sorted in similarity order. If it is set to 'chronological', the results are sorted in chronological order with the most recent message first.\n",
      "        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\n",
      "        \"\"\"\n",
      "        # Validate order_by parameter\n",
      "        if order_by not in (\"similarity\", \"chronological\"):\n",
      "            raise ValueError(\n",
      "                \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
      "            )\n",
      "\n",
      "        # Get similarity-based results\n",
      "        sim_messages, sim_scores, sim_indices = self.sorted_query(\n",
      "            query, k, max_tokens=max_tokens\n",
      "        )\n",
      "\n",
      "        # Get token-bound history\n",
      "        hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\n",
      "\n",
      "        # Combine messages and indices\n",
      "        combined_messages = sim_messages + hist_messages\n",
      "        combined_indices = sim_indices + hist_indices\n",
      "\n",
      "        # Create the local_index and populate it\n",
      "        self.local_index = MemoryIndex(name=\"local_index\")\n",
      "        for message in combined_messages:\n",
      "            self.local_index.add_to_index(value=message, verbose=False)\n",
      "\n",
      "        # Perform a new query on the combined index\n",
      "        (\n",
      "            new_query_results,\n",
      "            new_query_scores,\n",
      "            new_query_indices,\n",
      "        ) = self.local_index.token_bound_query(\n",
      "            query, k=len(combined_messages), max_tokens=max_tokens\n",
      "        )\n",
      "\n",
      "        # Compute temporal weights\n",
      "        temporal_weights = [\n",
      "            np.exp(-decay_factor * i) for i in range(len(combined_messages))\n",
      "        ]\n",
      "        temporal_weights = [\n",
      "            w / sum(temporal_weights) for w in temporal_weights\n",
      "        ]  # Normalize the temporal weights\n",
      "\n",
      "        # Combine similarity scores and temporal weights\n",
      "        weighted_scores = []\n",
      "        for i in range(len(new_query_scores)):\n",
      "            sim_score = new_query_scores[i]\n",
      "            temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\n",
      "            weighted_score = (\n",
      "                1 - temporal_weight\n",
      "            ) * sim_score + temporal_weight * temp_weight\n",
      "            weighted_scores.append(weighted_score)\n",
      "\n",
      "        # Sort the results based on the order_by parameter\n",
      "        if order_by == \"similarity\":\n",
      "            sorting_key = lambda k: weighted_scores[k]\n",
      "        elif order_by == \"chronological\":  # order_by == 'chronological'\n",
      "            sorting_key = lambda k: new_query_indices[k]\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
      "            )\n",
      "\n",
      "        sorted_indices = [\n",
      "            new_query_indices[i]\n",
      "            for i in sorted(\n",
      "                range(len(new_query_indices)), key=sorting_key, reverse=not reverse\n",
      "            )\n",
      "        ]\n",
      "        sorted_results = [\n",
      "            new_query_results[i]\n",
      "            for i in sorted(\n",
      "                range(len(new_query_results)), key=sorting_key, reverse=not reverse\n",
      "            )\n",
      "        ]\n",
      "        sorted_scores = [\n",
      "            weighted_scores[i]\n",
      "            for i in sorted(\n",
      "                range(len(weighted_scores)), key=sorting_key, reverse=not reverse\n",
      "            )\n",
      "        ]\n",
      "\n",
      "        # Return only the top k results without exceeding max_tokens\n",
      "        final_results, final_scores, final_indices = [], [], []\n",
      "        current_tokens = 0\n",
      "        for i in range(min(k, len(sorted_results))):\n",
      "            message_tokens = self.get_message_tokens(sorted_results[i])\n",
      "            if current_tokens + message_tokens <= max_tokens:\n",
      "                final_results.append(sorted_results[i])\n",
      "                final_scores.append(sorted_scores[i])\n",
      "                final_indices.append(sorted_indices[i])\n",
      "                current_tokens += message_tokens\n",
      "            else:\n",
      "                break\n",
      "\n",
      "        return final_results, final_scores, final_indices\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseThread, \", \"MemoryIndex\"]\n",
      "]\n",
      "Function call: check_dict\n",
      "Function call: mark_question\n",
      "Function call: min\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: int\n",
      "Function call: sorted\n",
      "Function call: range\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: ValueError\n",
      "Function call: MemoryIndex\n",
      "Function call: len\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: sum\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: sorted\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: sorted\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: sorted\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: range\n",
      "Function call: min\n",
      "Function call: len\n",
      "Related codes: ['\\n\\ndef check_dict(message_dict):\\n    if (\\n        type(message_dict) is list\\n        and len(message_dict) == 1\\n        and type(message_dict[0]) is dict\\n    ):\\n        message_dict = message_dict[0]\\n    elif type(message_dict) is not dict:\\n        raise Exception(\\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\\n            message_dict,\\n            type(message_dict),\\n        )\\n    return message_dict\\n', '\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\n\\n\\nclass MemoryIndex(NpIndex):\\n    \"\"\"\\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \\n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\\n    \"\"\"\\n    @staticmethod\\n    def check_uniform_context_type(context: List[Any]) -> None:\\n        \"\"\"Check if all context elements are of the same type.\"\"\"\\n        if not all(isinstance(x, type(context[0])) for x in context):\\n            raise ValueError(\"All context elements must be of the same type.\")\\n\\n    def __init__(\\n        self,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        context: Optional[List[Any]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ):\\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\\n        if context is not None and len(context) != len(values):\\n            raise ValueError(\"The context must have the same length as the values\")\\n\\n\\n        self.markdown = markdown\\n        if context is not None and values is not None:\\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\\n\\n        if context is not None:\\n            self.check_uniform_context_type(context)\\n            self.context_type = type(context[0])\\n\\n\\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\\n        if isinstance(identifier, list):\\n            return [self.get_context(value) for value in identifier]\\n        else:\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n            return self.context[value]\\n\\n    def clean_context(self):\\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\\n        self.context = {value: self.context[value] for value in self.values}\\n\\n    def add_to_context(self, value: str, context: Any):\\n        \"\"\" add a context to a value \"\"\"\\n        if not isinstance(context, self.context_type):\\n            raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if value in self.values:\\n            if value not in self.context:\\n                self.context[value] = []\\n            self.context[value].append(context)\\n\\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\\n        if isinstance(values, str):\\n            values = [values]\\n        NpIndex.add(self, values, embedding)\\n        if context is not None:\\n            for value, cont in zip(values, context):\\n                self.add_to_context(value, cont)\\n\\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\\n        if not isinstance(identifier, list):\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n        NpIndex.remove(self, identifier)\\n        if not isinstance(identifier, list):\\n            self.context.pop(value)\\n\\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\\n        #recover value from old_identifier\\n        if new_context is not None:\\n            if not isinstance(new_context, self.context_type):\\n                raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if not isinstance(old_identifier, list):\\n            old_id = self.identify_input(old_identifier)\\n            old_value = self.values[old_id]\\n            # Only perform the update if the old_value is not the same as the new_value.\\n            if old_value != new_value:\\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\\n\\n            if new_context is not None:\\n                self.context[new_value] = [new_context]\\n            else:\\n                self.context[new_value] = self.context.pop(old_value)\\n        else:\\n            self.context[new_value] = self.context.pop(old_value)\\n\\n    @classmethod\\n    def from_pandas(\\n        cls,\\n        data_frame: Union[pd.DataFrame, str],\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        if (\\n            isinstance(data_frame, str)\\n            and data_frame.endswith(\".csv\")\\n            and os.path.isfile(data_frame)\\n        ):\\n            logger.info(\"Loading the CSV file\")\\n            data_frame = pd.read_csv(data_frame)\\n            name = os.path.basename(data_frame).split(\".\")[0]\\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\\n            logger.info(\"Loading the pandas DataFrame\")\\n        else:\\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\\n\\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_pandas(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_hf_dataset(\\n        cls,\\n        dataset_url: str,\\n        value_column: str,\\n        data_split: str = \"train\",\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        dataset = load_dataset(dataset_url)[data_split]\\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_hf(dataset, context_columns)\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_polars(\\n        cls,\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        print(\"Loading the Polars DataFrame\")\\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_polars(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n\\n    @classmethod\\n    def from_python(\\n        cls,\\n        directory_path: str,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        markdown: str = \"python/markdown\",\\n        resolution: str = \"both\",\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\n",
      "    BaseThread.__init__(self, name=name, max_memory=None)\n",
      "    MemoryIndex.__init__(self, index=None, name=name)\n",
      "    self.max_context = max_context\n",
      "    self.use_mark = use_mark\n",
      "    self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def index_message(self, message: str, verbose: bool = False):\n",
      "    \"\"\"index a message in the faiss index, the message is embedded and added to the index\n",
      "        self.values and self.embeddings and self.index are updated\n",
      "        \"\"\"\n",
      "\n",
      "    self.add_to_index(value=message, verbose=verbose)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def add_message(self, message_dict: dict, verbose: bool = False):\n",
      "    \"\"\"add a message to the memory thread, the message is embedded and added to the index\n",
      "        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\n",
      "        \"\"\"\n",
      "    # print(\"checking the dict\")\n",
      "    message_dict = check_dict(message_dict)\n",
      "    # print(\"trying to add the message\")\n",
      "    BaseThread.add_message(self, message_dict)\n",
      "    # print(message_dict)\n",
      "    message = message_dict[\"content\"]\n",
      "    self.index_message(message, verbose=verbose)\n",
      "    return True\n",
      "\n",
      "Function call: check_dict\n",
      "Related codes: ['\\n\\ndef check_dict(message_dict):\\n    if (\\n        type(message_dict) is list\\n        and len(message_dict) == 1\\n        and type(message_dict[0]) is dict\\n    ):\\n        message_dict = message_dict[0]\\n    elif type(message_dict) is not dict:\\n        raise Exception(\\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\\n            message_dict,\\n            type(message_dict),\\n        )\\n    return message_dict\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\n",
      "    \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\n",
      "    if self.use_mark:\n",
      "        query = mark_question(query)\n",
      "    return MemoryIndex.token_bound_query(self, query, k, max_tokens)\n",
      "\n",
      "Function call: mark_question\n",
      "Related codes: ['\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def sorted_query(\n",
      "    self,\n",
      "    query,\n",
      "    k: int = 10,\n",
      "    max_tokens: int = 4000,\n",
      "    reverse: bool = False,\n",
      "    return_from_thread=True,\n",
      ") -> Tuple[List[str], List[float], List[int]]:\n",
      "    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\n",
      "        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\n",
      "        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\n",
      "        \"\"\"\n",
      "    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\n",
      "\n",
      "    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\n",
      "    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\n",
      "    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\n",
      "\n",
      "    # Sort the indices\n",
      "    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\n",
      "    \n",
      "    print(sorted_indices)\n",
      "    print(type(sorted_indices))\n",
      "\n",
      "    if reverse:\n",
      "        sorted_indices.reverse()\n",
      "\n",
      "    # Fetch the sorted messages, scores, and indices based on sorted_indices\n",
      "    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\n",
      "    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\n",
      "    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\n",
      "\n",
      "    if return_from_thread:\n",
      "        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\n",
      "\n",
      "    return sorted_messages, sorted_scores, sorted_indices\n",
      "\n",
      "Function call: min\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: int\n",
      "Function call: sorted\n",
      "Function call: range\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: type\n",
      "Related codes: ['\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def weighted_query(\n",
      "    self,\n",
      "    query,\n",
      "    k: int = 10,\n",
      "    max_tokens: int = 4000,\n",
      "    decay_factor: float = 0.1,\n",
      "    temporal_weight: float = 0.5,\n",
      "    order_by: str = \"chronological\",\n",
      "    reverse: bool = False,\n",
      ") -> list:\n",
      "    \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\n",
      "        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\n",
      "        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\n",
      "        The order_by parameter controls the order of the results. If it is set to 'similarity', the results are sorted in similarity order. If it is set to 'chronological', the results are sorted in chronological order with the most recent message first.\n",
      "        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\n",
      "        \"\"\"\n",
      "    # Validate order_by parameter\n",
      "    if order_by not in (\"similarity\", \"chronological\"):\n",
      "        raise ValueError(\n",
      "            \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
      "        )\n",
      "\n",
      "    # Get similarity-based results\n",
      "    sim_messages, sim_scores, sim_indices = self.sorted_query(\n",
      "        query, k, max_tokens=max_tokens\n",
      "    )\n",
      "\n",
      "    # Get token-bound history\n",
      "    hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\n",
      "\n",
      "    # Combine messages and indices\n",
      "    combined_messages = sim_messages + hist_messages\n",
      "    combined_indices = sim_indices + hist_indices\n",
      "\n",
      "    # Create the local_index and populate it\n",
      "    self.local_index = MemoryIndex(name=\"local_index\")\n",
      "    for message in combined_messages:\n",
      "        self.local_index.add_to_index(value=message, verbose=False)\n",
      "\n",
      "    # Perform a new query on the combined index\n",
      "    (\n",
      "        new_query_results,\n",
      "        new_query_scores,\n",
      "        new_query_indices,\n",
      "    ) = self.local_index.token_bound_query(\n",
      "        query, k=len(combined_messages), max_tokens=max_tokens\n",
      "    )\n",
      "\n",
      "    # Compute temporal weights\n",
      "    temporal_weights = [\n",
      "        np.exp(-decay_factor * i) for i in range(len(combined_messages))\n",
      "    ]\n",
      "    temporal_weights = [\n",
      "        w / sum(temporal_weights) for w in temporal_weights\n",
      "    ]  # Normalize the temporal weights\n",
      "\n",
      "    # Combine similarity scores and temporal weights\n",
      "    weighted_scores = []\n",
      "    for i in range(len(new_query_scores)):\n",
      "        sim_score = new_query_scores[i]\n",
      "        temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\n",
      "        weighted_score = (\n",
      "            1 - temporal_weight\n",
      "        ) * sim_score + temporal_weight * temp_weight\n",
      "        weighted_scores.append(weighted_score)\n",
      "\n",
      "    # Sort the results based on the order_by parameter\n",
      "    if order_by == \"similarity\":\n",
      "        sorting_key = lambda k: weighted_scores[k]\n",
      "    elif order_by == \"chronological\":  # order_by == 'chronological'\n",
      "        sorting_key = lambda k: new_query_indices[k]\n",
      "    else:\n",
      "        raise ValueError(\n",
      "            \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
      "        )\n",
      "\n",
      "    sorted_indices = [\n",
      "        new_query_indices[i]\n",
      "        for i in sorted(\n",
      "            range(len(new_query_indices)), key=sorting_key, reverse=not reverse\n",
      "        )\n",
      "    ]\n",
      "    sorted_results = [\n",
      "        new_query_results[i]\n",
      "        for i in sorted(\n",
      "            range(len(new_query_results)), key=sorting_key, reverse=not reverse\n",
      "        )\n",
      "    ]\n",
      "    sorted_scores = [\n",
      "        weighted_scores[i]\n",
      "        for i in sorted(\n",
      "            range(len(weighted_scores)), key=sorting_key, reverse=not reverse\n",
      "        )\n",
      "    ]\n",
      "\n",
      "    # Return only the top k results without exceeding max_tokens\n",
      "    final_results, final_scores, final_indices = [], [], []\n",
      "    current_tokens = 0\n",
      "    for i in range(min(k, len(sorted_results))):\n",
      "        message_tokens = self.get_message_tokens(sorted_results[i])\n",
      "        if current_tokens + message_tokens <= max_tokens:\n",
      "            final_results.append(sorted_results[i])\n",
      "            final_scores.append(sorted_scores[i])\n",
      "            final_indices.append(sorted_indices[i])\n",
      "            current_tokens += message_tokens\n",
      "        else:\n",
      "            break\n",
      "\n",
      "    return final_results, final_scores, final_indices\n",
      "\n",
      "Function call: ValueError\n",
      "Function call: MemoryIndex\n",
      "Function call: len\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: sum\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: sorted\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: sorted\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: sorted\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: range\n",
      "Function call: min\n",
      "Function call: len\n",
      "Related codes: ['\\n\\n\\nclass MemoryIndex(NpIndex):\\n    \"\"\"\\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \\n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\\n    \"\"\"\\n    @staticmethod\\n    def check_uniform_context_type(context: List[Any]) -> None:\\n        \"\"\"Check if all context elements are of the same type.\"\"\"\\n        if not all(isinstance(x, type(context[0])) for x in context):\\n            raise ValueError(\"All context elements must be of the same type.\")\\n\\n    def __init__(\\n        self,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        context: Optional[List[Any]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ):\\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\\n        if context is not None and len(context) != len(values):\\n            raise ValueError(\"The context must have the same length as the values\")\\n\\n\\n        self.markdown = markdown\\n        if context is not None and values is not None:\\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\\n\\n        if context is not None:\\n            self.check_uniform_context_type(context)\\n            self.context_type = type(context[0])\\n\\n\\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\\n        if isinstance(identifier, list):\\n            return [self.get_context(value) for value in identifier]\\n        else:\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n            return self.context[value]\\n\\n    def clean_context(self):\\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\\n        self.context = {value: self.context[value] for value in self.values}\\n\\n    def add_to_context(self, value: str, context: Any):\\n        \"\"\" add a context to a value \"\"\"\\n        if not isinstance(context, self.context_type):\\n            raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if value in self.values:\\n            if value not in self.context:\\n                self.context[value] = []\\n            self.context[value].append(context)\\n\\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\\n        if isinstance(values, str):\\n            values = [values]\\n        NpIndex.add(self, values, embedding)\\n        if context is not None:\\n            for value, cont in zip(values, context):\\n                self.add_to_context(value, cont)\\n\\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\\n        if not isinstance(identifier, list):\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n        NpIndex.remove(self, identifier)\\n        if not isinstance(identifier, list):\\n            self.context.pop(value)\\n\\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\\n        #recover value from old_identifier\\n        if new_context is not None:\\n            if not isinstance(new_context, self.context_type):\\n                raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if not isinstance(old_identifier, list):\\n            old_id = self.identify_input(old_identifier)\\n            old_value = self.values[old_id]\\n            # Only perform the update if the old_value is not the same as the new_value.\\n            if old_value != new_value:\\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\\n\\n            if new_context is not None:\\n                self.context[new_value] = [new_context]\\n            else:\\n                self.context[new_value] = self.context.pop(old_value)\\n        else:\\n            self.context[new_value] = self.context.pop(old_value)\\n\\n    @classmethod\\n    def from_pandas(\\n        cls,\\n        data_frame: Union[pd.DataFrame, str],\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        if (\\n            isinstance(data_frame, str)\\n            and data_frame.endswith(\".csv\")\\n            and os.path.isfile(data_frame)\\n        ):\\n            logger.info(\"Loading the CSV file\")\\n            data_frame = pd.read_csv(data_frame)\\n            name = os.path.basename(data_frame).split(\".\")[0]\\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\\n            logger.info(\"Loading the pandas DataFrame\")\\n        else:\\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\\n\\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_pandas(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_hf_dataset(\\n        cls,\\n        dataset_url: str,\\n        value_column: str,\\n        data_split: str = \"train\",\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        dataset = load_dataset(dataset_url)[data_split]\\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_hf(dataset, context_columns)\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_polars(\\n        cls,\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        print(\"Loading the Polars DataFrame\")\\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_polars(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n\\n    @classmethod\\n    def from_python(\\n        cls,\\n        directory_path: str,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        markdown: str = \"python/markdown\",\\n        resolution: str = \"both\",\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class BaseThread:\n",
      "    \"\"\"\n",
      "    This class is used to keep track of the memory thread of a conversation and the total number of tokens.\n",
      "    All conversation memories should subclass this class. If max_memory is None, it has\n",
      "    no limit to the number of tokens that can be stored in the memory thread.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        name: str = \"memory\",\n",
      "        max_memory: Optional[int] = None,\n",
      "        tokenizer: Optional[Any] = None,\n",
      "        save_path: str = 'threads'\n",
      "\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        Initialize the BaseThread instance.\n",
      "\n",
      "        :param name: The name of the memory thread. Defaults to 'memory'.\n",
      "        :param max_memory: The maximum number of tokens allowed in the memory thread.\n",
      "                           Defaults to None, which means no limit.\n",
      "        :param tokenizer: The tokenizer to be used for tokenizing messages.\n",
      "                          Defaults to None, which means using the tiktoken encoding for the 'gpt-3.5-turbo' model.\n",
      "        \"\"\"\n",
      "        self.name = name\n",
      "        self.max_memory = max_memory\n",
      "        self.memory_schema = {\"role\": pl.Utf8, \"content\": pl.Utf8,\"timestamp\":pl.Float64,\"tokens_count\":pl.UInt16}\n",
      "        self.memory_thread: pl.DataFrame = pl.DataFrame(schema=self.memory_schema)\n",
      "        \"\"\" self.time_stamps = [] \"\"\"\n",
      "        \"\"\" self.message_tokens = [] \"\"\"\n",
      "        self.total_tokens = self.get_total_tokens_from_thread()\n",
      "        self.save_path = save_path\n",
      "        if tokenizer is None:\n",
      "            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        return self.memory_thread[idx]\n",
      "\n",
      "    def __len__(self):\n",
      "        return self.memory_thread.shape[0]\n",
      "\n",
      "    def save(self, path: Union[str,None]) -> None:\n",
      "        if path is None:\n",
      "            path = os.path.join(self.save_path,f'{self.name}.parquet')\n",
      "        self.memory_thread.write_parquet(\n",
      "            file=path,\n",
      "            compression='zstd',\n",
      "            compression_level=None,\n",
      "            statistics=False,\n",
      "            row_group_size=None,\n",
      "            use_pyarrow=True,\n",
      "            pyarrow_options=None\n",
      "        )\n",
      "\n",
      "    def load(self, path: Union[str,None]) -> None:\n",
      "        if path is None:\n",
      "            path = os.path.join(self.save_path,f'{self.name}.parquet')\n",
      "        self.memory_thread = pl.read_parquet(source = path,\n",
      "                        use_pyarrow= True,\n",
      "                        memory_map = True,\n",
      "                        )\n",
      "        \n",
      "    def get_total_tokens_from_thread(self):\n",
      "        return self.memory_thread[\"tokens_count\"].sum()\n",
      "\n",
      "    def reset_memory(self) -> None:\n",
      "        self.memory_thread = pl.DataFrame(schema=self.memory_schema) \n",
      "\n",
      "    def dict_to_row(self, message_dict:Dict[str,str]) -> pl.DataFrame:\n",
      "        timestamp = message_dict['timestamp'] if 'timestamp' in message_dict else [now()]\n",
      "        return pl.DataFrame(schema=self.memory_schema,\n",
      "                            data={ \"role\":[message_dict[\"role\"]],\n",
      "                                  \"content\":[message_dict[\"content\"]],\n",
      "                                  \"timestamp\": timestamp,\n",
      "                                  \"tokens_count\":[len(self.tokenizer.encode(message_dict[\"content\"]))+7]\n",
      "                                  })\n",
      "\n",
      "    def get_message_tokens_from_dict(self, message_dict: dict) -> int:\n",
      "        \"\"\"\n",
      "        Calculate the number of tokens in a message, including the role token.\n",
      "\n",
      "        :param message_dict: A dictionary containing the role and content of the message.\n",
      "        :return: The total number of tokens in the message.\n",
      "        \"\"\"\n",
      "        message_dict = check_dict(message_dict)\n",
      "        message = message_dict[\"content\"]\n",
      "        return len(self.tokenizer.encode(message)) + 7  # +7 for the role token\n",
      "\n",
      "    def get_message_role_from_dict(self, message_dict: dict) -> str:\n",
      "        \"\"\"\n",
      "        Get the role of the message from a message dictionary.\n",
      "\n",
      "        :param message_dict: A dictionary containing the role and content of the message.\n",
      "        :return: The role of the message.\n",
      "        \"\"\"\n",
      "        message_dict = check_dict(message_dict)\n",
      "        return message_dict[\"role\"]\n",
      "\n",
      "    def add_dict_to_thread(self, message_dict: dict) -> None:\n",
      "        \"\"\"\n",
      "        Add a message to the memory thread.\n",
      "\n",
      "        :param message_dict: A dictionary containing the role and content of the message.\n",
      "        \"\"\"\n",
      "        \n",
      "        new_message_row = self.dict_to_row(message_dict)\n",
      "\n",
      "        if (\n",
      "            self.max_memory is None\n",
      "            or self.total_tokens + new_message_row['tokens_count'] <= self.max_memory\n",
      "        ):\n",
      "            self.memory_thread = pl.concat([self.memory_thread, new_message_row], rechunk=True)\n",
      "            self.total_tokens = self.get_total_tokens_from_thread()\n",
      "        else:\n",
      "            print(\"The memory BaseThread is full, the last message was not added\")\n",
      "            \n",
      "\n",
      "    def remove_dict_from_thread(\n",
      "        self, message_dict: Union[Dict, None] = None, idx: Union[int, None] = None\n",
      "    ) -> None:\n",
      "        \n",
      "        if message_dict is None and idx is None:\n",
      "            raise Exception(\"You need to provide either a message_dict or an idx\")\n",
      "\n",
      "        elif idx is not None and idx < len(self.memory_thread):\n",
      "            \n",
      "            self.memory_thread = self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\"index\") != idx).drop(\"index\").collect()\n",
      "            self.total_tokens = self.get_total_tokens_from_thread() \n",
      "            return\n",
      "\n",
      "        elif message_dict is not None:\n",
      "            message_dict = check_dict(message_dict)\n",
      "            self.memory_thread = self.memory_thread.lazy().filter(self.memory_thread[\"content\"] != message_dict[\"content\"]).collect()\n",
      "            self.total_tokens = self.get_total_tokens_from_thread()\n",
      "            return\n",
      "\n",
      "        else:\n",
      "            raise Exception(\"Index was out bound and no corresponding content found.\")\n",
      "\n",
      "\n",
      "    def find_message(\n",
      "        self, message: Union[dict, str]\n",
      "        ) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\n",
      "        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\n",
      "\n",
      "        message = message if isinstance(message, str) else check_dict(message)\n",
      "        \n",
      "        if isinstance(message,str):\n",
      "            return self.memory_thread.lazy().filter(pl.col(\"content\") == message).collect()\n",
      "\n",
      "        else:\n",
      "            return self.memory_thread.lazy().filter((pl.col(\"content\") == message[\"content\"]) & (pl.col(\"role\") == message['role'])).collect()\n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "    def last_message(self, role: Union[str, None] = None) ->  pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get the last message in the memory thread with a specific role.\"\"\"\n",
      "        if role is None:\n",
      "            return self.memory_thread[-1]\n",
      "        else:\n",
      "            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[-1]\n",
      "\n",
      "    def first_message(self, role: Union[str, None] = None) -> pl.DataFrame: \n",
      "        \"\"\"\n",
      "        Get the first message in the memory thread with a specific role.\"\"\"\n",
      "        if role is None:\n",
      "            return self.memory_thread[0]\n",
      "        else:\n",
      "            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[0]\n",
      "\n",
      "\n",
      "    def messages_before( self, message: dict   ) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages before a specific message in the memory thread.\"\"\"\n",
      "        \n",
      "        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==message['content']) & (pl.col('role')==message['role'])).select('index').collect()[0][0]\n",
      "        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col('index')<index).collect()\n",
      "    \n",
      "    def messages_after( self, message: dict   ) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages after a specific message in the memory thread.\"\"\"\n",
      "        \n",
      "        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==message['content']) & (pl.col('role')==message['role'])).select('index').collect()[0][0]\n",
      "        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col('index')>index).collect()\n",
      "\n",
      "    def messages_between(\n",
      "        self, start_message: dict, end_message: dict ) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\n",
      "        start_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==start_message['content']) & (pl.col('role')==start_message['role'])).select('index').collect()[0][0]\n",
      "        end_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==end_message['content']) & (pl.col('role')==end_message['role'])).select('index').collect()[0][0]\n",
      "        return self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('index')<end_index) & (pl.col('index'))>start_index).collect()\n",
      "\n",
      "    def messages_more_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "        \n",
      "        return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('tokens_count'))>tokens).collect()\n",
      "\n",
      "    def messages_less_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "        \n",
      "        return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('tokens_count'))<tokens).collect()\n",
      "\n",
      "\n",
      "    def messages_after_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "        \n",
      "        return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('timestamp')>timestamp)).collect()\n",
      "\n",
      "    def messages_before_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "        \"\"\"\n",
      "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "        \n",
      "        return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('timestamp')<timestamp)).collect()\n",
      "\n",
      "    def select_col(self, feature: Union[List[str],str]):\n",
      "        return self.memory_thread[feature]\n",
      "\n",
      "    def filter_col(self, feature: str, filter: str):\n",
      "        try:\n",
      "            return self.memory_thread.lazy().filter(pl.col(feature) == filter).collect()\n",
      "        except Exception as e:\n",
      "            return str(e)\n",
      "\n",
      "\n",
      "\n",
      "    def token_bound_history(\n",
      "        self, max_tokens: int, role: Union[str, None] = None\n",
      "    ):\n",
      "\n",
      "        reversed_df = self.memory_thread.lazy().with_row_count(\"index\").reverse()   \n",
      "        reversed_df = reversed_df.with_columns(pl.col(\"tokens_count\").cumsum().alias(\"cum_tokens_count\"))  \n",
      "        filtered_df = reversed_df.filter((pl.col(\"cum_tokens_count\") <= max_tokens) & (pl.col(\"role\") != role)).collect()\n",
      "        messages = filtered_df['content']\n",
      "        indexes = filtered_df['index']\n",
      "\n",
      "        return messages,indexes\n",
      "\n",
      "    \n",
      "    def load_from_gpt_url(self, url: str):\n",
      "\n",
      "        response = requests.get(url)\n",
      "        if response.status_code == 200:\n",
      "        \n",
      "            soup = BeautifulSoup(response.text, 'html.parser')\n",
      "        else:\n",
      "            raise ValueError(f\"Non è stato possibile accedere alla pagina. Codice di stato: {response.status_code}\")\n",
      "            \n",
      "        \n",
      "        next_data = soup.find('script', id='__NEXT_DATA__')\n",
      "\n",
      "        if next_data is not None:\n",
      "\n",
      "            data_string = next_data.string  # pyright: ignore \n",
      "\n",
      "            json_obj = json.loads(data_string)  # pyright: ignore\n",
      "\n",
      "            conversation_data = json_obj['props']['pageProps']['serverResponse']['data']\n",
      "\n",
      "            messages = conversation_data['mapping']\n",
      "\n",
      "            for _,value in messages.items():\n",
      "                if ('parent' in value.keys()) and (value['message']['content']['parts'][0] != ''):\n",
      "                    message_dict = {'role':value['message']['author']['role'],\n",
      "                                    'content': value['message']['content']['parts'][0],\n",
      "                                    'timestamp': value['message']['create_time']}\n",
      "                    self.add_dict_to_thread(message_dict)\n",
      "            print(\"Conversation loaded succesfully\")\n",
      "        else:\n",
      "            raise ValueError(f\"Nessuna conversazione trovata a questo link!\")\n",
      "\n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: now\n",
      "Function call: len\n",
      "Function call: check_dict\n",
      "Function call: len\n",
      "Function call: check_dict\n",
      "Function call: print\n",
      "Function call: Exception\n",
      "Function call: len\n",
      "Function call: check_dict\n",
      "Function call: Exception\n",
      "Function call: isinstance\n",
      "Function call: check_dict\n",
      "Function call: isinstance\n",
      "Function call: str\n",
      "Function call: BeautifulSoup\n",
      "Function call: ValueError\n",
      "Function call: print\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef check_dict(message_dict):\\n    if (\\n        type(message_dict) is list\\n        and len(message_dict) == 1\\n        and type(message_dict[0]) is dict\\n    ):\\n        message_dict = message_dict[0]\\n    elif type(message_dict) is not dict:\\n        raise Exception(\\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\\n            message_dict,\\n            type(message_dict),\\n        )\\n    return message_dict\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef check_dict(message_dict):\\n    if (\\n        type(message_dict) is list\\n        and len(message_dict) == 1\\n        and type(message_dict[0]) is dict\\n    ):\\n        message_dict = message_dict[0]\\n    elif type(message_dict) is not dict:\\n        raise Exception(\\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\\n            message_dict,\\n            type(message_dict),\\n        )\\n    return message_dict\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef check_dict(message_dict):\\n    if (\\n        type(message_dict) is list\\n        and len(message_dict) == 1\\n        and type(message_dict[0]) is dict\\n    ):\\n        message_dict = message_dict[0]\\n    elif type(message_dict) is not dict:\\n        raise Exception(\\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\\n            message_dict,\\n            type(message_dict),\\n        )\\n    return message_dict\\n', '\\n\\ndef check_dict(message_dict):\\n    if (\\n        type(message_dict) is list\\n        and len(message_dict) == 1\\n        and type(message_dict[0]) is dict\\n    ):\\n        message_dict = message_dict[0]\\n    elif type(message_dict) is not dict:\\n        raise Exception(\\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\\n            message_dict,\\n            type(message_dict),\\n        )\\n    return message_dict\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    name: str = \"memory\",\n",
      "    max_memory: Optional[int] = None,\n",
      "    tokenizer: Optional[Any] = None,\n",
      "    save_path: str = 'threads'\n",
      "\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "        Initialize the BaseThread instance.\n",
      "\n",
      "        :param name: The name of the memory thread. Defaults to 'memory'.\n",
      "        :param max_memory: The maximum number of tokens allowed in the memory thread.\n",
      "                           Defaults to None, which means no limit.\n",
      "        :param tokenizer: The tokenizer to be used for tokenizing messages.\n",
      "                          Defaults to None, which means using the tiktoken encoding for the 'gpt-3.5-turbo' model.\n",
      "        \"\"\"\n",
      "    self.name = name\n",
      "    self.max_memory = max_memory\n",
      "    self.memory_schema = {\"role\": pl.Utf8, \"content\": pl.Utf8,\"timestamp\":pl.Float64,\"tokens_count\":pl.UInt16}\n",
      "    self.memory_thread: pl.DataFrame = pl.DataFrame(schema=self.memory_schema)\n",
      "    \"\"\" self.time_stamps = [] \"\"\"\n",
      "    \"\"\" self.message_tokens = [] \"\"\"\n",
      "    self.total_tokens = self.get_total_tokens_from_thread()\n",
      "    self.save_path = save_path\n",
      "    if tokenizer is None:\n",
      "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __getitem__(self, idx):\n",
      "    return self.memory_thread[idx]\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __len__(self):\n",
      "    return self.memory_thread.shape[0]\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def save(self, path: Union[str,None]) -> None:\n",
      "    if path is None:\n",
      "        path = os.path.join(self.save_path,f'{self.name}.parquet')\n",
      "    self.memory_thread.write_parquet(\n",
      "        file=path,\n",
      "        compression='zstd',\n",
      "        compression_level=None,\n",
      "        statistics=False,\n",
      "        row_group_size=None,\n",
      "        use_pyarrow=True,\n",
      "        pyarrow_options=None\n",
      "    )\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def load(self, path: Union[str,None]) -> None:\n",
      "    if path is None:\n",
      "        path = os.path.join(self.save_path,f'{self.name}.parquet')\n",
      "    self.memory_thread = pl.read_parquet(source = path,\n",
      "                    use_pyarrow= True,\n",
      "                    memory_map = True,\n",
      "                    )\n",
      "    \n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "def get_total_tokens_from_thread(self):\n",
      "    return self.memory_thread[\"tokens_count\"].sum()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def reset_memory(self) -> None:\n",
      "    self.memory_thread = pl.DataFrame(schema=self.memory_schema) \n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def dict_to_row(self, message_dict:Dict[str,str]) -> pl.DataFrame:\n",
      "    timestamp = message_dict['timestamp'] if 'timestamp' in message_dict else [now()]\n",
      "    return pl.DataFrame(schema=self.memory_schema,\n",
      "                        data={ \"role\":[message_dict[\"role\"]],\n",
      "                              \"content\":[message_dict[\"content\"]],\n",
      "                              \"timestamp\": timestamp,\n",
      "                              \"tokens_count\":[len(self.tokenizer.encode(message_dict[\"content\"]))+7]\n",
      "                              })\n",
      "\n",
      "Function call: now\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_message_tokens_from_dict(self, message_dict: dict) -> int:\n",
      "    \"\"\"\n",
      "        Calculate the number of tokens in a message, including the role token.\n",
      "\n",
      "        :param message_dict: A dictionary containing the role and content of the message.\n",
      "        :return: The total number of tokens in the message.\n",
      "        \"\"\"\n",
      "    message_dict = check_dict(message_dict)\n",
      "    message = message_dict[\"content\"]\n",
      "    return len(self.tokenizer.encode(message)) + 7  # +7 for the role token\n",
      "\n",
      "Function call: check_dict\n",
      "Function call: len\n",
      "Related codes: ['\\n\\ndef check_dict(message_dict):\\n    if (\\n        type(message_dict) is list\\n        and len(message_dict) == 1\\n        and type(message_dict[0]) is dict\\n    ):\\n        message_dict = message_dict[0]\\n    elif type(message_dict) is not dict:\\n        raise Exception(\\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\\n            message_dict,\\n            type(message_dict),\\n        )\\n    return message_dict\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_message_role_from_dict(self, message_dict: dict) -> str:\n",
      "    \"\"\"\n",
      "        Get the role of the message from a message dictionary.\n",
      "\n",
      "        :param message_dict: A dictionary containing the role and content of the message.\n",
      "        :return: The role of the message.\n",
      "        \"\"\"\n",
      "    message_dict = check_dict(message_dict)\n",
      "    return message_dict[\"role\"]\n",
      "\n",
      "Function call: check_dict\n",
      "Related codes: ['\\n\\ndef check_dict(message_dict):\\n    if (\\n        type(message_dict) is list\\n        and len(message_dict) == 1\\n        and type(message_dict[0]) is dict\\n    ):\\n        message_dict = message_dict[0]\\n    elif type(message_dict) is not dict:\\n        raise Exception(\\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\\n            message_dict,\\n            type(message_dict),\\n        )\\n    return message_dict\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def add_dict_to_thread(self, message_dict: dict) -> None:\n",
      "    \"\"\"\n",
      "        Add a message to the memory thread.\n",
      "\n",
      "        :param message_dict: A dictionary containing the role and content of the message.\n",
      "        \"\"\"\n",
      "    \n",
      "    new_message_row = self.dict_to_row(message_dict)\n",
      "\n",
      "    if (\n",
      "        self.max_memory is None\n",
      "        or self.total_tokens + new_message_row['tokens_count'] <= self.max_memory\n",
      "    ):\n",
      "        self.memory_thread = pl.concat([self.memory_thread, new_message_row], rechunk=True)\n",
      "        self.total_tokens = self.get_total_tokens_from_thread()\n",
      "    else:\n",
      "        print(\"The memory BaseThread is full, the last message was not added\")\n",
      "        \n",
      "\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def remove_dict_from_thread(\n",
      "    self, message_dict: Union[Dict, None] = None, idx: Union[int, None] = None\n",
      ") -> None:\n",
      "    \n",
      "    if message_dict is None and idx is None:\n",
      "        raise Exception(\"You need to provide either a message_dict or an idx\")\n",
      "\n",
      "    elif idx is not None and idx < len(self.memory_thread):\n",
      "        \n",
      "        self.memory_thread = self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\"index\") != idx).drop(\"index\").collect()\n",
      "        self.total_tokens = self.get_total_tokens_from_thread() \n",
      "        return\n",
      "\n",
      "    elif message_dict is not None:\n",
      "        message_dict = check_dict(message_dict)\n",
      "        self.memory_thread = self.memory_thread.lazy().filter(self.memory_thread[\"content\"] != message_dict[\"content\"]).collect()\n",
      "        self.total_tokens = self.get_total_tokens_from_thread()\n",
      "        return\n",
      "\n",
      "    else:\n",
      "        raise Exception(\"Index was out bound and no corresponding content found.\")\n",
      "\n",
      "Function call: Exception\n",
      "Function call: len\n",
      "Function call: check_dict\n",
      "Function call: Exception\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef check_dict(message_dict):\\n    if (\\n        type(message_dict) is list\\n        and len(message_dict) == 1\\n        and type(message_dict[0]) is dict\\n    ):\\n        message_dict = message_dict[0]\\n    elif type(message_dict) is not dict:\\n        raise Exception(\\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\\n            message_dict,\\n            type(message_dict),\\n        )\\n    return message_dict\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def find_message(\n",
      "    self, message: Union[dict, str]\n",
      "    ) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\n",
      "        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\n",
      "\n",
      "    message = message if isinstance(message, str) else check_dict(message)\n",
      "    \n",
      "    if isinstance(message,str):\n",
      "        return self.memory_thread.lazy().filter(pl.col(\"content\") == message).collect()\n",
      "\n",
      "    else:\n",
      "        return self.memory_thread.lazy().filter((pl.col(\"content\") == message[\"content\"]) & (pl.col(\"role\") == message['role'])).collect()\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "Function call: isinstance\n",
      "Function call: check_dict\n",
      "Function call: isinstance\n",
      "Related codes: ['\\n\\ndef check_dict(message_dict):\\n    if (\\n        type(message_dict) is list\\n        and len(message_dict) == 1\\n        and type(message_dict[0]) is dict\\n    ):\\n        message_dict = message_dict[0]\\n    elif type(message_dict) is not dict:\\n        raise Exception(\\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\\n            message_dict,\\n            type(message_dict),\\n        )\\n    return message_dict\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def last_message(self, role: Union[str, None] = None) ->  pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get the last message in the memory thread with a specific role.\"\"\"\n",
      "    if role is None:\n",
      "        return self.memory_thread[-1]\n",
      "    else:\n",
      "        return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[-1]\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def first_message(self, role: Union[str, None] = None) -> pl.DataFrame: \n",
      "    \"\"\"\n",
      "        Get the first message in the memory thread with a specific role.\"\"\"\n",
      "    if role is None:\n",
      "        return self.memory_thread[0]\n",
      "    else:\n",
      "        return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[0]\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def messages_before( self, message: dict   ) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages before a specific message in the memory thread.\"\"\"\n",
      "    \n",
      "    index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==message['content']) & (pl.col('role')==message['role'])).select('index').collect()[0][0]\n",
      "    return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col('index')<index).collect()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def messages_after( self, message: dict   ) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages after a specific message in the memory thread.\"\"\"\n",
      "    \n",
      "    index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==message['content']) & (pl.col('role')==message['role'])).select('index').collect()[0][0]\n",
      "    return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col('index')>index).collect()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def messages_between(\n",
      "    self, start_message: dict, end_message: dict ) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\n",
      "    start_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==start_message['content']) & (pl.col('role')==start_message['role'])).select('index').collect()[0][0]\n",
      "    end_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('content')==end_message['content']) & (pl.col('role')==end_message['role'])).select('index').collect()[0][0]\n",
      "    return self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col('index')<end_index) & (pl.col('index'))>start_index).collect()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def messages_more_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "    \n",
      "    return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('tokens_count'))>tokens).collect()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def messages_less_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "    \n",
      "    return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('tokens_count'))<tokens).collect()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def messages_after_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "    \n",
      "    return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('timestamp')>timestamp)).collect()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def messages_before_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\n",
      "    \"\"\"\n",
      "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
      "    \n",
      "    return self.memory_thread.lazy().filter((pl.col('role')==role) & (pl.col('timestamp')<timestamp)).collect()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def select_col(self, feature: Union[List[str],str]):\n",
      "    return self.memory_thread[feature]\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def filter_col(self, feature: str, filter: str):\n",
      "    try:\n",
      "        return self.memory_thread.lazy().filter(pl.col(feature) == filter).collect()\n",
      "    except Exception as e:\n",
      "        return str(e)\n",
      "\n",
      "Function call: str\n",
      "Related codes: ['\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "def token_bound_history(\n",
      "    self, max_tokens: int, role: Union[str, None] = None\n",
      "):\n",
      "\n",
      "    reversed_df = self.memory_thread.lazy().with_row_count(\"index\").reverse()   \n",
      "    reversed_df = reversed_df.with_columns(pl.col(\"tokens_count\").cumsum().alias(\"cum_tokens_count\"))  \n",
      "    filtered_df = reversed_df.filter((pl.col(\"cum_tokens_count\") <= max_tokens) & (pl.col(\"role\") != role)).collect()\n",
      "    messages = filtered_df['content']\n",
      "    indexes = filtered_df['index']\n",
      "\n",
      "    return messages,indexes\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def load_from_gpt_url(self, url: str):\n",
      "\n",
      "    response = requests.get(url)\n",
      "    if response.status_code == 200:\n",
      "        \n",
      "        soup = BeautifulSoup(response.text, 'html.parser')\n",
      "    else:\n",
      "        raise ValueError(f\"Non è stato possibile accedere alla pagina. Codice di stato: {response.status_code}\")\n",
      "        \n",
      "    \n",
      "    next_data = soup.find('script', id='__NEXT_DATA__')\n",
      "\n",
      "    if next_data is not None:\n",
      "\n",
      "        data_string = next_data.string  # pyright: ignore \n",
      "\n",
      "        json_obj = json.loads(data_string)  # pyright: ignore\n",
      "\n",
      "        conversation_data = json_obj['props']['pageProps']['serverResponse']['data']\n",
      "\n",
      "        messages = conversation_data['mapping']\n",
      "\n",
      "        for _,value in messages.items():\n",
      "            if ('parent' in value.keys()) and (value['message']['content']['parts'][0] != ''):\n",
      "                message_dict = {'role':value['message']['author']['role'],\n",
      "                                'content': value['message']['content']['parts'][0],\n",
      "                                'timestamp': value['message']['create_time']}\n",
      "                self.add_dict_to_thread(message_dict)\n",
      "        print(\"Conversation loaded succesfully\")\n",
      "    else:\n",
      "        raise ValueError(f\"Nessuna conversazione trovata a questo link!\")\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "Function call: BeautifulSoup\n",
      "Function call: ValueError\n",
      "Function call: print\n",
      "Function call: ValueError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class FifoThread(BaseThread):\n",
      "    \"\"\"FIFO Memory BaseThread, the oldest messages are removed first when reaching the max_memory limit, the memory is defined in terms of tokens,\n",
      "    outs are passe to the longterm_memory, lucid_memory is a redundant memory that stores all the messages\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self, name=\"fifo_memory\", max_memory=None, longterm_thread=None, redundant=True\n",
      "    ):\n",
      "\n",
      "        BaseThread.__init__(self, name=name, max_memory=None)\n",
      "        if redundant is True:\n",
      "            self.redundant_thread = BaseThread(name=\"lucid_memory\", max_memory=None)\n",
      "        else:\n",
      "            self.redundant_thread = None\n",
      "        if longterm_thread is None:\n",
      "            self.longterm_thread = BaseThread(name=\"longterm_memory\", max_memory=None)\n",
      "        else:\n",
      "            self.longterm_thread = longterm_thread\n",
      "        # create an alias for the memory_thread to make the code more readable\n",
      "        self.fifo_thread = self.memory_thread\n",
      "        self.max_memory = max_memory\n",
      "\n",
      "    def to_longterm(self, idx: int):\n",
      "        \"\"\"move the message at the index idx to the longterm_memory\"\"\"\n",
      "        # move the message at the index idx to the longterm_memory\n",
      "        display(\n",
      "            Markdown(\n",
      "                \"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(\n",
      "                    idx\n",
      "                )\n",
      "            )\n",
      "        )\n",
      "        # this should not remove everything\n",
      "        # there should be a check to make sure the thread isnt left empty\n",
      "        message = copy.deepcopy(self.memory_thread[idx])\n",
      "        # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\n",
      "        self.longterm_thread.add_message(message)\n",
      "        self.remove_message(idx=idx)\n",
      "\n",
      "    def add_message(self, message_dict: dict):\n",
      "        \"\"\"add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\n",
      "        # message_dict = {\"role\": role, \"content\": content}\n",
      "        # chek that the message_dict is a dictionary or a list of dictionaries\n",
      "        message_dict = check_dict(message_dict)\n",
      "        if self.redundant_thread is not None:\n",
      "            self.redundant_thread.add_message(message_dict)\n",
      "        message_tokens = self.get_message_tokens(message_dict)\n",
      "\n",
      "        if self.total_tokens + message_tokens > self.max_memory:\n",
      "            while self.total_tokens + message_tokens > self.max_memory:\n",
      "                if len(self.memory_thread) > 0:\n",
      "                    self.to_longterm(idx=0)\n",
      "                    message_tokens = self.get_message_tokens(message_dict)  # Update message_tokens\n",
      "                    self.total_tokens -= message_tokens  # Update self.total_tokens\n",
      "\n",
      "            super().add_message(message_dict)\n",
      "\n",
      "        else:\n",
      "            # add the message_dict to the memory_thread\n",
      "            # update the total number of tokens\n",
      "            super().add_message(message_dict)\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseThread\"]\n",
      "]\n",
      "Function call: BaseThread\n",
      "Function call: BaseThread\n",
      "Function call: display\n",
      "Function call: Markdown\n",
      "Function call: check_dict\n",
      "Function call: len\n",
      "Function call: super\n",
      "Function call: super\n",
      "Related codes: ['\\n\\nclass BaseThread:\\n    \"\"\"\\n    This class is used to keep track of the memory thread of a conversation and the total number of tokens.\\n    All conversation memories should subclass this class. If max_memory is None, it has\\n    no limit to the number of tokens that can be stored in the memory thread.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        name: str = \"memory\",\\n        max_memory: Optional[int] = None,\\n        tokenizer: Optional[Any] = None,\\n        save_path: str = \\'threads\\'\\n\\n    ) -> None:\\n        \"\"\"\\n        Initialize the BaseThread instance.\\n\\n        :param name: The name of the memory thread. Defaults to \\'memory\\'.\\n        :param max_memory: The maximum number of tokens allowed in the memory thread.\\n                           Defaults to None, which means no limit.\\n        :param tokenizer: The tokenizer to be used for tokenizing messages.\\n                          Defaults to None, which means using the tiktoken encoding for the \\'gpt-3.5-turbo\\' model.\\n        \"\"\"\\n        self.name = name\\n        self.max_memory = max_memory\\n        self.memory_schema = {\"role\": pl.Utf8, \"content\": pl.Utf8,\"timestamp\":pl.Float64,\"tokens_count\":pl.UInt16}\\n        self.memory_thread: pl.DataFrame = pl.DataFrame(schema=self.memory_schema)\\n        \"\"\" self.time_stamps = [] \"\"\"\\n        \"\"\" self.message_tokens = [] \"\"\"\\n        self.total_tokens = self.get_total_tokens_from_thread()\\n        self.save_path = save_path\\n        if tokenizer is None:\\n            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n\\n    def __getitem__(self, idx):\\n        return self.memory_thread[idx]\\n\\n    def __len__(self):\\n        return self.memory_thread.shape[0]\\n\\n    def save(self, path: Union[str,None]) -> None:\\n        if path is None:\\n            path = os.path.join(self.save_path,f\\'{self.name}.parquet\\')\\n        self.memory_thread.write_parquet(\\n            file=path,\\n            compression=\\'zstd\\',\\n            compression_level=None,\\n            statistics=False,\\n            row_group_size=None,\\n            use_pyarrow=True,\\n            pyarrow_options=None\\n        )\\n\\n    def load(self, path: Union[str,None]) -> None:\\n        if path is None:\\n            path = os.path.join(self.save_path,f\\'{self.name}.parquet\\')\\n        self.memory_thread = pl.read_parquet(source = path,\\n                        use_pyarrow= True,\\n                        memory_map = True,\\n                        )\\n        \\n    def get_total_tokens_from_thread(self):\\n        return self.memory_thread[\"tokens_count\"].sum()\\n\\n    def reset_memory(self) -> None:\\n        self.memory_thread = pl.DataFrame(schema=self.memory_schema) \\n\\n    def dict_to_row(self, message_dict:Dict[str,str]) -> pl.DataFrame:\\n        timestamp = message_dict[\\'timestamp\\'] if \\'timestamp\\' in message_dict else [now()]\\n        return pl.DataFrame(schema=self.memory_schema,\\n                            data={ \"role\":[message_dict[\"role\"]],\\n                                  \"content\":[message_dict[\"content\"]],\\n                                  \"timestamp\": timestamp,\\n                                  \"tokens_count\":[len(self.tokenizer.encode(message_dict[\"content\"]))+7]\\n                                  })\\n\\n    def get_message_tokens_from_dict(self, message_dict: dict) -> int:\\n        \"\"\"\\n        Calculate the number of tokens in a message, including the role token.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The total number of tokens in the message.\\n        \"\"\"\\n        message_dict = check_dict(message_dict)\\n        message = message_dict[\"content\"]\\n        return len(self.tokenizer.encode(message)) + 7  # +7 for the role token\\n\\n    def get_message_role_from_dict(self, message_dict: dict) -> str:\\n        \"\"\"\\n        Get the role of the message from a message dictionary.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The role of the message.\\n        \"\"\"\\n        message_dict = check_dict(message_dict)\\n        return message_dict[\"role\"]\\n\\n    def add_dict_to_thread(self, message_dict: dict) -> None:\\n        \"\"\"\\n        Add a message to the memory thread.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        \"\"\"\\n        \\n        new_message_row = self.dict_to_row(message_dict)\\n\\n        if (\\n            self.max_memory is None\\n            or self.total_tokens + new_message_row[\\'tokens_count\\'] <= self.max_memory\\n        ):\\n            self.memory_thread = pl.concat([self.memory_thread, new_message_row], rechunk=True)\\n            self.total_tokens = self.get_total_tokens_from_thread()\\n        else:\\n            print(\"The memory BaseThread is full, the last message was not added\")\\n            \\n\\n    def remove_dict_from_thread(\\n        self, message_dict: Union[Dict, None] = None, idx: Union[int, None] = None\\n    ) -> None:\\n        \\n        if message_dict is None and idx is None:\\n            raise Exception(\"You need to provide either a message_dict or an idx\")\\n\\n        elif idx is not None and idx < len(self.memory_thread):\\n            \\n            self.memory_thread = self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\"index\") != idx).drop(\"index\").collect()\\n            self.total_tokens = self.get_total_tokens_from_thread() \\n            return\\n\\n        elif message_dict is not None:\\n            message_dict = check_dict(message_dict)\\n            self.memory_thread = self.memory_thread.lazy().filter(self.memory_thread[\"content\"] != message_dict[\"content\"]).collect()\\n            self.total_tokens = self.get_total_tokens_from_thread()\\n            return\\n\\n        else:\\n            raise Exception(\"Index was out bound and no corresponding content found.\")\\n\\n\\n    def find_message(\\n        self, message: Union[dict, str]\\n        ) -> pl.DataFrame:\\n        \"\"\"\\n        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\\n        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\\n\\n        message = message if isinstance(message, str) else check_dict(message)\\n        \\n        if isinstance(message,str):\\n            return self.memory_thread.lazy().filter(pl.col(\"content\") == message).collect()\\n\\n        else:\\n            return self.memory_thread.lazy().filter((pl.col(\"content\") == message[\"content\"]) & (pl.col(\"role\") == message[\\'role\\'])).collect()\\n\\n\\n        \\n\\n    def last_message(self, role: Union[str, None] = None) ->  pl.DataFrame:\\n        \"\"\"\\n        Get the last message in the memory thread with a specific role.\"\"\"\\n        if role is None:\\n            return self.memory_thread[-1]\\n        else:\\n            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[-1]\\n\\n    def first_message(self, role: Union[str, None] = None) -> pl.DataFrame: \\n        \"\"\"\\n        Get the first message in the memory thread with a specific role.\"\"\"\\n        if role is None:\\n            return self.memory_thread[0]\\n        else:\\n            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[0]\\n\\n\\n    def messages_before( self, message: dict   ) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages before a specific message in the memory thread.\"\"\"\\n        \\n        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==message[\\'content\\']) & (pl.col(\\'role\\')==message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\\'index\\')<index).collect()\\n    \\n    def messages_after( self, message: dict   ) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages after a specific message in the memory thread.\"\"\"\\n        \\n        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==message[\\'content\\']) & (pl.col(\\'role\\')==message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\\'index\\')>index).collect()\\n\\n    def messages_between(\\n        self, start_message: dict, end_message: dict ) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\\n        start_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==start_message[\\'content\\']) & (pl.col(\\'role\\')==start_message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        end_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==end_message[\\'content\\']) & (pl.col(\\'role\\')==end_message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        return self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'index\\')<end_index) & (pl.col(\\'index\\'))>start_index).collect()\\n\\n    def messages_more_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'tokens_count\\'))>tokens).collect()\\n\\n    def messages_less_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'tokens_count\\'))<tokens).collect()\\n\\n\\n    def messages_after_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'timestamp\\')>timestamp)).collect()\\n\\n    def messages_before_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'timestamp\\')<timestamp)).collect()\\n\\n    def select_col(self, feature: Union[List[str],str]):\\n        return self.memory_thread[feature]\\n\\n    def filter_col(self, feature: str, filter: str):\\n        try:\\n            return self.memory_thread.lazy().filter(pl.col(feature) == filter).collect()\\n        except Exception as e:\\n            return str(e)\\n\\n\\n\\n    def token_bound_history(\\n        self, max_tokens: int, role: Union[str, None] = None\\n    ):\\n\\n        reversed_df = self.memory_thread.lazy().with_row_count(\"index\").reverse()   \\n        reversed_df = reversed_df.with_columns(pl.col(\"tokens_count\").cumsum().alias(\"cum_tokens_count\"))  \\n        filtered_df = reversed_df.filter((pl.col(\"cum_tokens_count\") <= max_tokens) & (pl.col(\"role\") != role)).collect()\\n        messages = filtered_df[\\'content\\']\\n        indexes = filtered_df[\\'index\\']\\n\\n        return messages,indexes\\n\\n    \\n    def load_from_gpt_url(self, url: str):\\n\\n        response = requests.get(url)\\n        if response.status_code == 200:\\n        \\n            soup = BeautifulSoup(response.text, \\'html.parser\\')\\n        else:\\n            raise ValueError(f\"Non è stato possibile accedere alla pagina. Codice di stato: {response.status_code}\")\\n            \\n        \\n        next_data = soup.find(\\'script\\', id=\\'__NEXT_DATA__\\')\\n\\n        if next_data is not None:\\n\\n            data_string = next_data.string  # pyright: ignore \\n\\n            json_obj = json.loads(data_string)  # pyright: ignore\\n\\n            conversation_data = json_obj[\\'props\\'][\\'pageProps\\'][\\'serverResponse\\'][\\'data\\']\\n\\n            messages = conversation_data[\\'mapping\\']\\n\\n            for _,value in messages.items():\\n                if (\\'parent\\' in value.keys()) and (value[\\'message\\'][\\'content\\'][\\'parts\\'][0] != \\'\\'):\\n                    message_dict = {\\'role\\':value[\\'message\\'][\\'author\\'][\\'role\\'],\\n                                    \\'content\\': value[\\'message\\'][\\'content\\'][\\'parts\\'][0],\\n                                    \\'timestamp\\': value[\\'message\\'][\\'create_time\\']}\\n                    self.add_dict_to_thread(message_dict)\\n            print(\"Conversation loaded succesfully\")\\n        else:\\n            raise ValueError(f\"Nessuna conversazione trovata a questo link!\")\\n\\n\\n\\n        \\n', '\\n\\nclass BaseThread:\\n    \"\"\"\\n    This class is used to keep track of the memory thread of a conversation and the total number of tokens.\\n    All conversation memories should subclass this class. If max_memory is None, it has\\n    no limit to the number of tokens that can be stored in the memory thread.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        name: str = \"memory\",\\n        max_memory: Optional[int] = None,\\n        tokenizer: Optional[Any] = None,\\n        save_path: str = \\'threads\\'\\n\\n    ) -> None:\\n        \"\"\"\\n        Initialize the BaseThread instance.\\n\\n        :param name: The name of the memory thread. Defaults to \\'memory\\'.\\n        :param max_memory: The maximum number of tokens allowed in the memory thread.\\n                           Defaults to None, which means no limit.\\n        :param tokenizer: The tokenizer to be used for tokenizing messages.\\n                          Defaults to None, which means using the tiktoken encoding for the \\'gpt-3.5-turbo\\' model.\\n        \"\"\"\\n        self.name = name\\n        self.max_memory = max_memory\\n        self.memory_schema = {\"role\": pl.Utf8, \"content\": pl.Utf8,\"timestamp\":pl.Float64,\"tokens_count\":pl.UInt16}\\n        self.memory_thread: pl.DataFrame = pl.DataFrame(schema=self.memory_schema)\\n        \"\"\" self.time_stamps = [] \"\"\"\\n        \"\"\" self.message_tokens = [] \"\"\"\\n        self.total_tokens = self.get_total_tokens_from_thread()\\n        self.save_path = save_path\\n        if tokenizer is None:\\n            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n\\n    def __getitem__(self, idx):\\n        return self.memory_thread[idx]\\n\\n    def __len__(self):\\n        return self.memory_thread.shape[0]\\n\\n    def save(self, path: Union[str,None]) -> None:\\n        if path is None:\\n            path = os.path.join(self.save_path,f\\'{self.name}.parquet\\')\\n        self.memory_thread.write_parquet(\\n            file=path,\\n            compression=\\'zstd\\',\\n            compression_level=None,\\n            statistics=False,\\n            row_group_size=None,\\n            use_pyarrow=True,\\n            pyarrow_options=None\\n        )\\n\\n    def load(self, path: Union[str,None]) -> None:\\n        if path is None:\\n            path = os.path.join(self.save_path,f\\'{self.name}.parquet\\')\\n        self.memory_thread = pl.read_parquet(source = path,\\n                        use_pyarrow= True,\\n                        memory_map = True,\\n                        )\\n        \\n    def get_total_tokens_from_thread(self):\\n        return self.memory_thread[\"tokens_count\"].sum()\\n\\n    def reset_memory(self) -> None:\\n        self.memory_thread = pl.DataFrame(schema=self.memory_schema) \\n\\n    def dict_to_row(self, message_dict:Dict[str,str]) -> pl.DataFrame:\\n        timestamp = message_dict[\\'timestamp\\'] if \\'timestamp\\' in message_dict else [now()]\\n        return pl.DataFrame(schema=self.memory_schema,\\n                            data={ \"role\":[message_dict[\"role\"]],\\n                                  \"content\":[message_dict[\"content\"]],\\n                                  \"timestamp\": timestamp,\\n                                  \"tokens_count\":[len(self.tokenizer.encode(message_dict[\"content\"]))+7]\\n                                  })\\n\\n    def get_message_tokens_from_dict(self, message_dict: dict) -> int:\\n        \"\"\"\\n        Calculate the number of tokens in a message, including the role token.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The total number of tokens in the message.\\n        \"\"\"\\n        message_dict = check_dict(message_dict)\\n        message = message_dict[\"content\"]\\n        return len(self.tokenizer.encode(message)) + 7  # +7 for the role token\\n\\n    def get_message_role_from_dict(self, message_dict: dict) -> str:\\n        \"\"\"\\n        Get the role of the message from a message dictionary.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The role of the message.\\n        \"\"\"\\n        message_dict = check_dict(message_dict)\\n        return message_dict[\"role\"]\\n\\n    def add_dict_to_thread(self, message_dict: dict) -> None:\\n        \"\"\"\\n        Add a message to the memory thread.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        \"\"\"\\n        \\n        new_message_row = self.dict_to_row(message_dict)\\n\\n        if (\\n            self.max_memory is None\\n            or self.total_tokens + new_message_row[\\'tokens_count\\'] <= self.max_memory\\n        ):\\n            self.memory_thread = pl.concat([self.memory_thread, new_message_row], rechunk=True)\\n            self.total_tokens = self.get_total_tokens_from_thread()\\n        else:\\n            print(\"The memory BaseThread is full, the last message was not added\")\\n            \\n\\n    def remove_dict_from_thread(\\n        self, message_dict: Union[Dict, None] = None, idx: Union[int, None] = None\\n    ) -> None:\\n        \\n        if message_dict is None and idx is None:\\n            raise Exception(\"You need to provide either a message_dict or an idx\")\\n\\n        elif idx is not None and idx < len(self.memory_thread):\\n            \\n            self.memory_thread = self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\"index\") != idx).drop(\"index\").collect()\\n            self.total_tokens = self.get_total_tokens_from_thread() \\n            return\\n\\n        elif message_dict is not None:\\n            message_dict = check_dict(message_dict)\\n            self.memory_thread = self.memory_thread.lazy().filter(self.memory_thread[\"content\"] != message_dict[\"content\"]).collect()\\n            self.total_tokens = self.get_total_tokens_from_thread()\\n            return\\n\\n        else:\\n            raise Exception(\"Index was out bound and no corresponding content found.\")\\n\\n\\n    def find_message(\\n        self, message: Union[dict, str]\\n        ) -> pl.DataFrame:\\n        \"\"\"\\n        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\\n        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\\n\\n        message = message if isinstance(message, str) else check_dict(message)\\n        \\n        if isinstance(message,str):\\n            return self.memory_thread.lazy().filter(pl.col(\"content\") == message).collect()\\n\\n        else:\\n            return self.memory_thread.lazy().filter((pl.col(\"content\") == message[\"content\"]) & (pl.col(\"role\") == message[\\'role\\'])).collect()\\n\\n\\n        \\n\\n    def last_message(self, role: Union[str, None] = None) ->  pl.DataFrame:\\n        \"\"\"\\n        Get the last message in the memory thread with a specific role.\"\"\"\\n        if role is None:\\n            return self.memory_thread[-1]\\n        else:\\n            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[-1]\\n\\n    def first_message(self, role: Union[str, None] = None) -> pl.DataFrame: \\n        \"\"\"\\n        Get the first message in the memory thread with a specific role.\"\"\"\\n        if role is None:\\n            return self.memory_thread[0]\\n        else:\\n            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[0]\\n\\n\\n    def messages_before( self, message: dict   ) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages before a specific message in the memory thread.\"\"\"\\n        \\n        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==message[\\'content\\']) & (pl.col(\\'role\\')==message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\\'index\\')<index).collect()\\n    \\n    def messages_after( self, message: dict   ) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages after a specific message in the memory thread.\"\"\"\\n        \\n        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==message[\\'content\\']) & (pl.col(\\'role\\')==message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\\'index\\')>index).collect()\\n\\n    def messages_between(\\n        self, start_message: dict, end_message: dict ) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\\n        start_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==start_message[\\'content\\']) & (pl.col(\\'role\\')==start_message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        end_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==end_message[\\'content\\']) & (pl.col(\\'role\\')==end_message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        return self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'index\\')<end_index) & (pl.col(\\'index\\'))>start_index).collect()\\n\\n    def messages_more_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'tokens_count\\'))>tokens).collect()\\n\\n    def messages_less_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'tokens_count\\'))<tokens).collect()\\n\\n\\n    def messages_after_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'timestamp\\')>timestamp)).collect()\\n\\n    def messages_before_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'timestamp\\')<timestamp)).collect()\\n\\n    def select_col(self, feature: Union[List[str],str]):\\n        return self.memory_thread[feature]\\n\\n    def filter_col(self, feature: str, filter: str):\\n        try:\\n            return self.memory_thread.lazy().filter(pl.col(feature) == filter).collect()\\n        except Exception as e:\\n            return str(e)\\n\\n\\n\\n    def token_bound_history(\\n        self, max_tokens: int, role: Union[str, None] = None\\n    ):\\n\\n        reversed_df = self.memory_thread.lazy().with_row_count(\"index\").reverse()   \\n        reversed_df = reversed_df.with_columns(pl.col(\"tokens_count\").cumsum().alias(\"cum_tokens_count\"))  \\n        filtered_df = reversed_df.filter((pl.col(\"cum_tokens_count\") <= max_tokens) & (pl.col(\"role\") != role)).collect()\\n        messages = filtered_df[\\'content\\']\\n        indexes = filtered_df[\\'index\\']\\n\\n        return messages,indexes\\n\\n    \\n    def load_from_gpt_url(self, url: str):\\n\\n        response = requests.get(url)\\n        if response.status_code == 200:\\n        \\n            soup = BeautifulSoup(response.text, \\'html.parser\\')\\n        else:\\n            raise ValueError(f\"Non è stato possibile accedere alla pagina. Codice di stato: {response.status_code}\")\\n            \\n        \\n        next_data = soup.find(\\'script\\', id=\\'__NEXT_DATA__\\')\\n\\n        if next_data is not None:\\n\\n            data_string = next_data.string  # pyright: ignore \\n\\n            json_obj = json.loads(data_string)  # pyright: ignore\\n\\n            conversation_data = json_obj[\\'props\\'][\\'pageProps\\'][\\'serverResponse\\'][\\'data\\']\\n\\n            messages = conversation_data[\\'mapping\\']\\n\\n            for _,value in messages.items():\\n                if (\\'parent\\' in value.keys()) and (value[\\'message\\'][\\'content\\'][\\'parts\\'][0] != \\'\\'):\\n                    message_dict = {\\'role\\':value[\\'message\\'][\\'author\\'][\\'role\\'],\\n                                    \\'content\\': value[\\'message\\'][\\'content\\'][\\'parts\\'][0],\\n                                    \\'timestamp\\': value[\\'message\\'][\\'create_time\\']}\\n                    self.add_dict_to_thread(message_dict)\\n            print(\"Conversation loaded succesfully\")\\n        else:\\n            raise ValueError(f\"Nessuna conversazione trovata a questo link!\")\\n\\n\\n\\n        \\n', '\\n\\ndef check_dict(message_dict):\\n    if (\\n        type(message_dict) is list\\n        and len(message_dict) == 1\\n        and type(message_dict[0]) is dict\\n    ):\\n        message_dict = message_dict[0]\\n    elif type(message_dict) is not dict:\\n        raise Exception(\\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\\n            message_dict,\\n            type(message_dict),\\n        )\\n    return message_dict\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(\n",
      "    self, name=\"fifo_memory\", max_memory=None, longterm_thread=None, redundant=True\n",
      "):\n",
      "\n",
      "    BaseThread.__init__(self, name=name, max_memory=None)\n",
      "    if redundant is True:\n",
      "        self.redundant_thread = BaseThread(name=\"lucid_memory\", max_memory=None)\n",
      "    else:\n",
      "        self.redundant_thread = None\n",
      "    if longterm_thread is None:\n",
      "        self.longterm_thread = BaseThread(name=\"longterm_memory\", max_memory=None)\n",
      "    else:\n",
      "        self.longterm_thread = longterm_thread\n",
      "    # create an alias for the memory_thread to make the code more readable\n",
      "    self.fifo_thread = self.memory_thread\n",
      "    self.max_memory = max_memory\n",
      "\n",
      "Function call: BaseThread\n",
      "Function call: BaseThread\n",
      "Related codes: ['\\n\\nclass BaseThread:\\n    \"\"\"\\n    This class is used to keep track of the memory thread of a conversation and the total number of tokens.\\n    All conversation memories should subclass this class. If max_memory is None, it has\\n    no limit to the number of tokens that can be stored in the memory thread.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        name: str = \"memory\",\\n        max_memory: Optional[int] = None,\\n        tokenizer: Optional[Any] = None,\\n        save_path: str = \\'threads\\'\\n\\n    ) -> None:\\n        \"\"\"\\n        Initialize the BaseThread instance.\\n\\n        :param name: The name of the memory thread. Defaults to \\'memory\\'.\\n        :param max_memory: The maximum number of tokens allowed in the memory thread.\\n                           Defaults to None, which means no limit.\\n        :param tokenizer: The tokenizer to be used for tokenizing messages.\\n                          Defaults to None, which means using the tiktoken encoding for the \\'gpt-3.5-turbo\\' model.\\n        \"\"\"\\n        self.name = name\\n        self.max_memory = max_memory\\n        self.memory_schema = {\"role\": pl.Utf8, \"content\": pl.Utf8,\"timestamp\":pl.Float64,\"tokens_count\":pl.UInt16}\\n        self.memory_thread: pl.DataFrame = pl.DataFrame(schema=self.memory_schema)\\n        \"\"\" self.time_stamps = [] \"\"\"\\n        \"\"\" self.message_tokens = [] \"\"\"\\n        self.total_tokens = self.get_total_tokens_from_thread()\\n        self.save_path = save_path\\n        if tokenizer is None:\\n            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n\\n    def __getitem__(self, idx):\\n        return self.memory_thread[idx]\\n\\n    def __len__(self):\\n        return self.memory_thread.shape[0]\\n\\n    def save(self, path: Union[str,None]) -> None:\\n        if path is None:\\n            path = os.path.join(self.save_path,f\\'{self.name}.parquet\\')\\n        self.memory_thread.write_parquet(\\n            file=path,\\n            compression=\\'zstd\\',\\n            compression_level=None,\\n            statistics=False,\\n            row_group_size=None,\\n            use_pyarrow=True,\\n            pyarrow_options=None\\n        )\\n\\n    def load(self, path: Union[str,None]) -> None:\\n        if path is None:\\n            path = os.path.join(self.save_path,f\\'{self.name}.parquet\\')\\n        self.memory_thread = pl.read_parquet(source = path,\\n                        use_pyarrow= True,\\n                        memory_map = True,\\n                        )\\n        \\n    def get_total_tokens_from_thread(self):\\n        return self.memory_thread[\"tokens_count\"].sum()\\n\\n    def reset_memory(self) -> None:\\n        self.memory_thread = pl.DataFrame(schema=self.memory_schema) \\n\\n    def dict_to_row(self, message_dict:Dict[str,str]) -> pl.DataFrame:\\n        timestamp = message_dict[\\'timestamp\\'] if \\'timestamp\\' in message_dict else [now()]\\n        return pl.DataFrame(schema=self.memory_schema,\\n                            data={ \"role\":[message_dict[\"role\"]],\\n                                  \"content\":[message_dict[\"content\"]],\\n                                  \"timestamp\": timestamp,\\n                                  \"tokens_count\":[len(self.tokenizer.encode(message_dict[\"content\"]))+7]\\n                                  })\\n\\n    def get_message_tokens_from_dict(self, message_dict: dict) -> int:\\n        \"\"\"\\n        Calculate the number of tokens in a message, including the role token.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The total number of tokens in the message.\\n        \"\"\"\\n        message_dict = check_dict(message_dict)\\n        message = message_dict[\"content\"]\\n        return len(self.tokenizer.encode(message)) + 7  # +7 for the role token\\n\\n    def get_message_role_from_dict(self, message_dict: dict) -> str:\\n        \"\"\"\\n        Get the role of the message from a message dictionary.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The role of the message.\\n        \"\"\"\\n        message_dict = check_dict(message_dict)\\n        return message_dict[\"role\"]\\n\\n    def add_dict_to_thread(self, message_dict: dict) -> None:\\n        \"\"\"\\n        Add a message to the memory thread.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        \"\"\"\\n        \\n        new_message_row = self.dict_to_row(message_dict)\\n\\n        if (\\n            self.max_memory is None\\n            or self.total_tokens + new_message_row[\\'tokens_count\\'] <= self.max_memory\\n        ):\\n            self.memory_thread = pl.concat([self.memory_thread, new_message_row], rechunk=True)\\n            self.total_tokens = self.get_total_tokens_from_thread()\\n        else:\\n            print(\"The memory BaseThread is full, the last message was not added\")\\n            \\n\\n    def remove_dict_from_thread(\\n        self, message_dict: Union[Dict, None] = None, idx: Union[int, None] = None\\n    ) -> None:\\n        \\n        if message_dict is None and idx is None:\\n            raise Exception(\"You need to provide either a message_dict or an idx\")\\n\\n        elif idx is not None and idx < len(self.memory_thread):\\n            \\n            self.memory_thread = self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\"index\") != idx).drop(\"index\").collect()\\n            self.total_tokens = self.get_total_tokens_from_thread() \\n            return\\n\\n        elif message_dict is not None:\\n            message_dict = check_dict(message_dict)\\n            self.memory_thread = self.memory_thread.lazy().filter(self.memory_thread[\"content\"] != message_dict[\"content\"]).collect()\\n            self.total_tokens = self.get_total_tokens_from_thread()\\n            return\\n\\n        else:\\n            raise Exception(\"Index was out bound and no corresponding content found.\")\\n\\n\\n    def find_message(\\n        self, message: Union[dict, str]\\n        ) -> pl.DataFrame:\\n        \"\"\"\\n        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\\n        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\\n\\n        message = message if isinstance(message, str) else check_dict(message)\\n        \\n        if isinstance(message,str):\\n            return self.memory_thread.lazy().filter(pl.col(\"content\") == message).collect()\\n\\n        else:\\n            return self.memory_thread.lazy().filter((pl.col(\"content\") == message[\"content\"]) & (pl.col(\"role\") == message[\\'role\\'])).collect()\\n\\n\\n        \\n\\n    def last_message(self, role: Union[str, None] = None) ->  pl.DataFrame:\\n        \"\"\"\\n        Get the last message in the memory thread with a specific role.\"\"\"\\n        if role is None:\\n            return self.memory_thread[-1]\\n        else:\\n            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[-1]\\n\\n    def first_message(self, role: Union[str, None] = None) -> pl.DataFrame: \\n        \"\"\"\\n        Get the first message in the memory thread with a specific role.\"\"\"\\n        if role is None:\\n            return self.memory_thread[0]\\n        else:\\n            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[0]\\n\\n\\n    def messages_before( self, message: dict   ) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages before a specific message in the memory thread.\"\"\"\\n        \\n        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==message[\\'content\\']) & (pl.col(\\'role\\')==message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\\'index\\')<index).collect()\\n    \\n    def messages_after( self, message: dict   ) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages after a specific message in the memory thread.\"\"\"\\n        \\n        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==message[\\'content\\']) & (pl.col(\\'role\\')==message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\\'index\\')>index).collect()\\n\\n    def messages_between(\\n        self, start_message: dict, end_message: dict ) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\\n        start_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==start_message[\\'content\\']) & (pl.col(\\'role\\')==start_message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        end_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==end_message[\\'content\\']) & (pl.col(\\'role\\')==end_message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        return self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'index\\')<end_index) & (pl.col(\\'index\\'))>start_index).collect()\\n\\n    def messages_more_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'tokens_count\\'))>tokens).collect()\\n\\n    def messages_less_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'tokens_count\\'))<tokens).collect()\\n\\n\\n    def messages_after_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'timestamp\\')>timestamp)).collect()\\n\\n    def messages_before_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'timestamp\\')<timestamp)).collect()\\n\\n    def select_col(self, feature: Union[List[str],str]):\\n        return self.memory_thread[feature]\\n\\n    def filter_col(self, feature: str, filter: str):\\n        try:\\n            return self.memory_thread.lazy().filter(pl.col(feature) == filter).collect()\\n        except Exception as e:\\n            return str(e)\\n\\n\\n\\n    def token_bound_history(\\n        self, max_tokens: int, role: Union[str, None] = None\\n    ):\\n\\n        reversed_df = self.memory_thread.lazy().with_row_count(\"index\").reverse()   \\n        reversed_df = reversed_df.with_columns(pl.col(\"tokens_count\").cumsum().alias(\"cum_tokens_count\"))  \\n        filtered_df = reversed_df.filter((pl.col(\"cum_tokens_count\") <= max_tokens) & (pl.col(\"role\") != role)).collect()\\n        messages = filtered_df[\\'content\\']\\n        indexes = filtered_df[\\'index\\']\\n\\n        return messages,indexes\\n\\n    \\n    def load_from_gpt_url(self, url: str):\\n\\n        response = requests.get(url)\\n        if response.status_code == 200:\\n        \\n            soup = BeautifulSoup(response.text, \\'html.parser\\')\\n        else:\\n            raise ValueError(f\"Non è stato possibile accedere alla pagina. Codice di stato: {response.status_code}\")\\n            \\n        \\n        next_data = soup.find(\\'script\\', id=\\'__NEXT_DATA__\\')\\n\\n        if next_data is not None:\\n\\n            data_string = next_data.string  # pyright: ignore \\n\\n            json_obj = json.loads(data_string)  # pyright: ignore\\n\\n            conversation_data = json_obj[\\'props\\'][\\'pageProps\\'][\\'serverResponse\\'][\\'data\\']\\n\\n            messages = conversation_data[\\'mapping\\']\\n\\n            for _,value in messages.items():\\n                if (\\'parent\\' in value.keys()) and (value[\\'message\\'][\\'content\\'][\\'parts\\'][0] != \\'\\'):\\n                    message_dict = {\\'role\\':value[\\'message\\'][\\'author\\'][\\'role\\'],\\n                                    \\'content\\': value[\\'message\\'][\\'content\\'][\\'parts\\'][0],\\n                                    \\'timestamp\\': value[\\'message\\'][\\'create_time\\']}\\n                    self.add_dict_to_thread(message_dict)\\n            print(\"Conversation loaded succesfully\")\\n        else:\\n            raise ValueError(f\"Nessuna conversazione trovata a questo link!\")\\n\\n\\n\\n        \\n', '\\n\\nclass BaseThread:\\n    \"\"\"\\n    This class is used to keep track of the memory thread of a conversation and the total number of tokens.\\n    All conversation memories should subclass this class. If max_memory is None, it has\\n    no limit to the number of tokens that can be stored in the memory thread.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        name: str = \"memory\",\\n        max_memory: Optional[int] = None,\\n        tokenizer: Optional[Any] = None,\\n        save_path: str = \\'threads\\'\\n\\n    ) -> None:\\n        \"\"\"\\n        Initialize the BaseThread instance.\\n\\n        :param name: The name of the memory thread. Defaults to \\'memory\\'.\\n        :param max_memory: The maximum number of tokens allowed in the memory thread.\\n                           Defaults to None, which means no limit.\\n        :param tokenizer: The tokenizer to be used for tokenizing messages.\\n                          Defaults to None, which means using the tiktoken encoding for the \\'gpt-3.5-turbo\\' model.\\n        \"\"\"\\n        self.name = name\\n        self.max_memory = max_memory\\n        self.memory_schema = {\"role\": pl.Utf8, \"content\": pl.Utf8,\"timestamp\":pl.Float64,\"tokens_count\":pl.UInt16}\\n        self.memory_thread: pl.DataFrame = pl.DataFrame(schema=self.memory_schema)\\n        \"\"\" self.time_stamps = [] \"\"\"\\n        \"\"\" self.message_tokens = [] \"\"\"\\n        self.total_tokens = self.get_total_tokens_from_thread()\\n        self.save_path = save_path\\n        if tokenizer is None:\\n            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n\\n    def __getitem__(self, idx):\\n        return self.memory_thread[idx]\\n\\n    def __len__(self):\\n        return self.memory_thread.shape[0]\\n\\n    def save(self, path: Union[str,None]) -> None:\\n        if path is None:\\n            path = os.path.join(self.save_path,f\\'{self.name}.parquet\\')\\n        self.memory_thread.write_parquet(\\n            file=path,\\n            compression=\\'zstd\\',\\n            compression_level=None,\\n            statistics=False,\\n            row_group_size=None,\\n            use_pyarrow=True,\\n            pyarrow_options=None\\n        )\\n\\n    def load(self, path: Union[str,None]) -> None:\\n        if path is None:\\n            path = os.path.join(self.save_path,f\\'{self.name}.parquet\\')\\n        self.memory_thread = pl.read_parquet(source = path,\\n                        use_pyarrow= True,\\n                        memory_map = True,\\n                        )\\n        \\n    def get_total_tokens_from_thread(self):\\n        return self.memory_thread[\"tokens_count\"].sum()\\n\\n    def reset_memory(self) -> None:\\n        self.memory_thread = pl.DataFrame(schema=self.memory_schema) \\n\\n    def dict_to_row(self, message_dict:Dict[str,str]) -> pl.DataFrame:\\n        timestamp = message_dict[\\'timestamp\\'] if \\'timestamp\\' in message_dict else [now()]\\n        return pl.DataFrame(schema=self.memory_schema,\\n                            data={ \"role\":[message_dict[\"role\"]],\\n                                  \"content\":[message_dict[\"content\"]],\\n                                  \"timestamp\": timestamp,\\n                                  \"tokens_count\":[len(self.tokenizer.encode(message_dict[\"content\"]))+7]\\n                                  })\\n\\n    def get_message_tokens_from_dict(self, message_dict: dict) -> int:\\n        \"\"\"\\n        Calculate the number of tokens in a message, including the role token.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The total number of tokens in the message.\\n        \"\"\"\\n        message_dict = check_dict(message_dict)\\n        message = message_dict[\"content\"]\\n        return len(self.tokenizer.encode(message)) + 7  # +7 for the role token\\n\\n    def get_message_role_from_dict(self, message_dict: dict) -> str:\\n        \"\"\"\\n        Get the role of the message from a message dictionary.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The role of the message.\\n        \"\"\"\\n        message_dict = check_dict(message_dict)\\n        return message_dict[\"role\"]\\n\\n    def add_dict_to_thread(self, message_dict: dict) -> None:\\n        \"\"\"\\n        Add a message to the memory thread.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        \"\"\"\\n        \\n        new_message_row = self.dict_to_row(message_dict)\\n\\n        if (\\n            self.max_memory is None\\n            or self.total_tokens + new_message_row[\\'tokens_count\\'] <= self.max_memory\\n        ):\\n            self.memory_thread = pl.concat([self.memory_thread, new_message_row], rechunk=True)\\n            self.total_tokens = self.get_total_tokens_from_thread()\\n        else:\\n            print(\"The memory BaseThread is full, the last message was not added\")\\n            \\n\\n    def remove_dict_from_thread(\\n        self, message_dict: Union[Dict, None] = None, idx: Union[int, None] = None\\n    ) -> None:\\n        \\n        if message_dict is None and idx is None:\\n            raise Exception(\"You need to provide either a message_dict or an idx\")\\n\\n        elif idx is not None and idx < len(self.memory_thread):\\n            \\n            self.memory_thread = self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\"index\") != idx).drop(\"index\").collect()\\n            self.total_tokens = self.get_total_tokens_from_thread() \\n            return\\n\\n        elif message_dict is not None:\\n            message_dict = check_dict(message_dict)\\n            self.memory_thread = self.memory_thread.lazy().filter(self.memory_thread[\"content\"] != message_dict[\"content\"]).collect()\\n            self.total_tokens = self.get_total_tokens_from_thread()\\n            return\\n\\n        else:\\n            raise Exception(\"Index was out bound and no corresponding content found.\")\\n\\n\\n    def find_message(\\n        self, message: Union[dict, str]\\n        ) -> pl.DataFrame:\\n        \"\"\"\\n        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\\n        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\\n\\n        message = message if isinstance(message, str) else check_dict(message)\\n        \\n        if isinstance(message,str):\\n            return self.memory_thread.lazy().filter(pl.col(\"content\") == message).collect()\\n\\n        else:\\n            return self.memory_thread.lazy().filter((pl.col(\"content\") == message[\"content\"]) & (pl.col(\"role\") == message[\\'role\\'])).collect()\\n\\n\\n        \\n\\n    def last_message(self, role: Union[str, None] = None) ->  pl.DataFrame:\\n        \"\"\"\\n        Get the last message in the memory thread with a specific role.\"\"\"\\n        if role is None:\\n            return self.memory_thread[-1]\\n        else:\\n            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[-1]\\n\\n    def first_message(self, role: Union[str, None] = None) -> pl.DataFrame: \\n        \"\"\"\\n        Get the first message in the memory thread with a specific role.\"\"\"\\n        if role is None:\\n            return self.memory_thread[0]\\n        else:\\n            return self.memory_thread.lazy().filter(pl.col(\"role\") == role).collect()[0]\\n\\n\\n    def messages_before( self, message: dict   ) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages before a specific message in the memory thread.\"\"\"\\n        \\n        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==message[\\'content\\']) & (pl.col(\\'role\\')==message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\\'index\\')<index).collect()\\n    \\n    def messages_after( self, message: dict   ) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages after a specific message in the memory thread.\"\"\"\\n        \\n        index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==message[\\'content\\']) & (pl.col(\\'role\\')==message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        return self.memory_thread.lazy().with_row_count(\"index\").filter(pl.col(\\'index\\')>index).collect()\\n\\n    def messages_between(\\n        self, start_message: dict, end_message: dict ) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\\n        start_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==start_message[\\'content\\']) & (pl.col(\\'role\\')==start_message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        end_index = self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'content\\')==end_message[\\'content\\']) & (pl.col(\\'role\\')==end_message[\\'role\\'])).select(\\'index\\').collect()[0][0]\\n        return self.memory_thread.lazy().with_row_count(\"index\").filter((pl.col(\\'index\\')<end_index) & (pl.col(\\'index\\'))>start_index).collect()\\n\\n    def messages_more_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'tokens_count\\'))>tokens).collect()\\n\\n    def messages_less_tokens(self, tokens: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'tokens_count\\'))<tokens).collect()\\n\\n\\n    def messages_after_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'timestamp\\')>timestamp)).collect()\\n\\n    def messages_before_time(self, timestamp: int, role: Union[str, None] = None) -> pl.DataFrame:\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        \\n        return self.memory_thread.lazy().filter((pl.col(\\'role\\')==role) & (pl.col(\\'timestamp\\')<timestamp)).collect()\\n\\n    def select_col(self, feature: Union[List[str],str]):\\n        return self.memory_thread[feature]\\n\\n    def filter_col(self, feature: str, filter: str):\\n        try:\\n            return self.memory_thread.lazy().filter(pl.col(feature) == filter).collect()\\n        except Exception as e:\\n            return str(e)\\n\\n\\n\\n    def token_bound_history(\\n        self, max_tokens: int, role: Union[str, None] = None\\n    ):\\n\\n        reversed_df = self.memory_thread.lazy().with_row_count(\"index\").reverse()   \\n        reversed_df = reversed_df.with_columns(pl.col(\"tokens_count\").cumsum().alias(\"cum_tokens_count\"))  \\n        filtered_df = reversed_df.filter((pl.col(\"cum_tokens_count\") <= max_tokens) & (pl.col(\"role\") != role)).collect()\\n        messages = filtered_df[\\'content\\']\\n        indexes = filtered_df[\\'index\\']\\n\\n        return messages,indexes\\n\\n    \\n    def load_from_gpt_url(self, url: str):\\n\\n        response = requests.get(url)\\n        if response.status_code == 200:\\n        \\n            soup = BeautifulSoup(response.text, \\'html.parser\\')\\n        else:\\n            raise ValueError(f\"Non è stato possibile accedere alla pagina. Codice di stato: {response.status_code}\")\\n            \\n        \\n        next_data = soup.find(\\'script\\', id=\\'__NEXT_DATA__\\')\\n\\n        if next_data is not None:\\n\\n            data_string = next_data.string  # pyright: ignore \\n\\n            json_obj = json.loads(data_string)  # pyright: ignore\\n\\n            conversation_data = json_obj[\\'props\\'][\\'pageProps\\'][\\'serverResponse\\'][\\'data\\']\\n\\n            messages = conversation_data[\\'mapping\\']\\n\\n            for _,value in messages.items():\\n                if (\\'parent\\' in value.keys()) and (value[\\'message\\'][\\'content\\'][\\'parts\\'][0] != \\'\\'):\\n                    message_dict = {\\'role\\':value[\\'message\\'][\\'author\\'][\\'role\\'],\\n                                    \\'content\\': value[\\'message\\'][\\'content\\'][\\'parts\\'][0],\\n                                    \\'timestamp\\': value[\\'message\\'][\\'create_time\\']}\\n                    self.add_dict_to_thread(message_dict)\\n            print(\"Conversation loaded succesfully\")\\n        else:\\n            raise ValueError(f\"Nessuna conversazione trovata a questo link!\")\\n\\n\\n\\n        \\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def to_longterm(self, idx: int):\n",
      "    \"\"\"move the message at the index idx to the longterm_memory\"\"\"\n",
      "    # move the message at the index idx to the longterm_memory\n",
      "    display(\n",
      "        Markdown(\n",
      "            \"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(\n",
      "                idx\n",
      "            )\n",
      "        )\n",
      "    )\n",
      "    # this should not remove everything\n",
      "    # there should be a check to make sure the thread isnt left empty\n",
      "    message = copy.deepcopy(self.memory_thread[idx])\n",
      "    # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\n",
      "    self.longterm_thread.add_message(message)\n",
      "    self.remove_message(idx=idx)\n",
      "\n",
      "Function call: display\n",
      "Function call: Markdown\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def add_message(self, message_dict: dict):\n",
      "    \"\"\"add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\n",
      "    # message_dict = {\"role\": role, \"content\": content}\n",
      "    # chek that the message_dict is a dictionary or a list of dictionaries\n",
      "    message_dict = check_dict(message_dict)\n",
      "    if self.redundant_thread is not None:\n",
      "        self.redundant_thread.add_message(message_dict)\n",
      "    message_tokens = self.get_message_tokens(message_dict)\n",
      "\n",
      "    if self.total_tokens + message_tokens > self.max_memory:\n",
      "        while self.total_tokens + message_tokens > self.max_memory:\n",
      "            if len(self.memory_thread) > 0:\n",
      "                self.to_longterm(idx=0)\n",
      "                message_tokens = self.get_message_tokens(message_dict)  # Update message_tokens\n",
      "                self.total_tokens -= message_tokens  # Update self.total_tokens\n",
      "\n",
      "        super().add_message(message_dict)\n",
      "\n",
      "    else:\n",
      "        # add the message_dict to the memory_thread\n",
      "        # update the total number of tokens\n",
      "        super().add_message(message_dict)\n",
      "\n",
      "Function call: check_dict\n",
      "Function call: len\n",
      "Function call: super\n",
      "Function call: super\n",
      "Related codes: ['\\n\\ndef check_dict(message_dict):\\n    if (\\n        type(message_dict) is list\\n        and len(message_dict) == 1\\n        and type(message_dict[0]) is dict\\n    ):\\n        message_dict = message_dict[0]\\n    elif type(message_dict) is not dict:\\n        raise Exception(\\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\\n            message_dict,\\n            type(message_dict),\\n        )\\n    return message_dict\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "class MemoryIndex(NpIndex):\n",
      "    \"\"\"\n",
      "    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\n",
      "    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\n",
      "    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \n",
      "    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\n",
      "    \"\"\"\n",
      "    @staticmethod\n",
      "    def check_uniform_context_type(context: List[Any]) -> None:\n",
      "        \"\"\"Check if all context elements are of the same type.\"\"\"\n",
      "        if not all(isinstance(x, type(context[0])) for x in context):\n",
      "            raise ValueError(\"All context elements must be of the same type.\")\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        values: Optional[List[str]] = None,\n",
      "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "        context: Optional[List[Any]] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        load: bool = False,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "        markdown: str = \"text/markdown\",\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ):\n",
      "        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\n",
      "        if context is not None and len(context) != len(values):\n",
      "            raise ValueError(\"The context must have the same length as the values\")\n",
      "\n",
      "\n",
      "        self.markdown = markdown\n",
      "        if context is not None and values is not None:\n",
      "            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\n",
      "\n",
      "        if context is not None:\n",
      "            self.check_uniform_context_type(context)\n",
      "            self.context_type = type(context[0])\n",
      "\n",
      "\n",
      "    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\n",
      "        \"\"\" get the context of a value id or embedding or a list of \"\"\"\n",
      "        if isinstance(identifier, list):\n",
      "            return [self.get_context(value) for value in identifier]\n",
      "        else:\n",
      "            id = self.identify_input(identifier)\n",
      "            value = self.values[id]\n",
      "            return self.context[value]\n",
      "\n",
      "    def clean_context(self):\n",
      "        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\n",
      "        self.context = {value: self.context[value] for value in self.values}\n",
      "\n",
      "    def add_to_context(self, value: str, context: Any):\n",
      "        \"\"\" add a context to a value \"\"\"\n",
      "        if not isinstance(context, self.context_type):\n",
      "            raise ValueError(\"The context must be of the same type as the other contexts\")\n",
      "        if value in self.values:\n",
      "            if value not in self.context:\n",
      "                self.context[value] = []\n",
      "            self.context[value].append(context)\n",
      "\n",
      "    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\n",
      "        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\n",
      "        if isinstance(values, str):\n",
      "            values = [values]\n",
      "        NpIndex.add(self, values, embedding)\n",
      "        if context is not None:\n",
      "            for value, cont in zip(values, context):\n",
      "                self.add_to_context(value, cont)\n",
      "\n",
      "    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\n",
      "        if not isinstance(identifier, list):\n",
      "            id = self.identify_input(identifier)\n",
      "            value = self.values[id]\n",
      "        NpIndex.remove(self, identifier)\n",
      "        if not isinstance(identifier, list):\n",
      "            self.context.pop(value)\n",
      "\n",
      "    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n",
      "        #recover value from old_identifier\n",
      "        if new_context is not None:\n",
      "            if not isinstance(new_context, self.context_type):\n",
      "                raise ValueError(\"The context must be of the same type as the other contexts\")\n",
      "        if not isinstance(old_identifier, list):\n",
      "            old_id = self.identify_input(old_identifier)\n",
      "            old_value = self.values[old_id]\n",
      "            # Only perform the update if the old_value is not the same as the new_value.\n",
      "            if old_value != new_value:\n",
      "                NpIndex.update(self, old_identifier, new_value, new_embedding)\n",
      "\n",
      "            if new_context is not None:\n",
      "                self.context[new_value] = [new_context]\n",
      "            else:\n",
      "                self.context[new_value] = self.context.pop(old_value)\n",
      "        else:\n",
      "            self.context[new_value] = self.context.pop(old_value)\n",
      "\n",
      "    @classmethod\n",
      "    def from_pandas(\n",
      "        cls,\n",
      "        data_frame: Union[pd.DataFrame, str],\n",
      "        value_column: str,\n",
      "        embeddings_column: Optional[str] = None,\n",
      "        context_columns: Optional[List[str]] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "        markdown: str = \"text/markdown\",\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ) -> \"MemoryIndex\":\n",
      "        if (\n",
      "            isinstance(data_frame, str)\n",
      "            and data_frame.endswith(\".csv\")\n",
      "            and os.path.isfile(data_frame)\n",
      "        ):\n",
      "            logger.info(\"Loading the CSV file\")\n",
      "            data_frame = pd.read_csv(data_frame)\n",
      "            name = os.path.basename(data_frame).split(\".\")[0]\n",
      "        elif isinstance(data_frame, pd.core.frame.DataFrame):\n",
      "            logger.info(\"Loading the pandas DataFrame\")\n",
      "        else:\n",
      "            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\n",
      "\n",
      "        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\n",
      "        if context_columns is not None:\n",
      "            context = get_context_from_pandas(data_frame, context_columns)\n",
      "\n",
      "        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "    @classmethod\n",
      "    def from_hf_dataset(\n",
      "        cls,\n",
      "        dataset_url: str,\n",
      "        value_column: str,\n",
      "        data_split: str = \"train\",\n",
      "        embeddings_column: Optional[str] = None,\n",
      "        context_columns: Optional[List[str]] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "        markdown: str = \"text/markdown\",\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ) -> \"MemoryIndex\":\n",
      "        dataset = load_dataset(dataset_url)[data_split]\n",
      "        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\n",
      "        if context_columns is not None:\n",
      "            context = get_context_from_hf(dataset, context_columns)\n",
      "        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "    @classmethod\n",
      "    def from_polars(\n",
      "        cls,\n",
      "        data_frame: pl.DataFrame,\n",
      "        value_column: str,\n",
      "        embeddings_column: Optional[str] = None,\n",
      "        context_columns: Optional[List[str]] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "        markdown: str = \"text/markdown\",\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ) -> \"MemoryIndex\":\n",
      "        print(\"Loading the Polars DataFrame\")\n",
      "        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\n",
      "        if context_columns is not None:\n",
      "            context = get_context_from_polars(data_frame, context_columns)\n",
      "\n",
      "        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "\n",
      "    @classmethod\n",
      "    def from_python(\n",
      "        cls,\n",
      "        directory_path: str,\n",
      "        minify_code: bool = False,\n",
      "        remove_docstrings: bool = False,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        markdown: str = \"python/markdown\",\n",
      "        resolution: str = \"both\",\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "    ) -> \"MemoryIndex\":\n",
      "        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n",
      "        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n",
      "        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"NpIndex\"]\n",
      "]\n",
      "Function call: all\n",
      "Function call: isinstance\n",
      "Function call: type\n",
      "Function call: ValueError\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: type\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: zip\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: extract_values_and_embeddings_pd\n",
      "Function call: get_context_from_pandas\n",
      "Function call: cls\n",
      "Function call: load_dataset\n",
      "Function call: extract_values_and_embeddings_hf\n",
      "Function call: get_context_from_hf\n",
      "Function call: cls\n",
      "Function call: print\n",
      "Function call: extract_values_and_embeddings_polars\n",
      "Function call: get_context_from_polars\n",
      "Function call: cls\n",
      "Function call: extract_values_and_embeddings_python\n",
      "Function call: len\n",
      "Function call: cls\n",
      "Related codes: ['\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef extract_values_and_embeddings_pd(\\n        data_frame: pd.DataFrame,\\n        value_column: str,\\n        embeddings_col: Optional[str],\\n    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n        Extract values and embeddings from a pandas DataFrame.\\n\\n        Args:\\n            data_frame: The DataFrame to extract values and embeddings from.\\n            value_column: The column of the DataFrame to use as values.\\n            embeddings_col: The column name containing the embeddings.\\n\\n        Returns:\\n            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n        \"\"\"\\n    values = data_frame[value_column].tolist()\\n    embeddings = data_frame[embeddings_col].tolist() if embeddings_col else None\\n\\n    return values, embeddings\\n', '\\ndef get_context_from_pandas(\\n          data_frame: pd.DataFrame,\\n          context_columns: List[str]):\\n    \"\"\"Extract context information from a pandas DataFrame.\"\"\"\\n    # This function will convert the row of the DataFrame into a dictionary where the keys are the column names.\\n    def row_to_dict(row: pd.Series) -> Dict[str, Any]:\\n        return {column: row[column] for column in context_columns}\\n\\n    # Apply the function to each row of the DataFrame and convert the result into a list.\\n    context = data_frame.apply(row_to_dict, axis=1).tolist()\\n\\n    return context\\n', '\\ndef extract_values_and_embeddings_hf(\\n    dataset: datasets.Dataset,\\n    value_column: Union[str, List[str]],\\n    embeddings_column: Optional[str],\\n) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n    Extract values and embeddings from a Hugging Face dataset.\\n\\n    Args:\\n        dataset: The Hugging Face dataset to extract values and embeddings from.\\n        value_column: The column(s) of the dataset to use as values.\\n        embeddings_column: The column name containing the embeddings.\\n\\n    Returns:\\n        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n    \"\"\"\\n    if isinstance(value_column, str):\\n        value_column = [value_column]\\n    print(\"Merging values: Start\")\\n    merged_docs = dataset.map(concat_columns, with_indices=True)\\n    print(\"Merging values: Done\")\\n    values = merged_docs[\\'merged_column\\']\\n    embeddings = dataset[embeddings_column]\\n    return values, embeddings if embeddings_column is not None else None\\n', '\\ndef get_context_from_hf(\\n          data_frame: datasets.Dataset,\\n          context_columns: List[str]):\\n    \"\"\"return a list dictionaries with the keys the column name and value the context columns values\"\"\"\\n    context_data = {column: data_frame[column] for column in context_columns}\\n    context = []\\n    data_frame_len = len(data_frame)\\n    for row in range(data_frame_len):\\n         context.append({column: context_data[column][row] for column in context_columns})\\n    return context\\n', '\\ndef extract_values_and_embeddings_polars(\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str]\\n    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n    Extract values and embeddings from a Polars DataFrame.\\n\\n    Args:\\n        data_frame: The DataFrame to extract values and embeddings from.\\n        value_column: The column of the DataFrame to use as values.\\n        embeddings_column: The column name containing the embeddings.\\n\\n    Returns:\\n        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n    \"\"\"\\n    values = data_frame[value_column].to_list()\\n    embeddings = data_frame[embeddings_column].to_list() if embeddings_column else None\\n\\n    return values, embeddings\\n', '\\ndef get_context_from_polars(\\n          data_frame: pl.DataFrame,\\n          context_columns: List[str]) -> List[Dict[str, Any]]:\\n    \"\"\"Extract context information from a Polars DataFrame.\"\"\"\\n\\n    context = []\\n    \\n    # Convert each row to a dictionary\\n    for i in range(len(data_frame)):\\n        row_dict = data_frame[i]\\n        context.append({column: row_dict[column] for column in context_columns})\\n\\n    return context\\n', '\\ndef extract_values_and_embeddings_python(\\n    directory_path: str,\\n    minify_code: bool = False,\\n    remove_docstrings: bool = False,\\n    resolution: str = \"function\"\\n) -> Tuple[List[str], List[Dict[str, str]]]:\\n    values = []\\n    context = []\\n\\n    parser = PythonParser(\\n        directory_path=directory_path,\\n        minify_code=minify_code,\\n        remove_docstrings=remove_docstrings\\n    )\\n\\n    results_dict = parser.process_directory()\\n\\n    if resolution == \"function\":\\n        source_codes = results_dict[\\'function_source_codes\\']\\n        nodes = results_dict[\\'function_nodes\\']\\n    elif resolution == \"class\":\\n        source_codes = results_dict[\\'class_source_codes\\']\\n        nodes = results_dict[\\'class_nodes\\']\\n    elif resolution == \"both\":\\n        source_codes = results_dict[\\'full_source\\']\\n        nodes = results_dict[\\'full_nodes\\']\\n    else:\\n        raise ValueError(f\"Invalid resolution: {resolution}\")\\n    if resolution in [\\'function\\', \\'class\\']:\\n        for source_code, node in zip(source_codes, nodes):\\n            values.append(source_code)\\n            context.append({\"libcst tree\": str(node)})\\n    elif resolution == \"both\":\\n        for source_code, node, filename in zip(source_codes, nodes, results_dict[\\'file_map\\']):\\n            values.append(source_code)\\n            context.append({\"libcst tree\": str(node), \"filename\": filename})\\n    else:\\n        raise ValueError(f\"Invalid resolution: {resolution}\")\\n    return values, context\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "@staticmethod\n",
      "def check_uniform_context_type(context: List[Any]) -> None:\n",
      "    \"\"\"Check if all context elements are of the same type.\"\"\"\n",
      "    if not all(isinstance(x, type(context[0])) for x in context):\n",
      "        raise ValueError(\"All context elements must be of the same type.\")\n",
      "\n",
      "Function call: all\n",
      "Function call: isinstance\n",
      "Function call: type\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    values: Optional[List[str]] = None,\n",
      "    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "    context: Optional[List[Any]] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    load: bool = False,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "    markdown: str = \"text/markdown\",\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      "):\n",
      "    NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\n",
      "    if context is not None and len(context) != len(values):\n",
      "        raise ValueError(\"The context must have the same length as the values\")\n",
      "\n",
      "\n",
      "    self.markdown = markdown\n",
      "    if context is not None and values is not None:\n",
      "        self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\n",
      "\n",
      "    if context is not None:\n",
      "        self.check_uniform_context_type(context)\n",
      "        self.context_type = type(context[0])\n",
      "\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: type\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\n",
      "    \"\"\" get the context of a value id or embedding or a list of \"\"\"\n",
      "    if isinstance(identifier, list):\n",
      "        return [self.get_context(value) for value in identifier]\n",
      "    else:\n",
      "        id = self.identify_input(identifier)\n",
      "        value = self.values[id]\n",
      "        return self.context[value]\n",
      "\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def clean_context(self):\n",
      "    \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\n",
      "    self.context = {value: self.context[value] for value in self.values}\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def add_to_context(self, value: str, context: Any):\n",
      "    \"\"\" add a context to a value \"\"\"\n",
      "    if not isinstance(context, self.context_type):\n",
      "        raise ValueError(\"The context must be of the same type as the other contexts\")\n",
      "    if value in self.values:\n",
      "        if value not in self.context:\n",
      "            self.context[value] = []\n",
      "        self.context[value].append(context)\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\n",
      "    \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\n",
      "    if isinstance(values, str):\n",
      "        values = [values]\n",
      "    NpIndex.add(self, values, embedding)\n",
      "    if context is not None:\n",
      "        for value, cont in zip(values, context):\n",
      "            self.add_to_context(value, cont)\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: zip\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\n",
      "    if not isinstance(identifier, list):\n",
      "        id = self.identify_input(identifier)\n",
      "        value = self.values[id]\n",
      "    NpIndex.remove(self, identifier)\n",
      "    if not isinstance(identifier, list):\n",
      "        self.context.pop(value)\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n",
      "    #recover value from old_identifier\n",
      "    if new_context is not None:\n",
      "        if not isinstance(new_context, self.context_type):\n",
      "            raise ValueError(\"The context must be of the same type as the other contexts\")\n",
      "    if not isinstance(old_identifier, list):\n",
      "        old_id = self.identify_input(old_identifier)\n",
      "        old_value = self.values[old_id]\n",
      "        # Only perform the update if the old_value is not the same as the new_value.\n",
      "        if old_value != new_value:\n",
      "            NpIndex.update(self, old_identifier, new_value, new_embedding)\n",
      "\n",
      "        if new_context is not None:\n",
      "            self.context[new_value] = [new_context]\n",
      "        else:\n",
      "            self.context[new_value] = self.context.pop(old_value)\n",
      "    else:\n",
      "        self.context[new_value] = self.context.pop(old_value)\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@classmethod\n",
      "def from_pandas(\n",
      "    cls,\n",
      "    data_frame: Union[pd.DataFrame, str],\n",
      "    value_column: str,\n",
      "    embeddings_column: Optional[str] = None,\n",
      "    context_columns: Optional[List[str]] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "    markdown: str = \"text/markdown\",\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      ") -> \"MemoryIndex\":\n",
      "    if (\n",
      "        isinstance(data_frame, str)\n",
      "        and data_frame.endswith(\".csv\")\n",
      "        and os.path.isfile(data_frame)\n",
      "    ):\n",
      "        logger.info(\"Loading the CSV file\")\n",
      "        data_frame = pd.read_csv(data_frame)\n",
      "        name = os.path.basename(data_frame).split(\".\")[0]\n",
      "    elif isinstance(data_frame, pd.core.frame.DataFrame):\n",
      "        logger.info(\"Loading the pandas DataFrame\")\n",
      "    else:\n",
      "        raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\n",
      "\n",
      "    values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\n",
      "    if context_columns is not None:\n",
      "        context = get_context_from_pandas(data_frame, context_columns)\n",
      "\n",
      "    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: extract_values_and_embeddings_pd\n",
      "Function call: get_context_from_pandas\n",
      "Function call: cls\n",
      "Related codes: ['\\ndef extract_values_and_embeddings_pd(\\n        data_frame: pd.DataFrame,\\n        value_column: str,\\n        embeddings_col: Optional[str],\\n    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n        Extract values and embeddings from a pandas DataFrame.\\n\\n        Args:\\n            data_frame: The DataFrame to extract values and embeddings from.\\n            value_column: The column of the DataFrame to use as values.\\n            embeddings_col: The column name containing the embeddings.\\n\\n        Returns:\\n            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n        \"\"\"\\n    values = data_frame[value_column].tolist()\\n    embeddings = data_frame[embeddings_col].tolist() if embeddings_col else None\\n\\n    return values, embeddings\\n', '\\ndef get_context_from_pandas(\\n          data_frame: pd.DataFrame,\\n          context_columns: List[str]):\\n    \"\"\"Extract context information from a pandas DataFrame.\"\"\"\\n    # This function will convert the row of the DataFrame into a dictionary where the keys are the column names.\\n    def row_to_dict(row: pd.Series) -> Dict[str, Any]:\\n        return {column: row[column] for column in context_columns}\\n\\n    # Apply the function to each row of the DataFrame and convert the result into a list.\\n    context = data_frame.apply(row_to_dict, axis=1).tolist()\\n\\n    return context\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@classmethod\n",
      "def from_hf_dataset(\n",
      "    cls,\n",
      "    dataset_url: str,\n",
      "    value_column: str,\n",
      "    data_split: str = \"train\",\n",
      "    embeddings_column: Optional[str] = None,\n",
      "    context_columns: Optional[List[str]] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "    markdown: str = \"text/markdown\",\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      ") -> \"MemoryIndex\":\n",
      "    dataset = load_dataset(dataset_url)[data_split]\n",
      "    values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\n",
      "    if context_columns is not None:\n",
      "        context = get_context_from_hf(dataset, context_columns)\n",
      "    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "Function call: load_dataset\n",
      "Function call: extract_values_and_embeddings_hf\n",
      "Function call: get_context_from_hf\n",
      "Function call: cls\n",
      "Related codes: ['\\ndef extract_values_and_embeddings_hf(\\n    dataset: datasets.Dataset,\\n    value_column: Union[str, List[str]],\\n    embeddings_column: Optional[str],\\n) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n    Extract values and embeddings from a Hugging Face dataset.\\n\\n    Args:\\n        dataset: The Hugging Face dataset to extract values and embeddings from.\\n        value_column: The column(s) of the dataset to use as values.\\n        embeddings_column: The column name containing the embeddings.\\n\\n    Returns:\\n        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n    \"\"\"\\n    if isinstance(value_column, str):\\n        value_column = [value_column]\\n    print(\"Merging values: Start\")\\n    merged_docs = dataset.map(concat_columns, with_indices=True)\\n    print(\"Merging values: Done\")\\n    values = merged_docs[\\'merged_column\\']\\n    embeddings = dataset[embeddings_column]\\n    return values, embeddings if embeddings_column is not None else None\\n', '\\ndef get_context_from_hf(\\n          data_frame: datasets.Dataset,\\n          context_columns: List[str]):\\n    \"\"\"return a list dictionaries with the keys the column name and value the context columns values\"\"\"\\n    context_data = {column: data_frame[column] for column in context_columns}\\n    context = []\\n    data_frame_len = len(data_frame)\\n    for row in range(data_frame_len):\\n         context.append({column: context_data[column][row] for column in context_columns})\\n    return context\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@classmethod\n",
      "def from_polars(\n",
      "    cls,\n",
      "    data_frame: pl.DataFrame,\n",
      "    value_column: str,\n",
      "    embeddings_column: Optional[str] = None,\n",
      "    context_columns: Optional[List[str]] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "    markdown: str = \"text/markdown\",\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      ") -> \"MemoryIndex\":\n",
      "    print(\"Loading the Polars DataFrame\")\n",
      "    values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\n",
      "    if context_columns is not None:\n",
      "        context = get_context_from_polars(data_frame, context_columns)\n",
      "\n",
      "    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "Function call: print\n",
      "Function call: extract_values_and_embeddings_polars\n",
      "Function call: get_context_from_polars\n",
      "Function call: cls\n",
      "Related codes: ['\\ndef extract_values_and_embeddings_polars(\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str]\\n    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n    Extract values and embeddings from a Polars DataFrame.\\n\\n    Args:\\n        data_frame: The DataFrame to extract values and embeddings from.\\n        value_column: The column of the DataFrame to use as values.\\n        embeddings_column: The column name containing the embeddings.\\n\\n    Returns:\\n        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n    \"\"\"\\n    values = data_frame[value_column].to_list()\\n    embeddings = data_frame[embeddings_column].to_list() if embeddings_column else None\\n\\n    return values, embeddings\\n', '\\ndef get_context_from_polars(\\n          data_frame: pl.DataFrame,\\n          context_columns: List[str]) -> List[Dict[str, Any]]:\\n    \"\"\"Extract context information from a Polars DataFrame.\"\"\"\\n\\n    context = []\\n    \\n    # Convert each row to a dictionary\\n    for i in range(len(data_frame)):\\n        row_dict = data_frame[i]\\n        context.append({column: row_dict[column] for column in context_columns})\\n\\n    return context\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "@classmethod\n",
      "def from_python(\n",
      "    cls,\n",
      "    directory_path: str,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    markdown: str = \"python/markdown\",\n",
      "    resolution: str = \"both\",\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\n",
      "    token_overflow_strategy: str = \"ignore\",\n",
      ") -> \"MemoryIndex\":\n",
      "    values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\n",
      "    logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\n",
      "    return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "Function call: extract_values_and_embeddings_python\n",
      "Function call: len\n",
      "Function call: cls\n",
      "Related codes: ['\\ndef extract_values_and_embeddings_python(\\n    directory_path: str,\\n    minify_code: bool = False,\\n    remove_docstrings: bool = False,\\n    resolution: str = \"function\"\\n) -> Tuple[List[str], List[Dict[str, str]]]:\\n    values = []\\n    context = []\\n\\n    parser = PythonParser(\\n        directory_path=directory_path,\\n        minify_code=minify_code,\\n        remove_docstrings=remove_docstrings\\n    )\\n\\n    results_dict = parser.process_directory()\\n\\n    if resolution == \"function\":\\n        source_codes = results_dict[\\'function_source_codes\\']\\n        nodes = results_dict[\\'function_nodes\\']\\n    elif resolution == \"class\":\\n        source_codes = results_dict[\\'class_source_codes\\']\\n        nodes = results_dict[\\'class_nodes\\']\\n    elif resolution == \"both\":\\n        source_codes = results_dict[\\'full_source\\']\\n        nodes = results_dict[\\'full_nodes\\']\\n    else:\\n        raise ValueError(f\"Invalid resolution: {resolution}\")\\n    if resolution in [\\'function\\', \\'class\\']:\\n        for source_code, node in zip(source_codes, nodes):\\n            values.append(source_code)\\n            context.append({\"libcst tree\": str(node)})\\n    elif resolution == \"both\":\\n        for source_code, node, filename in zip(source_codes, nodes, results_dict[\\'file_map\\']):\\n            values.append(source_code)\\n            context.append({\"libcst tree\": str(node), \"filename\": filename})\\n    else:\\n        raise ValueError(f\"Invalid resolution: {resolution}\")\\n    return values, context\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class PythonIndex(MemoryIndex, PythonParser):\n",
      "    def __init__(\n",
      "        self,\n",
      "        directory_path: str,\n",
      "        name: str = \"python_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        load: bool = False,\n",
      "        minify_code: bool = False,\n",
      "        remove_docstrings: bool = False,\n",
      "        tokenizer: Optional[tiktoken.Encoding] = None,\n",
      "        max_workers: int = 1,\n",
      "        backup: bool = False,\n",
      "        filter: str = \"class_function\"\n",
      "    ):\n",
      "\n",
      "        PythonParser.__init__(\n",
      "            self,\n",
      "            directory_path=directory_path,\n",
      "            minify_code=minify_code,\n",
      "            remove_docstrings=remove_docstrings,\n",
      "        )\n",
      "        #check if load folder exists\n",
      "        if save_path is None:\n",
      "            save_path = \"storage\"\n",
      "        load_directory = os.path.join(save_path, name)\n",
      "        loadcheck = not load or not os.path.exists(load_directory)\n",
      "        if load and not os.path.exists(load_directory):\n",
      "            print(\"No python-index found even if load=True, indexing from scratch\")\n",
      "        if loadcheck:\n",
      "            # Extract functions and classes source code\n",
      "            function_source_codes, class_source_codes, _, _ = self.process_directory()\n",
      "            print(\n",
      "                \"Indexing {} functions and {} classes\".format(\n",
      "                    len(function_source_codes), len(class_source_codes)\n",
      "                )\n",
      "            )\n",
      "            # Concatenate function and class source code and index them\n",
      "            if filter == \"function\":\n",
      "                codes = function_source_codes\n",
      "            elif filter == \"class\":\n",
      "                codes = class_source_codes\n",
      "            elif filter == \"class_function\":\n",
      "                codes = function_source_codes + class_source_codes\n",
      "            load = False\n",
      "            self.function_source_codes = function_source_codes\n",
      "            self.class_source_codes = class_source_codes\n",
      "\n",
      "         # Initialize the MemoryIndex\n",
      "        MemoryIndex.__init__(\n",
      "            self,\n",
      "            name=name,\n",
      "            values=codes if loadcheck else None,\n",
      "            save_path=save_path,\n",
      "            load=load,\n",
      "            tokenizer=tokenizer,\n",
      "            max_workers=max_workers,\n",
      "            backup=backup,\n",
      "        )\n",
      "        self.markdown = \"python\"\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"MemoryIndex, \", \"PythonParser\"]\n",
      "]\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    directory_path: str,\n",
      "    name: str = \"python_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    load: bool = False,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "    tokenizer: Optional[tiktoken.Encoding] = None,\n",
      "    max_workers: int = 1,\n",
      "    backup: bool = False,\n",
      "    filter: str = \"class_function\"\n",
      "):\n",
      "\n",
      "    PythonParser.__init__(\n",
      "        self,\n",
      "        directory_path=directory_path,\n",
      "        minify_code=minify_code,\n",
      "        remove_docstrings=remove_docstrings,\n",
      "    )\n",
      "    #check if load folder exists\n",
      "    if save_path is None:\n",
      "        save_path = \"storage\"\n",
      "    load_directory = os.path.join(save_path, name)\n",
      "    loadcheck = not load or not os.path.exists(load_directory)\n",
      "    if load and not os.path.exists(load_directory):\n",
      "        print(\"No python-index found even if load=True, indexing from scratch\")\n",
      "    if loadcheck:\n",
      "        # Extract functions and classes source code\n",
      "        function_source_codes, class_source_codes, _, _ = self.process_directory()\n",
      "        print(\n",
      "            \"Indexing {} functions and {} classes\".format(\n",
      "                len(function_source_codes), len(class_source_codes)\n",
      "            )\n",
      "        )\n",
      "        # Concatenate function and class source code and index them\n",
      "        if filter == \"function\":\n",
      "            codes = function_source_codes\n",
      "        elif filter == \"class\":\n",
      "            codes = class_source_codes\n",
      "        elif filter == \"class_function\":\n",
      "            codes = function_source_codes + class_source_codes\n",
      "        load = False\n",
      "        self.function_source_codes = function_source_codes\n",
      "        self.class_source_codes = class_source_codes\n",
      "\n",
      "     # Initialize the MemoryIndex\n",
      "    MemoryIndex.__init__(\n",
      "        self,\n",
      "        name=name,\n",
      "        values=codes if loadcheck else None,\n",
      "        save_path=save_path,\n",
      "        load=load,\n",
      "        tokenizer=tokenizer,\n",
      "        max_workers=max_workers,\n",
      "        backup=backup,\n",
      "    )\n",
      "    self.markdown = \"python\"\n",
      "\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def prune_index(\n",
      "    cls: \"MemoryIndex\",\n",
      "    constraint: Optional[str] = None,\n",
      "    regex_pattern: Optional[str] = None,\n",
      "    length_constraint: Optional[Tuple[int,int]] = None,\n",
      "    tokenizer: Optional[tiktoken.Encoding] = None,\n",
      ") -> \"MemoryIndex\" :\n",
      "    if constraint is not None:\n",
      "        if constraint == \"regex\":\n",
      "            if regex_pattern is None:\n",
      "                raise ValueError(\"regex_pattern must be provided for regex constraint.\")\n",
      "            pruned_values, pruned_embeddings = _prune_by_regex(cls, regex_pattern)\n",
      "        elif constraint == \"length\":\n",
      "            if length_constraint is None:\n",
      "                raise ValueError(\"length_constraint must be provided for length constraint.\")\n",
      "            pruned_values, pruned_embeddings = _prune_by_length(cls, length_constraint, tokenizer)\n",
      "        else:\n",
      "            raise ValueError(\"Invalid constraint type provided.\")\n",
      "    else:\n",
      "        raise ValueError(\"constraint must be provided for pruning the index.\")\n",
      "\n",
      "    pruned_memory_index = cls.__class__(\n",
      "        values=pruned_values,\n",
      "        embeddings=pruned_embeddings,\n",
      "        name=cls.name + \"_pruned\",\n",
      "    )\n",
      "\n",
      "    return pruned_memory_index\n",
      "\n",
      "Function call: ValueError\n",
      "Function call: _prune_by_regex\n",
      "Function call: ValueError\n",
      "Function call: _prune_by_length\n",
      "Function call: ValueError\n",
      "Function call: ValueError\n",
      "Related codes: ['\\n\\ndef _prune_by_regex(cls: \"MemoryIndex\", regex_pattern: str) -> Tuple[List[str], List[np.ndarray]]:\\n    pruned_values = []\\n    pruned_embeddings = []\\n\\n    for value in cls.values:\\n        if re.search(regex_pattern, value):\\n            pruned_values.append(value)\\n            pruned_embeddings.append(cls.get_embedding_by_value(value))\\n\\n    return pruned_values, pruned_embeddings\\n', '\\n\\ndef _prune_by_length(cls: \"MemoryIndex\", length_constraint: Tuple[int,int], tokenizer) -> Tuple[List[str], List[np.ndarray]]:\\n    pruned_values = []\\n    pruned_embeddings = []\\n    if tokenizer is None:\\n        len_func = len\\n    else:\\n        def len_func(value):\\n            return len(tokenizer.encode(value))\\n    print(\"Pruning by length\")\\n    print(\"Length constraint: \", length_constraint)\\n    print(\"Number of values: \", len(cls.values))\\n    print(\"tokenizer: \", tokenizer)\\n    for value in cls.values:\\n        if len_func(value) <= length_constraint[1] and len_func(value) >= length_constraint[0]:\\n            print(f\"value {value} is in range {length_constraint}\")\\n            pruned_values.append(value)\\n            pruned_embeddings.append(cls.get_embedding_by_value(value))\\n\\n    return pruned_values, pruned_embeddings\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def _prune_by_regex(cls: \"MemoryIndex\", regex_pattern: str) -> Tuple[List[str], List[np.ndarray]]:\n",
      "    pruned_values = []\n",
      "    pruned_embeddings = []\n",
      "\n",
      "    for value in cls.values:\n",
      "        if re.search(regex_pattern, value):\n",
      "            pruned_values.append(value)\n",
      "            pruned_embeddings.append(cls.get_embedding_by_value(value))\n",
      "\n",
      "    return pruned_values, pruned_embeddings\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def _prune_by_length(cls: \"MemoryIndex\", length_constraint: Tuple[int,int], tokenizer) -> Tuple[List[str], List[np.ndarray]]:\n",
      "    pruned_values = []\n",
      "    pruned_embeddings = []\n",
      "    if tokenizer is None:\n",
      "        len_func = len\n",
      "    else:\n",
      "        def len_func(value):\n",
      "            return len(tokenizer.encode(value))\n",
      "    print(\"Pruning by length\")\n",
      "    print(\"Length constraint: \", length_constraint)\n",
      "    print(\"Number of values: \", len(cls.values))\n",
      "    print(\"tokenizer: \", tokenizer)\n",
      "    for value in cls.values:\n",
      "        if len_func(value) <= length_constraint[1] and len_func(value) >= length_constraint[0]:\n",
      "            print(f\"value {value} is in range {length_constraint}\")\n",
      "            pruned_values.append(value)\n",
      "            pruned_embeddings.append(cls.get_embedding_by_value(value))\n",
      "\n",
      "    return pruned_values, pruned_embeddings\n",
      "\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: len_func\n",
      "Function call: len_func\n",
      "Function call: print\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', 'def len_func(value):\\n    return len(tokenizer.encode(value))\\n', 'def len_func(value):\\n    return len(tokenizer.encode(value))\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def len_func(value):\n",
      "    return len(tokenizer.encode(value))\n",
      "\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def save(cls):\n",
      "    save_directory = os.path.join(cls.save_path, cls.name)\n",
      "    os.makedirs(save_directory, exist_ok=True)\n",
      "\n",
      "    index_filename = os.path.join(save_directory, f\"{cls.name}_index.faiss\")\n",
      "    faiss.write_index(cls.index, index_filename)\n",
      "\n",
      "    values_filename = os.path.join(save_directory, f\"{cls.name}_values.json\")\n",
      "    with open(values_filename, \"w\") as f:\n",
      "        json.dump(cls.values, f)\n",
      "\n",
      "    # embeddings_filename = os.path.join(save_directory, f\"{cls.name}_embeddings.npz\")\n",
      "    # np.savez_compressed(embeddings_filename, cls.get_all_embeddings())\n",
      "\n",
      "Function call: open\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def load(cls):\n",
      "    load_directory = os.path.join(cls.save_path, cls.name)\n",
      "    if not os.path.exists(load_directory):\n",
      "        cls.loaded = False\n",
      "        print(\"I did not find the directory to load the index from.\", load_directory)\n",
      "        return\n",
      "\n",
      "    print(f\"Loading index from {load_directory}\")\n",
      "\n",
      "    index_filename = os.path.join(load_directory, f\"{cls.name}_index.faiss\")\n",
      "    cls.index = faiss.read_index(index_filename)\n",
      "\n",
      "    values_filename = os.path.join(load_directory, f\"{cls.name}_values.json\")\n",
      "    with open(values_filename, \"r\") as f:\n",
      "        cls.values = json.load(f)\n",
      "    # embeddings_filename = os.path.join(load_directory, f\"{cls.name}_embeddings.npz\")\n",
      "    # print(embeddings_filename)\n",
      "    # embeddings_data = np.load(embeddings_filename)\n",
      "    # cls.embeddings = embeddings_data[\"arr_0\"]\n",
      "    cls.loaded = True\n",
      "\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: open\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class MemoryIndex:\n",
      "    \"\"\"\n",
      "    this class is a wrapper for a faiss index, it contains information about the format of the index the faiss index itself\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        index: Optional[faiss.Index] = None,\n",
      "        values: Optional[List[str]] = None,\n",
      "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        load: bool = False,\n",
      "        tokenizer: Optional[tiktoken.Encoding] = None,\n",
      "        max_workers: int = 1,\n",
      "        backup: bool = False,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "        is_batched: bool = False,\n",
      "        markdown: str = \"text/markdown\",\n",
      "    ):\n",
      "\n",
      "        self.name = name\n",
      "        self.embedder = embedder()\n",
      "        self.save_path = save_path if save_path is not None else \"storage\"\n",
      "        os.makedirs(self.save_path, exist_ok=True)\n",
      "        self.values = []\n",
      "        self.embeddings = []\n",
      "        self.max_workers = max_workers\n",
      "        self.is_batched = is_batched\n",
      "        self.markdown = markdown\n",
      "\n",
      "        if load is True:\n",
      "            self.load()\n",
      "        else:\n",
      "            self.loaded = False\n",
      "        if not self.loaded:\n",
      "            if (\n",
      "                self.max_workers > 1\n",
      "                and values is not None\n",
      "                and embeddings is None\n",
      "                and index is None\n",
      "            ):\n",
      "                embeddings = parallel_embeddings(self.embedder,\n",
      "                    values, max_workers, backup=backup, name=name\n",
      "                )\n",
      "            self.init_index(index, values, embeddings, is_embed_batched=is_batched)\n",
      "        if tokenizer is None:\n",
      "            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "        else:\n",
      "            self.tokenizer = tokenizer\n",
      "        self.query_history = []\n",
      "        if not self.loaded:\n",
      "            self.save()\n",
      "\n",
      "    def init_index(\n",
      "        self,\n",
      "        index: Optional[faiss.Index] = None,\n",
      "        values: Optional[List[str]] = None,\n",
      "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "        is_embed_batched: bool = False,\n",
      "    ) -> None:\n",
      "\n",
      "        \"\"\"\n",
      "        initializes the index, there are 4 cases:\n",
      "        1. we create a new index from scratch\n",
      "        2. we create a new index from a list of embeddings and values\n",
      "        3. we create a new index from a faiss index and values list\n",
      "        4. we load an index from a file\n",
      "        \"\"\"\n",
      "        if index is None and values is None and embeddings is None:\n",
      "            print(\"Creating a new index\")\n",
      "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "            self.values = []\n",
      "        elif (\n",
      "            index is None\n",
      "            and values is not None\n",
      "            and embeddings is not None\n",
      "            and len(values) == len(embeddings)\n",
      "        ):\n",
      "            print(\"Creating a new index from a list of embeddings and values\")\n",
      "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "            #add all the embeddings to the index\n",
      "            if is_embed_batched:\n",
      "                print(\"Adding batched embeddings to index\")\n",
      "                print(type(embeddings))\n",
      "                embeddings = np.array(embeddings)\n",
      "                self.add_batch_to_index(values=values, embeddings=embeddings)\n",
      "            else:\n",
      "                for embedding, value in zip(embeddings, values):\n",
      "                    self.add_to_index(value, embedding)\n",
      "\n",
      "        elif (\n",
      "            isinstance(index, faiss.Index)\n",
      "            and index.d == self.embedder.get_embedding_size()\n",
      "            and type(values) == list\n",
      "            and len(values) == index.ntotal\n",
      "        ):\n",
      "            print(\"Creating a new index from a faiss index and values list\")\n",
      "            self.index = index\n",
      "            self.values = values\n",
      "        elif index is None and values is not None and embeddings is None:\n",
      "            print(\"Creating a new index from a list of values\")\n",
      "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "            if is_embed_batched:\n",
      "                batch = []\n",
      "                i = 0\n",
      "                for value in values:\n",
      "                    batch.append(value)\n",
      "                    if len(batch) == 1000:\n",
      "                        start = time.time()\n",
      "                        self.add_batch_to_index(values=batch)\n",
      "                        print(f\"Embedding batch {i} took \", time.time() - start, \" seconds\")\n",
      "                        print(f\"Batch {i} of {len(values)//1000}\")\n",
      "                        i +=1\n",
      "                        batch = []\n",
      "                if len(batch) > 0:\n",
      "                    self.add_batch_to_index(values=batch)\n",
      "            else:\n",
      "                i = 0\n",
      "                for value in values:\n",
      "                    print(\"Embedding value \", i, \" of \", len(values))\n",
      "                    start = time.time()\n",
      "                    self.add_to_index(value)\n",
      "                    print(\"Embedding value \", i, \" took \", time.time() - start, \" seconds\")\n",
      "                    i += 1\n",
      "        else:\n",
      "            print(type(values))\n",
      "            print(type(embeddings))\n",
      "            print(type(index))\n",
      "            raise ValueError(\n",
      "                \"The index is not a valid faiss index or the embedding dimension is not correct\"\n",
      "            )\n",
      "        print(len(self.values), \" values in the index\")\n",
      "        print(self.index.ntotal, \" embeddings in the index\")\n",
      "\n",
      "    def __getstate__(self):\n",
      "        state = self.__dict__.copy()\n",
      "        del state[\"index\"]\n",
      "\n",
      "        index_buffer = io.BytesIO()\n",
      "        faiss.write_index(state[\"index\"], index_buffer)\n",
      "        state[\"index_bytes\"] = index_buffer.getvalue()\n",
      "\n",
      "        return state\n",
      "\n",
      "    def __setstate__(self, state):\n",
      "        index_buffer = io.BytesIO(state[\"index_bytes\"])\n",
      "        state[\"index\"] = faiss.read_index(index_buffer)\n",
      "\n",
      "        del state[\"index_bytes\"]\n",
      "\n",
      "        self.__dict__.update(state)\n",
      "\n",
      "    @classmethod\n",
      "    def from_pandas(\n",
      "        cls,\n",
      "        data_frame: Union[pd.DataFrame, str],\n",
      "        columns: Optional[Union[str, List[str]]] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        in_place: bool = True,\n",
      "        embeddings_col: Optional[str] = None,\n",
      "        markdown: str = \"text/markdown\",\n",
      "    ) -> \"MemoryIndex\":\n",
      "        \"\"\"\n",
      "        Initialize a MemoryIndex object from a pandas DataFrame.\n",
      "\n",
      "        Args:\n",
      "            data_frame: The DataFrame or path to a CSV file.\n",
      "            columns: The columns of the DataFrame to use as values.\n",
      "            name: The name of the index.\n",
      "            save_path: The path to save the index.\n",
      "            in_place: Whether to work on the DataFrame in place or create a copy.\n",
      "            embeddings_col: The column name containing the embeddings.\n",
      "\n",
      "        Returns:\n",
      "            A MemoryIndex object initialized with values and embeddings from the DataFrame.\n",
      "        \"\"\"\n",
      "\n",
      "        if (\n",
      "            isinstance(data_frame, str)\n",
      "            and data_frame.endswith(\".csv\")\n",
      "            and os.path.isfile(data_frame)\n",
      "        ):\n",
      "            print(\"Loading the CSV file\")\n",
      "            try:\n",
      "                data_frame = pd.read_csv(data_frame)\n",
      "            except:\n",
      "                raise ValueError(\"The CSV file is not valid\")\n",
      "            name = data_frame.split(\"/\")[-1].split(\".\")[0]\n",
      "        elif isinstance(data_frame, pd.core.frame.DataFrame) and columns is not None:\n",
      "            print(\"Loading the DataFrame\")\n",
      "            if not in_place:\n",
      "                data_frame = copy.deepcopy(data_frame)\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                \"The data_frame is not a valid pandas dataframe or the columns are not valid or the path is not valid\"\n",
      "            )\n",
      "\n",
      "        values, embeddings = extract_values_and_embeddings(\n",
      "            data_frame, columns, embeddings_col\n",
      "        )\n",
      "        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown)\n",
      "\n",
      "    @classmethod\n",
      "    def from_hf_dataset(\n",
      "        cls,\n",
      "        dataset_url: str,\n",
      "        value_column: str,\n",
      "        embeddings_column: Optional[str] = None,\n",
      "        name: str = \"memory_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= CohereEmbedder,\n",
      "        is_batched: bool = False,\n",
      "        markdown: str = \"text/markdown\"\n",
      "    ) -> \"MemoryIndex\":\n",
      "        \"\"\"\n",
      "        Initialize a MemoryIndex object from a Hugging Face dataset.\n",
      "\n",
      "        Args:\n",
      "            dataset_url: The URL of the Hugging Face dataset.\n",
      "            value_column: The column of the dataset to use as values.\n",
      "            embeddings_column: The column of the dataset containing the embeddings.\n",
      "            name: The name of the index.\n",
      "            save_path: The path to save the index.\n",
      "\n",
      "        Returns:\n",
      "            A MemoryIndex object initialized with values and embeddings from the Hugging Face dataset.\n",
      "        \"\"\"\n",
      "        dataset = load_dataset(dataset_url)['train']\n",
      "        if embeddings_column is not None:\n",
      "            values, embeddings = extract_values_and_embeddings_hf(\n",
      "                dataset, value_column, embeddings_column\n",
      "            )\n",
      "        elif embeddings_column is None:\n",
      "            values = extract_values_hf(dataset, value_column)\n",
      "            embeddings = None\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                \"The dataset is not a valid Hugging Face dataset or the columns are not valid\"\n",
      "            )\n",
      "        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, is_batched=is_batched, markdown=markdown)\n",
      "\n",
      "    def add_to_index(\n",
      "        self,\n",
      "        value: str,\n",
      "        embedding: Optional[Union[List[float], np.ndarray, str]] = None,\n",
      "        verbose: bool = False,\n",
      "        default_save: bool = False,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n",
      "        \"\"\"\n",
      "        if value not in self.values:\n",
      "            if embedding is None:\n",
      "                embedding = self.embedder.embed(value)\n",
      "                if verbose:\n",
      "                    display(\n",
      "                        Markdown(\"The value {value} was embedded\".format(value=value))\n",
      "                    )\n",
      "                if type(embedding) is list:\n",
      "                    embedding = np.array([embedding])\n",
      "                self.index.add(embedding)\n",
      "                self.values.append(value)\n",
      "            elif embedding is not None:\n",
      "                if type(embedding) is list:\n",
      "                    embedding = np.array([embedding])\n",
      "                elif type(embedding) is str:\n",
      "                    try:\n",
      "                        embedding = eval(embedding)\n",
      "                        embedding = np.array([embedding]).astype(np.float32)\n",
      "                    except (SyntaxError, ValueError):\n",
      "                        print(\"The string is not a valid list, probably an error:\", embedding)\n",
      "                        return\n",
      "                elif type(embedding) is not np.ndarray:\n",
      "                    raise ValueError(\"The embedding is not a valid type\")\n",
      "\n",
      "                if embedding.ndim == 1:\n",
      "                    embedding = embedding.reshape(1, -1)\n",
      "                self.index.add(embedding)\n",
      "                self.values.append(value)\n",
      "                if default_save:\n",
      "                    self.save()  # we should check here the save time is not too long\n",
      "        else:\n",
      "            if verbose:\n",
      "                display(\n",
      "                    Markdown(\n",
      "                        \"The value {value} was already in the index\".format(value=value)\n",
      "                    )\n",
      "                )\n",
      "    def add_batch_to_index(\n",
      "        self,\n",
      "        values: List[str],\n",
      "        embeddings: Optional[Union[List[float], np.ndarray, str]] = None,\n",
      "        verbose: bool = False,\n",
      "        default_save: bool = False,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n",
      "        \"\"\"\n",
      "\n",
      "        if embeddings is None:\n",
      "            embeddings = self.embedder.batch_embed(values)\n",
      "            if verbose:\n",
      "                display(\n",
      "                    Markdown(\"The value batch was embedded\")\n",
      "                )\n",
      "            if type(embeddings) is list:\n",
      "                embeddings = np.array(embeddings)\n",
      "            self.index.add(embeddings)\n",
      "            self.values.extend(values)\n",
      "        elif embeddings is not None:\n",
      "            if type(embeddings) is list:\n",
      "                embeddings = np.array([embeddings])\n",
      "            elif type(embeddings) is str:\n",
      "                try:\n",
      "                    embeddings = eval(embeddings)\n",
      "                    embeddings = np.array([embeddings]).astype(np.float32)\n",
      "                except (SyntaxError, ValueError):\n",
      "                    print(\"The string is not a valid list, probably an error:\", embeddings)\n",
      "                    return\n",
      "            elif type(embeddings) is not np.ndarray:\n",
      "                raise ValueError(\"The embedding is not a valid type\")\n",
      "\n",
      "            self.index.add(embeddings)\n",
      "            self.values.extend(values)\n",
      "            if default_save:\n",
      "                self.save()  # we should check here the save time is not too long\n",
      "\n",
      "    def remove_from_id(self, id: int) -> None:\n",
      "        \"\"\"\n",
      "        Remove a value from the index and the values list.\n",
      "        Args:\n",
      "            value: The value to remove from the index.\n",
      "        \"\"\"\n",
      "        if id is not None:\n",
      "            self.values.pop(id)\n",
      "            self.embeddings = np.array(list(self.embeddings).pop(id))\n",
      "            id_selector = faiss.IDSelectorArray(n=1, ids=np.array([id], dtype=np.int64))\n",
      "            self.index.remove_ids(id_selector)            \n",
      "            self.save()\n",
      "        else:\n",
      "            print(f\"The value '{id}' was not found in the index.\")\n",
      "\n",
      "    def remove_from_index(self, value: str) -> None:\n",
      "        \"\"\"\n",
      "        Remove a value from the index and the values list.\n",
      "        Args:\n",
      "            value: The value to remove from the index.\n",
      "        \"\"\"\n",
      "        index = self.get_index_by_value(value)\n",
      "        if index is not None:\n",
      "            self.values.pop(index)\n",
      "\n",
      "            id_selector = faiss.IDSelectorArray(np.array([index], dtype=np.int64))\n",
      "            self.index.remove_ids(id_selector)\n",
      "\n",
      "            self.save()\n",
      "        else:\n",
      "            print(f\"The value '{value}' was not found in the index.\")\n",
      "\n",
      "    def get_embedding_by_index(self, index: int) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Get the embedding corresponding to a certain index value.\n",
      "        \"\"\"\n",
      "        if index < 0 or index >= len(self.values):\n",
      "            raise ValueError(\"The index is out of range\")\n",
      "\n",
      "        embedding = self.index.reconstruct(index)\n",
      "\n",
      "        return embedding\n",
      "\n",
      "    def get_index_by_value(self, value: str) -> Optional[int]:\n",
      "        \"\"\"\n",
      "        Get the index corresponding to a value in self.values.\n",
      "        \"\"\"\n",
      "        if value in self.values:\n",
      "            index = self.values.index(value)\n",
      "            return index\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    def get_embedding_by_value(self, value: str) -> Optional[np.ndarray]:\n",
      "        \"\"\"\n",
      "        Get the embedding corresponding to a certain value in self.values.\n",
      "        \"\"\"\n",
      "        index = self.get_index_by_value(value)\n",
      "        if index is not None:\n",
      "            embedding = self.get_embedding_by_index(index)\n",
      "            return embedding\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    def get_all_embeddings(self) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Get all the embeddings in the index.\n",
      "        \"\"\"\n",
      "        embeddings = []\n",
      "        for i in range(len(self.values)):\n",
      "            embeddings.append(self.get_embedding_by_index(i))\n",
      "        self.embeddings = np.array(embeddings)\n",
      "        return self.embeddings\n",
      "\n",
      "    def faiss_query(self, query: str, k: int = 10) -> Tuple[List[str], List[float]]:\n",
      "        \"\"\"Query the faiss index for the top-k most similar values to the query\"\"\"\n",
      "\n",
      "        embedding = self.embedder.embed(query)\n",
      "        if k > len(self.values):\n",
      "            k = len(self.values)\n",
      "        D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\n",
      "        values = [self.values[i] for i in I[0]]\n",
      "        scores = [d for d in D[0]]\n",
      "        return values, scores, I\n",
      "\n",
      "    def token_bound_query(self, query, k=10, max_tokens=4000):\n",
      "        \"\"\"Query the faiss index for the top-k most similar values to the query, but bound the number of tokens retrieved by the max_tokens parameter\"\"\"\n",
      "        returned_tokens = 0\n",
      "        top_k_hint = []\n",
      "        scores = []\n",
      "        tokens = []\n",
      "        indices = []\n",
      "\n",
      "        if len(self.values) > 0:\n",
      "            top_k, scores, indices = self.faiss_query(query, k=min(k, len(self.values)))\n",
      "\n",
      "            for hint in top_k:\n",
      "                message_tokens = len(self.tokenizer.encode(hint))\n",
      "                tokens.append(message_tokens)\n",
      "                if returned_tokens + message_tokens <= max_tokens:\n",
      "                    top_k_hint += [hint]\n",
      "                    returned_tokens += message_tokens\n",
      "\n",
      "            self.query_history.append(\n",
      "                {\n",
      "                    \"query\": query,\n",
      "                    \"hints\": top_k_hint,\n",
      "                    \"scores\": scores,\n",
      "                    \"indices\": indices,\n",
      "                    \"hints_tokens\": tokens,\n",
      "                    \"returned_tokens\": returned_tokens,\n",
      "                    \"max_tokens\": max_tokens,\n",
      "                    \"k\": k,\n",
      "                }\n",
      "            )\n",
      "\n",
      "        return top_k_hint, scores, indices\n",
      "\n",
      "    def save(self):\n",
      "        save(self)\n",
      "\n",
      "    def load(self):\n",
      "        load(self)\n",
      "\n",
      "    def prune(\n",
      "        self,\n",
      "        constraint: Optional[str] = None,\n",
      "        regex_pattern: Optional[str] = None,\n",
      "        length_constraint: Optional[int] = None,\n",
      "        tokenizer: Optional[tiktoken.Encoding] = None,\n",
      "    ) -> \"MemoryIndex\":\n",
      "        if tokenizer is None:\n",
      "            tokenizer = self.tokenizer\n",
      "        return prune_index(self, constraint, regex_pattern, length_constraint,tokenizer)\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: embedder\n",
      "Function call: parallel_embeddings\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: zip\n",
      "Function call: isinstance\n",
      "Function call: type\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: ValueError\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "Function call: ValueError\n",
      "Function call: extract_values_and_embeddings\n",
      "Function call: cls\n",
      "Function call: load_dataset\n",
      "Function call: extract_values_and_embeddings_hf\n",
      "Function call: extract_values_hf\n",
      "Function call: ValueError\n",
      "Function call: cls\n",
      "Function call: display\n",
      "Function call: Markdown\n",
      "Function call: type\n",
      "Function call: type\n",
      "Function call: type\n",
      "Function call: eval\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: ValueError\n",
      "Function call: display\n",
      "Function call: Markdown\n",
      "Function call: display\n",
      "Function call: Markdown\n",
      "Function call: type\n",
      "Function call: type\n",
      "Function call: type\n",
      "Function call: eval\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: ValueError\n",
      "Function call: list\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: min\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: save\n",
      "Function call: load\n",
      "Function call: prune_index\n",
      "Related codes: [\"\\ndef numeric_embedder(column):\\n    # Implement the numeric embedding strategy\\n    # This will depend on the type of `column` (whether it's a string, Series, etc.)\\n    # Here we'll assume `column` is a pandas Series for simplicity\\n    return column.values\\n\", '\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef extract_values_and_embeddings(\\n        data_frame: pd.DataFrame,\\n        columns: Union[str, List[str]],\\n        embeddings_col: Optional[str],\\n    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n        Extract values and embeddings from a pandas DataFrame.\\n\\n        Args:\\n            data_frame: The DataFrame to extract values and embeddings from.\\n            columns: The columns of the DataFrame to use as values.\\n            embeddings_col: The column name containing the embeddings.\\n\\n        Returns:\\n            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n        \"\"\"\\n\\n    if isinstance(columns, list) and len(columns) > 1:\\n        data_frame[\"values_combined\"] = data_frame[columns].apply(\\n            lambda x: \" \".join(x), axis=1\\n        )\\n        columns = \"values_combined\"\\n    elif isinstance(columns, list) and len(columns) == 1:\\n        columns = columns[0]\\n    elif not isinstance(columns, str):\\n        raise ValueError(\"The columns are not valid\")\\n\\n    values = []\\n    embeddings = []\\n\\n    for _, row in data_frame.iterrows():\\n        value = row[columns]\\n        values.append(value)\\n\\n        if embeddings_col is not None:\\n            embedding = row[embeddings_col]\\n            embeddings.append(embedding)\\n\\n    return values, embeddings if embeddings_col is not None else None\\n', '\\ndef extract_values_and_embeddings_hf(\\n    dataset: datasets.Dataset,\\n    value_column: Union[str, List[str]],\\n    embeddings_column: Optional[str],\\n) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n    Extract values and embeddings from a Hugging Face dataset.\\n\\n    Args:\\n        dataset: The Hugging Face dataset to extract values and embeddings from.\\n        value_column: The column(s) of the dataset to use as values.\\n        embeddings_column: The column name containing the embeddings.\\n\\n    Returns:\\n        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n    \"\"\"\\n    if isinstance(value_column, str):\\n        value_column = [value_column]\\n    print(\"Merging values: Start\")\\n    merged_docs = dataset.map(concat_columns, with_indices=True)\\n    print(\"Merging values: Done\")\\n    values = merged_docs[\\'merged_column\\']\\n    embeddings = dataset[embeddings_column]\\n    return values, embeddings if embeddings_column is not None else None\\n', '\\ndef extract_values_hf(dataset: datasets.Dataset, value_column: Union[str, List[str]]) -> List[str]:\\n    \"\"\"\\n    Extract values from a Hugging Face dataset.\\n\\n    Args:\\n        dataset: The Hugging Face dataset to extract values from.\\n        value_column: The column(s) of the dataset to use as values.\\n\\n    Returns:\\n        A list with the extracted values.\\n    \"\"\"\\n    if isinstance(value_column, str):\\n        value_column = [value_column]\\n    if len(value_column) == 1:\\n        return dataset[value_column[0]]\\n    elif len(value_column) > 1:\\n        merged_docs = dataset.map(concat_columns)\\n        return merged_docs\\n    else:\\n        raise ValueError(\"No value column specified.\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef evaluate_stability(self) -> float:\\n    kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\\n    pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\\n    nmi_sum = 0\\n\\n    for kernel_label1, kernel_label2 in pairwise_combinations:\\n        nmi = self.compute_nmi(kernel_label1, kernel_label2)\\n        nmi_sum += nmi\\n\\n    stability_score = nmi_sum / len(pairwise_combinations)\\n    return stability_score\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef evaluate_stability(self) -> float:\\n    kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\\n    pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\\n    nmi_sum = 0\\n\\n    for kernel_label1, kernel_label2 in pairwise_combinations:\\n        nmi = self.compute_nmi(kernel_label1, kernel_label2)\\n        nmi_sum += nmi\\n\\n    stability_score = nmi_sum / len(pairwise_combinations)\\n    return stability_score\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef _save_results_to_file(self) -> None:\\n    os.makedirs(self.save_path, exist_ok=True)\\n    with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"w\") as f:\\n        json.dump(self.results, f)\\n', '\\ndef _load_results_from_file(self) -> None:\\n    if os.path.exists(os.path.join(self.save_path, f\"{self.task_id}_results.json\")):\\n        try:\\n            with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"r\") as f:\\n                self.results = json.load(f)\\n                print(f\"Loaded {len(self.results)} results from file.\")\\n        except Exception as e:\\n            print(f\"Error loading results from file: {e}\")\\n            print(\"Starting from scratch.\")\\n    else:\\n        print(\"No results file found, starting from scratch.\")\\n', '\\ndef prune_index(\\n    cls: \"MemoryIndex\",\\n    constraint: Optional[str] = None,\\n    regex_pattern: Optional[str] = None,\\n    length_constraint: Optional[Tuple[int,int]] = None,\\n    tokenizer: Optional[tiktoken.Encoding] = None,\\n) -> \"MemoryIndex\" :\\n    if constraint is not None:\\n        if constraint == \"regex\":\\n            if regex_pattern is None:\\n                raise ValueError(\"regex_pattern must be provided for regex constraint.\")\\n            pruned_values, pruned_embeddings = _prune_by_regex(cls, regex_pattern)\\n        elif constraint == \"length\":\\n            if length_constraint is None:\\n                raise ValueError(\"length_constraint must be provided for length constraint.\")\\n            pruned_values, pruned_embeddings = _prune_by_length(cls, length_constraint, tokenizer)\\n        else:\\n            raise ValueError(\"Invalid constraint type provided.\")\\n    else:\\n        raise ValueError(\"constraint must be provided for pruning the index.\")\\n\\n    pruned_memory_index = cls.__class__(\\n        values=pruned_values,\\n        embeddings=pruned_embeddings,\\n        name=cls.name + \"_pruned\",\\n    )\\n\\n    return pruned_memory_index\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    index: Optional[faiss.Index] = None,\n",
      "    values: Optional[List[str]] = None,\n",
      "    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    load: bool = False,\n",
      "    tokenizer: Optional[tiktoken.Encoding] = None,\n",
      "    max_workers: int = 1,\n",
      "    backup: bool = False,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\n",
      "    is_batched: bool = False,\n",
      "    markdown: str = \"text/markdown\",\n",
      "):\n",
      "\n",
      "    self.name = name\n",
      "    self.embedder = embedder()\n",
      "    self.save_path = save_path if save_path is not None else \"storage\"\n",
      "    os.makedirs(self.save_path, exist_ok=True)\n",
      "    self.values = []\n",
      "    self.embeddings = []\n",
      "    self.max_workers = max_workers\n",
      "    self.is_batched = is_batched\n",
      "    self.markdown = markdown\n",
      "\n",
      "    if load is True:\n",
      "        self.load()\n",
      "    else:\n",
      "        self.loaded = False\n",
      "    if not self.loaded:\n",
      "        if (\n",
      "            self.max_workers > 1\n",
      "            and values is not None\n",
      "            and embeddings is None\n",
      "            and index is None\n",
      "        ):\n",
      "            embeddings = parallel_embeddings(self.embedder,\n",
      "                values, max_workers, backup=backup, name=name\n",
      "            )\n",
      "        self.init_index(index, values, embeddings, is_embed_batched=is_batched)\n",
      "    if tokenizer is None:\n",
      "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "    else:\n",
      "        self.tokenizer = tokenizer\n",
      "    self.query_history = []\n",
      "    if not self.loaded:\n",
      "        self.save()\n",
      "\n",
      "Function call: embedder\n",
      "Function call: parallel_embeddings\n",
      "Related codes: [\"\\ndef numeric_embedder(column):\\n    # Implement the numeric embedding strategy\\n    # This will depend on the type of `column` (whether it's a string, Series, etc.)\\n    # Here we'll assume `column` is a pandas Series for simplicity\\n    return column.values\\n\", '\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def init_index(\n",
      "    self,\n",
      "    index: Optional[faiss.Index] = None,\n",
      "    values: Optional[List[str]] = None,\n",
      "    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "    is_embed_batched: bool = False,\n",
      ") -> None:\n",
      "\n",
      "    \"\"\"\n",
      "        initializes the index, there are 4 cases:\n",
      "        1. we create a new index from scratch\n",
      "        2. we create a new index from a list of embeddings and values\n",
      "        3. we create a new index from a faiss index and values list\n",
      "        4. we load an index from a file\n",
      "        \"\"\"\n",
      "    if index is None and values is None and embeddings is None:\n",
      "        print(\"Creating a new index\")\n",
      "        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "        self.values = []\n",
      "    elif (\n",
      "        index is None\n",
      "        and values is not None\n",
      "        and embeddings is not None\n",
      "        and len(values) == len(embeddings)\n",
      "    ):\n",
      "        print(\"Creating a new index from a list of embeddings and values\")\n",
      "        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "        #add all the embeddings to the index\n",
      "        if is_embed_batched:\n",
      "            print(\"Adding batched embeddings to index\")\n",
      "            print(type(embeddings))\n",
      "            embeddings = np.array(embeddings)\n",
      "            self.add_batch_to_index(values=values, embeddings=embeddings)\n",
      "        else:\n",
      "            for embedding, value in zip(embeddings, values):\n",
      "                self.add_to_index(value, embedding)\n",
      "\n",
      "    elif (\n",
      "        isinstance(index, faiss.Index)\n",
      "        and index.d == self.embedder.get_embedding_size()\n",
      "        and type(values) == list\n",
      "        and len(values) == index.ntotal\n",
      "    ):\n",
      "        print(\"Creating a new index from a faiss index and values list\")\n",
      "        self.index = index\n",
      "        self.values = values\n",
      "    elif index is None and values is not None and embeddings is None:\n",
      "        print(\"Creating a new index from a list of values\")\n",
      "        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "        if is_embed_batched:\n",
      "            batch = []\n",
      "            i = 0\n",
      "            for value in values:\n",
      "                batch.append(value)\n",
      "                if len(batch) == 1000:\n",
      "                    start = time.time()\n",
      "                    self.add_batch_to_index(values=batch)\n",
      "                    print(f\"Embedding batch {i} took \", time.time() - start, \" seconds\")\n",
      "                    print(f\"Batch {i} of {len(values)//1000}\")\n",
      "                    i +=1\n",
      "                    batch = []\n",
      "            if len(batch) > 0:\n",
      "                self.add_batch_to_index(values=batch)\n",
      "        else:\n",
      "            i = 0\n",
      "            for value in values:\n",
      "                print(\"Embedding value \", i, \" of \", len(values))\n",
      "                start = time.time()\n",
      "                self.add_to_index(value)\n",
      "                print(\"Embedding value \", i, \" took \", time.time() - start, \" seconds\")\n",
      "                i += 1\n",
      "    else:\n",
      "        print(type(values))\n",
      "        print(type(embeddings))\n",
      "        print(type(index))\n",
      "        raise ValueError(\n",
      "            \"The index is not a valid faiss index or the embedding dimension is not correct\"\n",
      "        )\n",
      "    print(len(self.values), \" values in the index\")\n",
      "    print(self.index.ntotal, \" embeddings in the index\")\n",
      "\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: zip\n",
      "Function call: isinstance\n",
      "Function call: type\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: ValueError\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __getstate__(self):\n",
      "    state = self.__dict__.copy()\n",
      "    del state[\"index\"]\n",
      "\n",
      "    index_buffer = io.BytesIO()\n",
      "    faiss.write_index(state[\"index\"], index_buffer)\n",
      "    state[\"index_bytes\"] = index_buffer.getvalue()\n",
      "\n",
      "    return state\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __setstate__(self, state):\n",
      "    index_buffer = io.BytesIO(state[\"index_bytes\"])\n",
      "    state[\"index\"] = faiss.read_index(index_buffer)\n",
      "\n",
      "    del state[\"index_bytes\"]\n",
      "\n",
      "    self.__dict__.update(state)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@classmethod\n",
      "def from_pandas(\n",
      "    cls,\n",
      "    data_frame: Union[pd.DataFrame, str],\n",
      "    columns: Optional[Union[str, List[str]]] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    in_place: bool = True,\n",
      "    embeddings_col: Optional[str] = None,\n",
      "    markdown: str = \"text/markdown\",\n",
      ") -> \"MemoryIndex\":\n",
      "    \"\"\"\n",
      "        Initialize a MemoryIndex object from a pandas DataFrame.\n",
      "\n",
      "        Args:\n",
      "            data_frame: The DataFrame or path to a CSV file.\n",
      "            columns: The columns of the DataFrame to use as values.\n",
      "            name: The name of the index.\n",
      "            save_path: The path to save the index.\n",
      "            in_place: Whether to work on the DataFrame in place or create a copy.\n",
      "            embeddings_col: The column name containing the embeddings.\n",
      "\n",
      "        Returns:\n",
      "            A MemoryIndex object initialized with values and embeddings from the DataFrame.\n",
      "        \"\"\"\n",
      "\n",
      "    if (\n",
      "        isinstance(data_frame, str)\n",
      "        and data_frame.endswith(\".csv\")\n",
      "        and os.path.isfile(data_frame)\n",
      "    ):\n",
      "        print(\"Loading the CSV file\")\n",
      "        try:\n",
      "            data_frame = pd.read_csv(data_frame)\n",
      "        except:\n",
      "            raise ValueError(\"The CSV file is not valid\")\n",
      "        name = data_frame.split(\"/\")[-1].split(\".\")[0]\n",
      "    elif isinstance(data_frame, pd.core.frame.DataFrame) and columns is not None:\n",
      "        print(\"Loading the DataFrame\")\n",
      "        if not in_place:\n",
      "            data_frame = copy.deepcopy(data_frame)\n",
      "    else:\n",
      "        raise ValueError(\n",
      "            \"The data_frame is not a valid pandas dataframe or the columns are not valid or the path is not valid\"\n",
      "        )\n",
      "\n",
      "    values, embeddings = extract_values_and_embeddings(\n",
      "        data_frame, columns, embeddings_col\n",
      "    )\n",
      "    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown)\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "Function call: ValueError\n",
      "Function call: extract_values_and_embeddings\n",
      "Function call: cls\n",
      "Related codes: ['\\ndef extract_values_and_embeddings(\\n        data_frame: pd.DataFrame,\\n        columns: Union[str, List[str]],\\n        embeddings_col: Optional[str],\\n    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n        Extract values and embeddings from a pandas DataFrame.\\n\\n        Args:\\n            data_frame: The DataFrame to extract values and embeddings from.\\n            columns: The columns of the DataFrame to use as values.\\n            embeddings_col: The column name containing the embeddings.\\n\\n        Returns:\\n            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n        \"\"\"\\n\\n    if isinstance(columns, list) and len(columns) > 1:\\n        data_frame[\"values_combined\"] = data_frame[columns].apply(\\n            lambda x: \" \".join(x), axis=1\\n        )\\n        columns = \"values_combined\"\\n    elif isinstance(columns, list) and len(columns) == 1:\\n        columns = columns[0]\\n    elif not isinstance(columns, str):\\n        raise ValueError(\"The columns are not valid\")\\n\\n    values = []\\n    embeddings = []\\n\\n    for _, row in data_frame.iterrows():\\n        value = row[columns]\\n        values.append(value)\\n\\n        if embeddings_col is not None:\\n            embedding = row[embeddings_col]\\n            embeddings.append(embedding)\\n\\n    return values, embeddings if embeddings_col is not None else None\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@classmethod\n",
      "def from_hf_dataset(\n",
      "    cls,\n",
      "    dataset_url: str,\n",
      "    value_column: str,\n",
      "    embeddings_column: Optional[str] = None,\n",
      "    name: str = \"memory_index\",\n",
      "    save_path: Optional[str] = None,\n",
      "    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= CohereEmbedder,\n",
      "    is_batched: bool = False,\n",
      "    markdown: str = \"text/markdown\"\n",
      ") -> \"MemoryIndex\":\n",
      "    \"\"\"\n",
      "        Initialize a MemoryIndex object from a Hugging Face dataset.\n",
      "\n",
      "        Args:\n",
      "            dataset_url: The URL of the Hugging Face dataset.\n",
      "            value_column: The column of the dataset to use as values.\n",
      "            embeddings_column: The column of the dataset containing the embeddings.\n",
      "            name: The name of the index.\n",
      "            save_path: The path to save the index.\n",
      "\n",
      "        Returns:\n",
      "            A MemoryIndex object initialized with values and embeddings from the Hugging Face dataset.\n",
      "        \"\"\"\n",
      "    dataset = load_dataset(dataset_url)['train']\n",
      "    if embeddings_column is not None:\n",
      "        values, embeddings = extract_values_and_embeddings_hf(\n",
      "            dataset, value_column, embeddings_column\n",
      "        )\n",
      "    elif embeddings_column is None:\n",
      "        values = extract_values_hf(dataset, value_column)\n",
      "        embeddings = None\n",
      "    else:\n",
      "        raise ValueError(\n",
      "            \"The dataset is not a valid Hugging Face dataset or the columns are not valid\"\n",
      "        )\n",
      "    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, is_batched=is_batched, markdown=markdown)\n",
      "\n",
      "Function call: load_dataset\n",
      "Function call: extract_values_and_embeddings_hf\n",
      "Function call: extract_values_hf\n",
      "Function call: ValueError\n",
      "Function call: cls\n",
      "Related codes: ['\\ndef extract_values_and_embeddings_hf(\\n    dataset: datasets.Dataset,\\n    value_column: Union[str, List[str]],\\n    embeddings_column: Optional[str],\\n) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n    Extract values and embeddings from a Hugging Face dataset.\\n\\n    Args:\\n        dataset: The Hugging Face dataset to extract values and embeddings from.\\n        value_column: The column(s) of the dataset to use as values.\\n        embeddings_column: The column name containing the embeddings.\\n\\n    Returns:\\n        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n    \"\"\"\\n    if isinstance(value_column, str):\\n        value_column = [value_column]\\n    print(\"Merging values: Start\")\\n    merged_docs = dataset.map(concat_columns, with_indices=True)\\n    print(\"Merging values: Done\")\\n    values = merged_docs[\\'merged_column\\']\\n    embeddings = dataset[embeddings_column]\\n    return values, embeddings if embeddings_column is not None else None\\n', '\\ndef extract_values_hf(dataset: datasets.Dataset, value_column: Union[str, List[str]]) -> List[str]:\\n    \"\"\"\\n    Extract values from a Hugging Face dataset.\\n\\n    Args:\\n        dataset: The Hugging Face dataset to extract values from.\\n        value_column: The column(s) of the dataset to use as values.\\n\\n    Returns:\\n        A list with the extracted values.\\n    \"\"\"\\n    if isinstance(value_column, str):\\n        value_column = [value_column]\\n    if len(value_column) == 1:\\n        return dataset[value_column[0]]\\n    elif len(value_column) > 1:\\n        merged_docs = dataset.map(concat_columns)\\n        return merged_docs\\n    else:\\n        raise ValueError(\"No value column specified.\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def add_to_index(\n",
      "    self,\n",
      "    value: str,\n",
      "    embedding: Optional[Union[List[float], np.ndarray, str]] = None,\n",
      "    verbose: bool = False,\n",
      "    default_save: bool = False,\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n",
      "        \"\"\"\n",
      "    if value not in self.values:\n",
      "        if embedding is None:\n",
      "            embedding = self.embedder.embed(value)\n",
      "            if verbose:\n",
      "                display(\n",
      "                    Markdown(\"The value {value} was embedded\".format(value=value))\n",
      "                )\n",
      "            if type(embedding) is list:\n",
      "                embedding = np.array([embedding])\n",
      "            self.index.add(embedding)\n",
      "            self.values.append(value)\n",
      "        elif embedding is not None:\n",
      "            if type(embedding) is list:\n",
      "                embedding = np.array([embedding])\n",
      "            elif type(embedding) is str:\n",
      "                try:\n",
      "                    embedding = eval(embedding)\n",
      "                    embedding = np.array([embedding]).astype(np.float32)\n",
      "                except (SyntaxError, ValueError):\n",
      "                    print(\"The string is not a valid list, probably an error:\", embedding)\n",
      "                    return\n",
      "            elif type(embedding) is not np.ndarray:\n",
      "                raise ValueError(\"The embedding is not a valid type\")\n",
      "\n",
      "            if embedding.ndim == 1:\n",
      "                embedding = embedding.reshape(1, -1)\n",
      "            self.index.add(embedding)\n",
      "            self.values.append(value)\n",
      "            if default_save:\n",
      "                self.save()  # we should check here the save time is not too long\n",
      "    else:\n",
      "        if verbose:\n",
      "            display(\n",
      "                Markdown(\n",
      "                    \"The value {value} was already in the index\".format(value=value)\n",
      "                )\n",
      "            )\n",
      "\n",
      "Function call: display\n",
      "Function call: Markdown\n",
      "Function call: type\n",
      "Function call: type\n",
      "Function call: type\n",
      "Function call: eval\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: ValueError\n",
      "Function call: display\n",
      "Function call: Markdown\n",
      "Related codes: ['\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef evaluate_stability(self) -> float:\\n    kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\\n    pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\\n    nmi_sum = 0\\n\\n    for kernel_label1, kernel_label2 in pairwise_combinations:\\n        nmi = self.compute_nmi(kernel_label1, kernel_label2)\\n        nmi_sum += nmi\\n\\n    stability_score = nmi_sum / len(pairwise_combinations)\\n    return stability_score\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def add_batch_to_index(\n",
      "    self,\n",
      "    values: List[str],\n",
      "    embeddings: Optional[Union[List[float], np.ndarray, str]] = None,\n",
      "    verbose: bool = False,\n",
      "    default_save: bool = False,\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n",
      "        \"\"\"\n",
      "\n",
      "    if embeddings is None:\n",
      "        embeddings = self.embedder.batch_embed(values)\n",
      "        if verbose:\n",
      "            display(\n",
      "                Markdown(\"The value batch was embedded\")\n",
      "            )\n",
      "        if type(embeddings) is list:\n",
      "            embeddings = np.array(embeddings)\n",
      "        self.index.add(embeddings)\n",
      "        self.values.extend(values)\n",
      "    elif embeddings is not None:\n",
      "        if type(embeddings) is list:\n",
      "            embeddings = np.array([embeddings])\n",
      "        elif type(embeddings) is str:\n",
      "            try:\n",
      "                embeddings = eval(embeddings)\n",
      "                embeddings = np.array([embeddings]).astype(np.float32)\n",
      "            except (SyntaxError, ValueError):\n",
      "                print(\"The string is not a valid list, probably an error:\", embeddings)\n",
      "                return\n",
      "        elif type(embeddings) is not np.ndarray:\n",
      "            raise ValueError(\"The embedding is not a valid type\")\n",
      "\n",
      "        self.index.add(embeddings)\n",
      "        self.values.extend(values)\n",
      "        if default_save:\n",
      "            self.save()  # we should check here the save time is not too long\n",
      "\n",
      "Function call: display\n",
      "Function call: Markdown\n",
      "Function call: type\n",
      "Function call: type\n",
      "Function call: type\n",
      "Function call: eval\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef evaluate_stability(self) -> float:\\n    kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\\n    pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\\n    nmi_sum = 0\\n\\n    for kernel_label1, kernel_label2 in pairwise_combinations:\\n        nmi = self.compute_nmi(kernel_label1, kernel_label2)\\n        nmi_sum += nmi\\n\\n    stability_score = nmi_sum / len(pairwise_combinations)\\n    return stability_score\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def remove_from_id(self, id: int) -> None:\n",
      "    \"\"\"\n",
      "        Remove a value from the index and the values list.\n",
      "        Args:\n",
      "            value: The value to remove from the index.\n",
      "        \"\"\"\n",
      "    if id is not None:\n",
      "        self.values.pop(id)\n",
      "        self.embeddings = np.array(list(self.embeddings).pop(id))\n",
      "        id_selector = faiss.IDSelectorArray(n=1, ids=np.array([id], dtype=np.int64))\n",
      "        self.index.remove_ids(id_selector)            \n",
      "        self.save()\n",
      "    else:\n",
      "        print(f\"The value '{id}' was not found in the index.\")\n",
      "\n",
      "Function call: list\n",
      "Function call: print\n",
      "Related codes: [\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\"]\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def remove_from_index(self, value: str) -> None:\n",
      "    \"\"\"\n",
      "        Remove a value from the index and the values list.\n",
      "        Args:\n",
      "            value: The value to remove from the index.\n",
      "        \"\"\"\n",
      "    index = self.get_index_by_value(value)\n",
      "    if index is not None:\n",
      "        self.values.pop(index)\n",
      "\n",
      "        id_selector = faiss.IDSelectorArray(np.array([index], dtype=np.int64))\n",
      "        self.index.remove_ids(id_selector)\n",
      "\n",
      "        self.save()\n",
      "    else:\n",
      "        print(f\"The value '{value}' was not found in the index.\")\n",
      "\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_embedding_by_index(self, index: int) -> np.ndarray:\n",
      "    \"\"\"\n",
      "        Get the embedding corresponding to a certain index value.\n",
      "        \"\"\"\n",
      "    if index < 0 or index >= len(self.values):\n",
      "        raise ValueError(\"The index is out of range\")\n",
      "\n",
      "    embedding = self.index.reconstruct(index)\n",
      "\n",
      "    return embedding\n",
      "\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_index_by_value(self, value: str) -> Optional[int]:\n",
      "    \"\"\"\n",
      "        Get the index corresponding to a value in self.values.\n",
      "        \"\"\"\n",
      "    if value in self.values:\n",
      "        index = self.values.index(value)\n",
      "        return index\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_embedding_by_value(self, value: str) -> Optional[np.ndarray]:\n",
      "    \"\"\"\n",
      "        Get the embedding corresponding to a certain value in self.values.\n",
      "        \"\"\"\n",
      "    index = self.get_index_by_value(value)\n",
      "    if index is not None:\n",
      "        embedding = self.get_embedding_by_index(index)\n",
      "        return embedding\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_all_embeddings(self) -> np.ndarray:\n",
      "    \"\"\"\n",
      "        Get all the embeddings in the index.\n",
      "        \"\"\"\n",
      "    embeddings = []\n",
      "    for i in range(len(self.values)):\n",
      "        embeddings.append(self.get_embedding_by_index(i))\n",
      "    self.embeddings = np.array(embeddings)\n",
      "    return self.embeddings\n",
      "\n",
      "Function call: range\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def faiss_query(self, query: str, k: int = 10) -> Tuple[List[str], List[float]]:\n",
      "    \"\"\"Query the faiss index for the top-k most similar values to the query\"\"\"\n",
      "\n",
      "    embedding = self.embedder.embed(query)\n",
      "    if k > len(self.values):\n",
      "        k = len(self.values)\n",
      "    D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\n",
      "    values = [self.values[i] for i in I[0]]\n",
      "    scores = [d for d in D[0]]\n",
      "    return values, scores, I\n",
      "\n",
      "Function call: len\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def token_bound_query(self, query, k=10, max_tokens=4000):\n",
      "    \"\"\"Query the faiss index for the top-k most similar values to the query, but bound the number of tokens retrieved by the max_tokens parameter\"\"\"\n",
      "    returned_tokens = 0\n",
      "    top_k_hint = []\n",
      "    scores = []\n",
      "    tokens = []\n",
      "    indices = []\n",
      "\n",
      "    if len(self.values) > 0:\n",
      "        top_k, scores, indices = self.faiss_query(query, k=min(k, len(self.values)))\n",
      "\n",
      "        for hint in top_k:\n",
      "            message_tokens = len(self.tokenizer.encode(hint))\n",
      "            tokens.append(message_tokens)\n",
      "            if returned_tokens + message_tokens <= max_tokens:\n",
      "                top_k_hint += [hint]\n",
      "                returned_tokens += message_tokens\n",
      "\n",
      "        self.query_history.append(\n",
      "            {\n",
      "                \"query\": query,\n",
      "                \"hints\": top_k_hint,\n",
      "                \"scores\": scores,\n",
      "                \"indices\": indices,\n",
      "                \"hints_tokens\": tokens,\n",
      "                \"returned_tokens\": returned_tokens,\n",
      "                \"max_tokens\": max_tokens,\n",
      "                \"k\": k,\n",
      "            }\n",
      "        )\n",
      "\n",
      "    return top_k_hint, scores, indices\n",
      "\n",
      "Function call: len\n",
      "Function call: min\n",
      "Function call: len\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def save(self):\n",
      "    save(self)\n",
      "\n",
      "Function call: save\n",
      "Related codes: ['\\ndef _save_results_to_file(self) -> None:\\n    os.makedirs(self.save_path, exist_ok=True)\\n    with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"w\") as f:\\n        json.dump(self.results, f)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def load(self):\n",
      "    load(self)\n",
      "\n",
      "Function call: load\n",
      "Related codes: ['\\ndef _load_results_from_file(self) -> None:\\n    if os.path.exists(os.path.join(self.save_path, f\"{self.task_id}_results.json\")):\\n        try:\\n            with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"r\") as f:\\n                self.results = json.load(f)\\n                print(f\"Loaded {len(self.results)} results from file.\")\\n        except Exception as e:\\n            print(f\"Error loading results from file: {e}\")\\n            print(\"Starting from scratch.\")\\n    else:\\n        print(\"No results file found, starting from scratch.\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def prune(\n",
      "    self,\n",
      "    constraint: Optional[str] = None,\n",
      "    regex_pattern: Optional[str] = None,\n",
      "    length_constraint: Optional[int] = None,\n",
      "    tokenizer: Optional[tiktoken.Encoding] = None,\n",
      ") -> \"MemoryIndex\":\n",
      "    if tokenizer is None:\n",
      "        tokenizer = self.tokenizer\n",
      "    return prune_index(self, constraint, regex_pattern, length_constraint,tokenizer)\n",
      "\n",
      "Function call: prune_index\n",
      "Related codes: ['\\ndef prune_index(\\n    cls: \"MemoryIndex\",\\n    constraint: Optional[str] = None,\\n    regex_pattern: Optional[str] = None,\\n    length_constraint: Optional[Tuple[int,int]] = None,\\n    tokenizer: Optional[tiktoken.Encoding] = None,\\n) -> \"MemoryIndex\" :\\n    if constraint is not None:\\n        if constraint == \"regex\":\\n            if regex_pattern is None:\\n                raise ValueError(\"regex_pattern must be provided for regex constraint.\")\\n            pruned_values, pruned_embeddings = _prune_by_regex(cls, regex_pattern)\\n        elif constraint == \"length\":\\n            if length_constraint is None:\\n                raise ValueError(\"length_constraint must be provided for length constraint.\")\\n            pruned_values, pruned_embeddings = _prune_by_length(cls, length_constraint, tokenizer)\\n        else:\\n            raise ValueError(\"Invalid constraint type provided.\")\\n    else:\\n        raise ValueError(\"constraint must be provided for pruning the index.\")\\n\\n    pruned_memory_index = cls.__class__(\\n        values=pruned_values,\\n        embeddings=pruned_embeddings,\\n        name=cls.name + \"_pruned\",\\n    )\\n\\n    return pruned_memory_index\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class PandasIndex(MemoryIndex):\n",
      "    \"\"\"\n",
      "    A class to create an index of a pandas DataFrame, allowing querying on specified columns.\n",
      "    Inherits from MemoryIndex class.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        df: pd.DataFrame,\n",
      "        row_func: Optional[Callable[[pd.Series], str]] = None,\n",
      "        name=\"pandas_index\",\n",
      "        columns: Optional[List[str]] = None,\n",
      "        load=False,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize a PandasIndex object.\n",
      "\n",
      "        Args:\n",
      "            df: A pandas DataFrame to index.\n",
      "            row_func: An optional function to process rows before adding them to the index.\n",
      "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
      "        \"\"\"\n",
      "        if row_func is None:\n",
      "            row_func = lambda row: str(row)\n",
      "        self.row_func = row_func\n",
      "\n",
      "        self.df = df\n",
      "        MemoryIndex.__init__(\n",
      "            self, name=name, load=load\n",
      "        )  # Initialize the parent MemoryIndex class\n",
      "\n",
      "        for _, row in df.iterrows():\n",
      "            self.add_to_index(row_func(row))\n",
      "\n",
      "        self.columns: Dict[str, MemoryIndex] = {}\n",
      "\n",
      "        # Set up columns during initialization\n",
      "        if columns is None:\n",
      "            self.setup_columns()\n",
      "        else:\n",
      "            self.setup_columns(columns)\n",
      "        self.save()\n",
      "        for col in self.columns:\n",
      "            self.columns[col].save()\n",
      "        self.executed_tasks = []\n",
      "\n",
      "    def setup_columns(self, columns: Optional[List[str]] = None):\n",
      "        \"\"\"\n",
      "        Set up columns for indexing.\n",
      "\n",
      "        Args:\n",
      "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
      "        \"\"\"\n",
      "        if columns is None:\n",
      "            # Use string columns or columns with lists containing a single string by default\n",
      "            columns = []\n",
      "\n",
      "        for col in columns:\n",
      "            self.columns[col] = MemoryIndex.from_pandas(\n",
      "                self.df, columns=col, name=f\"{self.name}_{col}\"\n",
      "            )\n",
      "\n",
      "    def query_columns(self, query: str, columns: List[str]) -> List[Tuple[str, float]]:\n",
      "        \"\"\"\n",
      "        Query the indexed columns of the DataFrame.\n",
      "\n",
      "        Args:\n",
      "            query: The search query as a string.\n",
      "            columns: A list of column names to query.\n",
      "\n",
      "        Returns:\n",
      "            A list of tuples containing the matched value and its similarity score.\n",
      "        \"\"\"\n",
      "        results = []\n",
      "        for col in columns:\n",
      "            if col in self.columns:\n",
      "                results.extend(self.columns[col].faiss_query(query))\n",
      "            else:\n",
      "                raise KeyError(\n",
      "                    f\"Column '{col}' not found in PandaDb columns dictionary.\"\n",
      "                )\n",
      "        return results\n",
      "\n",
      "    def add_row(self, row: pd.Series) -> None:\n",
      "        \"\"\"\n",
      "        Add a row to the DataFrame and update the row and column indexes.\n",
      "\n",
      "        Args:\n",
      "            row: A pandas Series representing the row to add.\n",
      "        \"\"\"\n",
      "        self.df = pd.concat([self.df, row.to_frame().T], ignore_index=True)\n",
      "        self.add_to_index(self.row_func(row))\n",
      "\n",
      "        for col in self.columns:\n",
      "            if col in row:\n",
      "                self.columns[col].add_to_index(row[col])\n",
      "\n",
      "    def remove_row(self, index: int) -> None:\n",
      "        \"\"\"\n",
      "        Remove a row from the DataFrame and update the row and column indexes.\n",
      "\n",
      "        Args:\n",
      "            index: The index of the row to remove.\n",
      "        \"\"\"\n",
      "        if 0 <= index < len(self.df):\n",
      "            self.remove_from_index(self.values[index])\n",
      "\n",
      "            for col in self.columns:\n",
      "                self.columns[col].remove_from_index(self.columns[col].values[index])\n",
      "\n",
      "            self.df.drop(index, inplace=True)\n",
      "            self.df.reset_index(drop=True, inplace=True)\n",
      "        else:\n",
      "            raise IndexError(\n",
      "                f\"Index {index} is out of bounds for DataFrame with length {len(self.df)}\"\n",
      "            )\n",
      "\n",
      "    def rows_from_value(\n",
      "        self, value: Union[str, int, float], column: Optional[str] = None\n",
      "    ) -> pd.DataFrame:\n",
      "        \"\"\"\n",
      "        Return all rows of the DataFrame that have a particular value in the row index or a column index.\n",
      "\n",
      "        Args:\n",
      "            value: The value to search for in the DataFrame.\n",
      "            column: The name of the column to search in. If None, search in the row index.\n",
      "\n",
      "        Returns:\n",
      "            A pandas DataFrame containing the rows with the specified value.\n",
      "        \"\"\"\n",
      "        if column is None:\n",
      "            return self.df.loc[self.df.index == value]\n",
      "        else:\n",
      "            if column in self.df.columns:\n",
      "                return self.df.loc[self.df[column] == value]\n",
      "            else:\n",
      "                raise KeyError(f\"Column '{column}' not found in the DataFrame.\")\n",
      "\n",
      "    def apply_llmtask(\n",
      "        self,\n",
      "        path: List[List[int]],\n",
      "        chatbot: Chat,\n",
      "        task_name=None,\n",
      "        write_func=None,\n",
      "        columns: Optional[List[str]] = None,\n",
      "        task_id=None,\n",
      "        max_workers=1,\n",
      "        calls_per_minute: int = 20,\n",
      "    ) -> pd.DataFrame:\n",
      "        \"\"\"\n",
      "        Apply a writing task to the specified columns or the main index, and create new modified indexes and a corresponding DataFrame with new values.\n",
      "\n",
      "        Args:\n",
      "            write_task: An instance of a writing task (subclass of BaseTask).\n",
      "            columns: A list of column names to apply the writing task to, or None (default) to apply the task to the main index.\n",
      "\n",
      "        Returns:\n",
      "            A pandas DataFrame containing the modified values in the specified columns or a new column with the modified values of the main index.\n",
      "        \"\"\"\n",
      "        modified_df = self.df.copy()\n",
      "        if task_name is None and task_id is None:\n",
      "            task_name = \"llm_task\"\n",
      "        elif task_name is None:\n",
      "            task_name = task_id\n",
      "\n",
      "        if columns is None:\n",
      "            # Apply the writing task to the main index\n",
      "            write_index = self\n",
      "            write_task = LLMWriter(\n",
      "                write_index,\n",
      "                path,\n",
      "                chatbot,\n",
      "                task_name=task_name,\n",
      "                write_func=write_func,\n",
      "                context=self.df,\n",
      "                task_id=task_id,\n",
      "                max_workers=max_workers,\n",
      "                calls_per_minute=calls_per_minute,\n",
      "            )\n",
      "\n",
      "            new_index = write_task.write()\n",
      "\n",
      "            # Create a mapping of old values to new values\n",
      "            old_to_new_values = dict(zip(self.values, new_index.values))\n",
      "\n",
      "            # Update the row values in the modified DataFrame\n",
      "            modified_df[\"new_column\"] = modified_df.apply(\n",
      "                lambda row: old_to_new_values.get(\n",
      "                    self.row_func(row), self.row_func(row)\n",
      "                ),\n",
      "                axis=1,\n",
      "            )\n",
      "        else:\n",
      "            # Iterate over the specified columns\n",
      "            for col in columns:\n",
      "                if col in self.columns:\n",
      "                    # Apply the writing task to the column\n",
      "                    write_index = self.columns[col]\n",
      "                    write_task = LLMWriter(\n",
      "                        write_index,\n",
      "                        path,\n",
      "                        chatbot,\n",
      "                        write_func=write_func,\n",
      "                        context=self.df,\n",
      "                        task_id=task_id,\n",
      "                        max_workers=max_workers,\n",
      "                        calls_per_minute=calls_per_minute,\n",
      "                    )\n",
      "                    new_index = write_task.write()\n",
      "\n",
      "                    # Create a mapping of old values to new values\n",
      "                    old_to_new_values = dict(\n",
      "                        zip(self.columns[col].values, new_index.values)\n",
      "                    )\n",
      "\n",
      "                    # Update the column values in the modified DataFrame\n",
      "                    modified_df[col] = modified_df[col].apply(\n",
      "                        lambda x: old_to_new_values.get(x, x)\n",
      "                    )\n",
      "\n",
      "                    # Update the column's MemoryIndex\n",
      "                    self.columns[col] = new_index\n",
      "                    self.columns[col].save()\n",
      "                else:\n",
      "                    raise KeyError(\n",
      "                        f\"Column '{col}' not found in PandasIndex columns dictionary.\"\n",
      "                    )\n",
      "        # remove context from the write_task to avoid memory leak\n",
      "        write_task.context = None\n",
      "        self.executed_tasks.append({\"task\": write_task, \"output\": modified_df})\n",
      "\n",
      "        return modified_df\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"MemoryIndex\"]\n",
      "]\n",
      "Function call: str\n",
      "Function call: row_func\n",
      "Function call: KeyError\n",
      "Function call: len\n",
      "Function call: IndexError\n",
      "Function call: len\n",
      "Function call: KeyError\n",
      "Function call: LLMWriter\n",
      "Function call: dict\n",
      "Function call: zip\n",
      "Function call: LLMWriter\n",
      "Function call: dict\n",
      "Function call: zip\n",
      "Function call: KeyError\n",
      "Related codes: ['\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\nclass LLMWriter(BaseTask):\\n    def __init__(\\n        self,\\n        index: MemoryIndex,\\n        path: List[List[int]],\\n        chatbot: Chat,\\n        write_func=None,\\n        context=None,\\n        task_name=\"summary\",\\n        max_workers: int = 1,\\n        task_id: str = \"LLMWriteTask\",\\n        calls_per_minute: int = 20,\\n        backup: bool = True,\\n    ):\\n        \"\"\"\\n        Initialize a LLMWriteTask instance.\\n\\n        :param index: List of strings representing the queries.\\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\\n        :param chatbot: Chatbot instance used for executing queries.\\n        :param max_workers: Maximum number of worker threads (default is 4).\\n        \"\"\"\\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\\n        self.index = index\\n        self.chatbot = chatbot\\n        self.write_func = write_func if write_func else self.llm_response\\n        self.new_index_name = self.index.name + f\"_{task_name}\"\\n        self.context = context\\n\\n    def llm_response(self, chatbot: Chat, message: str, context=None, id=None):\\n        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\\n        # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\\n        #     return \"the message is too long to be processed\"\\n        # moved the error catching to multi-threading but custom method could report the error here\\n        return chatbot.reply(message)\\n\\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\\n        \"\"\"\\n        Execute a sub-task using a separate copy of the chatbot instance.\\n\\n        :param sub_path: List of indices representing the sub-task\\'s sequence.\\n        :return: List of strings representing the responses for each query in the sub-task.\\n        \"\"\"\\n        if self.parallel:\\n            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\\n            chatbot_instance = copy.deepcopy(self.chatbot)\\n        else:\\n            chatbot_instance = self.chatbot\\n        if isinstance(self.chatbot, BaseThread):\\n            chatbot_instance.reset_memory()\\n\\n        sub_results = {}\\n        for i in sub_path:\\n            current_val = self.index.values[i]\\n            response = self.write_func(\\n                chatbot_instance, current_val, self.context, id=i\\n            )\\n            sub_results[i] = response\\n        return sub_results\\n\\n    def write(self):\\n        content_to_write = self.work()\\n        self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\\n        self.new_index.save()\\n        return self.new_index\\n', '\\n@field_validator(\\'values\\')\\ndef check_type_dictionaries(cls, values):\\n    first_type_dictionary = values[0].type_dictionary\\n    for value in values[1:]:\\n        if value.type_dictionary != first_type_dictionary:\\n            raise ValueError(\"All elements in \\'values\\' should have the same \\'type_dictionary\\'.\")\\n    return values\\n', '\\n\\nclass LLMWriter(BaseTask):\\n    def __init__(\\n        self,\\n        index: MemoryIndex,\\n        path: List[List[int]],\\n        chatbot: Chat,\\n        write_func=None,\\n        context=None,\\n        task_name=\"summary\",\\n        max_workers: int = 1,\\n        task_id: str = \"LLMWriteTask\",\\n        calls_per_minute: int = 20,\\n        backup: bool = True,\\n    ):\\n        \"\"\"\\n        Initialize a LLMWriteTask instance.\\n\\n        :param index: List of strings representing the queries.\\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\\n        :param chatbot: Chatbot instance used for executing queries.\\n        :param max_workers: Maximum number of worker threads (default is 4).\\n        \"\"\"\\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\\n        self.index = index\\n        self.chatbot = chatbot\\n        self.write_func = write_func if write_func else self.llm_response\\n        self.new_index_name = self.index.name + f\"_{task_name}\"\\n        self.context = context\\n\\n    def llm_response(self, chatbot: Chat, message: str, context=None, id=None):\\n        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\\n        # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\\n        #     return \"the message is too long to be processed\"\\n        # moved the error catching to multi-threading but custom method could report the error here\\n        return chatbot.reply(message)\\n\\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\\n        \"\"\"\\n        Execute a sub-task using a separate copy of the chatbot instance.\\n\\n        :param sub_path: List of indices representing the sub-task\\'s sequence.\\n        :return: List of strings representing the responses for each query in the sub-task.\\n        \"\"\"\\n        if self.parallel:\\n            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\\n            chatbot_instance = copy.deepcopy(self.chatbot)\\n        else:\\n            chatbot_instance = self.chatbot\\n        if isinstance(self.chatbot, BaseThread):\\n            chatbot_instance.reset_memory()\\n\\n        sub_results = {}\\n        for i in sub_path:\\n            current_val = self.index.values[i]\\n            response = self.write_func(\\n                chatbot_instance, current_val, self.context, id=i\\n            )\\n            sub_results[i] = response\\n        return sub_results\\n\\n    def write(self):\\n        content_to_write = self.work()\\n        self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\\n        self.new_index.save()\\n        return self.new_index\\n', '\\n@field_validator(\\'values\\')\\ndef check_type_dictionaries(cls, values):\\n    first_type_dictionary = values[0].type_dictionary\\n    for value in values[1:]:\\n        if value.type_dictionary != first_type_dictionary:\\n            raise ValueError(\"All elements in \\'values\\' should have the same \\'type_dictionary\\'.\")\\n    return values\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    df: pd.DataFrame,\n",
      "    row_func: Optional[Callable[[pd.Series], str]] = None,\n",
      "    name=\"pandas_index\",\n",
      "    columns: Optional[List[str]] = None,\n",
      "    load=False,\n",
      "):\n",
      "    \"\"\"\n",
      "        Initialize a PandasIndex object.\n",
      "\n",
      "        Args:\n",
      "            df: A pandas DataFrame to index.\n",
      "            row_func: An optional function to process rows before adding them to the index.\n",
      "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
      "        \"\"\"\n",
      "    if row_func is None:\n",
      "        row_func = lambda row: str(row)\n",
      "    self.row_func = row_func\n",
      "\n",
      "    self.df = df\n",
      "    MemoryIndex.__init__(\n",
      "        self, name=name, load=load\n",
      "    )  # Initialize the parent MemoryIndex class\n",
      "\n",
      "    for _, row in df.iterrows():\n",
      "        self.add_to_index(row_func(row))\n",
      "\n",
      "    self.columns: Dict[str, MemoryIndex] = {}\n",
      "\n",
      "    # Set up columns during initialization\n",
      "    if columns is None:\n",
      "        self.setup_columns()\n",
      "    else:\n",
      "        self.setup_columns(columns)\n",
      "    self.save()\n",
      "    for col in self.columns:\n",
      "        self.columns[col].save()\n",
      "    self.executed_tasks = []\n",
      "\n",
      "Function call: str\n",
      "Function call: row_func\n",
      "Related codes: ['\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def setup_columns(self, columns: Optional[List[str]] = None):\n",
      "    \"\"\"\n",
      "        Set up columns for indexing.\n",
      "\n",
      "        Args:\n",
      "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
      "        \"\"\"\n",
      "    if columns is None:\n",
      "        # Use string columns or columns with lists containing a single string by default\n",
      "        columns = []\n",
      "\n",
      "    for col in columns:\n",
      "        self.columns[col] = MemoryIndex.from_pandas(\n",
      "            self.df, columns=col, name=f\"{self.name}_{col}\"\n",
      "        )\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def query_columns(self, query: str, columns: List[str]) -> List[Tuple[str, float]]:\n",
      "    \"\"\"\n",
      "        Query the indexed columns of the DataFrame.\n",
      "\n",
      "        Args:\n",
      "            query: The search query as a string.\n",
      "            columns: A list of column names to query.\n",
      "\n",
      "        Returns:\n",
      "            A list of tuples containing the matched value and its similarity score.\n",
      "        \"\"\"\n",
      "    results = []\n",
      "    for col in columns:\n",
      "        if col in self.columns:\n",
      "            results.extend(self.columns[col].faiss_query(query))\n",
      "        else:\n",
      "            raise KeyError(\n",
      "                f\"Column '{col}' not found in PandaDb columns dictionary.\"\n",
      "            )\n",
      "    return results\n",
      "\n",
      "Function call: KeyError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def add_row(self, row: pd.Series) -> None:\n",
      "    \"\"\"\n",
      "        Add a row to the DataFrame and update the row and column indexes.\n",
      "\n",
      "        Args:\n",
      "            row: A pandas Series representing the row to add.\n",
      "        \"\"\"\n",
      "    self.df = pd.concat([self.df, row.to_frame().T], ignore_index=True)\n",
      "    self.add_to_index(self.row_func(row))\n",
      "\n",
      "    for col in self.columns:\n",
      "        if col in row:\n",
      "            self.columns[col].add_to_index(row[col])\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def remove_row(self, index: int) -> None:\n",
      "    \"\"\"\n",
      "        Remove a row from the DataFrame and update the row and column indexes.\n",
      "\n",
      "        Args:\n",
      "            index: The index of the row to remove.\n",
      "        \"\"\"\n",
      "    if 0 <= index < len(self.df):\n",
      "        self.remove_from_index(self.values[index])\n",
      "\n",
      "        for col in self.columns:\n",
      "            self.columns[col].remove_from_index(self.columns[col].values[index])\n",
      "\n",
      "        self.df.drop(index, inplace=True)\n",
      "        self.df.reset_index(drop=True, inplace=True)\n",
      "    else:\n",
      "        raise IndexError(\n",
      "            f\"Index {index} is out of bounds for DataFrame with length {len(self.df)}\"\n",
      "        )\n",
      "\n",
      "Function call: len\n",
      "Function call: IndexError\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def rows_from_value(\n",
      "    self, value: Union[str, int, float], column: Optional[str] = None\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "        Return all rows of the DataFrame that have a particular value in the row index or a column index.\n",
      "\n",
      "        Args:\n",
      "            value: The value to search for in the DataFrame.\n",
      "            column: The name of the column to search in. If None, search in the row index.\n",
      "\n",
      "        Returns:\n",
      "            A pandas DataFrame containing the rows with the specified value.\n",
      "        \"\"\"\n",
      "    if column is None:\n",
      "        return self.df.loc[self.df.index == value]\n",
      "    else:\n",
      "        if column in self.df.columns:\n",
      "            return self.df.loc[self.df[column] == value]\n",
      "        else:\n",
      "            raise KeyError(f\"Column '{column}' not found in the DataFrame.\")\n",
      "\n",
      "Function call: KeyError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def apply_llmtask(\n",
      "    self,\n",
      "    path: List[List[int]],\n",
      "    chatbot: Chat,\n",
      "    task_name=None,\n",
      "    write_func=None,\n",
      "    columns: Optional[List[str]] = None,\n",
      "    task_id=None,\n",
      "    max_workers=1,\n",
      "    calls_per_minute: int = 20,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "        Apply a writing task to the specified columns or the main index, and create new modified indexes and a corresponding DataFrame with new values.\n",
      "\n",
      "        Args:\n",
      "            write_task: An instance of a writing task (subclass of BaseTask).\n",
      "            columns: A list of column names to apply the writing task to, or None (default) to apply the task to the main index.\n",
      "\n",
      "        Returns:\n",
      "            A pandas DataFrame containing the modified values in the specified columns or a new column with the modified values of the main index.\n",
      "        \"\"\"\n",
      "    modified_df = self.df.copy()\n",
      "    if task_name is None and task_id is None:\n",
      "        task_name = \"llm_task\"\n",
      "    elif task_name is None:\n",
      "        task_name = task_id\n",
      "\n",
      "    if columns is None:\n",
      "        # Apply the writing task to the main index\n",
      "        write_index = self\n",
      "        write_task = LLMWriter(\n",
      "            write_index,\n",
      "            path,\n",
      "            chatbot,\n",
      "            task_name=task_name,\n",
      "            write_func=write_func,\n",
      "            context=self.df,\n",
      "            task_id=task_id,\n",
      "            max_workers=max_workers,\n",
      "            calls_per_minute=calls_per_minute,\n",
      "        )\n",
      "\n",
      "        new_index = write_task.write()\n",
      "\n",
      "        # Create a mapping of old values to new values\n",
      "        old_to_new_values = dict(zip(self.values, new_index.values))\n",
      "\n",
      "        # Update the row values in the modified DataFrame\n",
      "        modified_df[\"new_column\"] = modified_df.apply(\n",
      "            lambda row: old_to_new_values.get(\n",
      "                self.row_func(row), self.row_func(row)\n",
      "            ),\n",
      "            axis=1,\n",
      "        )\n",
      "    else:\n",
      "        # Iterate over the specified columns\n",
      "        for col in columns:\n",
      "            if col in self.columns:\n",
      "                # Apply the writing task to the column\n",
      "                write_index = self.columns[col]\n",
      "                write_task = LLMWriter(\n",
      "                    write_index,\n",
      "                    path,\n",
      "                    chatbot,\n",
      "                    write_func=write_func,\n",
      "                    context=self.df,\n",
      "                    task_id=task_id,\n",
      "                    max_workers=max_workers,\n",
      "                    calls_per_minute=calls_per_minute,\n",
      "                )\n",
      "                new_index = write_task.write()\n",
      "\n",
      "                # Create a mapping of old values to new values\n",
      "                old_to_new_values = dict(\n",
      "                    zip(self.columns[col].values, new_index.values)\n",
      "                )\n",
      "\n",
      "                # Update the column values in the modified DataFrame\n",
      "                modified_df[col] = modified_df[col].apply(\n",
      "                    lambda x: old_to_new_values.get(x, x)\n",
      "                )\n",
      "\n",
      "                # Update the column's MemoryIndex\n",
      "                self.columns[col] = new_index\n",
      "                self.columns[col].save()\n",
      "            else:\n",
      "                raise KeyError(\n",
      "                    f\"Column '{col}' not found in PandasIndex columns dictionary.\"\n",
      "                )\n",
      "    # remove context from the write_task to avoid memory leak\n",
      "    write_task.context = None\n",
      "    self.executed_tasks.append({\"task\": write_task, \"output\": modified_df})\n",
      "\n",
      "    return modified_df\n",
      "\n",
      "Function call: LLMWriter\n",
      "Function call: dict\n",
      "Function call: zip\n",
      "Function call: LLMWriter\n",
      "Function call: dict\n",
      "Function call: zip\n",
      "Function call: KeyError\n",
      "Related codes: ['\\n\\nclass LLMWriter(BaseTask):\\n    def __init__(\\n        self,\\n        index: MemoryIndex,\\n        path: List[List[int]],\\n        chatbot: Chat,\\n        write_func=None,\\n        context=None,\\n        task_name=\"summary\",\\n        max_workers: int = 1,\\n        task_id: str = \"LLMWriteTask\",\\n        calls_per_minute: int = 20,\\n        backup: bool = True,\\n    ):\\n        \"\"\"\\n        Initialize a LLMWriteTask instance.\\n\\n        :param index: List of strings representing the queries.\\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\\n        :param chatbot: Chatbot instance used for executing queries.\\n        :param max_workers: Maximum number of worker threads (default is 4).\\n        \"\"\"\\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\\n        self.index = index\\n        self.chatbot = chatbot\\n        self.write_func = write_func if write_func else self.llm_response\\n        self.new_index_name = self.index.name + f\"_{task_name}\"\\n        self.context = context\\n\\n    def llm_response(self, chatbot: Chat, message: str, context=None, id=None):\\n        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\\n        # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\\n        #     return \"the message is too long to be processed\"\\n        # moved the error catching to multi-threading but custom method could report the error here\\n        return chatbot.reply(message)\\n\\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\\n        \"\"\"\\n        Execute a sub-task using a separate copy of the chatbot instance.\\n\\n        :param sub_path: List of indices representing the sub-task\\'s sequence.\\n        :return: List of strings representing the responses for each query in the sub-task.\\n        \"\"\"\\n        if self.parallel:\\n            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\\n            chatbot_instance = copy.deepcopy(self.chatbot)\\n        else:\\n            chatbot_instance = self.chatbot\\n        if isinstance(self.chatbot, BaseThread):\\n            chatbot_instance.reset_memory()\\n\\n        sub_results = {}\\n        for i in sub_path:\\n            current_val = self.index.values[i]\\n            response = self.write_func(\\n                chatbot_instance, current_val, self.context, id=i\\n            )\\n            sub_results[i] = response\\n        return sub_results\\n\\n    def write(self):\\n        content_to_write = self.work()\\n        self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\\n        self.new_index.save()\\n        return self.new_index\\n', '\\n@field_validator(\\'values\\')\\ndef check_type_dictionaries(cls, values):\\n    first_type_dictionary = values[0].type_dictionary\\n    for value in values[1:]:\\n        if value.type_dictionary != first_type_dictionary:\\n            raise ValueError(\"All elements in \\'values\\' should have the same \\'type_dictionary\\'.\")\\n    return values\\n', '\\n\\nclass LLMWriter(BaseTask):\\n    def __init__(\\n        self,\\n        index: MemoryIndex,\\n        path: List[List[int]],\\n        chatbot: Chat,\\n        write_func=None,\\n        context=None,\\n        task_name=\"summary\",\\n        max_workers: int = 1,\\n        task_id: str = \"LLMWriteTask\",\\n        calls_per_minute: int = 20,\\n        backup: bool = True,\\n    ):\\n        \"\"\"\\n        Initialize a LLMWriteTask instance.\\n\\n        :param index: List of strings representing the queries.\\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\\n        :param chatbot: Chatbot instance used for executing queries.\\n        :param max_workers: Maximum number of worker threads (default is 4).\\n        \"\"\"\\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\\n        self.index = index\\n        self.chatbot = chatbot\\n        self.write_func = write_func if write_func else self.llm_response\\n        self.new_index_name = self.index.name + f\"_{task_name}\"\\n        self.context = context\\n\\n    def llm_response(self, chatbot: Chat, message: str, context=None, id=None):\\n        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\\n        # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\\n        #     return \"the message is too long to be processed\"\\n        # moved the error catching to multi-threading but custom method could report the error here\\n        return chatbot.reply(message)\\n\\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\\n        \"\"\"\\n        Execute a sub-task using a separate copy of the chatbot instance.\\n\\n        :param sub_path: List of indices representing the sub-task\\'s sequence.\\n        :return: List of strings representing the responses for each query in the sub-task.\\n        \"\"\"\\n        if self.parallel:\\n            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\\n            chatbot_instance = copy.deepcopy(self.chatbot)\\n        else:\\n            chatbot_instance = self.chatbot\\n        if isinstance(self.chatbot, BaseThread):\\n            chatbot_instance.reset_memory()\\n\\n        sub_results = {}\\n        for i in sub_path:\\n            current_val = self.index.values[i]\\n            response = self.write_func(\\n                chatbot_instance, current_val, self.context, id=i\\n            )\\n            sub_results[i] = response\\n        return sub_results\\n\\n    def write(self):\\n        content_to_write = self.work()\\n        self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\\n        self.new_index.save()\\n        return self.new_index\\n', '\\n@field_validator(\\'values\\')\\ndef check_type_dictionaries(cls, values):\\n    first_type_dictionary = values[0].type_dictionary\\n    for value in values[1:]:\\n        if value.type_dictionary != first_type_dictionary:\\n            raise ValueError(\"All elements in \\'values\\' should have the same \\'type_dictionary\\'.\")\\n    return values\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BaseIndex(ABC):\n",
      "    def __init__(\n",
      "            self,\n",
      "            values: Optional[List[str]] = None,\n",
      "            embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "            name: str = \"np_index\",\n",
      "            save_path: Optional[str] = None,\n",
      "            load: bool = False,\n",
      "            embedder: Optional[Union[OpenAiEmbedder, CohereEmbedder]] = OpenAiEmbedder,\n",
      "            token_overflow_strategy: str = \"ignore\",\n",
      "    ):\n",
      "        self.name = name\n",
      "        self.embedder = embedder()\n",
      "        self.save_path = save_path or \"storage\"\n",
      "        os.makedirs(self.save_path, exist_ok=True)\n",
      "        self.values = []\n",
      "        self.embeddings = None  # initialize embeddings as None\n",
      "        self.queries_embeddings = None  # initialize query embeddings as None\n",
      "        self.token_overflow_strategy = token_overflow_strategy\n",
      "        self.queries = []\n",
      "        self.queries_set = set()  # add this to quickly check for duplicates\n",
      "        self.index_set = set()  # add this to quickly check for duplicates\n",
      "        self.loaded = False\n",
      "        self.setup_index(values, embeddings, load)\n",
      "    \n",
      "    @staticmethod\n",
      "    @abstractmethod\n",
      "    def compare_embeddings(query: Any, targets: Any) -> Any:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abstractmethod\n",
      "    def batched_l2_distance(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\n",
      "        pass\n",
      "\n",
      "    @staticmethod\n",
      "    @abstractmethod\n",
      "    def batched_cosine_similarity(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def get(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Union[str, List[str]]:\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def add(self, values: List[str], embeddings: Optional[List[Union[List[float], np.ndarray]]] = None):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def search(self, query: Optional[str] = None, query_embedding: Optional[np.ndarray] = None, top_k: int = 10, metric: str = \"cosine\", filter_mask: Optional[np.ndarray] = None) -> Tuple[List[str], Optional[List[float]], List[int]]:\n",
      "        pass\n",
      "    \n",
      "    @abstractmethod\n",
      "    def setup_index(self, values: Optional[List[str]] = None, embeddings: Optional[List[Union[List[float], np.ndarray]]] = None, load: bool = False):\n",
      "        pass\n",
      "    \n",
      "    # Non-abstract method\n",
      "    def save_index(self):\n",
      "        save_directory = os.path.join(self.save_path, self.name)\n",
      "        os.makedirs(save_directory, exist_ok=True)\n",
      "\n",
      "        with open(os.path.join(save_directory, f\"{self.name}_values.json\"), \"w\") as f:\n",
      "            json.dump(self.values, f)\n",
      "        #check if queries exist\n",
      "        if len(self.queries) > 0:\n",
      "            with open(os.path.join(save_directory, f\"{self.name}_queries.json\"), \"w\") as f:\n",
      "                json.dump(self.queries, f)\n",
      "\n",
      "        # Save embeddings in a subclass-specific way\n",
      "        self._save_embeddings(save_directory)\n",
      "\n",
      "    def load_index(self):\n",
      "        load_directory = os.path.join(self.save_path, self.name)\n",
      "        if not os.path.exists(load_directory):\n",
      "            print(f\"I did not find the directory to load the index from: {load_directory}\")\n",
      "            return\n",
      "\n",
      "        print(f\"Loading index from {load_directory}\")\n",
      "\n",
      "        with open(os.path.join(load_directory, f\"{self.name}_values.json\"), \"r\") as f:\n",
      "            self.values = json.load(f)\n",
      "        self.values_set = set(self.values)\n",
      "        #check that queries exist\n",
      "        if os.path.exists(os.path.join(load_directory, f\"{self.name}_queries.json\")):\n",
      "            with open(os.path.join(load_directory, f\"{self.name}_queries.json\"), \"r\") as f:\n",
      "                self.queries = json.load(f)\n",
      "            self.queries_set = set(self.queries)\n",
      "\n",
      "        # Load embeddings in a subclass-specific way\n",
      "        self._load_embeddings(load_directory)\n",
      "        self.loaded = True\n",
      "\n",
      "    @abstractmethod\n",
      "    def _save_embeddings(self, directory: str):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def _load_embeddings(self, directory: str):\n",
      "        pass\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"ABC\"]\n",
      "]\n",
      "Function call: embedder\n",
      "Function call: set\n",
      "Function call: set\n",
      "Function call: open\n",
      "Function call: len\n",
      "Function call: open\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: open\n",
      "Function call: set\n",
      "Function call: open\n",
      "Function call: set\n",
      "Related codes: [\"\\ndef numeric_embedder(column):\\n    # Implement the numeric embedding strategy\\n    # This will depend on the type of `column` (whether it's a string, Series, etc.)\\n    # Here we'll assume `column` is a pandas Series for simplicity\\n    return column.values\\n\", '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "        self,\n",
      "        values: Optional[List[str]] = None,\n",
      "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "        name: str = \"np_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        load: bool = False,\n",
      "        embedder: Optional[Union[OpenAiEmbedder, CohereEmbedder]] = OpenAiEmbedder,\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "):\n",
      "    self.name = name\n",
      "    self.embedder = embedder()\n",
      "    self.save_path = save_path or \"storage\"\n",
      "    os.makedirs(self.save_path, exist_ok=True)\n",
      "    self.values = []\n",
      "    self.embeddings = None  # initialize embeddings as None\n",
      "    self.queries_embeddings = None  # initialize query embeddings as None\n",
      "    self.token_overflow_strategy = token_overflow_strategy\n",
      "    self.queries = []\n",
      "    self.queries_set = set()  # add this to quickly check for duplicates\n",
      "    self.index_set = set()  # add this to quickly check for duplicates\n",
      "    self.loaded = False\n",
      "    self.setup_index(values, embeddings, load)\n",
      "\n",
      "Function call: embedder\n",
      "Function call: set\n",
      "Function call: set\n",
      "Related codes: [\"\\ndef numeric_embedder(column):\\n    # Implement the numeric embedding strategy\\n    # This will depend on the type of `column` (whether it's a string, Series, etc.)\\n    # Here we'll assume `column` is a pandas Series for simplicity\\n    return column.values\\n\", '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@staticmethod\n",
      "@abstractmethod\n",
      "def compare_embeddings(query: Any, targets: Any) -> Any:\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@staticmethod\n",
      "@abstractmethod\n",
      "def batched_l2_distance(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@staticmethod\n",
      "@abstractmethod\n",
      "def batched_cosine_similarity(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def get(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Union[str, List[str]]:\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def add(self, values: List[str], embeddings: Optional[List[Union[List[float], np.ndarray]]] = None):\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def search(self, query: Optional[str] = None, query_embedding: Optional[np.ndarray] = None, top_k: int = 10, metric: str = \"cosine\", filter_mask: Optional[np.ndarray] = None) -> Tuple[List[str], Optional[List[float]], List[int]]:\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def setup_index(self, values: Optional[List[str]] = None, embeddings: Optional[List[Union[List[float], np.ndarray]]] = None, load: bool = False):\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Non-abstract method\n",
      "def save_index(self):\n",
      "    save_directory = os.path.join(self.save_path, self.name)\n",
      "    os.makedirs(save_directory, exist_ok=True)\n",
      "\n",
      "    with open(os.path.join(save_directory, f\"{self.name}_values.json\"), \"w\") as f:\n",
      "        json.dump(self.values, f)\n",
      "    #check if queries exist\n",
      "    if len(self.queries) > 0:\n",
      "        with open(os.path.join(save_directory, f\"{self.name}_queries.json\"), \"w\") as f:\n",
      "            json.dump(self.queries, f)\n",
      "\n",
      "    # Save embeddings in a subclass-specific way\n",
      "    self._save_embeddings(save_directory)\n",
      "\n",
      "Function call: open\n",
      "Function call: len\n",
      "Function call: open\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def load_index(self):\n",
      "    load_directory = os.path.join(self.save_path, self.name)\n",
      "    if not os.path.exists(load_directory):\n",
      "        print(f\"I did not find the directory to load the index from: {load_directory}\")\n",
      "        return\n",
      "\n",
      "    print(f\"Loading index from {load_directory}\")\n",
      "\n",
      "    with open(os.path.join(load_directory, f\"{self.name}_values.json\"), \"r\") as f:\n",
      "        self.values = json.load(f)\n",
      "    self.values_set = set(self.values)\n",
      "    #check that queries exist\n",
      "    if os.path.exists(os.path.join(load_directory, f\"{self.name}_queries.json\")):\n",
      "        with open(os.path.join(load_directory, f\"{self.name}_queries.json\"), \"r\") as f:\n",
      "            self.queries = json.load(f)\n",
      "        self.queries_set = set(self.queries)\n",
      "\n",
      "    # Load embeddings in a subclass-specific way\n",
      "    self._load_embeddings(load_directory)\n",
      "    self.loaded = True\n",
      "\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: open\n",
      "Function call: set\n",
      "Function call: open\n",
      "Function call: set\n",
      "Related codes: ['\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def _save_embeddings(self, directory: str):\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@abstractmethod\n",
      "def _load_embeddings(self, directory: str):\n",
      "    pass\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class NpIndex(BaseIndex):\n",
      "    def __init__(\n",
      "            self,\n",
      "            values: Optional[List[str]] = None,\n",
      "            embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "            name: str = \"np_index\",\n",
      "            save_path: Optional[str] = None,\n",
      "            load: bool = False,\n",
      "            embedder: Optional[Union[OpenAiEmbedder, CohereEmbedder]] = OpenAiEmbedder,\n",
      "            token_overflow_strategy: str = \"ignore\",\n",
      "    ):\n",
      "        self.old_ids = collections.OrderedDict()\n",
      "        BaseIndex.__init__(self,values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "\n",
      "    @staticmethod\n",
      "    def compare_embeddings(query: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
      "        return np.array([np.allclose(query, target, rtol=1e-05, atol=1e-08) for target in targets])\n",
      "\n",
      "    @staticmethod\n",
      "    def batched_l2_distance(query_embedding: np.ndarray, embeddings: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
      "        scores = np.linalg.norm(embeddings - query_embedding, axis=1)\n",
      "        if mask is not None:\n",
      "            scores[~mask.astype(bool)] = np.inf  # set scores of excluded embeddings to infinity\n",
      "        return scores\n",
      "\n",
      "    @staticmethod\n",
      "    def batched_cosine_similarity(query_embedding: np.ndarray, embeddings: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
      "        scores = np.dot(embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True),\n",
      "                        query_embedding / np.linalg.norm(query_embedding))\n",
      "        if mask is not None:\n",
      "            scores[~mask.astype(bool)] = -np.inf  # set scores of excluded embeddings to negative infinity\n",
      "        return scores\n",
      "\n",
      "    def _save_embeddings(self, directory: str):\n",
      "        np.save(os.path.join(directory, f\"{self.name}_embeddings.npy\"), self.embeddings)\n",
      "        if self.queries_embeddings is not None:\n",
      "            np.save(os.path.join(directory, f\"{self.name}_queries_embeddings.npy\"), self.queries_embeddings)\n",
      "\n",
      "\n",
      "    def _load_embeddings(self, directory: str):\n",
      "        load_directory = os.path.join(self.save_path, self.name)\n",
      "        if not os.path.exists(load_directory):\n",
      "            print(f\"I did not find the directory to load the embe from: {load_directory}\")\n",
      "            return\n",
      "\n",
      "        self.embeddings = np.load(os.path.join(load_directory, f\"{self.name}_embeddings.npy\"))\n",
      "        if len(self.values) != len(self.embeddings):\n",
      "            raise ValueError(\"Loaded values and embeddings are not the same length.\")\n",
      "        #check that queries embeddings exist\n",
      "        if os.path.exists(os.path.join(load_directory, f\"{self.name}_queries_embeddings.npy\")):\n",
      "            self.queries_embeddings = np.load(os.path.join(load_directory, f\"{self.name}_queries_embeddings.npy\"),allow_pickle=True)\n",
      "            print(self.embeddings.shape, len(self.values))\n",
      "            print(self.queries_embeddings.shape, len(self.queries))\n",
      "            print(self.queries, self.queries_embeddings, self.queries_embeddings is not None, type(self.queries_embeddings), self.queries_embeddings.shape)\n",
      "\n",
      "            if self.queries_embeddings is not None and len(self.queries) != len(self.queries_embeddings):\n",
      "                raise ValueError(\"Loaded queries and queries embeddings are not the same length.\")\n",
      "\n",
      "\n",
      "    def setup_index(self, input_values: Optional[List[str]], embeddings: Optional[List[Union[List[float], np.ndarray]]], load: bool):\n",
      "        if load and os.path.exists(os.path.join(self.save_path, self.name)):\n",
      "            self.load_index()\n",
      "\n",
      "        elif input_values and embeddings and len(input_values) == len(embeddings):\n",
      "            # Check that input_values and embeddings are the same length\n",
      "            unique_dict = collections.defaultdict(list)\n",
      "            for val, emb in zip(input_values, embeddings):\n",
      "                unique_dict[val].append(emb)\n",
      "\n",
      "            # Ensure that all embeddings for each value are identical\n",
      "            for val in unique_dict.keys():\n",
      "                if len(unique_dict[val]) > 1 and not all(np.array_equal(unique_dict[val][0], emb) for emb in unique_dict[val]):\n",
      "                    raise ValueError(f'Different embeddings for the same value \"{val}\" found.')\n",
      "\n",
      "            self.add(list(unique_dict.keys()), [unique_dict[val][0] for val in unique_dict.keys()])\n",
      "            self.save_index()\n",
      "\n",
      "        elif input_values:\n",
      "            # Embed the input_values\n",
      "            self.add(list(set(input_values)))\n",
      "            self.save_index()\n",
      "\n",
      "    def get(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], output_type: str = \"value\") -> Union[int, str, np.ndarray, Dict[str, Union[int, str, np.ndarray]]]:\n",
      "\n",
      "        index = self.identify_input(identifier)\n",
      "        # Define output types\n",
      "        output_types = {\n",
      "            'index': index,\n",
      "            'value': self.values[index],\n",
      "            'embedding': self.embeddings[index],\n",
      "            'all': {\n",
      "                'index': index,\n",
      "                'value': self.values[index],\n",
      "                'embedding': self.embeddings[index]}\n",
      "        }\n",
      "\n",
      "        # Check output type is valid\n",
      "        if output_type not in output_types:\n",
      "            raise ValueError(\"Invalid output_type. Expected 'index', 'value', or 'embedding'.\")\n",
      "\n",
      "        return output_types[output_type]\n",
      "\n",
      "    def validate_value_length(self, value: str, tokens: List[int]) -> Union[bool, str]:\n",
      "        if not isinstance(value, str):\n",
      "            raise TypeError(\"Value must be a string.\")\n",
      "        token_len = len(tokens)\n",
      "        overflow_check = token_len > MAX_CONTEXT_LENGTH\n",
      "        match self.token_overflow_strategy:\n",
      "            case \"ignore\":\n",
      "                if overflow_check:\n",
      "                    return False, value\n",
      "                else:\n",
      "                    return True, value\n",
      "            case \"truncate\":\n",
      "                if overflow_check:\n",
      "                    return True, TOKENIZER.decode(tokens[:MAX_CONTEXT_LENGTH])\n",
      "                else:\n",
      "                    return True, value\n",
      "            case \"error\":\n",
      "                if overflow_check:\n",
      "                    raise ValueError(f\" The input is too long for OpenAI, num tokens is {token_len}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "                else:\n",
      "                    return True, value\n",
      "            case _:\n",
      "                raise ValueError(\"Invalid token_overflow_strategy. Expected 'ignore', 'truncate', or 'error'.\")\n",
      "\n",
      "    def add(self, values: List[str], embeddings: Optional[List[Union[List[float], np.ndarray]]] = None):\n",
      "\n",
      "        if embeddings is not None and len(values) != len(embeddings):\n",
      "            raise ValueError(\"values and embeddings must be the same length\")\n",
      "\n",
      "        # Check for duplicates and only add unique values\n",
      "        unique_values = []\n",
      "        unique_embeddings = []\n",
      "        token_batch = TOKENIZER.encode_batch(values, allowed_special=\"all\")\n",
      "        #extract the max value in old_ids consider that each old_ids is a list take the max of the list itself\n",
      "        for i, (val, tokens) in enumerate(zip(values, token_batch)):\n",
      "            is_valid, value = self.validate_value_length(val, tokens)\n",
      "            if not is_valid:\n",
      "                print(f\"Value '{value[:50]}...' is too long and will be ignored.\")\n",
      "                continue\n",
      "            if value not in self.index_set:\n",
      "                unique_values.append(value)\n",
      "                self.index_set.add(value)\n",
      "\n",
      "                self.old_ids[value] = [i]\n",
      "                if embeddings is not None:\n",
      "                    unique_embeddings.append(embeddings[i])\n",
      "            else:\n",
      "                self.old_ids[value].append(i)\n",
      "\n",
      "        if not unique_values:\n",
      "            print(\"All values already exist in the index. No values were added.\")\n",
      "            return\n",
      "        if embeddings is None:\n",
      "            unique_embeddings = self.embedder.embed(unique_values)\n",
      "\n",
      "        # Add unique values to the set\n",
      "        self.index_set.update(unique_values)\n",
      "\n",
      "        # If embeddings array is not yet created, initialize it, else append to it\n",
      "        if self.embeddings is None:\n",
      "            logger.info(\"Initializing embeddings array\")\n",
      "            self.embeddings = np.vstack(unique_embeddings)\n",
      "        else:\n",
      "            self.embeddings = np.vstack((self.embeddings, unique_embeddings))\n",
      "\n",
      "        self.values.extend(unique_values)\n",
      "\n",
      "    def identify_input(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Union[int, str, np.ndarray]:\n",
      "        if isinstance(identifier, int):  # if given an index\n",
      "            if identifier < len(self.values):  # if valid index\n",
      "                index = identifier\n",
      "            else:\n",
      "                raise ValueError(\"Invalid index given.\")\n",
      "        elif isinstance(identifier, str):  # if given a value\n",
      "            if identifier in self.values:\n",
      "                index = self.values.index(identifier)\n",
      "            else:\n",
      "                raise ValueError(\"Value not found.\")\n",
      "        elif isinstance(identifier, np.ndarray):  # if given an embedding\n",
      "            indices = np.where(self.compare_embeddings(identifier, self.embeddings))[0]\n",
      "            if len(indices) == 0:\n",
      "                raise ValueError(\"Embedding not found.\")\n",
      "            index = indices[0]\n",
      "        else:\n",
      "            raise TypeError(\"Invalid identifier type. Expected int, str, np.ndarray, or list of these types\")\n",
      "\n",
      "        return index\n",
      "\n",
      "\n",
      "    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[str, np.ndarray]]]) -> None:\n",
      "\n",
      "        if isinstance(identifier, list):\n",
      "            if all(isinstance(i, type(identifier[0])) for i in identifier) and not isinstance(identifier[0], list) and not isinstance(identifier[0], int):\n",
      "                for i in identifier:\n",
      "                    self.remove(i)\n",
      "            else:\n",
      "                raise TypeError(\"All elements in the list must be of the same type.\")\n",
      "            return\n",
      "        id = self.identify_input(identifier)\n",
      "        value = self.values[id]\n",
      "        self.index_set.remove(value)\n",
      "        self.old_ids.pop(value)\n",
      "        self.values.pop(id)\n",
      "        self.embeddings = np.delete(self.embeddings, [id], axis=0)\n",
      "\n",
      "\n",
      "    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[str, np.ndarray]]], new_value: Union[str, List[str]], new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n",
      "        if isinstance(new_value,str) and new_value in self.index_set:\n",
      "            raise ValueError(\"new_value already exists in the index. Please remove it first.\")\n",
      "        elif isinstance(new_value, list) and any(v in self.index_set for v in new_value):\n",
      "            raise ValueError(\"One or more new_value already exists in the index. Please remove them first.\")\n",
      "        if isinstance(old_identifier, list) and not isinstance(old_identifier[0], list) and not isinstance(old_identifier[0], int):\n",
      "            if not isinstance(new_value, list) or len(old_identifier) != len(new_value):\n",
      "                raise ValueError(\"For list inputs, old_identifier and new_value must all be lists of the same length.\")\n",
      "            if new_embedding is not None:\n",
      "                if not isinstance(new_embedding, list) or len(old_identifier) != len(new_embedding):\n",
      "                    raise ValueError(\"new_embedding must be a list of same length as old_identifier and new_value.\")\n",
      "                # If new_embedding is a 2D array or list of lists\n",
      "                if isinstance(new_embedding[0], list) or isinstance(new_embedding[0], np.ndarray):\n",
      "                    if len(new_embedding[0]) != len(self.embeddings[0]):\n",
      "                        raise ValueError(\"Each row in new_embedding must have the same dimension as the original embeddings.\")\n",
      "            else:\n",
      "                new_embedding = self.embedder.embed(new_value)\n",
      "            for old_id, new_val, new_emb in zip(old_identifier, new_value, new_embedding):\n",
      "                self.update(old_id, new_val, new_emb)\n",
      "            return\n",
      "        old_id = self.identify_input(old_identifier)\n",
      "        self.index_set.remove(self.values[old_id])\n",
      "        self.old_ids[new_value] = self.old_ids.pop(self.values[old_id])\n",
      "        self.values[old_id] = new_value\n",
      "        self.index_set.add(new_value)\n",
      "        self.embeddings[old_id] = self.embedder.embed([new_value]) if new_embedding is None else new_embedding\n",
      "\n",
      "    def search(self, query: Optional[str] = None, query_embedding: Optional[np.ndarray] = None, top_k: int = 10, metric: str = \"cosine\", filter_mask: Optional[np.ndarray] = None) -> Tuple[List[str], Optional[List[float]], List[int]]:\n",
      "\n",
      "        # create a 2D numpy array from the embeddings list\n",
      "        embeddings_array = self.embeddings\n",
      "\n",
      "        # if no query or query embedding is provided, return random samples\n",
      "        if query is None and query_embedding is None:\n",
      "            indices = np.random.choice(len(self.values), size=top_k, replace=False)\n",
      "            return [self.values[i] for i in indices],None, indices  # return indices with dummy scores\n",
      "\n",
      "        # if the query is in the queries set, use the stored embedding\n",
      "        if query in self.queries_set:\n",
      "            print(\"Query is in queries set.\")\n",
      "            query_embedding = self.queries_embeddings[self.queries.index(query)]\n",
      "        # else if a query string is provided but not in queries set, compute its embedding\n",
      "        elif query is not None:\n",
      "            print(\"Query is not in queries set. Computing embedding...\")\n",
      "            query_embedding = self.embedder.embed([query])\n",
      "            # print(query_embedding)\n",
      "            self.queries_set.add(query)\n",
      "            self.queries.append(query)\n",
      "            # If queries_embeddings array is not yet created, initialize it, else append to it\n",
      "            if self.queries_embeddings is None:\n",
      "                self.queries_embeddings = np.array([query_embedding])\n",
      "            else:\n",
      "                self.queries_embeddings = np.vstack((self.queries_embeddings, query_embedding))\n",
      "\n",
      "        # compute distances or similarities\n",
      "        if metric == \"l2\":\n",
      "            scores = self.batched_l2_distance(query_embedding, embeddings_array, filter_mask)\n",
      "        elif metric == \"cosine\":\n",
      "            scores = self.batched_cosine_similarity(query_embedding, embeddings_array, filter_mask)\n",
      "        else:\n",
      "            raise ValueError(\"Invalid metric. Expected 'l2' or 'cosine'.\")\n",
      "\n",
      "        # sort by scores\n",
      "        sorted_indices = np.argsort(scores)\n",
      "\n",
      "        # for L2 distance, closer vectors are better (smaller distance)\n",
      "        # for cosine similarity, further vectors are better (larger similarity)\n",
      "        top_k = min(top_k, len(self.values))\n",
      "        top_indices = sorted_indices[:top_k] if metric == \"l2\" else sorted_indices[-top_k:][::-1]\n",
      "\n",
      "        # return indices and scores\n",
      "        return  [self.values[i] for i in top_indices],[scores[i] for i in top_indices], [i for i in top_indices]\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseIndex\"]\n",
      "]\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: zip\n",
      "Function call: len\n",
      "Function call: all\n",
      "Function call: ValueError\n",
      "Function call: list\n",
      "Function call: list\n",
      "Function call: set\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: TypeError\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: ValueError\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: enumerate\n",
      "Function call: zip\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: TypeError\n",
      "Function call: isinstance\n",
      "Function call: all\n",
      "Function call: isinstance\n",
      "Function call: type\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: TypeError\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: any\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: zip\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: ValueError\n",
      "Function call: min\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "        self,\n",
      "        values: Optional[List[str]] = None,\n",
      "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
      "        name: str = \"np_index\",\n",
      "        save_path: Optional[str] = None,\n",
      "        load: bool = False,\n",
      "        embedder: Optional[Union[OpenAiEmbedder, CohereEmbedder]] = OpenAiEmbedder,\n",
      "        token_overflow_strategy: str = \"ignore\",\n",
      "):\n",
      "    self.old_ids = collections.OrderedDict()\n",
      "    BaseIndex.__init__(self,values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "@staticmethod\n",
      "def compare_embeddings(query: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
      "    return np.array([np.allclose(query, target, rtol=1e-05, atol=1e-08) for target in targets])\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@staticmethod\n",
      "def batched_l2_distance(query_embedding: np.ndarray, embeddings: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
      "    scores = np.linalg.norm(embeddings - query_embedding, axis=1)\n",
      "    if mask is not None:\n",
      "        scores[~mask.astype(bool)] = np.inf  # set scores of excluded embeddings to infinity\n",
      "    return scores\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@staticmethod\n",
      "def batched_cosine_similarity(query_embedding: np.ndarray, embeddings: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
      "    scores = np.dot(embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True),\n",
      "                    query_embedding / np.linalg.norm(query_embedding))\n",
      "    if mask is not None:\n",
      "        scores[~mask.astype(bool)] = -np.inf  # set scores of excluded embeddings to negative infinity\n",
      "    return scores\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _save_embeddings(self, directory: str):\n",
      "    np.save(os.path.join(directory, f\"{self.name}_embeddings.npy\"), self.embeddings)\n",
      "    if self.queries_embeddings is not None:\n",
      "        np.save(os.path.join(directory, f\"{self.name}_queries_embeddings.npy\"), self.queries_embeddings)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def _load_embeddings(self, directory: str):\n",
      "    load_directory = os.path.join(self.save_path, self.name)\n",
      "    if not os.path.exists(load_directory):\n",
      "        print(f\"I did not find the directory to load the embe from: {load_directory}\")\n",
      "        return\n",
      "\n",
      "    self.embeddings = np.load(os.path.join(load_directory, f\"{self.name}_embeddings.npy\"))\n",
      "    if len(self.values) != len(self.embeddings):\n",
      "        raise ValueError(\"Loaded values and embeddings are not the same length.\")\n",
      "    #check that queries embeddings exist\n",
      "    if os.path.exists(os.path.join(load_directory, f\"{self.name}_queries_embeddings.npy\")):\n",
      "        self.queries_embeddings = np.load(os.path.join(load_directory, f\"{self.name}_queries_embeddings.npy\"),allow_pickle=True)\n",
      "        print(self.embeddings.shape, len(self.values))\n",
      "        print(self.queries_embeddings.shape, len(self.queries))\n",
      "        print(self.queries, self.queries_embeddings, self.queries_embeddings is not None, type(self.queries_embeddings), self.queries_embeddings.shape)\n",
      "\n",
      "        if self.queries_embeddings is not None and len(self.queries) != len(self.queries_embeddings):\n",
      "            raise ValueError(\"Loaded queries and queries embeddings are not the same length.\")\n",
      "\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: type\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def setup_index(self, input_values: Optional[List[str]], embeddings: Optional[List[Union[List[float], np.ndarray]]], load: bool):\n",
      "    if load and os.path.exists(os.path.join(self.save_path, self.name)):\n",
      "        self.load_index()\n",
      "\n",
      "    elif input_values and embeddings and len(input_values) == len(embeddings):\n",
      "        # Check that input_values and embeddings are the same length\n",
      "        unique_dict = collections.defaultdict(list)\n",
      "        for val, emb in zip(input_values, embeddings):\n",
      "            unique_dict[val].append(emb)\n",
      "\n",
      "        # Ensure that all embeddings for each value are identical\n",
      "        for val in unique_dict.keys():\n",
      "            if len(unique_dict[val]) > 1 and not all(np.array_equal(unique_dict[val][0], emb) for emb in unique_dict[val]):\n",
      "                raise ValueError(f'Different embeddings for the same value \"{val}\" found.')\n",
      "\n",
      "        self.add(list(unique_dict.keys()), [unique_dict[val][0] for val in unique_dict.keys()])\n",
      "        self.save_index()\n",
      "\n",
      "    elif input_values:\n",
      "        # Embed the input_values\n",
      "        self.add(list(set(input_values)))\n",
      "        self.save_index()\n",
      "\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: zip\n",
      "Function call: len\n",
      "Function call: all\n",
      "Function call: ValueError\n",
      "Function call: list\n",
      "Function call: list\n",
      "Function call: set\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], output_type: str = \"value\") -> Union[int, str, np.ndarray, Dict[str, Union[int, str, np.ndarray]]]:\n",
      "\n",
      "    index = self.identify_input(identifier)\n",
      "    # Define output types\n",
      "    output_types = {\n",
      "        'index': index,\n",
      "        'value': self.values[index],\n",
      "        'embedding': self.embeddings[index],\n",
      "        'all': {\n",
      "            'index': index,\n",
      "            'value': self.values[index],\n",
      "            'embedding': self.embeddings[index]}\n",
      "    }\n",
      "\n",
      "    # Check output type is valid\n",
      "    if output_type not in output_types:\n",
      "        raise ValueError(\"Invalid output_type. Expected 'index', 'value', or 'embedding'.\")\n",
      "\n",
      "    return output_types[output_type]\n",
      "\n",
      "Function call: ValueError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def validate_value_length(self, value: str, tokens: List[int]) -> Union[bool, str]:\n",
      "    if not isinstance(value, str):\n",
      "        raise TypeError(\"Value must be a string.\")\n",
      "    token_len = len(tokens)\n",
      "    overflow_check = token_len > MAX_CONTEXT_LENGTH\n",
      "    match self.token_overflow_strategy:\n",
      "        case \"ignore\":\n",
      "            if overflow_check:\n",
      "                return False, value\n",
      "            else:\n",
      "                return True, value\n",
      "        case \"truncate\":\n",
      "            if overflow_check:\n",
      "                return True, TOKENIZER.decode(tokens[:MAX_CONTEXT_LENGTH])\n",
      "            else:\n",
      "                return True, value\n",
      "        case \"error\":\n",
      "            if overflow_check:\n",
      "                raise ValueError(f\" The input is too long for OpenAI, num tokens is {token_len}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "            else:\n",
      "                return True, value\n",
      "        case _:\n",
      "            raise ValueError(\"Invalid token_overflow_strategy. Expected 'ignore', 'truncate', or 'error'.\")\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: TypeError\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def add(self, values: List[str], embeddings: Optional[List[Union[List[float], np.ndarray]]] = None):\n",
      "\n",
      "    if embeddings is not None and len(values) != len(embeddings):\n",
      "        raise ValueError(\"values and embeddings must be the same length\")\n",
      "\n",
      "    # Check for duplicates and only add unique values\n",
      "    unique_values = []\n",
      "    unique_embeddings = []\n",
      "    token_batch = TOKENIZER.encode_batch(values, allowed_special=\"all\")\n",
      "    #extract the max value in old_ids consider that each old_ids is a list take the max of the list itself\n",
      "    for i, (val, tokens) in enumerate(zip(values, token_batch)):\n",
      "        is_valid, value = self.validate_value_length(val, tokens)\n",
      "        if not is_valid:\n",
      "            print(f\"Value '{value[:50]}...' is too long and will be ignored.\")\n",
      "            continue\n",
      "        if value not in self.index_set:\n",
      "            unique_values.append(value)\n",
      "            self.index_set.add(value)\n",
      "\n",
      "            self.old_ids[value] = [i]\n",
      "            if embeddings is not None:\n",
      "                unique_embeddings.append(embeddings[i])\n",
      "        else:\n",
      "            self.old_ids[value].append(i)\n",
      "\n",
      "    if not unique_values:\n",
      "        print(\"All values already exist in the index. No values were added.\")\n",
      "        return\n",
      "    if embeddings is None:\n",
      "        unique_embeddings = self.embedder.embed(unique_values)\n",
      "\n",
      "    # Add unique values to the set\n",
      "    self.index_set.update(unique_values)\n",
      "\n",
      "    # If embeddings array is not yet created, initialize it, else append to it\n",
      "    if self.embeddings is None:\n",
      "        logger.info(\"Initializing embeddings array\")\n",
      "        self.embeddings = np.vstack(unique_embeddings)\n",
      "    else:\n",
      "        self.embeddings = np.vstack((self.embeddings, unique_embeddings))\n",
      "\n",
      "    self.values.extend(unique_values)\n",
      "\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: enumerate\n",
      "Function call: zip\n",
      "Function call: print\n",
      "Function call: print\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def identify_input(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Union[int, str, np.ndarray]:\n",
      "    if isinstance(identifier, int):  # if given an index\n",
      "        if identifier < len(self.values):  # if valid index\n",
      "            index = identifier\n",
      "        else:\n",
      "            raise ValueError(\"Invalid index given.\")\n",
      "    elif isinstance(identifier, str):  # if given a value\n",
      "        if identifier in self.values:\n",
      "            index = self.values.index(identifier)\n",
      "        else:\n",
      "            raise ValueError(\"Value not found.\")\n",
      "    elif isinstance(identifier, np.ndarray):  # if given an embedding\n",
      "        indices = np.where(self.compare_embeddings(identifier, self.embeddings))[0]\n",
      "        if len(indices) == 0:\n",
      "            raise ValueError(\"Embedding not found.\")\n",
      "        index = indices[0]\n",
      "    else:\n",
      "        raise TypeError(\"Invalid identifier type. Expected int, str, np.ndarray, or list of these types\")\n",
      "\n",
      "    return index\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: TypeError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def remove(self, identifier: Union[int, str, np.ndarray, List[Union[str, np.ndarray]]]) -> None:\n",
      "\n",
      "    if isinstance(identifier, list):\n",
      "        if all(isinstance(i, type(identifier[0])) for i in identifier) and not isinstance(identifier[0], list) and not isinstance(identifier[0], int):\n",
      "            for i in identifier:\n",
      "                self.remove(i)\n",
      "        else:\n",
      "            raise TypeError(\"All elements in the list must be of the same type.\")\n",
      "        return\n",
      "    id = self.identify_input(identifier)\n",
      "    value = self.values[id]\n",
      "    self.index_set.remove(value)\n",
      "    self.old_ids.pop(value)\n",
      "    self.values.pop(id)\n",
      "    self.embeddings = np.delete(self.embeddings, [id], axis=0)\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: all\n",
      "Function call: isinstance\n",
      "Function call: type\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: TypeError\n",
      "Related codes: ['\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[str, np.ndarray]]], new_value: Union[str, List[str]], new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\n",
      "    if isinstance(new_value,str) and new_value in self.index_set:\n",
      "        raise ValueError(\"new_value already exists in the index. Please remove it first.\")\n",
      "    elif isinstance(new_value, list) and any(v in self.index_set for v in new_value):\n",
      "        raise ValueError(\"One or more new_value already exists in the index. Please remove them first.\")\n",
      "    if isinstance(old_identifier, list) and not isinstance(old_identifier[0], list) and not isinstance(old_identifier[0], int):\n",
      "        if not isinstance(new_value, list) or len(old_identifier) != len(new_value):\n",
      "            raise ValueError(\"For list inputs, old_identifier and new_value must all be lists of the same length.\")\n",
      "        if new_embedding is not None:\n",
      "            if not isinstance(new_embedding, list) or len(old_identifier) != len(new_embedding):\n",
      "                raise ValueError(\"new_embedding must be a list of same length as old_identifier and new_value.\")\n",
      "            # If new_embedding is a 2D array or list of lists\n",
      "            if isinstance(new_embedding[0], list) or isinstance(new_embedding[0], np.ndarray):\n",
      "                if len(new_embedding[0]) != len(self.embeddings[0]):\n",
      "                    raise ValueError(\"Each row in new_embedding must have the same dimension as the original embeddings.\")\n",
      "        else:\n",
      "            new_embedding = self.embedder.embed(new_value)\n",
      "        for old_id, new_val, new_emb in zip(old_identifier, new_value, new_embedding):\n",
      "            self.update(old_id, new_val, new_emb)\n",
      "        return\n",
      "    old_id = self.identify_input(old_identifier)\n",
      "    self.index_set.remove(self.values[old_id])\n",
      "    self.old_ids[new_value] = self.old_ids.pop(self.values[old_id])\n",
      "    self.values[old_id] = new_value\n",
      "    self.index_set.add(new_value)\n",
      "    self.embeddings[old_id] = self.embedder.embed([new_value]) if new_embedding is None else new_embedding\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: any\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: zip\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def search(self, query: Optional[str] = None, query_embedding: Optional[np.ndarray] = None, top_k: int = 10, metric: str = \"cosine\", filter_mask: Optional[np.ndarray] = None) -> Tuple[List[str], Optional[List[float]], List[int]]:\n",
      "\n",
      "    # create a 2D numpy array from the embeddings list\n",
      "    embeddings_array = self.embeddings\n",
      "\n",
      "    # if no query or query embedding is provided, return random samples\n",
      "    if query is None and query_embedding is None:\n",
      "        indices = np.random.choice(len(self.values), size=top_k, replace=False)\n",
      "        return [self.values[i] for i in indices],None, indices  # return indices with dummy scores\n",
      "\n",
      "    # if the query is in the queries set, use the stored embedding\n",
      "    if query in self.queries_set:\n",
      "        print(\"Query is in queries set.\")\n",
      "        query_embedding = self.queries_embeddings[self.queries.index(query)]\n",
      "    # else if a query string is provided but not in queries set, compute its embedding\n",
      "    elif query is not None:\n",
      "        print(\"Query is not in queries set. Computing embedding...\")\n",
      "        query_embedding = self.embedder.embed([query])\n",
      "        # print(query_embedding)\n",
      "        self.queries_set.add(query)\n",
      "        self.queries.append(query)\n",
      "        # If queries_embeddings array is not yet created, initialize it, else append to it\n",
      "        if self.queries_embeddings is None:\n",
      "            self.queries_embeddings = np.array([query_embedding])\n",
      "        else:\n",
      "            self.queries_embeddings = np.vstack((self.queries_embeddings, query_embedding))\n",
      "\n",
      "    # compute distances or similarities\n",
      "    if metric == \"l2\":\n",
      "        scores = self.batched_l2_distance(query_embedding, embeddings_array, filter_mask)\n",
      "    elif metric == \"cosine\":\n",
      "        scores = self.batched_cosine_similarity(query_embedding, embeddings_array, filter_mask)\n",
      "    else:\n",
      "        raise ValueError(\"Invalid metric. Expected 'l2' or 'cosine'.\")\n",
      "\n",
      "    # sort by scores\n",
      "    sorted_indices = np.argsort(scores)\n",
      "\n",
      "    # for L2 distance, closer vectors are better (smaller distance)\n",
      "    # for cosine similarity, further vectors are better (larger similarity)\n",
      "    top_k = min(top_k, len(self.values))\n",
      "    top_indices = sorted_indices[:top_k] if metric == \"l2\" else sorted_indices[-top_k:][::-1]\n",
      "\n",
      "    # return indices and scores\n",
      "    return  [self.values[i] for i in top_indices],[scores[i] for i in top_indices], [i for i in top_indices]\n",
      "\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: ValueError\n",
      "Function call: min\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "class FifoChat(FifoThread, Chat):\n",
      "    \"\"\"\n",
      "    A chatbot class that combines FIFO Memory Thread, BaseChat, and Prompter. The oldest messages are removed first\n",
      "    when reaching the max_memory limit. The memory is defined in terms of tokens, and outs are passed to the\n",
      "    longterm_memory. The lucid_memory is a redundant memory that stores all the messages.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        model: Optional[str] = None,\n",
      "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "        system_prompt: Optional[str] = None,\n",
      "        user_prompt: Optional[str] = None,\n",
      "        name: str = \"fifo_memory\",\n",
      "        max_index_memory: int = 400,\n",
      "        max_fifo_memory: int = 2048,\n",
      "        max_output_tokens: int = 1000,\n",
      "        longterm_thread: Optional[BaseThread] = None,\n",
      "    ):\n",
      "\n",
      "        FifoThread.__init__(\n",
      "            self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread\n",
      "        )\n",
      "        Chat.__init__(\n",
      "            self,\n",
      "            model=model,\n",
      "            index_dict=index_dict,\n",
      "            max_output_tokens=max_output_tokens,\n",
      "            max_index_memory=max_index_memory,\n",
      "            system_prompt=system_prompt,\n",
      "            user_prompt=user_prompt,\n",
      "            name=name,\n",
      "        )\n",
      "\n",
      "        self.prompt_func = self.fifo_memory_prompt\n",
      "\n",
      "    def fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\n",
      "        \"\"\"\n",
      "        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "        marked_question = mark_question(self.user_prompt(message))\n",
      "        prompt = (\n",
      "            [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\n",
      "        )\n",
      "        return prompt, marked_question\n",
      "\n",
      "    def query(self, question: str, verbose: bool = True, stream: bool = False) -> Union[Generator,str]:\n",
      "        \"\"\"\n",
      "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
      "        and added to the memory.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "        marked_question = mark_question(question)\n",
      "        self.add_message(marked_question)\n",
      "        answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n",
      "        if stream:\n",
      "            return answer\n",
      "        else:\n",
      "            self.add_message(answer)\n",
      "            return answer\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"FifoThread, \", \"Chat\"]\n",
      "]\n",
      "Function call: mark_question\n",
      "Function call: mark_system\n",
      "Function call: mark_question\n",
      "Related codes: ['\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n', '\\ndef mark_system(system_prompt):\\n    return {\"role\": \"system\", \"content\": system_prompt}\\n', '\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    model: Optional[str] = None,\n",
      "    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "    system_prompt: Optional[str] = None,\n",
      "    user_prompt: Optional[str] = None,\n",
      "    name: str = \"fifo_memory\",\n",
      "    max_index_memory: int = 400,\n",
      "    max_fifo_memory: int = 2048,\n",
      "    max_output_tokens: int = 1000,\n",
      "    longterm_thread: Optional[BaseThread] = None,\n",
      "):\n",
      "\n",
      "    FifoThread.__init__(\n",
      "        self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread\n",
      "    )\n",
      "    Chat.__init__(\n",
      "        self,\n",
      "        model=model,\n",
      "        index_dict=index_dict,\n",
      "        max_output_tokens=max_output_tokens,\n",
      "        max_index_memory=max_index_memory,\n",
      "        system_prompt=system_prompt,\n",
      "        user_prompt=user_prompt,\n",
      "        name=name,\n",
      "    )\n",
      "\n",
      "    self.prompt_func = self.fifo_memory_prompt\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\n",
      "    \"\"\"\n",
      "        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "    marked_question = mark_question(self.user_prompt(message))\n",
      "    prompt = (\n",
      "        [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\n",
      "    )\n",
      "    return prompt, marked_question\n",
      "\n",
      "Function call: mark_question\n",
      "Function call: mark_system\n",
      "Related codes: ['\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n', '\\ndef mark_system(system_prompt):\\n    return {\"role\": \"system\", \"content\": system_prompt}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def query(self, question: str, verbose: bool = True, stream: bool = False) -> Union[Generator,str]:\n",
      "    \"\"\"\n",
      "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
      "        and added to the memory.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "    marked_question = mark_question(question)\n",
      "    self.add_message(marked_question)\n",
      "    answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n",
      "    if stream:\n",
      "        return answer\n",
      "    else:\n",
      "        self.add_message(answer)\n",
      "        return answer\n",
      "\n",
      "Function call: mark_question\n",
      "Related codes: ['\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class VectorChat(VectorThread, Chat):\n",
      "    \"\"\"\n",
      "    A chatbot class that combines Vector Memory Thread, BaseChat, and Prompter. Memory prompt is constructed by\n",
      "    filling the memory with the k most similar messages to the question until the max prompt memory tokens are reached.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        model: Optional[str] = None,\n",
      "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "        name: str = \"vector_memory\",\n",
      "        max_index_memory: int = 400,\n",
      "        max_vector_memory: int = 2048,\n",
      "        max_output_tokens: int = 1000,\n",
      "        system_prompt: str = None,\n",
      "        user_prompt: str = None,\n",
      "    \n",
      "    ):\n",
      "        VectorThread.__init__(self, name=name, max_context=max_vector_memory)\n",
      "        Chat.__init__(\n",
      "            self,\n",
      "            model=model,\n",
      "            index_dict=index_dict,\n",
      "            max_output_tokens=max_output_tokens,\n",
      "            max_index_memory=max_index_memory,\n",
      "            system_prompt=system_prompt,\n",
      "            user_prompt=user_prompt,\n",
      "            name=name,\n",
      "        )\n",
      "        self.max_vector_memory = self.max_context\n",
      "        self.prompt_func = self.vector_memory_prompt\n",
      "\n",
      "    def vector_memory_prompt(\n",
      "        self, message: str, k: int = 10\n",
      "    ) -> Tuple[List[dict], dict]:\n",
      "        \"\"\"\n",
      "        Combine system prompt, k most similar messages to the question, and the user prompt.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include in the prompt.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "        sorted_messages, sorted_scores, sorted_indices = self.sorted_query(\n",
      "            message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
      "        )\n",
      "        marked_question = mark_question(self.user_prompt(message))\n",
      "        prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\n",
      "        return prompt, marked_question\n",
      "\n",
      "    def weighted_memory_prompt(\n",
      "        self,\n",
      "        message: str,\n",
      "        k: int = 10,\n",
      "        decay_factor: float = 0.1,\n",
      "        temporal_weight: float = 0.5,\n",
      "    ) -> Tuple[List[dict], dict]:\n",
      "        \"\"\"\n",
      "        Combine system prompt, weighted k most similar messages to the question, and the user prompt.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include in the prompt.\n",
      "        :param decay_factor: A float representing the decay factor for weighting.\n",
      "        :param temporal_weight: A float representing the weight of the temporal aspect.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "        weighted_messages, weighted_scores, weighted_indices = self.weighted_query(\n",
      "            message,\n",
      "            k=k,\n",
      "            max_tokens=self.max_vector_memory,\n",
      "            decay_factor=decay_factor,\n",
      "            temporal_weight=temporal_weight,\n",
      "            order_by=\"chronological\",\n",
      "            reverse=True,\n",
      "        )\n",
      "        marked_question = mark_question(self.user_prompt(message))\n",
      "        prompt = (\n",
      "            [mark_system(self.system_prompt)] + weighted_messages + [marked_question]\n",
      "        )\n",
      "        return prompt, marked_question\n",
      "\n",
      "    def query(self, question: str, verbose: bool = False, stream:bool = False) -> Union[Generator,str]:\n",
      "        \"\"\"\n",
      "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
      "        and added to the memory.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "                \n",
      "        marked_question = mark_question(question)\n",
      "        self.add_message(marked_question)\n",
      "        answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n",
      "        if stream:\n",
      "            return answer\n",
      "        else:\n",
      "            self.add_message(answer)\n",
      "            return answer\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"VectorThread, \", \"Chat\"]\n",
      "]\n",
      "Function call: mark_question\n",
      "Function call: mark_system\n",
      "Function call: mark_question\n",
      "Function call: mark_system\n",
      "Function call: mark_question\n",
      "Related codes: ['\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n', '\\ndef mark_system(system_prompt):\\n    return {\"role\": \"system\", \"content\": system_prompt}\\n', '\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n', '\\ndef mark_system(system_prompt):\\n    return {\"role\": \"system\", \"content\": system_prompt}\\n', '\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    model: Optional[str] = None,\n",
      "    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "    name: str = \"vector_memory\",\n",
      "    max_index_memory: int = 400,\n",
      "    max_vector_memory: int = 2048,\n",
      "    max_output_tokens: int = 1000,\n",
      "    system_prompt: str = None,\n",
      "    user_prompt: str = None,\n",
      "\n",
      "):\n",
      "    VectorThread.__init__(self, name=name, max_context=max_vector_memory)\n",
      "    Chat.__init__(\n",
      "        self,\n",
      "        model=model,\n",
      "        index_dict=index_dict,\n",
      "        max_output_tokens=max_output_tokens,\n",
      "        max_index_memory=max_index_memory,\n",
      "        system_prompt=system_prompt,\n",
      "        user_prompt=user_prompt,\n",
      "        name=name,\n",
      "    )\n",
      "    self.max_vector_memory = self.max_context\n",
      "    self.prompt_func = self.vector_memory_prompt\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def vector_memory_prompt(\n",
      "    self, message: str, k: int = 10\n",
      ") -> Tuple[List[dict], dict]:\n",
      "    \"\"\"\n",
      "        Combine system prompt, k most similar messages to the question, and the user prompt.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include in the prompt.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "    sorted_messages, sorted_scores, sorted_indices = self.sorted_query(\n",
      "        message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
      "    )\n",
      "    marked_question = mark_question(self.user_prompt(message))\n",
      "    prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\n",
      "    return prompt, marked_question\n",
      "\n",
      "Function call: mark_question\n",
      "Function call: mark_system\n",
      "Related codes: ['\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n', '\\ndef mark_system(system_prompt):\\n    return {\"role\": \"system\", \"content\": system_prompt}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def weighted_memory_prompt(\n",
      "    self,\n",
      "    message: str,\n",
      "    k: int = 10,\n",
      "    decay_factor: float = 0.1,\n",
      "    temporal_weight: float = 0.5,\n",
      ") -> Tuple[List[dict], dict]:\n",
      "    \"\"\"\n",
      "        Combine system prompt, weighted k most similar messages to the question, and the user prompt.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include in the prompt.\n",
      "        :param decay_factor: A float representing the decay factor for weighting.\n",
      "        :param temporal_weight: A float representing the weight of the temporal aspect.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "    weighted_messages, weighted_scores, weighted_indices = self.weighted_query(\n",
      "        message,\n",
      "        k=k,\n",
      "        max_tokens=self.max_vector_memory,\n",
      "        decay_factor=decay_factor,\n",
      "        temporal_weight=temporal_weight,\n",
      "        order_by=\"chronological\",\n",
      "        reverse=True,\n",
      "    )\n",
      "    marked_question = mark_question(self.user_prompt(message))\n",
      "    prompt = (\n",
      "        [mark_system(self.system_prompt)] + weighted_messages + [marked_question]\n",
      "    )\n",
      "    return prompt, marked_question\n",
      "\n",
      "Function call: mark_question\n",
      "Function call: mark_system\n",
      "Related codes: ['\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n', '\\ndef mark_system(system_prompt):\\n    return {\"role\": \"system\", \"content\": system_prompt}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def query(self, question: str, verbose: bool = False, stream:bool = False) -> Union[Generator,str]:\n",
      "    \"\"\"\n",
      "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
      "        and added to the memory.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "            \n",
      "    marked_question = mark_question(question)\n",
      "    self.add_message(marked_question)\n",
      "    answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n",
      "    if stream:\n",
      "        return answer\n",
      "    else:\n",
      "        self.add_message(answer)\n",
      "        return answer\n",
      "\n",
      "Function call: mark_question\n",
      "Related codes: ['\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class FifoVectorChat(FifoThread, Chat):\n",
      "    \"\"\"\n",
      "    A chatbot class that combines FIFO Memory Thread, Vector Memory Thread, BaseChat, and Prompter.\n",
      "    The memory prompt is constructed by including both FIFO memory and Vector memory.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        model: str = None,\n",
      "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "        system_prompt: str = None,\n",
      "        user_prompt: str = None,\n",
      "        name: str = \"fifo_vector_memory\",\n",
      "        max_memory: int = 2048,\n",
      "        max_index_memory: int = 400,\n",
      "        max_output_tokens: int = 1000,\n",
      "        longterm_thread: Optional[VectorThread] = None,\n",
      "        longterm_frac: float = 0.5,\n",
      "    ):\n",
      "        self.total_max_memory = max_memory\n",
      "\n",
      "        self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n",
      "        FifoThread.__init__(\n",
      "            self,\n",
      "            name=name,\n",
      "            max_memory=self.max_fifo_memory,\n",
      "            longterm_thread=self.longterm_thread,\n",
      "        )\n",
      "        Chat.__init__(\n",
      "            self,\n",
      "            model=model,\n",
      "            index_dict=index_dict,\n",
      "            max_output_tokens=max_output_tokens,\n",
      "            max_index_memory=max_index_memory,\n",
      "            system_prompt=system_prompt,\n",
      "            user_prompt=user_prompt,\n",
      "            name=name,\n",
      "        )\n",
      "        \n",
      "        self.prompt_func = self.fifovector_memory_prompt\n",
      "        self.prompt_list = []\n",
      "\n",
      "    def setup_longterm_memory(\n",
      "        self,\n",
      "        longterm_thread: Optional[VectorThread],\n",
      "        max_memory: int,\n",
      "        longterm_frac: float,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n",
      "\n",
      "        :param longterm_thread: An optional VectorThread for long-term memory.\n",
      "        :param max_memory: The maximum amount of memory for the chatbot.\n",
      "        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n",
      "        \"\"\"\n",
      "        if longterm_thread is None:\n",
      "            self.longterm_frac = longterm_frac\n",
      "            self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n",
      "            self.max_vector_memory = max_memory - self.max_fifo_memory\n",
      "            self.longterm_thread = VectorThread(\n",
      "                name=\"longterm_memory\", max_context=self.max_vector_memory\n",
      "            )\n",
      "        else:\n",
      "            self.longterm_thread = longterm_thread\n",
      "            self.max_vector_memory = self.longterm_thread.max_context\n",
      "            self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n",
      "            self.longterm_frac = self.max_vector_memory / self.total_max_memory\n",
      "\n",
      "    def fifovector_memory_prompt(\n",
      "        self, message: str, k: int = 10\n",
      "    ) -> Tuple[List[dict], dict]:\n",
      "        \"\"\"\n",
      "        Combine the system prompt, long-term memory (vector memory), short-term memory (FIFO memory), and the user prompt.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include from the long-term memory.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "        prompt = [mark_system(self.system_prompt)]\n",
      "        if (\n",
      "            len(self.longterm_thread.memory_thread) > 0\n",
      "            and self.longterm_thread.total_tokens <= self.max_vector_memory\n",
      "        ):\n",
      "            prompt += self.longterm_thread.memory_thread\n",
      "        elif (\n",
      "            len(self.longterm_thread.memory_thread) > 0\n",
      "            and self.longterm_thread.total_tokens > self.max_vector_memory\n",
      "        ):\n",
      "            (\n",
      "                sorted_messages,\n",
      "                sorted_scores,\n",
      "                sorted_indices,\n",
      "            ) = self.longterm_thread.sorted_query(\n",
      "                message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
      "            )\n",
      "            prompt += sorted_messages\n",
      "\n",
      "        prompt += self.memory_thread\n",
      "        marked_question = mark_question(self.user_prompt(message))\n",
      "        prompt += [marked_question]\n",
      "        return prompt, marked_question\n",
      "\n",
      "    def query(self, question: str, verbose: bool = False, stream:bool = False) -> Union[Generator,str]:\n",
      "        \"\"\"\n",
      "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
      "        and added to the memory.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "        prompt, marked_question = self.fifovector_memory_prompt(question)\n",
      "\n",
      "        self.add_message(marked_question)\n",
      "\n",
      "        answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n",
      "        if stream:\n",
      "            return answer\n",
      "        else:\n",
      "            self.add_message(answer)\n",
      "            return answer\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"FifoThread, \", \"Chat\"]\n",
      "]\n",
      "Function call: int\n",
      "Function call: VectorThread\n",
      "Function call: mark_system\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: mark_question\n",
      "Related codes: ['\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\n\\nclass VectorThread(BaseThread, MemoryIndex):\\n    \"\"\"vector BaseThread, creates a faiss index with the messages and allows to search for similar messages, memory BaseThread can return messages in either similarity or chronological order\\n    add a parameter to choose the order of the messages\\n    \"\"\"\\n\\n    def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\\n        BaseThread.__init__(self, name=name, max_memory=None)\\n        MemoryIndex.__init__(self, index=None, name=name)\\n        self.max_context = max_context\\n        self.use_mark = use_mark\\n        self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n\\n    def index_message(self, message: str, verbose: bool = False):\\n        \"\"\"index a message in the faiss index, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated\\n        \"\"\"\\n\\n        self.add_to_index(value=message, verbose=verbose)\\n\\n    def add_message(self, message_dict: dict, verbose: bool = False):\\n        \"\"\"add a message to the memory thread, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\\n        \"\"\"\\n        # print(\"checking the dict\")\\n        message_dict = check_dict(message_dict)\\n        # print(\"trying to add the message\")\\n        BaseThread.add_message(self, message_dict)\\n        # print(message_dict)\\n        message = message_dict[\"content\"]\\n        self.index_message(message, verbose=verbose)\\n        return True\\n\\n    def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\\n        \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\\n        if self.use_mark:\\n            query = mark_question(query)\\n        return MemoryIndex.token_bound_query(self, query, k, max_tokens)\\n\\n    def sorted_query(\\n        self,\\n        query,\\n        k: int = 10,\\n        max_tokens: int = 4000,\\n        reverse: bool = False,\\n        return_from_thread=True,\\n    ) -> Tuple[List[str], List[float], List[int]]:\\n        \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n        unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n        num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n        # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n        unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n        # Sort the indices\\n        sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n        \\n        print(sorted_indices)\\n        print(type(sorted_indices))\\n\\n        if reverse:\\n            sorted_indices.reverse()\\n\\n        # Fetch the sorted messages, scores, and indices based on sorted_indices\\n        sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n        sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n        sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n        if return_from_thread:\\n            sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n        return sorted_messages, sorted_scores, sorted_indices\\n    def weighted_query(\\n        self,\\n        query,\\n        k: int = 10,\\n        max_tokens: int = 4000,\\n        decay_factor: float = 0.1,\\n        temporal_weight: float = 0.5,\\n        order_by: str = \"chronological\",\\n        reverse: bool = False,\\n    ) -> list:\\n        \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\\n        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\\n        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\\n        The order_by parameter controls the order of the results. If it is set to \\'similarity\\', the results are sorted in similarity order. If it is set to \\'chronological\\', the results are sorted in chronological order with the most recent message first.\\n        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\\n        \"\"\"\\n        # Validate order_by parameter\\n        if order_by not in (\"similarity\", \"chronological\"):\\n            raise ValueError(\\n                \"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\"\\n            )\\n\\n        # Get similarity-based results\\n        sim_messages, sim_scores, sim_indices = self.sorted_query(\\n            query, k, max_tokens=max_tokens\\n        )\\n\\n        # Get token-bound history\\n        hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\\n\\n        # Combine messages and indices\\n        combined_messages = sim_messages + hist_messages\\n        combined_indices = sim_indices + hist_indices\\n\\n        # Create the local_index and populate it\\n        self.local_index = MemoryIndex(name=\"local_index\")\\n        for message in combined_messages:\\n            self.local_index.add_to_index(value=message, verbose=False)\\n\\n        # Perform a new query on the combined index\\n        (\\n            new_query_results,\\n            new_query_scores,\\n            new_query_indices,\\n        ) = self.local_index.token_bound_query(\\n            query, k=len(combined_messages), max_tokens=max_tokens\\n        )\\n\\n        # Compute temporal weights\\n        temporal_weights = [\\n            np.exp(-decay_factor * i) for i in range(len(combined_messages))\\n        ]\\n        temporal_weights = [\\n            w / sum(temporal_weights) for w in temporal_weights\\n        ]  # Normalize the temporal weights\\n\\n        # Combine similarity scores and temporal weights\\n        weighted_scores = []\\n        for i in range(len(new_query_scores)):\\n            sim_score = new_query_scores[i]\\n            temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\\n            weighted_score = (\\n                1 - temporal_weight\\n            ) * sim_score + temporal_weight * temp_weight\\n            weighted_scores.append(weighted_score)\\n\\n        # Sort the results based on the order_by parameter\\n        if order_by == \"similarity\":\\n            sorting_key = lambda k: weighted_scores[k]\\n        elif order_by == \"chronological\":  # order_by == \\'chronological\\'\\n            sorting_key = lambda k: new_query_indices[k]\\n        else:\\n            raise ValueError(\\n                \"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\"\\n            )\\n\\n        sorted_indices = [\\n            new_query_indices[i]\\n            for i in sorted(\\n                range(len(new_query_indices)), key=sorting_key, reverse=not reverse\\n            )\\n        ]\\n        sorted_results = [\\n            new_query_results[i]\\n            for i in sorted(\\n                range(len(new_query_results)), key=sorting_key, reverse=not reverse\\n            )\\n        ]\\n        sorted_scores = [\\n            weighted_scores[i]\\n            for i in sorted(\\n                range(len(weighted_scores)), key=sorting_key, reverse=not reverse\\n            )\\n        ]\\n\\n        # Return only the top k results without exceeding max_tokens\\n        final_results, final_scores, final_indices = [], [], []\\n        current_tokens = 0\\n        for i in range(min(k, len(sorted_results))):\\n            message_tokens = self.get_message_tokens(sorted_results[i])\\n            if current_tokens + message_tokens <= max_tokens:\\n                final_results.append(sorted_results[i])\\n                final_scores.append(sorted_scores[i])\\n                final_indices.append(sorted_indices[i])\\n                current_tokens += message_tokens\\n            else:\\n                break\\n\\n        return final_results, final_scores, final_indices\\n', '\\ndef mark_system(system_prompt):\\n    return {\"role\": \"system\", \"content\": system_prompt}\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    model: str = None,\n",
      "    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "    system_prompt: str = None,\n",
      "    user_prompt: str = None,\n",
      "    name: str = \"fifo_vector_memory\",\n",
      "    max_memory: int = 2048,\n",
      "    max_index_memory: int = 400,\n",
      "    max_output_tokens: int = 1000,\n",
      "    longterm_thread: Optional[VectorThread] = None,\n",
      "    longterm_frac: float = 0.5,\n",
      "):\n",
      "    self.total_max_memory = max_memory\n",
      "\n",
      "    self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n",
      "    FifoThread.__init__(\n",
      "        self,\n",
      "        name=name,\n",
      "        max_memory=self.max_fifo_memory,\n",
      "        longterm_thread=self.longterm_thread,\n",
      "    )\n",
      "    Chat.__init__(\n",
      "        self,\n",
      "        model=model,\n",
      "        index_dict=index_dict,\n",
      "        max_output_tokens=max_output_tokens,\n",
      "        max_index_memory=max_index_memory,\n",
      "        system_prompt=system_prompt,\n",
      "        user_prompt=user_prompt,\n",
      "        name=name,\n",
      "    )\n",
      "    \n",
      "    self.prompt_func = self.fifovector_memory_prompt\n",
      "    self.prompt_list = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def setup_longterm_memory(\n",
      "    self,\n",
      "    longterm_thread: Optional[VectorThread],\n",
      "    max_memory: int,\n",
      "    longterm_frac: float,\n",
      "):\n",
      "    \"\"\"\n",
      "        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n",
      "\n",
      "        :param longterm_thread: An optional VectorThread for long-term memory.\n",
      "        :param max_memory: The maximum amount of memory for the chatbot.\n",
      "        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n",
      "        \"\"\"\n",
      "    if longterm_thread is None:\n",
      "        self.longterm_frac = longterm_frac\n",
      "        self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n",
      "        self.max_vector_memory = max_memory - self.max_fifo_memory\n",
      "        self.longterm_thread = VectorThread(\n",
      "            name=\"longterm_memory\", max_context=self.max_vector_memory\n",
      "        )\n",
      "    else:\n",
      "        self.longterm_thread = longterm_thread\n",
      "        self.max_vector_memory = self.longterm_thread.max_context\n",
      "        self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n",
      "        self.longterm_frac = self.max_vector_memory / self.total_max_memory\n",
      "\n",
      "Function call: int\n",
      "Function call: VectorThread\n",
      "Related codes: ['\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\n\\nclass VectorThread(BaseThread, MemoryIndex):\\n    \"\"\"vector BaseThread, creates a faiss index with the messages and allows to search for similar messages, memory BaseThread can return messages in either similarity or chronological order\\n    add a parameter to choose the order of the messages\\n    \"\"\"\\n\\n    def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\\n        BaseThread.__init__(self, name=name, max_memory=None)\\n        MemoryIndex.__init__(self, index=None, name=name)\\n        self.max_context = max_context\\n        self.use_mark = use_mark\\n        self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n\\n    def index_message(self, message: str, verbose: bool = False):\\n        \"\"\"index a message in the faiss index, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated\\n        \"\"\"\\n\\n        self.add_to_index(value=message, verbose=verbose)\\n\\n    def add_message(self, message_dict: dict, verbose: bool = False):\\n        \"\"\"add a message to the memory thread, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\\n        \"\"\"\\n        # print(\"checking the dict\")\\n        message_dict = check_dict(message_dict)\\n        # print(\"trying to add the message\")\\n        BaseThread.add_message(self, message_dict)\\n        # print(message_dict)\\n        message = message_dict[\"content\"]\\n        self.index_message(message, verbose=verbose)\\n        return True\\n\\n    def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\\n        \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\\n        if self.use_mark:\\n            query = mark_question(query)\\n        return MemoryIndex.token_bound_query(self, query, k, max_tokens)\\n\\n    def sorted_query(\\n        self,\\n        query,\\n        k: int = 10,\\n        max_tokens: int = 4000,\\n        reverse: bool = False,\\n        return_from_thread=True,\\n    ) -> Tuple[List[str], List[float], List[int]]:\\n        \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n        unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n        num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n        # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n        unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n        # Sort the indices\\n        sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n        \\n        print(sorted_indices)\\n        print(type(sorted_indices))\\n\\n        if reverse:\\n            sorted_indices.reverse()\\n\\n        # Fetch the sorted messages, scores, and indices based on sorted_indices\\n        sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n        sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n        sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n        if return_from_thread:\\n            sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n        return sorted_messages, sorted_scores, sorted_indices\\n    def weighted_query(\\n        self,\\n        query,\\n        k: int = 10,\\n        max_tokens: int = 4000,\\n        decay_factor: float = 0.1,\\n        temporal_weight: float = 0.5,\\n        order_by: str = \"chronological\",\\n        reverse: bool = False,\\n    ) -> list:\\n        \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\\n        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\\n        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\\n        The order_by parameter controls the order of the results. If it is set to \\'similarity\\', the results are sorted in similarity order. If it is set to \\'chronological\\', the results are sorted in chronological order with the most recent message first.\\n        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\\n        \"\"\"\\n        # Validate order_by parameter\\n        if order_by not in (\"similarity\", \"chronological\"):\\n            raise ValueError(\\n                \"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\"\\n            )\\n\\n        # Get similarity-based results\\n        sim_messages, sim_scores, sim_indices = self.sorted_query(\\n            query, k, max_tokens=max_tokens\\n        )\\n\\n        # Get token-bound history\\n        hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\\n\\n        # Combine messages and indices\\n        combined_messages = sim_messages + hist_messages\\n        combined_indices = sim_indices + hist_indices\\n\\n        # Create the local_index and populate it\\n        self.local_index = MemoryIndex(name=\"local_index\")\\n        for message in combined_messages:\\n            self.local_index.add_to_index(value=message, verbose=False)\\n\\n        # Perform a new query on the combined index\\n        (\\n            new_query_results,\\n            new_query_scores,\\n            new_query_indices,\\n        ) = self.local_index.token_bound_query(\\n            query, k=len(combined_messages), max_tokens=max_tokens\\n        )\\n\\n        # Compute temporal weights\\n        temporal_weights = [\\n            np.exp(-decay_factor * i) for i in range(len(combined_messages))\\n        ]\\n        temporal_weights = [\\n            w / sum(temporal_weights) for w in temporal_weights\\n        ]  # Normalize the temporal weights\\n\\n        # Combine similarity scores and temporal weights\\n        weighted_scores = []\\n        for i in range(len(new_query_scores)):\\n            sim_score = new_query_scores[i]\\n            temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\\n            weighted_score = (\\n                1 - temporal_weight\\n            ) * sim_score + temporal_weight * temp_weight\\n            weighted_scores.append(weighted_score)\\n\\n        # Sort the results based on the order_by parameter\\n        if order_by == \"similarity\":\\n            sorting_key = lambda k: weighted_scores[k]\\n        elif order_by == \"chronological\":  # order_by == \\'chronological\\'\\n            sorting_key = lambda k: new_query_indices[k]\\n        else:\\n            raise ValueError(\\n                \"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\"\\n            )\\n\\n        sorted_indices = [\\n            new_query_indices[i]\\n            for i in sorted(\\n                range(len(new_query_indices)), key=sorting_key, reverse=not reverse\\n            )\\n        ]\\n        sorted_results = [\\n            new_query_results[i]\\n            for i in sorted(\\n                range(len(new_query_results)), key=sorting_key, reverse=not reverse\\n            )\\n        ]\\n        sorted_scores = [\\n            weighted_scores[i]\\n            for i in sorted(\\n                range(len(weighted_scores)), key=sorting_key, reverse=not reverse\\n            )\\n        ]\\n\\n        # Return only the top k results without exceeding max_tokens\\n        final_results, final_scores, final_indices = [], [], []\\n        current_tokens = 0\\n        for i in range(min(k, len(sorted_results))):\\n            message_tokens = self.get_message_tokens(sorted_results[i])\\n            if current_tokens + message_tokens <= max_tokens:\\n                final_results.append(sorted_results[i])\\n                final_scores.append(sorted_scores[i])\\n                final_indices.append(sorted_indices[i])\\n                current_tokens += message_tokens\\n            else:\\n                break\\n\\n        return final_results, final_scores, final_indices\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def fifovector_memory_prompt(\n",
      "    self, message: str, k: int = 10\n",
      ") -> Tuple[List[dict], dict]:\n",
      "    \"\"\"\n",
      "        Combine the system prompt, long-term memory (vector memory), short-term memory (FIFO memory), and the user prompt.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include from the long-term memory.\n",
      "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
      "        \"\"\"\n",
      "    prompt = [mark_system(self.system_prompt)]\n",
      "    if (\n",
      "        len(self.longterm_thread.memory_thread) > 0\n",
      "        and self.longterm_thread.total_tokens <= self.max_vector_memory\n",
      "    ):\n",
      "        prompt += self.longterm_thread.memory_thread\n",
      "    elif (\n",
      "        len(self.longterm_thread.memory_thread) > 0\n",
      "        and self.longterm_thread.total_tokens > self.max_vector_memory\n",
      "    ):\n",
      "        (\n",
      "            sorted_messages,\n",
      "            sorted_scores,\n",
      "            sorted_indices,\n",
      "        ) = self.longterm_thread.sorted_query(\n",
      "            message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
      "        )\n",
      "        prompt += sorted_messages\n",
      "\n",
      "    prompt += self.memory_thread\n",
      "    marked_question = mark_question(self.user_prompt(message))\n",
      "    prompt += [marked_question]\n",
      "    return prompt, marked_question\n",
      "\n",
      "Function call: mark_system\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: mark_question\n",
      "Related codes: ['\\ndef mark_system(system_prompt):\\n    return {\"role\": \"system\", \"content\": system_prompt}\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def query(self, question: str, verbose: bool = False, stream:bool = False) -> Union[Generator,str]:\n",
      "    \"\"\"\n",
      "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
      "        and added to the memory.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "    prompt, marked_question = self.fifovector_memory_prompt(question)\n",
      "\n",
      "    self.add_message(marked_question)\n",
      "\n",
      "    answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\n",
      "    if stream:\n",
      "        return answer\n",
      "    else:\n",
      "        self.add_message(answer)\n",
      "        return answer\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class ContextManagedFifoVectorChat(FifoThread, Chat):\n",
      "    def __init__(\n",
      "        self,\n",
      "        model: str = None,\n",
      "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "        system_prompt: str = None,\n",
      "        user_prompt: str = None,\n",
      "        name: str = \"fifo_vector_memory\",\n",
      "        max_memory: int = 2048,\n",
      "        max_index_memory: int = 400,\n",
      "        max_output_tokens: int = 1000,\n",
      "        longterm_thread: Optional[VectorThread] = None,\n",
      "        longterm_frac: float = 0.5,\n",
      "    ):\n",
      "        self.total_max_memory = max_memory\n",
      "        \n",
      "\n",
      "        self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n",
      "        FifoThread.__init__(\n",
      "            self,\n",
      "            name=name,\n",
      "            max_memory=self.max_fifo_memory,\n",
      "            longterm_thread=self.longterm_thread,\n",
      "        )\n",
      "        Chat.__init__(\n",
      "            self,\n",
      "            model=model,\n",
      "            index_dict=index_dict,\n",
      "            max_output_tokens=max_output_tokens,\n",
      "            max_index_memory=max_index_memory,\n",
      "            system_prompt=system_prompt,\n",
      "            user_prompt=user_prompt,\n",
      "            name=name,\n",
      "        )\n",
      "        #self.context_manager = ContextManager(index_dict)\n",
      "        #deep copy of index_dict\n",
      "        '''\n",
      "        deep_copied_index_dict = copy.deepcopy(self.index_dict)\n",
      "        \n",
      "        self.multi_kernel = CreateMultiKernel(deep_copied_index_dict).create_multi_kernel()\n",
      "        self.memory_kernel_dict = self.multi_kernel.memory_kernel_dict\n",
      "        self.path_group = self.multi_kernel.path_group\n",
      "        \n",
      "        self.prompt_func = self.fifovector_memory_prompt\n",
      "        '''\n",
      "        self.prompt_list = []\n",
      "    \n",
      "    def setup_longterm_memory(\n",
      "        self,\n",
      "        longterm_thread: Optional[VectorThread],\n",
      "        max_memory: int,\n",
      "        longterm_frac: float,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n",
      "\n",
      "        :param longterm_thread: An optional VectorThread for long-term memory.\n",
      "        :param max_memory: The maximum amount of memory for the chatbot.\n",
      "        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n",
      "        \"\"\"\n",
      "        #TODO preload longterm memory with index summaries\n",
      "        if longterm_thread is None:\n",
      "            self.longterm_frac = longterm_frac\n",
      "            self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n",
      "            self.max_vector_memory = max_memory - self.max_fifo_memory\n",
      "            self.longterm_thread = VectorThread(\n",
      "                name=\"longterm_memory\", max_context=self.max_vector_memory\n",
      "            )\n",
      "        else:\n",
      "            self.longterm_thread = longterm_thread\n",
      "            self.max_vector_memory = self.longterm_thread.max_context\n",
      "            self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n",
      "            self.longterm_frac = self.max_vector_memory / self.total_max_memory\n",
      "\n",
      "    def calculate_stability(self, boundaries, adjacency_matrix_with_message):\n",
      "        boundary_stability = np.zeros(len(boundaries) - 1)\n",
      "        for i in range(len(boundaries) - 1):\n",
      "            boundary_connections = adjacency_matrix_with_message[boundaries[i]:boundaries[i+1], :][:, boundaries[i]:boundaries[i+1]]\n",
      "            boundary_stability[i] = np.mean(boundary_connections) - np.std(boundary_connections)**2\n",
      "        logging.info(f\"Boundary Stability: {boundary_stability}\")\n",
      "        return boundary_stability\n",
      "\n",
      "    def query_hints(self, message, index, k):\n",
      "        top_k, _, indices = index.token_bound_query(message, k=k, max_tokens=5000)\n",
      "        top_k_embeddings = [index.embeddings[i] for i in indices]\n",
      "        return top_k, top_k_embeddings\n",
      "\n",
      "    def query_mk_hints(self, message, index_key, index, k):\n",
      "        top_k, _, indices = index.token_bound_query(message, k=k, max_tokens=3000)\n",
      "        top_k_embeddings = [self.memory_kernel_dict[index_key].node_embeddings[i] for i in indices]\n",
      "        return top_k, top_k_embeddings\n",
      "\n",
      "\n",
      "    def heat_trajectory(self, message, k=20):\n",
      "        \"\"\"\n",
      "        This function gets the top k embeddings from all indexes including longterm, merges into numpy matrix,\n",
      "        record boundaries, computes kernel adj matrix, and sums all connections in each index section.\n",
      "        It returns the heat trajectory.\n",
      "        \"\"\"\n",
      "        embeddings = []\n",
      "        boundaries = [0]  # Start boundary\n",
      "        top_k_hints = {}\n",
      "        # Gather top k embeddings from each index\n",
      "        logging.info(\"Computing Trajectory for the current state of memory.\")\n",
      "        for index_key, index in self.index_dict.items():\n",
      "            top_k, top_k_embeddings = self.query_hints(message, index, k)\n",
      "            embeddings.extend(top_k_embeddings)\n",
      "            boundaries.append(boundaries[-1] + k)\n",
      "            top_k_hints[index_key] = top_k\n",
      "\n",
      "\n",
      "        if len(self.longterm_thread.values) >= k:\n",
      "            if len(self.longterm_thread.embeddings) != len(self.longterm_thread.values):\n",
      "                self.longterm_thread.compute_embeddings()\n",
      "            top_k, top_k_embeddings = self.query_hints(message, index, k)\n",
      "            embeddings.extend(top_k_embeddings)\n",
      "            boundaries.append(boundaries[-1] + k)\n",
      "            top_k_hints['longterm_thread'] = top_k\n",
      "        mean = np.sum(embeddings, axis=1)\n",
      "        mean = mean / np.linalg.norm(mean)\n",
      "        #should be shape (,1536) currently (4, 1536)\n",
      "        mean = np.sum(mean, axis=0)\n",
      "\n",
      "\n",
      "        logging.info(f\"Mean: {mean.shape}\")\n",
      "        embeddings_matrix = np.vstack(embeddings)\n",
      "        adjacency_matrix = cosine_similarity(embeddings_matrix)\n",
      "        message_embedding = EMBEDDER.embed(data=message)\n",
      "        #get residuals\n",
      "        message_embedding = message_embedding - mean\n",
      "        top_k_embeddings_with_message = embeddings_matrix.copy()\n",
      "        top_k_embeddings_with_message[:-1] += message_embedding\n",
      "        norms = np.linalg.norm(top_k_embeddings_with_message, axis=1)\n",
      "        normalized_embeddings = top_k_embeddings_with_message / norms[:, np.newaxis]\n",
      "        adjacency_matrix_with_message = cosine_similarity(normalized_embeddings)\n",
      "\n",
      "        adjacency_matrix_with_message = adjacency_matrix_with_message - adjacency_matrix\n",
      "        adjacency_matrix_with_message =  adjacency_matrix_with_message**3\n",
      "\n",
      "        degrees = np.sum(adjacency_matrix_with_message, axis=1)\n",
      "        heat_trajectory = [np.sum(degrees[boundaries[i]:boundaries[i + 1]]) for i in range(len(boundaries) - 1)]\n",
      "        logging.info(f\"Heat Trajectory: {heat_trajectory}\")\n",
      "        heat_dict = dict(zip(list(self.index_dict.keys()) + ['longterm_thread'], heat_trajectory))\n",
      "        sum_of_vals = sum(heat_dict.values())\n",
      "        heat_dict = {k: v / sum_of_vals for k, v in heat_dict.items()}\n",
      "        hdict = {k: v for k, v in sorted(heat_dict.items(), key=lambda item: item[1])}\n",
      "        logging.info(f\"Heat dict: {heat_dict}\")\n",
      "        return hdict, top_k_hints\n",
      "\n",
      "\n",
      "    def fifovector_memory_prompt(\n",
      "        self, message: str, k: int = 5\n",
      "    ) -> Tuple[List[dict], dict]:\n",
      "        hdict, top_k_hint_dict = self.heat_trajectory(message)\n",
      "        min_heat_index = list(hdict.keys())[0]\n",
      "        second_min_heat_index = list(hdict.keys())[1]\n",
      "        if len(hdict) > 2:\n",
      "            third_min_heat_index = list(hdict.keys())[2]\n",
      "        if min_heat_index == 'longterm_thread':\n",
      "            logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from long-term memory.\")\n",
      "            logging.info(f\"Number of values in Index: {len(self.longterm_thread.values)}\")\n",
      "            if len(hdict) > 2:\n",
      "                if hdict[second_min_heat_index] - hdict[third_min_heat_index] < 0.20:\n",
      "                    top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-2] + top_k_hint_dict[third_min_heat_index][:1]\n",
      "                else:\n",
      "                    top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-1]\n",
      "            else:\n",
      "                top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-1]\n",
      "\n",
      "            logging.info(f\"Top K Hint: {top_k_hint}\")\n",
      "            prompt =f'[LONG TERM MEMORY]{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n",
      "        elif min_heat_index in self.index_dict.keys():\n",
      "            # if the difference between min and second min is less than 0.25, merge hints from both indexes\n",
      "            if hdict[min_heat_index] - hdict[second_min_heat_index] < 0.25:\n",
      "                logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from index.\")\n",
      "                logging.info(f\"Number of values in Index {self.index_dict[min_heat_index].name}: {len(self.index_dict[min_heat_index].values)}\")\n",
      "                #take 2 from first index topk and 1 from second index topk\n",
      "                top_k_hint = top_k_hint_dict[min_heat_index][:k-2] + top_k_hint_dict[second_min_heat_index][:2]\n",
      "                logging.info(f\"Top K Hint: {top_k_hint}\")\n",
      "                prompt =f'{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n",
      "            else:\n",
      "                logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from index.\")\n",
      "                logging.info(f\"Number of values in Index {self.index_dict[min_heat_index].name}: {len(self.index_dict[min_heat_index].values)}\")\n",
      "                top_k_hint = top_k_hint_dict[min_heat_index][:k]\n",
      "                logging.info(f\"Top K Hint: {top_k_hint}\")\n",
      "                prompt =f'{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n",
      "        else:\n",
      "            raise ValueError(\"The provided index name is not available.\")\n",
      "\n",
      "        return prompt\n",
      "\n",
      "    def context_query(self, question: str, verbose: bool = False, stream: bool = False) -> Union[Generator, str]:\n",
      "        prompt = self.fifovector_memory_prompt(question)\n",
      "        modified_question = mark_question(prompt)\n",
      "        self.add_message(modified_question)\n",
      "\n",
      "        answer = BaseChat.query(self, message=prompt, verbose=verbose, stream=stream)\n",
      "\n",
      "        if stream:\n",
      "            return answer\n",
      "        else:\n",
      "            self.add_message(answer)\n",
      "            return answer\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"FifoThread, \", \"Chat\"]\n",
      "]\n",
      "Function call: int\n",
      "Function call: VectorThread\n",
      "Function call: len\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: cosine_similarity\n",
      "Function call: cosine_similarity\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: dict\n",
      "Function call: zip\n",
      "Function call: list\n",
      "Function call: sum\n",
      "Function call: sorted\n",
      "Function call: list\n",
      "Function call: list\n",
      "Function call: len\n",
      "Function call: list\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: str\n",
      "Function call: len\n",
      "Function call: str\n",
      "Function call: len\n",
      "Function call: str\n",
      "Function call: ValueError\n",
      "Function call: mark_question\n",
      "Related codes: ['\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\n\\nclass VectorThread(BaseThread, MemoryIndex):\\n    \"\"\"vector BaseThread, creates a faiss index with the messages and allows to search for similar messages, memory BaseThread can return messages in either similarity or chronological order\\n    add a parameter to choose the order of the messages\\n    \"\"\"\\n\\n    def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\\n        BaseThread.__init__(self, name=name, max_memory=None)\\n        MemoryIndex.__init__(self, index=None, name=name)\\n        self.max_context = max_context\\n        self.use_mark = use_mark\\n        self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n\\n    def index_message(self, message: str, verbose: bool = False):\\n        \"\"\"index a message in the faiss index, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated\\n        \"\"\"\\n\\n        self.add_to_index(value=message, verbose=verbose)\\n\\n    def add_message(self, message_dict: dict, verbose: bool = False):\\n        \"\"\"add a message to the memory thread, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\\n        \"\"\"\\n        # print(\"checking the dict\")\\n        message_dict = check_dict(message_dict)\\n        # print(\"trying to add the message\")\\n        BaseThread.add_message(self, message_dict)\\n        # print(message_dict)\\n        message = message_dict[\"content\"]\\n        self.index_message(message, verbose=verbose)\\n        return True\\n\\n    def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\\n        \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\\n        if self.use_mark:\\n            query = mark_question(query)\\n        return MemoryIndex.token_bound_query(self, query, k, max_tokens)\\n\\n    def sorted_query(\\n        self,\\n        query,\\n        k: int = 10,\\n        max_tokens: int = 4000,\\n        reverse: bool = False,\\n        return_from_thread=True,\\n    ) -> Tuple[List[str], List[float], List[int]]:\\n        \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n        unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n        num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n        # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n        unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n        # Sort the indices\\n        sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n        \\n        print(sorted_indices)\\n        print(type(sorted_indices))\\n\\n        if reverse:\\n            sorted_indices.reverse()\\n\\n        # Fetch the sorted messages, scores, and indices based on sorted_indices\\n        sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n        sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n        sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n        if return_from_thread:\\n            sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n        return sorted_messages, sorted_scores, sorted_indices\\n    def weighted_query(\\n        self,\\n        query,\\n        k: int = 10,\\n        max_tokens: int = 4000,\\n        decay_factor: float = 0.1,\\n        temporal_weight: float = 0.5,\\n        order_by: str = \"chronological\",\\n        reverse: bool = False,\\n    ) -> list:\\n        \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\\n        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\\n        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\\n        The order_by parameter controls the order of the results. If it is set to \\'similarity\\', the results are sorted in similarity order. If it is set to \\'chronological\\', the results are sorted in chronological order with the most recent message first.\\n        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\\n        \"\"\"\\n        # Validate order_by parameter\\n        if order_by not in (\"similarity\", \"chronological\"):\\n            raise ValueError(\\n                \"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\"\\n            )\\n\\n        # Get similarity-based results\\n        sim_messages, sim_scores, sim_indices = self.sorted_query(\\n            query, k, max_tokens=max_tokens\\n        )\\n\\n        # Get token-bound history\\n        hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\\n\\n        # Combine messages and indices\\n        combined_messages = sim_messages + hist_messages\\n        combined_indices = sim_indices + hist_indices\\n\\n        # Create the local_index and populate it\\n        self.local_index = MemoryIndex(name=\"local_index\")\\n        for message in combined_messages:\\n            self.local_index.add_to_index(value=message, verbose=False)\\n\\n        # Perform a new query on the combined index\\n        (\\n            new_query_results,\\n            new_query_scores,\\n            new_query_indices,\\n        ) = self.local_index.token_bound_query(\\n            query, k=len(combined_messages), max_tokens=max_tokens\\n        )\\n\\n        # Compute temporal weights\\n        temporal_weights = [\\n            np.exp(-decay_factor * i) for i in range(len(combined_messages))\\n        ]\\n        temporal_weights = [\\n            w / sum(temporal_weights) for w in temporal_weights\\n        ]  # Normalize the temporal weights\\n\\n        # Combine similarity scores and temporal weights\\n        weighted_scores = []\\n        for i in range(len(new_query_scores)):\\n            sim_score = new_query_scores[i]\\n            temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\\n            weighted_score = (\\n                1 - temporal_weight\\n            ) * sim_score + temporal_weight * temp_weight\\n            weighted_scores.append(weighted_score)\\n\\n        # Sort the results based on the order_by parameter\\n        if order_by == \"similarity\":\\n            sorting_key = lambda k: weighted_scores[k]\\n        elif order_by == \"chronological\":  # order_by == \\'chronological\\'\\n            sorting_key = lambda k: new_query_indices[k]\\n        else:\\n            raise ValueError(\\n                \"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\"\\n            )\\n\\n        sorted_indices = [\\n            new_query_indices[i]\\n            for i in sorted(\\n                range(len(new_query_indices)), key=sorting_key, reverse=not reverse\\n            )\\n        ]\\n        sorted_results = [\\n            new_query_results[i]\\n            for i in sorted(\\n                range(len(new_query_results)), key=sorting_key, reverse=not reverse\\n            )\\n        ]\\n        sorted_scores = [\\n            weighted_scores[i]\\n            for i in sorted(\\n                range(len(weighted_scores)), key=sorting_key, reverse=not reverse\\n            )\\n        ]\\n\\n        # Return only the top k results without exceeding max_tokens\\n        final_results, final_scores, final_indices = [], [], []\\n        current_tokens = 0\\n        for i in range(min(k, len(sorted_results))):\\n            message_tokens = self.get_message_tokens(sorted_results[i])\\n            if current_tokens + message_tokens <= max_tokens:\\n                final_results.append(sorted_results[i])\\n                final_scores.append(sorted_scores[i])\\n                final_indices.append(sorted_indices[i])\\n                current_tokens += message_tokens\\n            else:\\n                break\\n\\n        return final_results, final_scores, final_indices\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n@staticmethod\\n@abstractmethod\\ndef batched_cosine_similarity(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\\n    pass\\n', '\\n@staticmethod\\n@abstractmethod\\ndef batched_cosine_similarity(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\\n    pass\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n@field_validator(\\'values\\')\\ndef check_type_dictionaries(cls, values):\\n    first_type_dictionary = values[0].type_dictionary\\n    for value in values[1:]:\\n        if value.type_dictionary != first_type_dictionary:\\n            raise ValueError(\"All elements in \\'values\\' should have the same \\'type_dictionary\\'.\")\\n    return values\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    model: str = None,\n",
      "    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
      "    system_prompt: str = None,\n",
      "    user_prompt: str = None,\n",
      "    name: str = \"fifo_vector_memory\",\n",
      "    max_memory: int = 2048,\n",
      "    max_index_memory: int = 400,\n",
      "    max_output_tokens: int = 1000,\n",
      "    longterm_thread: Optional[VectorThread] = None,\n",
      "    longterm_frac: float = 0.5,\n",
      "):\n",
      "    self.total_max_memory = max_memory\n",
      "    \n",
      "\n",
      "    self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n",
      "    FifoThread.__init__(\n",
      "        self,\n",
      "        name=name,\n",
      "        max_memory=self.max_fifo_memory,\n",
      "        longterm_thread=self.longterm_thread,\n",
      "    )\n",
      "    Chat.__init__(\n",
      "        self,\n",
      "        model=model,\n",
      "        index_dict=index_dict,\n",
      "        max_output_tokens=max_output_tokens,\n",
      "        max_index_memory=max_index_memory,\n",
      "        system_prompt=system_prompt,\n",
      "        user_prompt=user_prompt,\n",
      "        name=name,\n",
      "    )\n",
      "    #self.context_manager = ContextManager(index_dict)\n",
      "    #deep copy of index_dict\n",
      "    '''\n",
      "        deep_copied_index_dict = copy.deepcopy(self.index_dict)\n",
      "        \n",
      "        self.multi_kernel = CreateMultiKernel(deep_copied_index_dict).create_multi_kernel()\n",
      "        self.memory_kernel_dict = self.multi_kernel.memory_kernel_dict\n",
      "        self.path_group = self.multi_kernel.path_group\n",
      "        \n",
      "        self.prompt_func = self.fifovector_memory_prompt\n",
      "        '''\n",
      "    self.prompt_list = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def setup_longterm_memory(\n",
      "    self,\n",
      "    longterm_thread: Optional[VectorThread],\n",
      "    max_memory: int,\n",
      "    longterm_frac: float,\n",
      "):\n",
      "    \"\"\"\n",
      "        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n",
      "\n",
      "        :param longterm_thread: An optional VectorThread for long-term memory.\n",
      "        :param max_memory: The maximum amount of memory for the chatbot.\n",
      "        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n",
      "        \"\"\"\n",
      "    #TODO preload longterm memory with index summaries\n",
      "    if longterm_thread is None:\n",
      "        self.longterm_frac = longterm_frac\n",
      "        self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n",
      "        self.max_vector_memory = max_memory - self.max_fifo_memory\n",
      "        self.longterm_thread = VectorThread(\n",
      "            name=\"longterm_memory\", max_context=self.max_vector_memory\n",
      "        )\n",
      "    else:\n",
      "        self.longterm_thread = longterm_thread\n",
      "        self.max_vector_memory = self.longterm_thread.max_context\n",
      "        self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n",
      "        self.longterm_frac = self.max_vector_memory / self.total_max_memory\n",
      "\n",
      "Function call: int\n",
      "Function call: VectorThread\n",
      "Related codes: ['\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\n\\nclass VectorThread(BaseThread, MemoryIndex):\\n    \"\"\"vector BaseThread, creates a faiss index with the messages and allows to search for similar messages, memory BaseThread can return messages in either similarity or chronological order\\n    add a parameter to choose the order of the messages\\n    \"\"\"\\n\\n    def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\\n        BaseThread.__init__(self, name=name, max_memory=None)\\n        MemoryIndex.__init__(self, index=None, name=name)\\n        self.max_context = max_context\\n        self.use_mark = use_mark\\n        self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n\\n    def index_message(self, message: str, verbose: bool = False):\\n        \"\"\"index a message in the faiss index, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated\\n        \"\"\"\\n\\n        self.add_to_index(value=message, verbose=verbose)\\n\\n    def add_message(self, message_dict: dict, verbose: bool = False):\\n        \"\"\"add a message to the memory thread, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\\n        \"\"\"\\n        # print(\"checking the dict\")\\n        message_dict = check_dict(message_dict)\\n        # print(\"trying to add the message\")\\n        BaseThread.add_message(self, message_dict)\\n        # print(message_dict)\\n        message = message_dict[\"content\"]\\n        self.index_message(message, verbose=verbose)\\n        return True\\n\\n    def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\\n        \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\\n        if self.use_mark:\\n            query = mark_question(query)\\n        return MemoryIndex.token_bound_query(self, query, k, max_tokens)\\n\\n    def sorted_query(\\n        self,\\n        query,\\n        k: int = 10,\\n        max_tokens: int = 4000,\\n        reverse: bool = False,\\n        return_from_thread=True,\\n    ) -> Tuple[List[str], List[float], List[int]]:\\n        \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n        unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n        num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n        # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n        unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n        # Sort the indices\\n        sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n        \\n        print(sorted_indices)\\n        print(type(sorted_indices))\\n\\n        if reverse:\\n            sorted_indices.reverse()\\n\\n        # Fetch the sorted messages, scores, and indices based on sorted_indices\\n        sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n        sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n        sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n        if return_from_thread:\\n            sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n        return sorted_messages, sorted_scores, sorted_indices\\n    def weighted_query(\\n        self,\\n        query,\\n        k: int = 10,\\n        max_tokens: int = 4000,\\n        decay_factor: float = 0.1,\\n        temporal_weight: float = 0.5,\\n        order_by: str = \"chronological\",\\n        reverse: bool = False,\\n    ) -> list:\\n        \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\\n        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\\n        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\\n        The order_by parameter controls the order of the results. If it is set to \\'similarity\\', the results are sorted in similarity order. If it is set to \\'chronological\\', the results are sorted in chronological order with the most recent message first.\\n        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\\n        \"\"\"\\n        # Validate order_by parameter\\n        if order_by not in (\"similarity\", \"chronological\"):\\n            raise ValueError(\\n                \"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\"\\n            )\\n\\n        # Get similarity-based results\\n        sim_messages, sim_scores, sim_indices = self.sorted_query(\\n            query, k, max_tokens=max_tokens\\n        )\\n\\n        # Get token-bound history\\n        hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\\n\\n        # Combine messages and indices\\n        combined_messages = sim_messages + hist_messages\\n        combined_indices = sim_indices + hist_indices\\n\\n        # Create the local_index and populate it\\n        self.local_index = MemoryIndex(name=\"local_index\")\\n        for message in combined_messages:\\n            self.local_index.add_to_index(value=message, verbose=False)\\n\\n        # Perform a new query on the combined index\\n        (\\n            new_query_results,\\n            new_query_scores,\\n            new_query_indices,\\n        ) = self.local_index.token_bound_query(\\n            query, k=len(combined_messages), max_tokens=max_tokens\\n        )\\n\\n        # Compute temporal weights\\n        temporal_weights = [\\n            np.exp(-decay_factor * i) for i in range(len(combined_messages))\\n        ]\\n        temporal_weights = [\\n            w / sum(temporal_weights) for w in temporal_weights\\n        ]  # Normalize the temporal weights\\n\\n        # Combine similarity scores and temporal weights\\n        weighted_scores = []\\n        for i in range(len(new_query_scores)):\\n            sim_score = new_query_scores[i]\\n            temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\\n            weighted_score = (\\n                1 - temporal_weight\\n            ) * sim_score + temporal_weight * temp_weight\\n            weighted_scores.append(weighted_score)\\n\\n        # Sort the results based on the order_by parameter\\n        if order_by == \"similarity\":\\n            sorting_key = lambda k: weighted_scores[k]\\n        elif order_by == \"chronological\":  # order_by == \\'chronological\\'\\n            sorting_key = lambda k: new_query_indices[k]\\n        else:\\n            raise ValueError(\\n                \"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\"\\n            )\\n\\n        sorted_indices = [\\n            new_query_indices[i]\\n            for i in sorted(\\n                range(len(new_query_indices)), key=sorting_key, reverse=not reverse\\n            )\\n        ]\\n        sorted_results = [\\n            new_query_results[i]\\n            for i in sorted(\\n                range(len(new_query_results)), key=sorting_key, reverse=not reverse\\n            )\\n        ]\\n        sorted_scores = [\\n            weighted_scores[i]\\n            for i in sorted(\\n                range(len(weighted_scores)), key=sorting_key, reverse=not reverse\\n            )\\n        ]\\n\\n        # Return only the top k results without exceeding max_tokens\\n        final_results, final_scores, final_indices = [], [], []\\n        current_tokens = 0\\n        for i in range(min(k, len(sorted_results))):\\n            message_tokens = self.get_message_tokens(sorted_results[i])\\n            if current_tokens + message_tokens <= max_tokens:\\n                final_results.append(sorted_results[i])\\n                final_scores.append(sorted_scores[i])\\n                final_indices.append(sorted_indices[i])\\n                current_tokens += message_tokens\\n            else:\\n                break\\n\\n        return final_results, final_scores, final_indices\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def calculate_stability(self, boundaries, adjacency_matrix_with_message):\n",
      "    boundary_stability = np.zeros(len(boundaries) - 1)\n",
      "    for i in range(len(boundaries) - 1):\n",
      "        boundary_connections = adjacency_matrix_with_message[boundaries[i]:boundaries[i+1], :][:, boundaries[i]:boundaries[i+1]]\n",
      "        boundary_stability[i] = np.mean(boundary_connections) - np.std(boundary_connections)**2\n",
      "    logging.info(f\"Boundary Stability: {boundary_stability}\")\n",
      "    return boundary_stability\n",
      "\n",
      "Function call: len\n",
      "Function call: range\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def query_hints(self, message, index, k):\n",
      "    top_k, _, indices = index.token_bound_query(message, k=k, max_tokens=5000)\n",
      "    top_k_embeddings = [index.embeddings[i] for i in indices]\n",
      "    return top_k, top_k_embeddings\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def query_mk_hints(self, message, index_key, index, k):\n",
      "    top_k, _, indices = index.token_bound_query(message, k=k, max_tokens=3000)\n",
      "    top_k_embeddings = [self.memory_kernel_dict[index_key].node_embeddings[i] for i in indices]\n",
      "    return top_k, top_k_embeddings\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def heat_trajectory(self, message, k=20):\n",
      "    \"\"\"\n",
      "        This function gets the top k embeddings from all indexes including longterm, merges into numpy matrix,\n",
      "        record boundaries, computes kernel adj matrix, and sums all connections in each index section.\n",
      "        It returns the heat trajectory.\n",
      "        \"\"\"\n",
      "    embeddings = []\n",
      "    boundaries = [0]  # Start boundary\n",
      "    top_k_hints = {}\n",
      "    # Gather top k embeddings from each index\n",
      "    logging.info(\"Computing Trajectory for the current state of memory.\")\n",
      "    for index_key, index in self.index_dict.items():\n",
      "        top_k, top_k_embeddings = self.query_hints(message, index, k)\n",
      "        embeddings.extend(top_k_embeddings)\n",
      "        boundaries.append(boundaries[-1] + k)\n",
      "        top_k_hints[index_key] = top_k\n",
      "\n",
      "\n",
      "    if len(self.longterm_thread.values) >= k:\n",
      "        if len(self.longterm_thread.embeddings) != len(self.longterm_thread.values):\n",
      "            self.longterm_thread.compute_embeddings()\n",
      "        top_k, top_k_embeddings = self.query_hints(message, index, k)\n",
      "        embeddings.extend(top_k_embeddings)\n",
      "        boundaries.append(boundaries[-1] + k)\n",
      "        top_k_hints['longterm_thread'] = top_k\n",
      "    mean = np.sum(embeddings, axis=1)\n",
      "    mean = mean / np.linalg.norm(mean)\n",
      "    #should be shape (,1536) currently (4, 1536)\n",
      "    mean = np.sum(mean, axis=0)\n",
      "\n",
      "\n",
      "    logging.info(f\"Mean: {mean.shape}\")\n",
      "    embeddings_matrix = np.vstack(embeddings)\n",
      "    adjacency_matrix = cosine_similarity(embeddings_matrix)\n",
      "    message_embedding = EMBEDDER.embed(data=message)\n",
      "    #get residuals\n",
      "    message_embedding = message_embedding - mean\n",
      "    top_k_embeddings_with_message = embeddings_matrix.copy()\n",
      "    top_k_embeddings_with_message[:-1] += message_embedding\n",
      "    norms = np.linalg.norm(top_k_embeddings_with_message, axis=1)\n",
      "    normalized_embeddings = top_k_embeddings_with_message / norms[:, np.newaxis]\n",
      "    adjacency_matrix_with_message = cosine_similarity(normalized_embeddings)\n",
      "\n",
      "    adjacency_matrix_with_message = adjacency_matrix_with_message - adjacency_matrix\n",
      "    adjacency_matrix_with_message =  adjacency_matrix_with_message**3\n",
      "\n",
      "    degrees = np.sum(adjacency_matrix_with_message, axis=1)\n",
      "    heat_trajectory = [np.sum(degrees[boundaries[i]:boundaries[i + 1]]) for i in range(len(boundaries) - 1)]\n",
      "    logging.info(f\"Heat Trajectory: {heat_trajectory}\")\n",
      "    heat_dict = dict(zip(list(self.index_dict.keys()) + ['longterm_thread'], heat_trajectory))\n",
      "    sum_of_vals = sum(heat_dict.values())\n",
      "    heat_dict = {k: v / sum_of_vals for k, v in heat_dict.items()}\n",
      "    hdict = {k: v for k, v in sorted(heat_dict.items(), key=lambda item: item[1])}\n",
      "    logging.info(f\"Heat dict: {heat_dict}\")\n",
      "    return hdict, top_k_hints\n",
      "\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: cosine_similarity\n",
      "Function call: cosine_similarity\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: dict\n",
      "Function call: zip\n",
      "Function call: list\n",
      "Function call: sum\n",
      "Function call: sorted\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n@staticmethod\\n@abstractmethod\\ndef batched_cosine_similarity(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\\n    pass\\n', '\\n@staticmethod\\n@abstractmethod\\ndef batched_cosine_similarity(query_embedding: Any, embeddings: Any, mask: Optional[Any] = None) -> Any:\\n    pass\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n@field_validator(\\'values\\')\\ndef check_type_dictionaries(cls, values):\\n    first_type_dictionary = values[0].type_dictionary\\n    for value in values[1:]:\\n        if value.type_dictionary != first_type_dictionary:\\n            raise ValueError(\"All elements in \\'values\\' should have the same \\'type_dictionary\\'.\")\\n    return values\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def fifovector_memory_prompt(\n",
      "    self, message: str, k: int = 5\n",
      ") -> Tuple[List[dict], dict]:\n",
      "    hdict, top_k_hint_dict = self.heat_trajectory(message)\n",
      "    min_heat_index = list(hdict.keys())[0]\n",
      "    second_min_heat_index = list(hdict.keys())[1]\n",
      "    if len(hdict) > 2:\n",
      "        third_min_heat_index = list(hdict.keys())[2]\n",
      "    if min_heat_index == 'longterm_thread':\n",
      "        logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from long-term memory.\")\n",
      "        logging.info(f\"Number of values in Index: {len(self.longterm_thread.values)}\")\n",
      "        if len(hdict) > 2:\n",
      "            if hdict[second_min_heat_index] - hdict[third_min_heat_index] < 0.20:\n",
      "                top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-2] + top_k_hint_dict[third_min_heat_index][:1]\n",
      "            else:\n",
      "                top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-1]\n",
      "        else:\n",
      "            top_k_hint = top_k_hint_dict[min_heat_index][:1] + top_k_hint_dict[second_min_heat_index][:k-1]\n",
      "\n",
      "        logging.info(f\"Top K Hint: {top_k_hint}\")\n",
      "        prompt =f'[LONG TERM MEMORY]{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n",
      "    elif min_heat_index in self.index_dict.keys():\n",
      "        # if the difference between min and second min is less than 0.25, merge hints from both indexes\n",
      "        if hdict[min_heat_index] - hdict[second_min_heat_index] < 0.25:\n",
      "            logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from index.\")\n",
      "            logging.info(f\"Number of values in Index {self.index_dict[min_heat_index].name}: {len(self.index_dict[min_heat_index].values)}\")\n",
      "            #take 2 from first index topk and 1 from second index topk\n",
      "            top_k_hint = top_k_hint_dict[min_heat_index][:k-2] + top_k_hint_dict[second_min_heat_index][:2]\n",
      "            logging.info(f\"Top K Hint: {top_k_hint}\")\n",
      "            prompt =f'{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n",
      "        else:\n",
      "            logging.info(f\"Chosen Index: {min_heat_index} - Retrieving prompt from index.\")\n",
      "            logging.info(f\"Number of values in Index {self.index_dict[min_heat_index].name}: {len(self.index_dict[min_heat_index].values)}\")\n",
      "            top_k_hint = top_k_hint_dict[min_heat_index][:k]\n",
      "            logging.info(f\"Top K Hint: {top_k_hint}\")\n",
      "            prompt =f'{str(top_k_hint)}\\n\\n [QUESTION]: {message}'\n",
      "    else:\n",
      "        raise ValueError(\"The provided index name is not available.\")\n",
      "\n",
      "    return prompt\n",
      "\n",
      "Function call: list\n",
      "Function call: list\n",
      "Function call: len\n",
      "Function call: list\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: str\n",
      "Function call: len\n",
      "Function call: str\n",
      "Function call: len\n",
      "Function call: str\n",
      "Function call: ValueError\n",
      "Related codes: [\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def context_query(self, question: str, verbose: bool = False, stream: bool = False) -> Union[Generator, str]:\n",
      "    prompt = self.fifovector_memory_prompt(question)\n",
      "    modified_question = mark_question(prompt)\n",
      "    self.add_message(modified_question)\n",
      "\n",
      "    answer = BaseChat.query(self, message=prompt, verbose=verbose, stream=stream)\n",
      "\n",
      "    if stream:\n",
      "        return answer\n",
      "    else:\n",
      "        self.add_message(answer)\n",
      "        return answer\n",
      "\n",
      "Function call: mark_question\n",
      "Related codes: ['\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Set the variables\n",
      "\n",
      "\n",
      "async def process_chat_requests(\n",
      "    request_data: List[Dict],\n",
      "    save_filepath: str,\n",
      "    request_url: str,\n",
      "    api_key: str,\n",
      "    max_requests_per_minute: float,\n",
      "    max_tokens_per_minute: float,\n",
      "    token_encoding_name: str,\n",
      "    max_attempts: int,\n",
      "    logging_level: int,\n",
      "):\n",
      "    \"\"\"Processes chat requests in parallel, throttling to stay under rate limits.\"\"\"\n",
      "    # constants\n",
      "    seconds_to_pause_after_rate_limit_error = 15\n",
      "    seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n",
      "\n",
      "    # initialize logging\n",
      "    logging.basicConfig(level=logging_level)\n",
      "    logging.debug(f\"Logging initialized at level {logging_level}\")\n",
      "\n",
      "    # infer API endpoint and construct request header\n",
      "    api_endpoint = api_endpoint_from_url(request_url)\n",
      "    request_header = {\"Authorization\": f\"Bearer {api_key}\"}\n",
      "\n",
      "    # initialize trackers\n",
      "    queue_of_requests_to_retry = asyncio.Queue()\n",
      "    task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...\n",
      "    status_tracker = StatusTracker()  # single instance to track a collection of variables\n",
      "    next_request = None  # variable to hold the next request to call\n",
      "\n",
      "    # initialize available capacity counts\n",
      "    available_request_capacity = max_requests_per_minute\n",
      "    available_token_capacity = max_tokens_per_minute\n",
      "    last_update_time = time.time()\n",
      "\n",
      "    # initialize flags\n",
      "    file_not_finished = True  # after file is empty, we'll skip reading it\n",
      "    logging.debug(f\"Initialization complete.\")\n",
      "\n",
      "    # `requests` will provide requests one at a time\n",
      "    requests = iter(request_data)\n",
      "    logging.debug(f\"List initialized. Entering main loop\")\n",
      "\n",
      "    while True:\n",
      "        # get next request (if one is not already waiting for capacity)\n",
      "        if next_request is None:\n",
      "            if not queue_of_requests_to_retry.empty():\n",
      "                next_request = queue_of_requests_to_retry.get_nowait()\n",
      "                logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")\n",
      "            elif file_not_finished:\n",
      "                try:\n",
      "                    # get new request\n",
      "                    request_json = next(requests)\n",
      "                    next_request = APIRequest(\n",
      "                        task_id=next(task_id_generator),\n",
      "                        request_json=request_json,\n",
      "                        token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),\n",
      "                        attempts_left=max_attempts,\n",
      "                        metadata=request_json.pop(\"metadata\", None)\n",
      "                    )\n",
      "                    status_tracker.num_tasks_started += 1\n",
      "                    status_tracker.num_tasks_in_progress += 1\n",
      "                    logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")\n",
      "                except StopIteration:\n",
      "                    # if file runs out, set flag to stop reading it\n",
      "                    logging.debug(\"Read file exhausted\")\n",
      "                    file_not_finished = False\n",
      "\n",
      "        # update available capacity\n",
      "        current_time = time.time()\n",
      "        seconds_since_update = current_time - last_update_time\n",
      "        available_request_capacity = min(\n",
      "            available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,\n",
      "            max_requests_per_minute,\n",
      "        )\n",
      "        available_token_capacity = min(\n",
      "            available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,\n",
      "            max_tokens_per_minute,\n",
      "        )\n",
      "        last_update_time = current_time\n",
      "\n",
      "        # if enough capacity available, call API\n",
      "        if next_request:\n",
      "            next_request_tokens = next_request.token_consumption\n",
      "            if (\n",
      "                available_request_capacity >= 1\n",
      "                and available_token_capacity >= next_request_tokens\n",
      "            ):\n",
      "                # update counters\n",
      "                available_request_capacity -= 1\n",
      "                available_token_capacity -= next_request_tokens\n",
      "                next_request.attempts_left -= 1\n",
      "\n",
      "                # call API\n",
      "                asyncio.create_task(\n",
      "                    next_request.call_api(\n",
      "                        request_url=request_url,\n",
      "                        request_header=request_header,\n",
      "                        retry_queue=queue_of_requests_to_retry,\n",
      "                        save_filepath=save_filepath,\n",
      "                        status_tracker=status_tracker,\n",
      "                    )\n",
      "                )\n",
      "                next_request = None  # reset next_request to empty\n",
      "\n",
      "        # if all tasks are finished, break\n",
      "        if status_tracker.num_tasks_in_progress == 0:\n",
      "            break\n",
      "\n",
      "        # main loop sleeps briefly so concurrent tasks can run\n",
      "        await asyncio.sleep(seconds_to_sleep_each_loop)\n",
      "\n",
      "        # if a rate limit error was hit recently, pause to cool down\n",
      "        seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)\n",
      "        if seconds_since_rate_limit_error < seconds_to_pause_after_rate_limit_error:\n",
      "            remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n",
      "            await asyncio.sleep(remaining_seconds_to_pause)\n",
      "            # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n",
      "            logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")\n",
      "\n",
      "    # after finishing, log final status\n",
      "    logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")\n",
      "    if status_tracker.num_tasks_failed > 0:\n",
      "        logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")\n",
      "    if status_tracker.num_rate_limit_errors > 0:\n",
      "        logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n",
      "\n",
      "Function call: api_endpoint_from_url\n",
      "Function call: task_id_generator_function\n",
      "Function call: StatusTracker\n",
      "Function call: iter\n",
      "Function call: next\n",
      "Function call: APIRequest\n",
      "Function call: next\n",
      "Function call: num_tokens_consumed_from_request\n",
      "Function call: min\n",
      "Function call: min\n",
      "Related codes: ['\\n\\n# functions\\n\\n\\ndef api_endpoint_from_url(request_url):\\n    \"\"\"Extract the API endpoint from the request URL.\"\"\"\\n    match = re.search(\\'^https://[^/]+/v\\\\\\\\d+/(.+)$\\', request_url)\\n    return match[1]\\n', '\\n\\ndef task_id_generator_function():\\n    \"\"\"Generate integers 0, 1, 2, and so on.\"\"\"\\n    task_id = 0\\n    while True:\\n        yield task_id\\n        task_id += 1\\n', '\\n\\n# dataclasses\\n\\n\\n@dataclass\\nclass StatusTracker:\\n    \"\"\"Stores metadata about the script\\'s progress. Only one instance is created.\"\"\"\\n\\n    num_tasks_started: int = 0\\n    num_tasks_in_progress: int = 0  # script ends when this reaches 0\\n    num_tasks_succeeded: int = 0\\n    num_tasks_failed: int = 0\\n    num_rate_limit_errors: int = 0\\n    num_api_errors: int = 0  # excluding rate limit errors, counted above\\n    num_other_errors: int = 0\\n    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\\n', '\\n\\nclass LLMWriter(BaseTask):\\n    def __init__(\\n        self,\\n        index: MemoryIndex,\\n        path: List[List[int]],\\n        chatbot: Chat,\\n        write_func=None,\\n        context=None,\\n        task_name=\"summary\",\\n        max_workers: int = 1,\\n        task_id: str = \"LLMWriteTask\",\\n        calls_per_minute: int = 20,\\n        backup: bool = True,\\n    ):\\n        \"\"\"\\n        Initialize a LLMWriteTask instance.\\n\\n        :param index: List of strings representing the queries.\\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\\n        :param chatbot: Chatbot instance used for executing queries.\\n        :param max_workers: Maximum number of worker threads (default is 4).\\n        \"\"\"\\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\\n        self.index = index\\n        self.chatbot = chatbot\\n        self.write_func = write_func if write_func else self.llm_response\\n        self.new_index_name = self.index.name + f\"_{task_name}\"\\n        self.context = context\\n\\n    def llm_response(self, chatbot: Chat, message: str, context=None, id=None):\\n        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\\n        # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\\n        #     return \"the message is too long to be processed\"\\n        # moved the error catching to multi-threading but custom method could report the error here\\n        return chatbot.reply(message)\\n\\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\\n        \"\"\"\\n        Execute a sub-task using a separate copy of the chatbot instance.\\n\\n        :param sub_path: List of indices representing the sub-task\\'s sequence.\\n        :return: List of strings representing the responses for each query in the sub-task.\\n        \"\"\"\\n        if self.parallel:\\n            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\\n            chatbot_instance = copy.deepcopy(self.chatbot)\\n        else:\\n            chatbot_instance = self.chatbot\\n        if isinstance(self.chatbot, BaseThread):\\n            chatbot_instance.reset_memory()\\n\\n        sub_results = {}\\n        for i in sub_path:\\n            current_val = self.index.values[i]\\n            response = self.write_func(\\n                chatbot_instance, current_val, self.context, id=i\\n            )\\n            sub_results[i] = response\\n        return sub_results\\n\\n    def write(self):\\n        content_to_write = self.work()\\n        self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\\n        self.new_index.save()\\n        return self.new_index\\n', '\\n\\n@dataclass\\nclass APIRequest:\\n    \"\"\"Stores an API request\\'s inputs, outputs, and other metadata. Contains a method to make an API call.\"\"\"\\n\\n    task_id: int\\n    request_json: dict\\n    token_consumption: int\\n    attempts_left: int\\n    metadata: dict\\n    result: list = field(default_factory=list)\\n\\n    async def call_api(\\n        self,\\n        request_url: str,\\n        request_header: dict,\\n        retry_queue: asyncio.Queue,\\n        save_filepath: str,\\n        status_tracker: StatusTracker,\\n    ):\\n        \"\"\"Calls the OpenAI API and saves results.\"\"\"\\n        logging.info(f\"Starting request #{self.task_id}\")\\n        error = None\\n        try:\\n            async with aiohttp.ClientSession() as session:\\n                async with session.post(\\n                    url=request_url, headers=request_header, json=self.request_json\\n                ) as response:\\n                    response = await response.json()\\n            if \"error\" in response:\\n                logging.warning(\\n                    f\"Request {self.task_id} failed with error {response[\\'error\\']}\"\\n                )\\n                status_tracker.num_api_errors += 1\\n                error = response\\n                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\\n                    status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \\n                    status_tracker.num_rate_limit_errors += 1\\n                    status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\\n\\n        except Exception as e:  # catching naked exceptions is bad practice, but in this case we\\'ll log & save them\\n            logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\\n            status_tracker.num_other_errors += 1\\n            error = e\\n        if error:\\n            self.result.append(error)\\n            if self.attempts_left:\\n                retry_queue.put_nowait(self)\\n            else:\\n                logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\\n                data = (\\n                    [self.request_json, [str(e) for e in self.result], self.metadata]\\n                    if self.metadata\\n                    else [self.request_json, [str(e) for e in self.result]]\\n                )\\n                append_to_jsonl(data, save_filepath)\\n                status_tracker.num_tasks_in_progress -= 1\\n                status_tracker.num_tasks_failed += 1\\n        else:\\n            data = (\\n                [self.request_json, response, self.metadata]\\n                if self.metadata\\n                else [self.request_json, response]\\n            )\\n            append_to_jsonl(data, save_filepath)\\n            status_tracker.num_tasks_in_progress -= 1\\n            status_tracker.num_tasks_succeeded += 1\\n            logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\\n', '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class BatchChat(BaseChat):\n",
      "    def __init__(self, model: str = \"gpt-3.5-turbo\", max_output_tokens: int = 200, save_filepath: str = None, max_attempts: int = 5, api_key: str = None ):\n",
      "        #super init\n",
      "        super().__init__(model=model, max_output_tokens=max_output_tokens)\n",
      "        # Set the variables\n",
      "        if save_filepath is None:\n",
      "            self.save_filepath = \"results.jsonl\"  # Default filename if not using input file\n",
      "        else:\n",
      "            self.save_filepath = save_filepath\n",
      "        self.request_url = \"https://api.openai.com/v1/chat/completions\" # URL for the chat endpoint\n",
      "        self.max_requests_per_minute = 3_000 * 0.5\n",
      "        self.max_tokens_per_minute = 250_000 * 0.5\n",
      "        self.token_encoding_name = \"cl100k_base\"\n",
      "        self.api_key = api_key\n",
      "        self.max_attempts = max_attempts\n",
      "        self.logging_level = logging.INFO\n",
      "\n",
      "    async def batch_query(self, messages: List[str], system_prompts:List[str]) -> List[str]:\n",
      "        request_data = []\n",
      "        for message, system_prompt in zip(messages, system_prompts):\n",
      "            prompt, _ = self.prompt_func(message)\n",
      "            print(prompt)\n",
      "            request_data.append({\n",
      "                \"model\": self.model,\n",
      "                \"max_tokens\": self.max_output_tokens,\n",
      "                \"messages\": [\n",
      "                    {\"role\": \"system\", \"content\": system_prompt},\n",
      "                    {\"role\": \"user\", \"content\": message}\n",
      "                ]\n",
      "            })\n",
      "        responses = await process_chat_requests(request_data=request_data, request_url=self.request_url, api_key=self.api_key, save_filepath=self.save_filepath, max_requests_per_minute=self.max_requests_per_minute, max_tokens_per_minute=self.max_tokens_per_minute, token_encoding_name=self.token_encoding_name, max_attempts=self.max_attempts, logging_level=self.logging_level)\n",
      "        return responses\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseChat\"]\n",
      "]\n",
      "Function call: super\n",
      "Function call: zip\n",
      "Function call: print\n",
      "Function call: process_chat_requests\n",
      "Related codes: ['\\n# Set the variables\\n\\n\\nasync def process_chat_requests(\\n    request_data: List[Dict],\\n    save_filepath: str,\\n    request_url: str,\\n    api_key: str,\\n    max_requests_per_minute: float,\\n    max_tokens_per_minute: float,\\n    token_encoding_name: str,\\n    max_attempts: int,\\n    logging_level: int,\\n):\\n    \"\"\"Processes chat requests in parallel, throttling to stay under rate limits.\"\"\"\\n    # constants\\n    seconds_to_pause_after_rate_limit_error = 15\\n    seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\\n\\n    # initialize logging\\n    logging.basicConfig(level=logging_level)\\n    logging.debug(f\"Logging initialized at level {logging_level}\")\\n\\n    # infer API endpoint and construct request header\\n    api_endpoint = api_endpoint_from_url(request_url)\\n    request_header = {\"Authorization\": f\"Bearer {api_key}\"}\\n\\n    # initialize trackers\\n    queue_of_requests_to_retry = asyncio.Queue()\\n    task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...\\n    status_tracker = StatusTracker()  # single instance to track a collection of variables\\n    next_request = None  # variable to hold the next request to call\\n\\n    # initialize available capacity counts\\n    available_request_capacity = max_requests_per_minute\\n    available_token_capacity = max_tokens_per_minute\\n    last_update_time = time.time()\\n\\n    # initialize flags\\n    file_not_finished = True  # after file is empty, we\\'ll skip reading it\\n    logging.debug(f\"Initialization complete.\")\\n\\n    # `requests` will provide requests one at a time\\n    requests = iter(request_data)\\n    logging.debug(f\"List initialized. Entering main loop\")\\n\\n    while True:\\n        # get next request (if one is not already waiting for capacity)\\n        if next_request is None:\\n            if not queue_of_requests_to_retry.empty():\\n                next_request = queue_of_requests_to_retry.get_nowait()\\n                logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")\\n            elif file_not_finished:\\n                try:\\n                    # get new request\\n                    request_json = next(requests)\\n                    next_request = APIRequest(\\n                        task_id=next(task_id_generator),\\n                        request_json=request_json,\\n                        token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),\\n                        attempts_left=max_attempts,\\n                        metadata=request_json.pop(\"metadata\", None)\\n                    )\\n                    status_tracker.num_tasks_started += 1\\n                    status_tracker.num_tasks_in_progress += 1\\n                    logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")\\n                except StopIteration:\\n                    # if file runs out, set flag to stop reading it\\n                    logging.debug(\"Read file exhausted\")\\n                    file_not_finished = False\\n\\n        # update available capacity\\n        current_time = time.time()\\n        seconds_since_update = current_time - last_update_time\\n        available_request_capacity = min(\\n            available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,\\n            max_requests_per_minute,\\n        )\\n        available_token_capacity = min(\\n            available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,\\n            max_tokens_per_minute,\\n        )\\n        last_update_time = current_time\\n\\n        # if enough capacity available, call API\\n        if next_request:\\n            next_request_tokens = next_request.token_consumption\\n            if (\\n                available_request_capacity >= 1\\n                and available_token_capacity >= next_request_tokens\\n            ):\\n                # update counters\\n                available_request_capacity -= 1\\n                available_token_capacity -= next_request_tokens\\n                next_request.attempts_left -= 1\\n\\n                # call API\\n                asyncio.create_task(\\n                    next_request.call_api(\\n                        request_url=request_url,\\n                        request_header=request_header,\\n                        retry_queue=queue_of_requests_to_retry,\\n                        save_filepath=save_filepath,\\n                        status_tracker=status_tracker,\\n                    )\\n                )\\n                next_request = None  # reset next_request to empty\\n\\n        # if all tasks are finished, break\\n        if status_tracker.num_tasks_in_progress == 0:\\n            break\\n\\n        # main loop sleeps briefly so concurrent tasks can run\\n        await asyncio.sleep(seconds_to_sleep_each_loop)\\n\\n        # if a rate limit error was hit recently, pause to cool down\\n        seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)\\n        if seconds_since_rate_limit_error < seconds_to_pause_after_rate_limit_error:\\n            remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\\n            await asyncio.sleep(remaining_seconds_to_pause)\\n            # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\\n            logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")\\n\\n    # after finishing, log final status\\n    logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")\\n    if status_tracker.num_tasks_failed > 0:\\n        logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")\\n    if status_tracker.num_rate_limit_errors > 0:\\n        logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, model: str = \"gpt-3.5-turbo\", max_output_tokens: int = 200, save_filepath: str = None, max_attempts: int = 5, api_key: str = None ):\n",
      "    #super init\n",
      "    super().__init__(model=model, max_output_tokens=max_output_tokens)\n",
      "    # Set the variables\n",
      "    if save_filepath is None:\n",
      "        self.save_filepath = \"results.jsonl\"  # Default filename if not using input file\n",
      "    else:\n",
      "        self.save_filepath = save_filepath\n",
      "    self.request_url = \"https://api.openai.com/v1/chat/completions\" # URL for the chat endpoint\n",
      "    self.max_requests_per_minute = 3_000 * 0.5\n",
      "    self.max_tokens_per_minute = 250_000 * 0.5\n",
      "    self.token_encoding_name = \"cl100k_base\"\n",
      "    self.api_key = api_key\n",
      "    self.max_attempts = max_attempts\n",
      "    self.logging_level = logging.INFO\n",
      "\n",
      "Function call: super\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "async def batch_query(self, messages: List[str], system_prompts:List[str]) -> List[str]:\n",
      "    request_data = []\n",
      "    for message, system_prompt in zip(messages, system_prompts):\n",
      "        prompt, _ = self.prompt_func(message)\n",
      "        print(prompt)\n",
      "        request_data.append({\n",
      "            \"model\": self.model,\n",
      "            \"max_tokens\": self.max_output_tokens,\n",
      "            \"messages\": [\n",
      "                {\"role\": \"system\", \"content\": system_prompt},\n",
      "                {\"role\": \"user\", \"content\": message}\n",
      "            ]\n",
      "        })\n",
      "    responses = await process_chat_requests(request_data=request_data, request_url=self.request_url, api_key=self.api_key, save_filepath=self.save_filepath, max_requests_per_minute=self.max_requests_per_minute, max_tokens_per_minute=self.max_tokens_per_minute, token_encoding_name=self.token_encoding_name, max_attempts=self.max_attempts, logging_level=self.logging_level)\n",
      "    return responses\n",
      "\n",
      "Function call: zip\n",
      "Function call: print\n",
      "Function call: process_chat_requests\n",
      "Related codes: ['\\n# Set the variables\\n\\n\\nasync def process_chat_requests(\\n    request_data: List[Dict],\\n    save_filepath: str,\\n    request_url: str,\\n    api_key: str,\\n    max_requests_per_minute: float,\\n    max_tokens_per_minute: float,\\n    token_encoding_name: str,\\n    max_attempts: int,\\n    logging_level: int,\\n):\\n    \"\"\"Processes chat requests in parallel, throttling to stay under rate limits.\"\"\"\\n    # constants\\n    seconds_to_pause_after_rate_limit_error = 15\\n    seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\\n\\n    # initialize logging\\n    logging.basicConfig(level=logging_level)\\n    logging.debug(f\"Logging initialized at level {logging_level}\")\\n\\n    # infer API endpoint and construct request header\\n    api_endpoint = api_endpoint_from_url(request_url)\\n    request_header = {\"Authorization\": f\"Bearer {api_key}\"}\\n\\n    # initialize trackers\\n    queue_of_requests_to_retry = asyncio.Queue()\\n    task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...\\n    status_tracker = StatusTracker()  # single instance to track a collection of variables\\n    next_request = None  # variable to hold the next request to call\\n\\n    # initialize available capacity counts\\n    available_request_capacity = max_requests_per_minute\\n    available_token_capacity = max_tokens_per_minute\\n    last_update_time = time.time()\\n\\n    # initialize flags\\n    file_not_finished = True  # after file is empty, we\\'ll skip reading it\\n    logging.debug(f\"Initialization complete.\")\\n\\n    # `requests` will provide requests one at a time\\n    requests = iter(request_data)\\n    logging.debug(f\"List initialized. Entering main loop\")\\n\\n    while True:\\n        # get next request (if one is not already waiting for capacity)\\n        if next_request is None:\\n            if not queue_of_requests_to_retry.empty():\\n                next_request = queue_of_requests_to_retry.get_nowait()\\n                logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")\\n            elif file_not_finished:\\n                try:\\n                    # get new request\\n                    request_json = next(requests)\\n                    next_request = APIRequest(\\n                        task_id=next(task_id_generator),\\n                        request_json=request_json,\\n                        token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),\\n                        attempts_left=max_attempts,\\n                        metadata=request_json.pop(\"metadata\", None)\\n                    )\\n                    status_tracker.num_tasks_started += 1\\n                    status_tracker.num_tasks_in_progress += 1\\n                    logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")\\n                except StopIteration:\\n                    # if file runs out, set flag to stop reading it\\n                    logging.debug(\"Read file exhausted\")\\n                    file_not_finished = False\\n\\n        # update available capacity\\n        current_time = time.time()\\n        seconds_since_update = current_time - last_update_time\\n        available_request_capacity = min(\\n            available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,\\n            max_requests_per_minute,\\n        )\\n        available_token_capacity = min(\\n            available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,\\n            max_tokens_per_minute,\\n        )\\n        last_update_time = current_time\\n\\n        # if enough capacity available, call API\\n        if next_request:\\n            next_request_tokens = next_request.token_consumption\\n            if (\\n                available_request_capacity >= 1\\n                and available_token_capacity >= next_request_tokens\\n            ):\\n                # update counters\\n                available_request_capacity -= 1\\n                available_token_capacity -= next_request_tokens\\n                next_request.attempts_left -= 1\\n\\n                # call API\\n                asyncio.create_task(\\n                    next_request.call_api(\\n                        request_url=request_url,\\n                        request_header=request_header,\\n                        retry_queue=queue_of_requests_to_retry,\\n                        save_filepath=save_filepath,\\n                        status_tracker=status_tracker,\\n                    )\\n                )\\n                next_request = None  # reset next_request to empty\\n\\n        # if all tasks are finished, break\\n        if status_tracker.num_tasks_in_progress == 0:\\n            break\\n\\n        # main loop sleeps briefly so concurrent tasks can run\\n        await asyncio.sleep(seconds_to_sleep_each_loop)\\n\\n        # if a rate limit error was hit recently, pause to cool down\\n        seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)\\n        if seconds_since_rate_limit_error < seconds_to_pause_after_rate_limit_error:\\n            remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\\n            await asyncio.sleep(remaining_seconds_to_pause)\\n            # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\\n            logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")\\n\\n    # after finishing, log final status\\n    logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")\\n    if status_tracker.num_tasks_failed > 0:\\n        logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")\\n    if status_tracker.num_rate_limit_errors > 0:\\n        logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class ContextManager:\n",
      "    \"\"\"\n",
      "    A class that manages the context of user interactions and maps them to the appropriate memory index or thread.\n",
      "    \"\"\"\n",
      "    def __init__(\n",
      "        self, \n",
      "        index_dict: Dict[str, MemoryIndex], \n",
      "        longterm_thread_keywords: List[str] = [\"long ago\", \"in the past\", \"long term\", \"long-term\", \"longterm\"]\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        Initialize the ContextManager with the available indexes.\n",
      "\n",
      "        :param index_dict: A dictionary mapping index names to MemoryIndex instances.\n",
      "        :param fifo_thread_keywords: A list of keywords indicating user reference to recent conversation context.\n",
      "        :param longterm_thread_keywords: A list of keywords indicating user reference to long-term conversation context.\n",
      "        \"\"\"\n",
      "        self.index_dict = index_dict\n",
      "        self.longterm_thread_keywords = longterm_thread_keywords\n",
      "        self.keyword_to_index_map = self.create_keyword_to_index_map()\n",
      "\n",
      "    def create_keyword_to_index_map(self) -> Dict[str, str]:\n",
      "        \"\"\"\n",
      "        Create a mapping from keywords to index names.\n",
      "\n",
      "        :return: A dictionary mapping keywords to index names or threads.\n",
      "        \"\"\"\n",
      "        keyword_to_index_map = {index_name.split('_')[0]: index_name for index_name in self.index_dict.keys()}\n",
      "        for keyword in self.longterm_thread_keywords:\n",
      "            keyword_to_index_map[keyword] = \"longterm_thread\"\n",
      "        return keyword_to_index_map\n",
      "\n",
      "    def get_context_for_user_input(self, user_input: str) -> str:\n",
      "        \"\"\"\n",
      "        Get the appropriate context (index or thread) for the given user input.\n",
      "\n",
      "        :param user_input: A string representing the user's input.\n",
      "        :return: The name of the appropriate context (index or thread), or None if no specific context is appropriate.\n",
      "        \"\"\"\n",
      "        user_input_str = str(user_input)  # Ensure user_input is a string\n",
      "        for keyword, context in self.keyword_to_index_map.items():\n",
      "            if re.search(r'\\b' + keyword + r'\\b', user_input_str, re.I):\n",
      "                return context\n",
      "        return None\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: str\n",
      "Related codes: ['\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self, \n",
      "    index_dict: Dict[str, MemoryIndex], \n",
      "    longterm_thread_keywords: List[str] = [\"long ago\", \"in the past\", \"long term\", \"long-term\", \"longterm\"]\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "        Initialize the ContextManager with the available indexes.\n",
      "\n",
      "        :param index_dict: A dictionary mapping index names to MemoryIndex instances.\n",
      "        :param fifo_thread_keywords: A list of keywords indicating user reference to recent conversation context.\n",
      "        :param longterm_thread_keywords: A list of keywords indicating user reference to long-term conversation context.\n",
      "        \"\"\"\n",
      "    self.index_dict = index_dict\n",
      "    self.longterm_thread_keywords = longterm_thread_keywords\n",
      "    self.keyword_to_index_map = self.create_keyword_to_index_map()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def create_keyword_to_index_map(self) -> Dict[str, str]:\n",
      "    \"\"\"\n",
      "        Create a mapping from keywords to index names.\n",
      "\n",
      "        :return: A dictionary mapping keywords to index names or threads.\n",
      "        \"\"\"\n",
      "    keyword_to_index_map = {index_name.split('_')[0]: index_name for index_name in self.index_dict.keys()}\n",
      "    for keyword in self.longterm_thread_keywords:\n",
      "        keyword_to_index_map[keyword] = \"longterm_thread\"\n",
      "    return keyword_to_index_map\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_context_for_user_input(self, user_input: str) -> str:\n",
      "    \"\"\"\n",
      "        Get the appropriate context (index or thread) for the given user input.\n",
      "\n",
      "        :param user_input: A string representing the user's input.\n",
      "        :return: The name of the appropriate context (index or thread), or None if no specific context is appropriate.\n",
      "        \"\"\"\n",
      "    user_input_str = str(user_input)  # Ensure user_input is a string\n",
      "    for keyword, context in self.keyword_to_index_map.items():\n",
      "        if re.search(r'\\b' + keyword + r'\\b', user_input_str, re.I):\n",
      "            return context\n",
      "    return None\n",
      "\n",
      "Function call: str\n",
      "Related codes: ['\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class Chat(BaseChat, Prompter):\n",
      "    \"\"\"\n",
      "    This class combines the BaseChat and Prompter classes to create a oneshot chatbot with a system and user prompt,\n",
      "    and the ability to handle queries to multiple MemoryIndex through the index_dict.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        model: str = None,\n",
      "        max_output_tokens: int = 1000,\n",
      "        system_prompt: str = None,\n",
      "        user_prompt: str = None,\n",
      "        index_dict: Optional[Dict[str, MemoryIndex]] = None,\n",
      "        max_index_memory: int = 1000,\n",
      "        name: str = \"Chat\",\n",
      "    ) -> None:\n",
      "        BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\n",
      "        Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\n",
      "        self.index_dict = index_dict\n",
      "        self.setup_indices(max_index_memory)\n",
      "        self.name = name\n",
      "    \n",
      "    def add_user_defined_ids(self, id_dict: Dict[str, list]):\n",
      "        self.user_defined_ids.append(id_dict)\n",
      "        self.use_user_defined_ids = True\n",
      "        self.setup_index_prompts()\n",
      "\n",
      "\n",
      "    def setup_index_prompts(self):\n",
      "        if self.current_index is not None or self.use_user_defined_ids:\n",
      "            print(\"Index is available so using index prompts\")\n",
      "            self.system_prompt = (\n",
      "                INDEX_SYSTEM_PROMPT \n",
      "                if self.user_defined_system_prompt is None \n",
      "                else self.user_defined_system_prompt\n",
      "            )\n",
      "            self.user_prompt = (\n",
      "                self.get_index_hints\n",
      "                if self.user_defined_user_prompt is None\n",
      "                else self.user_defined_user_prompt\n",
      "            )\n",
      "        else:\n",
      "            if self.user_defined_system_prompt is None:\n",
      "                print(\"No user defined system prompt defaulting to default prompts\")\n",
      "                self.set_default_prompts()\n",
      "            else:\n",
      "                print(\"User defined system prompt and default user prompt\")\n",
      "                self.system_prompt = self.user_defined_system_prompt\n",
      "                self.user_prompt = self.default_user_prompt\n",
      "\n",
      "\n",
      "                \n",
      "\n",
      "    def setup_indices(self, max_index_memory):\n",
      "        \"\"\"setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index.\"\"\"\n",
      "        if self.index_dict is not None:\n",
      "            self.max_index_memory = max_index_memory\n",
      "            # set the last index to be the current index\n",
      "            self.current_index = list(self.index_dict.keys())[-1]\n",
      "            self.setup_index_prompts()    \n",
      "        else:\n",
      "            self.current_index = None\n",
      "\n",
      "    def get_index_hints(\n",
      "        self, question: str, k: int = 10, max_tokens: int = None\n",
      "    ) -> str:\n",
      "        \"\"\"\n",
      "        Get hints from the current index for the given question.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include from the index.\n",
      "        :param max_tokens: The maximum number of tokens to be retrieved from the index.\n",
      "        :return: A string representing the hint prompt with the question.\n",
      "        \"\"\"\n",
      "        if max_tokens is None:\n",
      "            max_tokens = self.max_index_memory\n",
      "        hints = []\n",
      "        if self.use_user_defined_ids is True:\n",
      "            user_defined_id = self.user_defined_ids[-1]\n",
      "            for index, ids in user_defined_id.items():\n",
      "                for i in ids:\n",
      "                    hints.append(self.index_dict[index].values[i])\n",
      "            self.use_user_defined_ids = False\n",
      "            self.setup_index_prompts()\n",
      "        elif self.current_index is not None:\n",
      "            index_instance = self.index_dict[self.current_index]\n",
      "            if isinstance(index_instance, MemoryIndex):\n",
      "                hints, _, _ = index_instance.token_bound_query(\n",
      "                    question, k=k, max_tokens=max_tokens\n",
      "                )\n",
      "            else:\n",
      "                raise ValueError(\"The current index is not a valid index instance.\")\n",
      "        if len(hints) == 0:\n",
      "            return question\n",
      "        else:\n",
      "            hints_string = \"\\n\".join(hints)\n",
      "            hint_prompt = INDEX_HINT_PROMPT\n",
      "            question_intro = QUESTION_INTRO\n",
      "            return hint_prompt.format(\n",
      "                hints_string=hints_string\n",
      "            ) + question_intro.format(question=question)\n",
      "\n",
      "    def set_current_index(self, index_name: Optional[str]) -> None:\n",
      "        \"\"\"\n",
      "        Set the current index to be used for hints.\n",
      "\n",
      "        :param index_name: A string representing the index name or None to clear the current index.\n",
      "        :raise ValueError: If the provided index name is not available.\n",
      "        \"\"\"\n",
      "        if self.index_dict is None:\n",
      "            raise ValueError(\"No index_dict are available.\")\n",
      "        elif index_name in self.index_dict:\n",
      "            self.current_index = index_name\n",
      "        elif index_name is None:\n",
      "            self.current_index = None\n",
      "        else:\n",
      "            raise ValueError(\"The provided index name is not available.\")\n",
      "        self.setup_index_prompts()\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseChat, \", \"Prompter\"]\n",
      "]\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: list\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: ValueError\n",
      "Related codes: [\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    model: str = None,\n",
      "    max_output_tokens: int = 1000,\n",
      "    system_prompt: str = None,\n",
      "    user_prompt: str = None,\n",
      "    index_dict: Optional[Dict[str, MemoryIndex]] = None,\n",
      "    max_index_memory: int = 1000,\n",
      "    name: str = \"Chat\",\n",
      ") -> None:\n",
      "    BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\n",
      "    Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\n",
      "    self.index_dict = index_dict\n",
      "    self.setup_indices(max_index_memory)\n",
      "    self.name = name\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def add_user_defined_ids(self, id_dict: Dict[str, list]):\n",
      "    self.user_defined_ids.append(id_dict)\n",
      "    self.use_user_defined_ids = True\n",
      "    self.setup_index_prompts()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def setup_index_prompts(self):\n",
      "    if self.current_index is not None or self.use_user_defined_ids:\n",
      "        print(\"Index is available so using index prompts\")\n",
      "        self.system_prompt = (\n",
      "            INDEX_SYSTEM_PROMPT \n",
      "            if self.user_defined_system_prompt is None \n",
      "            else self.user_defined_system_prompt\n",
      "        )\n",
      "        self.user_prompt = (\n",
      "            self.get_index_hints\n",
      "            if self.user_defined_user_prompt is None\n",
      "            else self.user_defined_user_prompt\n",
      "        )\n",
      "    else:\n",
      "        if self.user_defined_system_prompt is None:\n",
      "            print(\"No user defined system prompt defaulting to default prompts\")\n",
      "            self.set_default_prompts()\n",
      "        else:\n",
      "            print(\"User defined system prompt and default user prompt\")\n",
      "            self.system_prompt = self.user_defined_system_prompt\n",
      "            self.user_prompt = self.default_user_prompt\n",
      "\n",
      "\n",
      "            \n",
      "\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def setup_indices(self, max_index_memory):\n",
      "    \"\"\"setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index.\"\"\"\n",
      "    if self.index_dict is not None:\n",
      "        self.max_index_memory = max_index_memory\n",
      "        # set the last index to be the current index\n",
      "        self.current_index = list(self.index_dict.keys())[-1]\n",
      "        self.setup_index_prompts()    \n",
      "    else:\n",
      "        self.current_index = None\n",
      "\n",
      "Function call: list\n",
      "Related codes: [\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\"]\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_index_hints(\n",
      "    self, question: str, k: int = 10, max_tokens: int = None\n",
      ") -> str:\n",
      "    \"\"\"\n",
      "        Get hints from the current index for the given question.\n",
      "\n",
      "        :param question: A string representing the user question.\n",
      "        :param k: The number of most similar messages to include from the index.\n",
      "        :param max_tokens: The maximum number of tokens to be retrieved from the index.\n",
      "        :return: A string representing the hint prompt with the question.\n",
      "        \"\"\"\n",
      "    if max_tokens is None:\n",
      "        max_tokens = self.max_index_memory\n",
      "    hints = []\n",
      "    if self.use_user_defined_ids is True:\n",
      "        user_defined_id = self.user_defined_ids[-1]\n",
      "        for index, ids in user_defined_id.items():\n",
      "            for i in ids:\n",
      "                hints.append(self.index_dict[index].values[i])\n",
      "        self.use_user_defined_ids = False\n",
      "        self.setup_index_prompts()\n",
      "    elif self.current_index is not None:\n",
      "        index_instance = self.index_dict[self.current_index]\n",
      "        if isinstance(index_instance, MemoryIndex):\n",
      "            hints, _, _ = index_instance.token_bound_query(\n",
      "                question, k=k, max_tokens=max_tokens\n",
      "            )\n",
      "        else:\n",
      "            raise ValueError(\"The current index is not a valid index instance.\")\n",
      "    if len(hints) == 0:\n",
      "        return question\n",
      "    else:\n",
      "        hints_string = \"\\n\".join(hints)\n",
      "        hint_prompt = INDEX_HINT_PROMPT\n",
      "        question_intro = QUESTION_INTRO\n",
      "        return hint_prompt.format(\n",
      "            hints_string=hints_string\n",
      "        ) + question_intro.format(question=question)\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def set_current_index(self, index_name: Optional[str]) -> None:\n",
      "    \"\"\"\n",
      "        Set the current index to be used for hints.\n",
      "\n",
      "        :param index_name: A string representing the index name or None to clear the current index.\n",
      "        :raise ValueError: If the provided index name is not available.\n",
      "        \"\"\"\n",
      "    if self.index_dict is None:\n",
      "        raise ValueError(\"No index_dict are available.\")\n",
      "    elif index_name in self.index_dict:\n",
      "        self.current_index = index_name\n",
      "    elif index_name is None:\n",
      "        self.current_index = None\n",
      "    else:\n",
      "        raise ValueError(\"The provided index name is not available.\")\n",
      "    self.setup_index_prompts()\n",
      "\n",
      "Function call: ValueError\n",
      "Function call: ValueError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class Prompter:\n",
      "    \"\"\"\n",
      "    This class handles the system and user prompts and the prompt_func. By subclassing and overriding the\n",
      "    prompt_func, you can change the way the prompts are composed.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, system_prompt: Union[str,None] = None, user_prompt: Union[str,None] = None):\n",
      "        \"\"\"\n",
      "        Initialize the Prompter with system and user prompts.\n",
      "\n",
      "        :param system_prompt: A string representing the system prompt.\n",
      "        :param user_prompt: A string representing the user prompt.\n",
      "        \"\"\"\n",
      "        \n",
      "        if system_prompt is None:\n",
      "            self.system_prompt = DEFAULT_SYSTEM_PROMPT\n",
      "            self.user_defined_system_prompt = None\n",
      "        else:\n",
      "            self.system_prompt = system_prompt\n",
      "            self.user_defined_system_prompt = system_prompt\n",
      "        if user_prompt is None:\n",
      "            self.user_prompt = self.default_user_prompt\n",
      "            self.user_defined_user_prompt = None\n",
      "        else:\n",
      "            self.user_prompt = user_prompt\n",
      "            self.user_defined_user_prompt = user_prompt\n",
      "\n",
      "        self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\n",
      "        self.user_defined_ids = []\n",
      "        self.user_defined_values = []\n",
      "        self.use_user_defined_ids = False\n",
      "\n",
      "    def set_default_prompts(self):\n",
      "        self.system_prompt = DEFAULT_SYSTEM_PROMPT\n",
      "        self.user_prompt = self.default_user_prompt\n",
      "\n",
      "\n",
      "    def default_user_prompt(self, message: str) -> str:\n",
      "        return DEFAULT_USER_PROMPT.format(question=message)\n",
      "\n",
      "    def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\n",
      "        \"\"\"\n",
      "        Compose the prompt for the chat-gpt API.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\n",
      "        \"\"\"\n",
      "        marked_question = mark_question(self.user_prompt(message))\n",
      "        prompt = [mark_system(self.system_prompt)] + [marked_question]\n",
      "        return prompt, marked_question\n",
      "\n",
      "    def update_system_prompt(self, new_prompt: str) -> None:\n",
      "        \"\"\"\n",
      "        Update the system prompt.\n",
      "\n",
      "        :param new_prompt: A string representing the new system prompt.\n",
      "        \"\"\"\n",
      "        self.system_prompt = new_prompt\n",
      "\n",
      "    def update_user_prompt(self, new_prompt ) -> None:\n",
      "        \"\"\"\n",
      "        Update the user prompt.\n",
      "\n",
      "        :param new_prompt: A string representing the new user prompt.\n",
      "        \"\"\"\n",
      "        self.user_prompt = new_prompt\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: mark_question\n",
      "Function call: mark_system\n",
      "Related codes: ['\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n', '\\ndef mark_system(system_prompt):\\n    return {\"role\": \"system\", \"content\": system_prompt}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(self, system_prompt: Union[str,None] = None, user_prompt: Union[str,None] = None):\n",
      "    \"\"\"\n",
      "        Initialize the Prompter with system and user prompts.\n",
      "\n",
      "        :param system_prompt: A string representing the system prompt.\n",
      "        :param user_prompt: A string representing the user prompt.\n",
      "        \"\"\"\n",
      "    \n",
      "    if system_prompt is None:\n",
      "        self.system_prompt = DEFAULT_SYSTEM_PROMPT\n",
      "        self.user_defined_system_prompt = None\n",
      "    else:\n",
      "        self.system_prompt = system_prompt\n",
      "        self.user_defined_system_prompt = system_prompt\n",
      "    if user_prompt is None:\n",
      "        self.user_prompt = self.default_user_prompt\n",
      "        self.user_defined_user_prompt = None\n",
      "    else:\n",
      "        self.user_prompt = user_prompt\n",
      "        self.user_defined_user_prompt = user_prompt\n",
      "\n",
      "    self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\n",
      "    self.user_defined_ids = []\n",
      "    self.user_defined_values = []\n",
      "    self.use_user_defined_ids = False\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def set_default_prompts(self):\n",
      "    self.system_prompt = DEFAULT_SYSTEM_PROMPT\n",
      "    self.user_prompt = self.default_user_prompt\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def default_user_prompt(self, message: str) -> str:\n",
      "    return DEFAULT_USER_PROMPT.format(question=message)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\n",
      "    \"\"\"\n",
      "        Compose the prompt for the chat-gpt API.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\n",
      "        \"\"\"\n",
      "    marked_question = mark_question(self.user_prompt(message))\n",
      "    prompt = [mark_system(self.system_prompt)] + [marked_question]\n",
      "    return prompt, marked_question\n",
      "\n",
      "Function call: mark_question\n",
      "Function call: mark_system\n",
      "Related codes: ['\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n', '\\ndef mark_system(system_prompt):\\n    return {\"role\": \"system\", \"content\": system_prompt}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def update_system_prompt(self, new_prompt: str) -> None:\n",
      "    \"\"\"\n",
      "        Update the system prompt.\n",
      "\n",
      "        :param new_prompt: A string representing the new system prompt.\n",
      "        \"\"\"\n",
      "    self.system_prompt = new_prompt\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def update_user_prompt(self, new_prompt ) -> None:\n",
      "    \"\"\"\n",
      "        Update the user prompt.\n",
      "\n",
      "        :param new_prompt: A string representing the new user prompt.\n",
      "        \"\"\"\n",
      "    self.user_prompt = new_prompt\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class BaseChat:\n",
      "    \"\"\"\n",
      "    This is the base class for chatbots, defining the basic functions that a chatbot should have, mainly the calls to\n",
      "    chat-gpt API, and a basic Gradio interface. It has a prompt_func that acts as a placeholder for a call to chat-gpt\n",
      "    API without any additional messages. It can be overridden by subclasses to add additional messages to the prompt.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, model: Union[str,None] = None, max_output_tokens: int = 200):\n",
      "        \"\"\"\n",
      "        Initialize the BaseChat with a model and max_output_tokens.\n",
      "\n",
      "        :param model: A string representing the chat model to be used.\n",
      "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
      "        \"\"\"\n",
      "        if model is None:\n",
      "            self.model = \"gpt-3.5-turbo\"\n",
      "        else:\n",
      "            self.model = model\n",
      "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "        self.max_output_tokens = max_output_tokens\n",
      "        self.failed_responses = []\n",
      "        self.outputs = []\n",
      "        self.inputs = []\n",
      "        self.prompts = []\n",
      "        self.prompt_func = self.identity_prompter\n",
      "\n",
      "    def __getstate__(self):\n",
      "        state = self.__dict__.copy()\n",
      "        # Remove the tokenizer attribute from the state\n",
      "        del state[\"tokenizer\"]\n",
      "        return state\n",
      "\n",
      "    def __setstate__(self, state):\n",
      "        self.__dict__.update(state)\n",
      "        # Reinitialize the tokenizer attribute after unpickling\n",
      "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "\n",
      "    def identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\n",
      "        \"\"\"\n",
      "        A simple identity prompter that takes a message and returns the message marked as a question.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A tuple containing the marked question and the original message.\n",
      "        \"\"\"\n",
      "        return [mark_question(message)], mark_question(message)\n",
      "\n",
      "    def chat_response( self, prompt: List[dict], max_tokens: Union[int,None] = None, stream:bool = False ) -> Union[Generator,Tuple[Dict, bool]]:\n",
      "        if max_tokens is None:\n",
      "            max_tokens = self.max_output_tokens\n",
      "        if \"gpt\" in self.model:\n",
      "            logging.info(prompt)\n",
      "            response, status = chatgpt_response(prompt=prompt,model=self.model, max_tokens = 1000, stream=stream)\n",
      "            if status:\n",
      "                return response, True\n",
      "            else:\n",
      "                self.failed_responses.append(response)\n",
      "                return response, False\n",
      "\n",
      "        elif \"command\" in self.model:\n",
      "            response, status = cohere_response(prompt=prompt,model=self.model, max_tokens = 1000)  \n",
      "            if status:\n",
      "                return response, True\n",
      "            else:\n",
      "                self.failed_responses.append(response)\n",
      "                return response, False\n",
      "        else:\n",
      "            return {}, False \n",
      "\n",
      "    def reply(self, message: str, verbose: bool = True, stream: bool = False) -> Union[Generator, str]:\n",
      "        \"\"\"\n",
      "        Reply to a given message using the chatbot.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "        if stream:\n",
      "            return self.query(message, verbose, stream)\n",
      "        else:\n",
      "            return self.query(message, verbose)[\"content\"]\n",
      "\n",
      "    def query(self, message: str, verbose: bool = True, stream: bool = False) -> Union[Generator, str]:\n",
      "        \"\"\"\n",
      "        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "\n",
      "        prompt, _ = self.prompt_func(message)\n",
      "\n",
      "        if stream:\n",
      "            return self.chat_response(prompt=prompt, stream=stream)\n",
      "\n",
      "        response, success = self.chat_response(prompt)\n",
      "        if verbose:\n",
      "            display(Markdown(\"#### Question: \\n {question}\".format(question=message)))\n",
      "        if success:\n",
      "            answer = get_mark_from_response(response, self.model)\n",
      "            self.outputs.append(answer)\n",
      "            self.inputs.append(message)\n",
      "            self.prompts.append(prompt)\n",
      "            if verbose:\n",
      "                display(\n",
      "                    Markdown(\n",
      "                        \" #### Anwser: \\n {answer}\".format(\n",
      "                            answer=get_str_from_response(response, self.model)\n",
      "                        )\n",
      "                    )\n",
      "                )\n",
      "            return answer\n",
      "        else:\n",
      "            raise Exception(\"OpenAI API Error inside query function\")\n",
      "\n",
      "    def reset_logs(self):\n",
      "        \"\"\"\n",
      "        Reset the chatbot's memory.\n",
      "        \"\"\"\n",
      "        self.outputs = []\n",
      "        self.inputs = []\n",
      "        self.prompts = []\n",
      "\n",
      "    def get_logs(self):\n",
      "        \"\"\"\n",
      "        Get the chatbot's memory.\n",
      "\n",
      "        :return: A tuple containing the chatbot's memory as three lists of strings.\n",
      "        \"\"\"\n",
      "        return self.inputs, self.outputs, self.prompts\n",
      "\n",
      "    def run_text(\n",
      "        self, text: str, state: List[Tuple[str, str]]\n",
      "    ) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\n",
      "        \"\"\"\n",
      "        Process the user's text input and update the chat state.\n",
      "\n",
      "        :param text: A string representing the user input.\n",
      "        :param state: A list of tuples representing the current chat state.\n",
      "        :return: A tuple containing the updated chat state as two lists of tuples.\n",
      "        \"\"\"\n",
      "        print(\"===============Running run_text =============\")\n",
      "        print(\"Inputs:\", text)\n",
      "        try:\n",
      "            print(\"======>Current memory:\\n %s\" % self.memory_thread)\n",
      "        except:\n",
      "            print(\"======>No memory\")\n",
      "        print(\"failed here\")\n",
      "        response = self.reply(text)\n",
      "        state = state + [(text, response)]\n",
      "        print(\"Outputs:\", state)\n",
      "        return state, state\n",
      "\n",
      "    def gradio(self):\n",
      "        \"\"\"\n",
      "        Create and launch a Gradio interface for the chatbot.\n",
      "        \"\"\"\n",
      "        with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n",
      "            chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\n",
      "            state = gr.State([])\n",
      "            with gr.Row():\n",
      "                with gr.Column(scale=1):\n",
      "                    txt = gr.Textbox(\n",
      "                        show_label=False,\n",
      "                        placeholder=\"Enter text and press enter, or upload an image\",\n",
      "                    ).style(container=False)\n",
      "                with gr.Column(scale=0.15, min_width=0):\n",
      "                    clear = gr.Button(\"Clear️\")\n",
      "\n",
      "            txt.submit(lambda text, state: self.run_text(text, state), [txt, state], [chatbot, state])\n",
      "            txt.submit(lambda: \"\", None, txt)\n",
      "            demo.launch(server_name=\"localhost\", server_port=7860)\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: mark_question\n",
      "Function call: mark_question\n",
      "Function call: chatgpt_response\n",
      "Function call: cohere_response\n",
      "Function call: display\n",
      "Function call: Markdown\n",
      "Function call: get_mark_from_response\n",
      "Function call: display\n",
      "Function call: Markdown\n",
      "Function call: get_str_from_response\n",
      "Function call: Exception\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Related codes: ['\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n', '\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n', '\\ndef chatgpt_response(\\n     \\n    prompt: List[Dict[str, Union[str, Dict[str, str]]]], \\n    model: str = \"gpt-3.5-turbo\", \\n    max_tokens: int = 1000, \\n    stream: bool = False\\n) -> Union[Generator,Tuple]:\\n\\n    try:\\n        print(\"Trying to call OpenAI API...\")\\n        response = openai.ChatCompletion.create(\\n            model=model,\\n            messages=prompt,\\n            max_tokens=max_tokens,\\n            stream=stream\\n        )\\n        return response, True\\n\\n    except openai.APIError as e:\\n        logging.error(f\"Unexpected error in openai call: {e}\") \\n        fail_response = {\\n            \"choices\": [\\n                {\\n                    \"message\": {\\n                        \"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"\\n                    }\\n                }\\n            ]\\n        }\\n        return fail_response, False\\n', '\\n\\n\\ndef cohere_response(prompt: List[dict],model: str = \"command\", max_tokens: int = 1000\\n    ) -> Tuple[Dict, bool]:\\n    \"\"\"\\n        Call the Cohere API with the given prompt and maximum number of output tokens.\\n        \\n\\n        :param prompt: A list of strings representing the prompt to send to the API.\\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\\n        :param model: A string representing the model to use. either command or command-nightly\\n        :return: A tuple containing the API response cohere object and a boolean indicating success.\\n        \"\"\"             \\n    try:\\n        prompt= convert_mark_to_str_prompt(prompt)\\n        print(\"Trying to call Cohere API... using model:\", model)\\n        response = co.generate(\\n        model= model,\\n        prompt= prompt,\\n        max_tokens=max_tokens,\\n        #end_sequences=[\\'#SYSTEM:\\', \\'#USER:\\'],\\n        )\\n        return response, True\\n\\n    except:\\n          return None, False\\n    \\n', '\\n\\ndef get_mark_from_response(response, model = \"gpt\"):\\n    # return the answer from the response\\n    if \"gpt\" in model:\\n        role = response[\"choices\"][0][\"message\"][\"role\"]\\n        message = response[\"choices\"][0][\"message\"][\"content\"]\\n    elif \"command\" in model:\\n        role = \"assistant\"\\n        message = response[0].text \\n    else:\\n        raise Exception(\"Unknown model type\")\\n    return {\"role\": role, \"content\": message}\\n', '\\n\\ndef get_str_from_response(response, model = \"gpt\"):\\n    # return the answer from the response\\n    if \"gpt\" in model:\\n        return response[\"choices\"][0][\"message\"][\"content\"]\\n    elif \"command\" in model:\\n        text = response[0].text\\n        text_without_assistant = text.replace(\"#ASSISTANT\", \"\")\\n        return text_without_assistant\\n    else:\\n        raise Exception(\"Unknown model type\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(self, model: Union[str,None] = None, max_output_tokens: int = 200):\n",
      "    \"\"\"\n",
      "        Initialize the BaseChat with a model and max_output_tokens.\n",
      "\n",
      "        :param model: A string representing the chat model to be used.\n",
      "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
      "        \"\"\"\n",
      "    if model is None:\n",
      "        self.model = \"gpt-3.5-turbo\"\n",
      "    else:\n",
      "        self.model = model\n",
      "    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "    self.max_output_tokens = max_output_tokens\n",
      "    self.failed_responses = []\n",
      "    self.outputs = []\n",
      "    self.inputs = []\n",
      "    self.prompts = []\n",
      "    self.prompt_func = self.identity_prompter\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __getstate__(self):\n",
      "    state = self.__dict__.copy()\n",
      "    # Remove the tokenizer attribute from the state\n",
      "    del state[\"tokenizer\"]\n",
      "    return state\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __setstate__(self, state):\n",
      "    self.__dict__.update(state)\n",
      "    # Reinitialize the tokenizer attribute after unpickling\n",
      "    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\n",
      "    \"\"\"\n",
      "        A simple identity prompter that takes a message and returns the message marked as a question.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A tuple containing the marked question and the original message.\n",
      "        \"\"\"\n",
      "    return [mark_question(message)], mark_question(message)\n",
      "\n",
      "Function call: mark_question\n",
      "Function call: mark_question\n",
      "Related codes: ['\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n', '\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def chat_response( self, prompt: List[dict], max_tokens: Union[int,None] = None, stream:bool = False ) -> Union[Generator,Tuple[Dict, bool]]:\n",
      "    if max_tokens is None:\n",
      "        max_tokens = self.max_output_tokens\n",
      "    if \"gpt\" in self.model:\n",
      "        logging.info(prompt)\n",
      "        response, status = chatgpt_response(prompt=prompt,model=self.model, max_tokens = 1000, stream=stream)\n",
      "        if status:\n",
      "            return response, True\n",
      "        else:\n",
      "            self.failed_responses.append(response)\n",
      "            return response, False\n",
      "\n",
      "    elif \"command\" in self.model:\n",
      "        response, status = cohere_response(prompt=prompt,model=self.model, max_tokens = 1000)  \n",
      "        if status:\n",
      "            return response, True\n",
      "        else:\n",
      "            self.failed_responses.append(response)\n",
      "            return response, False\n",
      "    else:\n",
      "        return {}, False \n",
      "\n",
      "Function call: chatgpt_response\n",
      "Function call: cohere_response\n",
      "Related codes: ['\\ndef chatgpt_response(\\n     \\n    prompt: List[Dict[str, Union[str, Dict[str, str]]]], \\n    model: str = \"gpt-3.5-turbo\", \\n    max_tokens: int = 1000, \\n    stream: bool = False\\n) -> Union[Generator,Tuple]:\\n\\n    try:\\n        print(\"Trying to call OpenAI API...\")\\n        response = openai.ChatCompletion.create(\\n            model=model,\\n            messages=prompt,\\n            max_tokens=max_tokens,\\n            stream=stream\\n        )\\n        return response, True\\n\\n    except openai.APIError as e:\\n        logging.error(f\"Unexpected error in openai call: {e}\") \\n        fail_response = {\\n            \"choices\": [\\n                {\\n                    \"message\": {\\n                        \"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"\\n                    }\\n                }\\n            ]\\n        }\\n        return fail_response, False\\n', '\\n\\n\\ndef cohere_response(prompt: List[dict],model: str = \"command\", max_tokens: int = 1000\\n    ) -> Tuple[Dict, bool]:\\n    \"\"\"\\n        Call the Cohere API with the given prompt and maximum number of output tokens.\\n        \\n\\n        :param prompt: A list of strings representing the prompt to send to the API.\\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\\n        :param model: A string representing the model to use. either command or command-nightly\\n        :return: A tuple containing the API response cohere object and a boolean indicating success.\\n        \"\"\"             \\n    try:\\n        prompt= convert_mark_to_str_prompt(prompt)\\n        print(\"Trying to call Cohere API... using model:\", model)\\n        response = co.generate(\\n        model= model,\\n        prompt= prompt,\\n        max_tokens=max_tokens,\\n        #end_sequences=[\\'#SYSTEM:\\', \\'#USER:\\'],\\n        )\\n        return response, True\\n\\n    except:\\n          return None, False\\n    \\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def reply(self, message: str, verbose: bool = True, stream: bool = False) -> Union[Generator, str]:\n",
      "    \"\"\"\n",
      "        Reply to a given message using the chatbot.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "    if stream:\n",
      "        return self.query(message, verbose, stream)\n",
      "    else:\n",
      "        return self.query(message, verbose)[\"content\"]\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def query(self, message: str, verbose: bool = True, stream: bool = False) -> Union[Generator, str]:\n",
      "    \"\"\"\n",
      "        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\n",
      "\n",
      "        :param message: A string representing the user message.\n",
      "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
      "        :return: A string representing the chatbot's response.\n",
      "        \"\"\"\n",
      "\n",
      "    prompt, _ = self.prompt_func(message)\n",
      "\n",
      "    if stream:\n",
      "        return self.chat_response(prompt=prompt, stream=stream)\n",
      "\n",
      "    response, success = self.chat_response(prompt)\n",
      "    if verbose:\n",
      "        display(Markdown(\"#### Question: \\n {question}\".format(question=message)))\n",
      "    if success:\n",
      "        answer = get_mark_from_response(response, self.model)\n",
      "        self.outputs.append(answer)\n",
      "        self.inputs.append(message)\n",
      "        self.prompts.append(prompt)\n",
      "        if verbose:\n",
      "            display(\n",
      "                Markdown(\n",
      "                    \" #### Anwser: \\n {answer}\".format(\n",
      "                        answer=get_str_from_response(response, self.model)\n",
      "                    )\n",
      "                )\n",
      "            )\n",
      "        return answer\n",
      "    else:\n",
      "        raise Exception(\"OpenAI API Error inside query function\")\n",
      "\n",
      "Function call: display\n",
      "Function call: Markdown\n",
      "Function call: get_mark_from_response\n",
      "Function call: display\n",
      "Function call: Markdown\n",
      "Function call: get_str_from_response\n",
      "Function call: Exception\n",
      "Related codes: ['\\n\\ndef get_mark_from_response(response, model = \"gpt\"):\\n    # return the answer from the response\\n    if \"gpt\" in model:\\n        role = response[\"choices\"][0][\"message\"][\"role\"]\\n        message = response[\"choices\"][0][\"message\"][\"content\"]\\n    elif \"command\" in model:\\n        role = \"assistant\"\\n        message = response[0].text \\n    else:\\n        raise Exception(\"Unknown model type\")\\n    return {\"role\": role, \"content\": message}\\n', '\\n\\ndef get_str_from_response(response, model = \"gpt\"):\\n    # return the answer from the response\\n    if \"gpt\" in model:\\n        return response[\"choices\"][0][\"message\"][\"content\"]\\n    elif \"command\" in model:\\n        text = response[0].text\\n        text_without_assistant = text.replace(\"#ASSISTANT\", \"\")\\n        return text_without_assistant\\n    else:\\n        raise Exception(\"Unknown model type\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def reset_logs(self):\n",
      "    \"\"\"\n",
      "        Reset the chatbot's memory.\n",
      "        \"\"\"\n",
      "    self.outputs = []\n",
      "    self.inputs = []\n",
      "    self.prompts = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_logs(self):\n",
      "    \"\"\"\n",
      "        Get the chatbot's memory.\n",
      "\n",
      "        :return: A tuple containing the chatbot's memory as three lists of strings.\n",
      "        \"\"\"\n",
      "    return self.inputs, self.outputs, self.prompts\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def run_text(\n",
      "    self, text: str, state: List[Tuple[str, str]]\n",
      ") -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\n",
      "    \"\"\"\n",
      "        Process the user's text input and update the chat state.\n",
      "\n",
      "        :param text: A string representing the user input.\n",
      "        :param state: A list of tuples representing the current chat state.\n",
      "        :return: A tuple containing the updated chat state as two lists of tuples.\n",
      "        \"\"\"\n",
      "    print(\"===============Running run_text =============\")\n",
      "    print(\"Inputs:\", text)\n",
      "    try:\n",
      "        print(\"======>Current memory:\\n %s\" % self.memory_thread)\n",
      "    except:\n",
      "        print(\"======>No memory\")\n",
      "    print(\"failed here\")\n",
      "    response = self.reply(text)\n",
      "    state = state + [(text, response)]\n",
      "    print(\"Outputs:\", state)\n",
      "    return state, state\n",
      "\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def gradio(self):\n",
      "    \"\"\"\n",
      "        Create and launch a Gradio interface for the chatbot.\n",
      "        \"\"\"\n",
      "    with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n",
      "        chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\n",
      "        state = gr.State([])\n",
      "        with gr.Row():\n",
      "            with gr.Column(scale=1):\n",
      "                txt = gr.Textbox(\n",
      "                    show_label=False,\n",
      "                    placeholder=\"Enter text and press enter, or upload an image\",\n",
      "                ).style(container=False)\n",
      "            with gr.Column(scale=0.15, min_width=0):\n",
      "                clear = gr.Button(\"Clear️\")\n",
      "\n",
      "        txt.submit(lambda text, state: self.run_text(text, state), [txt, state], [chatbot, state])\n",
      "        txt.submit(lambda: \"\", None, txt)\n",
      "        demo.launch(server_name=\"localhost\", server_port=7860)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def list_subjects_and_perspective():\n",
      "    # read from the file\n",
      "    prompts = json.load(open(filename))\n",
      "    subject_and_perspective = list(prompts.keys())\n",
      "    subjects = set()\n",
      "    perspectives = set()\n",
      "    for item in subject_and_perspective:\n",
      "        subject, perspective = item.split('\\\\')\n",
      "        subjects.add(subject)\n",
      "        perspectives.add(perspective)\n",
      "    return subjects, perspectives\n",
      "\n",
      "Function call: open\n",
      "Function call: list\n",
      "Function call: set\n",
      "Function call: set\n",
      "Related codes: [\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def list_subjects():\n",
      "    # read from the file\n",
      "    prompts = json.load(open(filename))\n",
      "    subject_and_perspective = list(prompts.keys())\n",
      "    subjects = set()\n",
      "    for item in subject_and_perspective:\n",
      "        subject, perspective = item.split('\\\\')\n",
      "        subjects.add(subject)\n",
      "    return subjects\n",
      "\n",
      "Function call: open\n",
      "Function call: list\n",
      "Function call: set\n",
      "Related codes: [\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def list_perspectives():\n",
      "    # read from the file\n",
      "    prompts = json.load(open(filename))\n",
      "    subject_and_perspective = list(prompts.keys())\n",
      "    perspectives = set()\n",
      "    for item in subject_and_perspective:\n",
      "        subject, perspective = item.split('\\\\')\n",
      "        perspectives.add(perspective)\n",
      "    return perspectives\n",
      "\n",
      "Function call: open\n",
      "Function call: list\n",
      "Function call: set\n",
      "Related codes: [\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_perspective_prompt(subject, perspective):\n",
      "    # read from the file\n",
      "    prompts = json.load(open(filename))\n",
      "    key = subject + '\\\\' + perspective\n",
      "    if key in prompts:\n",
      "        return prompts[key]\n",
      "    else:\n",
      "        raise Exception('No prompt found for subject: ' + subject + ' and perspective: ' + perspective +' use list_subjects() and list_perspectives() to see available prompts')\n",
      "\n",
      "Function call: open\n",
      "Function call: Exception\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_random_perspective_prompt():\n",
      "    # read from the file\n",
      "    prompts = json.load(open(filename))\n",
      "    key = random.choice(list(prompts.keys()))\n",
      "    subject = key.split('\\\\')[0]\n",
      "    perspective = key.split('\\\\')[1]\n",
      "    return subject, perspective, prompts[key]\n",
      "\n",
      "Function call: open\n",
      "Function call: list\n",
      "Related codes: [\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\"]\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def extract_values_and_embeddings(\n",
      "        data_frame: pd.DataFrame,\n",
      "        columns: Union[str, List[str]],\n",
      "        embeddings_col: Optional[str],\n",
      "    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\n",
      "    \"\"\"\n",
      "        Extract values and embeddings from a pandas DataFrame.\n",
      "\n",
      "        Args:\n",
      "            data_frame: The DataFrame to extract values and embeddings from.\n",
      "            columns: The columns of the DataFrame to use as values.\n",
      "            embeddings_col: The column name containing the embeddings.\n",
      "\n",
      "        Returns:\n",
      "            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n",
      "        \"\"\"\n",
      "\n",
      "    if isinstance(columns, list) and len(columns) > 1:\n",
      "        data_frame[\"values_combined\"] = data_frame[columns].apply(\n",
      "            lambda x: \" \".join(x), axis=1\n",
      "        )\n",
      "        columns = \"values_combined\"\n",
      "    elif isinstance(columns, list) and len(columns) == 1:\n",
      "        columns = columns[0]\n",
      "    elif not isinstance(columns, str):\n",
      "        raise ValueError(\"The columns are not valid\")\n",
      "\n",
      "    values = []\n",
      "    embeddings = []\n",
      "\n",
      "    for _, row in data_frame.iterrows():\n",
      "        value = row[columns]\n",
      "        values.append(value)\n",
      "\n",
      "        if embeddings_col is not None:\n",
      "            embedding = row[embeddings_col]\n",
      "            embeddings.append(embedding)\n",
      "\n",
      "    return values, embeddings if embeddings_col is not None else None\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def convert_mark_to_str_prompt(messages: List[dict], prompt: str = \"\") -> str:\n",
      "    prompt = \"\"\n",
      "    for message in messages:\n",
      "        role = message[\"role\"].upper()\n",
      "        content = message[\"content\"]\n",
      "        prompt += f\" #{role}: {content}\"\n",
      "\n",
      "    return prompt\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def mark_system(system_prompt):\n",
      "    return {\"role\": \"system\", \"content\": system_prompt}\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def mark_answer(answer):\n",
      "    return {\"role\": \"assistant\", \"content\": answer}\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def mark_question(question):\n",
      "    return {\"role\": \"user\", \"content\": question}\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def check_dict(message_dict):\n",
      "    if (\n",
      "        type(message_dict) is list\n",
      "        and len(message_dict) == 1\n",
      "        and type(message_dict[0]) is dict\n",
      "    ):\n",
      "        message_dict = message_dict[0]\n",
      "    elif type(message_dict) is not dict:\n",
      "        raise Exception(\n",
      "            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\n",
      "            message_dict,\n",
      "            type(message_dict),\n",
      "        )\n",
      "    return message_dict\n",
      "\n",
      "Function call: type\n",
      "Function call: len\n",
      "Function call: type\n",
      "Function call: type\n",
      "Function call: Exception\n",
      "Function call: type\n",
      "Related codes: ['\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def get_mark_from_response(response, model = \"gpt\"):\n",
      "    # return the answer from the response\n",
      "    if \"gpt\" in model:\n",
      "        role = response[\"choices\"][0][\"message\"][\"role\"]\n",
      "        message = response[\"choices\"][0][\"message\"][\"content\"]\n",
      "    elif \"command\" in model:\n",
      "        role = \"assistant\"\n",
      "        message = response[0].text \n",
      "    else:\n",
      "        raise Exception(\"Unknown model type\")\n",
      "    return {\"role\": role, \"content\": message}\n",
      "\n",
      "Function call: Exception\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def get_str_from_response(response, model = \"gpt\"):\n",
      "    # return the answer from the response\n",
      "    if \"gpt\" in model:\n",
      "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
      "    elif \"command\" in model:\n",
      "        text = response[0].text\n",
      "        text_without_assistant = text.replace(\"#ASSISTANT\", \"\")\n",
      "        return text_without_assistant\n",
      "    else:\n",
      "        raise Exception(\"Unknown model type\")\n",
      "\n",
      "Function call: Exception\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def apply_sigmoid(matrix:np.ndarray):\n",
      "    \"\"\"\n",
      "    This function applies a sigmoid non-linearity to the matrix elements.\n",
      "    The sigmoid function maps any value to a value between 0 and 1.\n",
      "    \"\"\"\n",
      "    return expit(matrix)\n",
      "\n",
      "Function call: expit\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def apply_threshold(matrix, threshold=0.5):\n",
      "    \"\"\"\n",
      "    This function applies a threshold to the matrix elements.\n",
      "    All values above the threshold are set to 1, all values below or equal to the threshold are set to 0.\n",
      "    \"\"\"\n",
      "    matrix[matrix > threshold] = 1\n",
      "    matrix[matrix <= threshold] = 0\n",
      "    return matrix\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def concat_columns(example, index=None):\n",
      "    column1='title'\n",
      "    column2='text'\n",
      "    example['merged_column'] = example[column1] +\" - \" + example[column2]\n",
      "    return example\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def extract_values_and_embeddings_hf(\n",
      "    dataset: datasets.Dataset,\n",
      "    value_column: Union[str, List[str]],\n",
      "    embeddings_column: Optional[str],\n",
      ") -> Tuple[List[str], Optional[List[np.ndarray]]]:\n",
      "    \"\"\"\n",
      "    Extract values and embeddings from a Hugging Face dataset.\n",
      "\n",
      "    Args:\n",
      "        dataset: The Hugging Face dataset to extract values and embeddings from.\n",
      "        value_column: The column(s) of the dataset to use as values.\n",
      "        embeddings_column: The column name containing the embeddings.\n",
      "\n",
      "    Returns:\n",
      "        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n",
      "    \"\"\"\n",
      "    if isinstance(value_column, str):\n",
      "        value_column = [value_column]\n",
      "    print(\"Merging values: Start\")\n",
      "    merged_docs = dataset.map(concat_columns, with_indices=True)\n",
      "    print(\"Merging values: Done\")\n",
      "    values = merged_docs['merged_column']\n",
      "    embeddings = dataset[embeddings_column]\n",
      "    return values, embeddings if embeddings_column is not None else None\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def extract_values_hf(dataset: datasets.Dataset, value_column: Union[str, List[str]]) -> List[str]:\n",
      "    \"\"\"\n",
      "    Extract values from a Hugging Face dataset.\n",
      "\n",
      "    Args:\n",
      "        dataset: The Hugging Face dataset to extract values from.\n",
      "        value_column: The column(s) of the dataset to use as values.\n",
      "\n",
      "    Returns:\n",
      "        A list with the extracted values.\n",
      "    \"\"\"\n",
      "    if isinstance(value_column, str):\n",
      "        value_column = [value_column]\n",
      "    if len(value_column) == 1:\n",
      "        return dataset[value_column[0]]\n",
      "    elif len(value_column) > 1:\n",
      "        merged_docs = dataset.map(concat_columns)\n",
      "        return merged_docs\n",
      "    else:\n",
      "        raise ValueError(\"No value column specified.\")\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def extract_values_and_embeddings_python(\n",
      "    directory_path: str,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "    resolution: str = \"function\"\n",
      ") -> Tuple[List[str], List[Dict[str, str]]]:\n",
      "    values = []\n",
      "    context = []\n",
      "\n",
      "    parser = PythonParser(\n",
      "        directory_path=directory_path,\n",
      "        minify_code=minify_code,\n",
      "        remove_docstrings=remove_docstrings\n",
      "    )\n",
      "\n",
      "    results_dict = parser.process_directory()\n",
      "\n",
      "    if resolution == \"function\":\n",
      "        source_codes = results_dict['function_source_codes']\n",
      "        nodes = results_dict['function_nodes']\n",
      "    elif resolution == \"class\":\n",
      "        source_codes = results_dict['class_source_codes']\n",
      "        nodes = results_dict['class_nodes']\n",
      "    elif resolution == \"both\":\n",
      "        source_codes = results_dict['full_source']\n",
      "        nodes = results_dict['full_nodes']\n",
      "    else:\n",
      "        raise ValueError(f\"Invalid resolution: {resolution}\")\n",
      "    if resolution in ['function', 'class']:\n",
      "        for source_code, node in zip(source_codes, nodes):\n",
      "            values.append(source_code)\n",
      "            context.append({\"libcst tree\": str(node)})\n",
      "    elif resolution == \"both\":\n",
      "        for source_code, node, filename in zip(source_codes, nodes, results_dict['file_map']):\n",
      "            values.append(source_code)\n",
      "            context.append({\"libcst tree\": str(node), \"filename\": filename})\n",
      "    else:\n",
      "        raise ValueError(f\"Invalid resolution: {resolution}\")\n",
      "    return values, context\n",
      "\n",
      "Function call: PythonParser\n",
      "Function call: ValueError\n",
      "Function call: zip\n",
      "Function call: str\n",
      "Function call: zip\n",
      "Function call: str\n",
      "Function call: ValueError\n",
      "Related codes: ['\\n\\nclass PythonParser(OsProcessor):\\n    def __init__(\\n        self,\\n        directory_path: str,\\n        visitor: Optional[FunctionAndClassVisitor] = None,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n    ):\\n        super().__init__(directory_path)\\n        self.visitor = visitor if visitor else FunctionAndClassVisitor()\\n        self.minify_code = minify_code\\n        self.remove_docstrings = remove_docstrings\\n\\n    def remove_docstring(self, tree: cst.Module) -> str:\\n        \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\\n\\n        # Remove docstrings using a transformer\\n        class DocstringRemover(cst.CSTTransformer):\\n            def leave_FunctionDef(\\n                self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\\n            ) -> cst.FunctionDef:\\n                docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n                if docstring.startswith(\"No docstring\"):\\n                    return updated_node\\n\\n                return updated_node.with_changes(\\n                    body=updated_node.body.with_changes(\\n                        body=[\\n                            stmt\\n                            for stmt in updated_node.body.body\\n                            if not (\\n                                isinstance(stmt, cst.SimpleStatementLine)\\n                                and any(\\n                                    isinstance(expr, cst.Expr)\\n                                    and isinstance(expr.value, cst.SimpleString)\\n                                    for expr in stmt.body\\n                                )\\n                            )\\n                        ]\\n                    )\\n                )\\n\\n        tree = tree.visit(DocstringRemover())\\n        return tree.code\\n\\n    def _process_file(self, file_path: str):\\n        \"\"\"This method is called for every file in the directory.\\n        It does the following:\\n        1. Reads the file\\n        2. Parses the file\\n        3. Visits the file with the visitor\\n        \"\"\"\\n        #get current number of nodes in visitor\\n        current_node_count = self.visitor.function_count + self.visitor.class_count\\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\\n            source_code = file.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n        except cst.ParserSyntaxError:\\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n            return\\n\\n        tree.visit(self.visitor)\\n        #calculate how many new nodes were added\\n        new_node_counter = self.visitor.function_count + self.visitor.class_count - current_node_count\\n        self.visitor.filename_map.extend([file_path]*new_node_counter)\\n        # Remove docstrings if specified\\n        if self.remove_docstrings:\\n            source_code = self.remove_docstring(source_code, tree)\\n\\n        # Minify the code if specified\\n        if self.minify_code:\\n            minifier = PythonMinifier(source_code)\\n            source_code = minifier.get_minified_code()\\n\\n        # Add the processed code to the corresponding list in the visitor\\n        self.visitor.function_source_codes.append(source_code)\\n\\n    def process_file(self, file_path: str):\\n        \"\"\"This method is called for every file in the directory.\\n        It does the following:\\n        1. Runs flake8 on the file\\n        if flake8 returns a non-zero exit code, it means the file has a syntax error\\n        2. Reads the file\\n        3. Parses the file\\n        4. Visits the file with the visitor\\n\\n        \"\"\"\\n        result = subprocess.run(\\n            [\"flake8\", \"--select=E999\", file_path], capture_output=True\\n        )\\n\\n        if result.returncode != 0:\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n            print(result.stderr.decode(\"utf-8\"))\\n            return\\n\\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\\n            source_code = f.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n            tree.visit(self.visitor)\\n        except cst.ParserSyntaxError as e:\\n            print(f\"Syntax error: {e}\")\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n\\n    def process_directory(\\n        self,\\n    ) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\\n        \"\"\"This method is called for every directory.\\n        It does the following:\\n        1. Gets all the python files in the directory\\n        2. Processes each file\\n        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\\n        \"\"\"\\n\\n        python_files = self.get_files_with_extension(\".py\")\\n\\n        for file_path in python_files:\\n            self._process_file(file_path)\\n\\n        result_dict = {\\n            \\'function_source_codes\\': self.visitor.function_source_codes,\\n            \\'function_nodes\\': self.visitor.function_nodes,\\n            \\'class_source_codes\\': self.visitor.class_source_codes,\\n            \\'class_nodes\\': self.visitor.class_nodes,\\n            \\'file_map\\': self.visitor.filename_map,\\n            \\'full_nodes\\': self.visitor.full_node_list,\\n            \\'full_source\\': self.visitor.full_source_list\\n        }\\n\\n\\n        return result_dict\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "async def process_api_requests_from_file(\n",
      "    requests_filepath: str,\n",
      "    save_filepath: str,\n",
      "    request_url: str,\n",
      "    api_key: str,\n",
      "    max_requests_per_minute: float,\n",
      "    max_tokens_per_minute: float,\n",
      "    token_encoding_name: str,\n",
      "    max_attempts: int,\n",
      "    logging_level: int,\n",
      "):\n",
      "    \"\"\"Processes API requests in parallel, throttling to stay under rate limits.\"\"\"\n",
      "    # constants\n",
      "    seconds_to_pause_after_rate_limit_error = 15\n",
      "    seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n",
      "\n",
      "    # initialize logging\n",
      "    logging.basicConfig(level=logging_level)\n",
      "    logging.debug(f\"Logging initialized at level {logging_level}\")\n",
      "\n",
      "    # infer API endpoint and construct request header\n",
      "    api_endpoint = api_endpoint_from_url(request_url)\n",
      "    request_header = {\"Authorization\": f\"Bearer {api_key}\"}\n",
      "\n",
      "    # initialize trackers\n",
      "    queue_of_requests_to_retry = asyncio.Queue()\n",
      "    task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...\n",
      "    status_tracker = StatusTracker()  # single instance to track a collection of variables\n",
      "    next_request = None  # variable to hold the next request to call\n",
      "\n",
      "    # initialize available capacity counts\n",
      "    available_request_capacity = max_requests_per_minute\n",
      "    available_token_capacity = max_tokens_per_minute\n",
      "    last_update_time = time.time()\n",
      "\n",
      "    # initialize flags\n",
      "    file_not_finished = True  # after file is empty, we'll skip reading it\n",
      "    logging.debug(f\"Initialization complete.\")\n",
      "\n",
      "    # initialize file reading\n",
      "    with open(requests_filepath) as file:\n",
      "        # `requests` will provide requests one at a time\n",
      "        requests = file.__iter__()\n",
      "        logging.debug(f\"File opened. Entering main loop\")\n",
      "\n",
      "        while True:\n",
      "            # get next request (if one is not already waiting for capacity)\n",
      "            if next_request is None:\n",
      "                if not queue_of_requests_to_retry.empty():\n",
      "                    next_request = queue_of_requests_to_retry.get_nowait()\n",
      "                    logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")\n",
      "                elif file_not_finished:\n",
      "                    try:\n",
      "                        # get new request\n",
      "                        request_json = json.loads(next(requests))\n",
      "                        next_request = APIRequest(\n",
      "                            task_id=next(task_id_generator),\n",
      "                            request_json=request_json,\n",
      "                            token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),\n",
      "                            attempts_left=max_attempts,\n",
      "                            metadata=request_json.pop(\"metadata\", None)\n",
      "                        )\n",
      "                        status_tracker.num_tasks_started += 1\n",
      "                        status_tracker.num_tasks_in_progress += 1\n",
      "                        logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")\n",
      "                    except StopIteration:\n",
      "                        # if file runs out, set flag to stop reading it\n",
      "                        logging.debug(\"Read file exhausted\")\n",
      "                        file_not_finished = False\n",
      "\n",
      "            # update available capacity\n",
      "            current_time = time.time()\n",
      "            seconds_since_update = current_time - last_update_time\n",
      "            available_request_capacity = min(\n",
      "                available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,\n",
      "                max_requests_per_minute,\n",
      "            )\n",
      "            available_token_capacity = min(\n",
      "                available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,\n",
      "                max_tokens_per_minute,\n",
      "            )\n",
      "            last_update_time = current_time\n",
      "\n",
      "            # if enough capacity available, call API\n",
      "            if next_request:\n",
      "                next_request_tokens = next_request.token_consumption\n",
      "                if (\n",
      "                    available_request_capacity >= 1\n",
      "                    and available_token_capacity >= next_request_tokens\n",
      "                ):\n",
      "                    # update counters\n",
      "                    available_request_capacity -= 1\n",
      "                    available_token_capacity -= next_request_tokens\n",
      "                    next_request.attempts_left -= 1\n",
      "\n",
      "                    # call API\n",
      "                    asyncio.create_task(\n",
      "                        next_request.call_api(\n",
      "                            request_url=request_url,\n",
      "                            request_header=request_header,\n",
      "                            retry_queue=queue_of_requests_to_retry,\n",
      "                            save_filepath=save_filepath,\n",
      "                            status_tracker=status_tracker,\n",
      "                        )\n",
      "                    )\n",
      "                    next_request = None  # reset next_request to empty\n",
      "\n",
      "            # if all tasks are finished, break\n",
      "            if status_tracker.num_tasks_in_progress == 0:\n",
      "                break\n",
      "\n",
      "            # main loop sleeps briefly so concurrent tasks can run\n",
      "            await asyncio.sleep(seconds_to_sleep_each_loop)\n",
      "\n",
      "            # if a rate limit error was hit recently, pause to cool down\n",
      "            seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)\n",
      "            if seconds_since_rate_limit_error < seconds_to_pause_after_rate_limit_error:\n",
      "                remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n",
      "                await asyncio.sleep(remaining_seconds_to_pause)\n",
      "                # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n",
      "                logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")\n",
      "\n",
      "        # after finishing, log final status\n",
      "        logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")\n",
      "        if status_tracker.num_tasks_failed > 0:\n",
      "            logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")\n",
      "        if status_tracker.num_rate_limit_errors > 0:\n",
      "            logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n",
      "\n",
      "Function call: api_endpoint_from_url\n",
      "Function call: task_id_generator_function\n",
      "Function call: StatusTracker\n",
      "Function call: open\n",
      "Function call: next\n",
      "Function call: APIRequest\n",
      "Function call: next\n",
      "Function call: num_tokens_consumed_from_request\n",
      "Function call: min\n",
      "Function call: min\n",
      "Related codes: ['\\n\\n# functions\\n\\n\\ndef api_endpoint_from_url(request_url):\\n    \"\"\"Extract the API endpoint from the request URL.\"\"\"\\n    match = re.search(\\'^https://[^/]+/v\\\\\\\\d+/(.+)$\\', request_url)\\n    return match[1]\\n', '\\n\\ndef task_id_generator_function():\\n    \"\"\"Generate integers 0, 1, 2, and so on.\"\"\"\\n    task_id = 0\\n    while True:\\n        yield task_id\\n        task_id += 1\\n', '\\n\\n# dataclasses\\n\\n\\n@dataclass\\nclass StatusTracker:\\n    \"\"\"Stores metadata about the script\\'s progress. Only one instance is created.\"\"\"\\n\\n    num_tasks_started: int = 0\\n    num_tasks_in_progress: int = 0  # script ends when this reaches 0\\n    num_tasks_succeeded: int = 0\\n    num_tasks_failed: int = 0\\n    num_rate_limit_errors: int = 0\\n    num_api_errors: int = 0  # excluding rate limit errors, counted above\\n    num_other_errors: int = 0\\n    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\\n', '\\n\\n@dataclass\\nclass APIRequest:\\n    \"\"\"Stores an API request\\'s inputs, outputs, and other metadata. Contains a method to make an API call.\"\"\"\\n\\n    task_id: int\\n    request_json: dict\\n    token_consumption: int\\n    attempts_left: int\\n    metadata: dict\\n    result: list = field(default_factory=list)\\n\\n    async def call_api(\\n        self,\\n        request_url: str,\\n        request_header: dict,\\n        retry_queue: asyncio.Queue,\\n        save_filepath: str,\\n        status_tracker: StatusTracker,\\n    ):\\n        \"\"\"Calls the OpenAI API and saves results.\"\"\"\\n        logging.info(f\"Starting request #{self.task_id}\")\\n        error = None\\n        try:\\n            async with aiohttp.ClientSession() as session:\\n                async with session.post(\\n                    url=request_url, headers=request_header, json=self.request_json\\n                ) as response:\\n                    response = await response.json()\\n            if \"error\" in response:\\n                logging.warning(\\n                    f\"Request {self.task_id} failed with error {response[\\'error\\']}\"\\n                )\\n                status_tracker.num_api_errors += 1\\n                error = response\\n                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\\n                    status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \\n                    status_tracker.num_rate_limit_errors += 1\\n                    status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\\n\\n        except Exception as e:  # catching naked exceptions is bad practice, but in this case we\\'ll log & save them\\n            logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\\n            status_tracker.num_other_errors += 1\\n            error = e\\n        if error:\\n            self.result.append(error)\\n            if self.attempts_left:\\n                retry_queue.put_nowait(self)\\n            else:\\n                logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\\n                data = (\\n                    [self.request_json, [str(e) for e in self.result], self.metadata]\\n                    if self.metadata\\n                    else [self.request_json, [str(e) for e in self.result]]\\n                )\\n                append_to_jsonl(data, save_filepath)\\n                status_tracker.num_tasks_in_progress -= 1\\n                status_tracker.num_tasks_failed += 1\\n        else:\\n            data = (\\n                [self.request_json, response, self.metadata]\\n                if self.metadata\\n                else [self.request_json, response]\\n            )\\n            append_to_jsonl(data, save_filepath)\\n            status_tracker.num_tasks_in_progress -= 1\\n            status_tracker.num_tasks_succeeded += 1\\n            logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\\n', '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# dataclasses\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class StatusTracker:\n",
      "    \"\"\"Stores metadata about the script's progress. Only one instance is created.\"\"\"\n",
      "\n",
      "    num_tasks_started: int = 0\n",
      "    num_tasks_in_progress: int = 0  # script ends when this reaches 0\n",
      "    num_tasks_succeeded: int = 0\n",
      "    num_tasks_failed: int = 0\n",
      "    num_rate_limit_errors: int = 0\n",
      "    num_api_errors: int = 0  # excluding rate limit errors, counted above\n",
      "    num_other_errors: int = 0\n",
      "    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class APIRequest:\n",
      "    \"\"\"Stores an API request's inputs, outputs, and other metadata. Contains a method to make an API call.\"\"\"\n",
      "\n",
      "    task_id: int\n",
      "    request_json: dict\n",
      "    token_consumption: int\n",
      "    attempts_left: int\n",
      "    metadata: dict\n",
      "    result: list = field(default_factory=list)\n",
      "\n",
      "    async def call_api(\n",
      "        self,\n",
      "        request_url: str,\n",
      "        request_header: dict,\n",
      "        retry_queue: asyncio.Queue,\n",
      "        save_filepath: str,\n",
      "        status_tracker: StatusTracker,\n",
      "    ):\n",
      "        \"\"\"Calls the OpenAI API and saves results.\"\"\"\n",
      "        logging.info(f\"Starting request #{self.task_id}\")\n",
      "        error = None\n",
      "        try:\n",
      "            async with aiohttp.ClientSession() as session:\n",
      "                async with session.post(\n",
      "                    url=request_url, headers=request_header, json=self.request_json\n",
      "                ) as response:\n",
      "                    response = await response.json()\n",
      "            if \"error\" in response:\n",
      "                logging.warning(\n",
      "                    f\"Request {self.task_id} failed with error {response['error']}\"\n",
      "                )\n",
      "                status_tracker.num_api_errors += 1\n",
      "                error = response\n",
      "                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                    status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \n",
      "                    status_tracker.num_rate_limit_errors += 1\n",
      "                    status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n",
      "\n",
      "        except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "            logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n",
      "            status_tracker.num_other_errors += 1\n",
      "            error = e\n",
      "        if error:\n",
      "            self.result.append(error)\n",
      "            if self.attempts_left:\n",
      "                retry_queue.put_nowait(self)\n",
      "            else:\n",
      "                logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n",
      "                data = (\n",
      "                    [self.request_json, [str(e) for e in self.result], self.metadata]\n",
      "                    if self.metadata\n",
      "                    else [self.request_json, [str(e) for e in self.result]]\n",
      "                )\n",
      "                append_to_jsonl(data, save_filepath)\n",
      "                status_tracker.num_tasks_in_progress -= 1\n",
      "                status_tracker.num_tasks_failed += 1\n",
      "        else:\n",
      "            data = (\n",
      "                [self.request_json, response, self.metadata]\n",
      "                if self.metadata\n",
      "                else [self.request_json, response]\n",
      "            )\n",
      "            append_to_jsonl(data, save_filepath)\n",
      "            status_tracker.num_tasks_in_progress -= 1\n",
      "            status_tracker.num_tasks_succeeded += 1\n",
      "            logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: field\n",
      "Function call: str\n",
      "Function call: str\n",
      "Function call: append_to_jsonl\n",
      "Function call: append_to_jsonl\n",
      "Related codes: ['\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\n\\ndef append_to_jsonl(data, filename: str) -> None:\\n    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\\n    json_string = json.dumps(data)\\n    with open(filename, \"a\") as f:\\n        f.write(json_string + \"\\\\n\")\\n', '\\n\\ndef append_to_jsonl(data, filename: str) -> None:\\n    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\\n    json_string = json.dumps(data)\\n    with open(filename, \"a\") as f:\\n        f.write(json_string + \"\\\\n\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "async def call_api(\n",
      "    self,\n",
      "    request_url: str,\n",
      "    request_header: dict,\n",
      "    retry_queue: asyncio.Queue,\n",
      "    save_filepath: str,\n",
      "    status_tracker: StatusTracker,\n",
      "):\n",
      "    \"\"\"Calls the OpenAI API and saves results.\"\"\"\n",
      "    logging.info(f\"Starting request #{self.task_id}\")\n",
      "    error = None\n",
      "    try:\n",
      "        async with aiohttp.ClientSession() as session:\n",
      "            async with session.post(\n",
      "                url=request_url, headers=request_header, json=self.request_json\n",
      "            ) as response:\n",
      "                response = await response.json()\n",
      "        if \"error\" in response:\n",
      "            logging.warning(\n",
      "                f\"Request {self.task_id} failed with error {response['error']}\"\n",
      "            )\n",
      "            status_tracker.num_api_errors += 1\n",
      "            error = response\n",
      "            if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \n",
      "                status_tracker.num_rate_limit_errors += 1\n",
      "                status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n",
      "\n",
      "    except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "        logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n",
      "        status_tracker.num_other_errors += 1\n",
      "        error = e\n",
      "    if error:\n",
      "        self.result.append(error)\n",
      "        if self.attempts_left:\n",
      "            retry_queue.put_nowait(self)\n",
      "        else:\n",
      "            logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n",
      "            data = (\n",
      "                [self.request_json, [str(e) for e in self.result], self.metadata]\n",
      "                if self.metadata\n",
      "                else [self.request_json, [str(e) for e in self.result]]\n",
      "            )\n",
      "            append_to_jsonl(data, save_filepath)\n",
      "            status_tracker.num_tasks_in_progress -= 1\n",
      "            status_tracker.num_tasks_failed += 1\n",
      "    else:\n",
      "        data = (\n",
      "            [self.request_json, response, self.metadata]\n",
      "            if self.metadata\n",
      "            else [self.request_json, response]\n",
      "        )\n",
      "        append_to_jsonl(data, save_filepath)\n",
      "        status_tracker.num_tasks_in_progress -= 1\n",
      "        status_tracker.num_tasks_succeeded += 1\n",
      "        logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n",
      "\n",
      "Function call: str\n",
      "Function call: str\n",
      "Function call: append_to_jsonl\n",
      "Function call: append_to_jsonl\n",
      "Related codes: ['\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\n\\ndef append_to_jsonl(data, filename: str) -> None:\\n    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\\n    json_string = json.dumps(data)\\n    with open(filename, \"a\") as f:\\n        f.write(json_string + \"\\\\n\")\\n', '\\n\\ndef append_to_jsonl(data, filename: str) -> None:\\n    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\\n    json_string = json.dumps(data)\\n    with open(filename, \"a\") as f:\\n        f.write(json_string + \"\\\\n\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# functions\n",
      "\n",
      "\n",
      "def api_endpoint_from_url(request_url):\n",
      "    \"\"\"Extract the API endpoint from the request URL.\"\"\"\n",
      "    match = re.search('^https://[^/]+/v\\\\d+/(.+)$', request_url)\n",
      "    return match[1]\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def append_to_jsonl(data, filename: str) -> None:\n",
      "    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\n",
      "    json_string = json.dumps(data)\n",
      "    with open(filename, \"a\") as f:\n",
      "        f.write(json_string + \"\\n\")\n",
      "\n",
      "Function call: open\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def num_tokens_consumed_from_request(\n",
      "    request_json: dict,\n",
      "    api_endpoint: str,\n",
      "    token_encoding_name: str,\n",
      "):\n",
      "    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\n",
      "    encoding = tiktoken.get_encoding(token_encoding_name)\n",
      "    # if completions request, tokens = prompt + n * max_tokens\n",
      "    if api_endpoint.endswith(\"completions\"):\n",
      "        max_tokens = request_json.get(\"max_tokens\", 15)\n",
      "        n = request_json.get(\"n\", 1)\n",
      "        completion_tokens = n * max_tokens\n",
      "\n",
      "        # chat completions\n",
      "        if api_endpoint.startswith(\"chat/\"):\n",
      "            num_tokens = 0\n",
      "            for message in request_json[\"messages\"]:\n",
      "                num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
      "                for key, value in message.items():\n",
      "                    num_tokens += len(encoding.encode(value))\n",
      "                    if key == \"name\":  # if there's a name, the role is omitted\n",
      "                        num_tokens -= 1  # role is always required and always 1 token\n",
      "            num_tokens += 2  # every reply is primed with <im_start>assistant\n",
      "            return num_tokens + completion_tokens\n",
      "        # normal completions\n",
      "        else:\n",
      "            prompt = request_json[\"prompt\"]\n",
      "            if isinstance(prompt, str):  # single prompt\n",
      "                prompt_tokens = len(encoding.encode(prompt))\n",
      "                num_tokens = prompt_tokens + completion_tokens\n",
      "                return num_tokens\n",
      "            elif isinstance(prompt, list):  # multiple prompts\n",
      "                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\n",
      "                num_tokens = prompt_tokens + completion_tokens * len(prompt)\n",
      "                return num_tokens\n",
      "            else:\n",
      "                raise TypeError('Expecting either string or list of strings for \"prompt\" field in completion request')\n",
      "    # if embeddings request, tokens = input tokens\n",
      "    elif api_endpoint == \"embeddings\":\n",
      "        input = request_json[\"input\"]\n",
      "        if isinstance(input, str):  # single input\n",
      "            num_tokens = len(encoding.encode(input))\n",
      "            return num_tokens\n",
      "        elif isinstance(input, list):  # multiple inputs\n",
      "            num_tokens = sum([len(encoding.encode(i)) for i in input])\n",
      "            return num_tokens\n",
      "        else:\n",
      "            raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n",
      "    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\n",
      "    else:\n",
      "        raise NotImplementedError(f'API endpoint \"{api_endpoint}\" not implemented in this script')\n",
      "\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: sum\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: TypeError\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: sum\n",
      "Function call: len\n",
      "Function call: TypeError\n",
      "Function call: NotImplementedError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def task_id_generator_function():\n",
      "    \"\"\"Generate integers 0, 1, 2, and so on.\"\"\"\n",
      "    task_id = 0\n",
      "    while True:\n",
      "        yield task_id\n",
      "        task_id += 1\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def extract_values_and_embeddings_pd(\n",
      "        data_frame: pd.DataFrame,\n",
      "        value_column: str,\n",
      "        embeddings_col: Optional[str],\n",
      "    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\n",
      "    \"\"\"\n",
      "        Extract values and embeddings from a pandas DataFrame.\n",
      "\n",
      "        Args:\n",
      "            data_frame: The DataFrame to extract values and embeddings from.\n",
      "            value_column: The column of the DataFrame to use as values.\n",
      "            embeddings_col: The column name containing the embeddings.\n",
      "\n",
      "        Returns:\n",
      "            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n",
      "        \"\"\"\n",
      "    values = data_frame[value_column].tolist()\n",
      "    embeddings = data_frame[embeddings_col].tolist() if embeddings_col else None\n",
      "\n",
      "    return values, embeddings\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def extract_values_and_embeddings_hf(\n",
      "    dataset: datasets.Dataset,\n",
      "    value_column: str,\n",
      "    embeddings_column: Optional[str],\n",
      ") -> Tuple[List[str], Optional[List[np.ndarray]]]:\n",
      "    \"\"\"\n",
      "    Extract values and embeddings from a Hugging Face dataset.\n",
      "\n",
      "    Args:\n",
      "        dataset: The Hugging Face dataset to extract values and embeddings from.\n",
      "        value_column: The column of the dataset to use as values.\n",
      "        embeddings_column: The column name containing the embeddings.\n",
      "\n",
      "    Returns:\n",
      "        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n",
      "    \"\"\"\n",
      "    values = dataset[value_column]\n",
      "    embeddings = dataset[embeddings_column] if embeddings_column else None\n",
      "\n",
      "    return values, embeddings\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def extract_values_and_embeddings_polars(\n",
      "        data_frame: pl.DataFrame,\n",
      "        value_column: str,\n",
      "        embeddings_column: Optional[str]\n",
      "    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\n",
      "    \"\"\"\n",
      "    Extract values and embeddings from a Polars DataFrame.\n",
      "\n",
      "    Args:\n",
      "        data_frame: The DataFrame to extract values and embeddings from.\n",
      "        value_column: The column of the DataFrame to use as values.\n",
      "        embeddings_column: The column name containing the embeddings.\n",
      "\n",
      "    Returns:\n",
      "        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n",
      "    \"\"\"\n",
      "    values = data_frame[value_column].to_list()\n",
      "    embeddings = data_frame[embeddings_column].to_list() if embeddings_column else None\n",
      "\n",
      "    return values, embeddings\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_context_from_pandas(\n",
      "          data_frame: pd.DataFrame,\n",
      "          context_columns: List[str]):\n",
      "    \"\"\"Extract context information from a pandas DataFrame.\"\"\"\n",
      "    # This function will convert the row of the DataFrame into a dictionary where the keys are the column names.\n",
      "    def row_to_dict(row: pd.Series) -> Dict[str, Any]:\n",
      "        return {column: row[column] for column in context_columns}\n",
      "\n",
      "    # Apply the function to each row of the DataFrame and convert the result into a list.\n",
      "    context = data_frame.apply(row_to_dict, axis=1).tolist()\n",
      "\n",
      "    return context\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "# This function will convert the row of the DataFrame into a dictionary where the keys are the column names.\n",
      "def row_to_dict(row: pd.Series) -> Dict[str, Any]:\n",
      "    return {column: row[column] for column in context_columns}\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_context_from_hf(\n",
      "          data_frame: datasets.Dataset,\n",
      "          context_columns: List[str]):\n",
      "    \"\"\"return a list dictionaries with the keys the column name and value the context columns values\"\"\"\n",
      "    context_data = {column: data_frame[column] for column in context_columns}\n",
      "    context = []\n",
      "    data_frame_len = len(data_frame)\n",
      "    for row in range(data_frame_len):\n",
      "         context.append({column: context_data[column][row] for column in context_columns})\n",
      "    return context\n",
      "\n",
      "Function call: len\n",
      "Function call: range\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_context_from_polars(\n",
      "          data_frame: pl.DataFrame,\n",
      "          context_columns: List[str]) -> List[Dict[str, Any]]:\n",
      "    \"\"\"Extract context information from a Polars DataFrame.\"\"\"\n",
      "\n",
      "    context = []\n",
      "    \n",
      "    # Convert each row to a dictionary\n",
      "    for i in range(len(data_frame)):\n",
      "        row_dict = data_frame[i]\n",
      "        context.append({column: row_dict[column] for column in context_columns})\n",
      "\n",
      "    return context\n",
      "\n",
      "Function call: range\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class EmbeddingAnalysis:\n",
      "    def __init__(self, embedding_matrix: np.ndarray, kernel_matrix: np.ndarray) -> None:\n",
      "        \"\"\"\n",
      "        Initializes an instance of the EmbeddingAnalysis class.\n",
      "\n",
      "        Parameters:\n",
      "        kernel_matrix (np.ndarray): A square kernel matrix.\n",
      "\n",
      "        Raises:\n",
      "        ValueError: If the input kernel matrix is not square or symmetric.\n",
      "        \"\"\"\n",
      "        if kernel_matrix.shape[0] != kernel_matrix.shape[1]:\n",
      "            raise ValueError(\"The input kernel matrix must be square.\")\n",
      "        if not np.allclose(kernel_matrix, kernel_matrix.T, atol=1e-8):\n",
      "            raise ValueError(\"The input kernel matrix must be symmetric.\")\n",
      "        self.embedding_matrix = embedding_matrix\n",
      "        self.kernel_matrix = kernel_matrix\n",
      "        self.eigenvalues = np.linalg.eigvalsh(kernel_matrix)\n",
      "\n",
      "    def mean_center(self, A):\n",
      "        # A.shape = (embedding dim, num of articles)\n",
      "        avg_doc = np.mean(A, axis=1)\n",
      "        # avg_doc.shape = (embedding dim, 1)\n",
      "        # Compute eigenfaces on mean-subtracted training data\n",
      "        X = A - np.tile(avg_doc,(A.shape[1],1)).T\n",
      "        # X.shape = A.shape\n",
      "        return X, avg_doc\n",
      "\n",
      "    def eigen_topic(self, A, r1 = 5, r2=55):\n",
      "        X, avg_doc = self.mean_center(A)\n",
      "        # SVD on Mean-centered articles\n",
      "        U, _, _ = np.linalg.svd(X)\n",
      "        # U.shape = (embedding dim, embedding dim)\n",
      "        # np.diag(S.shape) = (embedding dim, embedding dim)\n",
      "        # VT.shape = (num of articles, num of articles)\n",
      "        econ_UT = U[:, r1:r2].T\n",
      "        # econ_UT.shape = (r2-r1, emmbedding dim)\n",
      "        transformed_matrix = econ_UT @ X\n",
      "        # transformed_matrix.shape = (r2-r1, num of articles)\n",
      "        return transformed_matrix, avg_doc, econ_UT\n",
      "\n",
      "    def check_symmetry(self) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if the kernel matrix is symmetric.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if the kernel matrix is symmetric, False otherwise.\n",
      "        \"\"\"\n",
      "        return np.allclose(self.kernel_matrix, self.kernel_matrix.T, atol=1e-8)\n",
      "\n",
      "    def is_positive_definite(self) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if the kernel matrix is positive definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are positive, False otherwise.\n",
      "        \"\"\"\n",
      "        return np.all(self.eigenvalues > 0)\n",
      "\n",
      "    def is_positive_semi_definite(self) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if the kernel matrix is positive semi-definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are non-negative, False otherwise.\n",
      "        \"\"\"\n",
      "        return np.all(self.eigenvalues >= 0)\n",
      "\n",
      "    def is_negative_definite(self) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if the kernel matrix is negative definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are negative, False otherwise.\n",
      "        \"\"\"\n",
      "        return np.all(self.eigenvalues < 0)\n",
      "\n",
      "    def is_negative_semi_definite(self) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if the kernel matrix is negative semi-definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are non-positive, False otherwise.\n",
      "        \"\"\"\n",
      "        return np.all(self.eigenvalues <= 0)\n",
      "\n",
      "    def is_indefinite(self) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if the kernel matrix is indefinite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if the matrix has both positive and negative eigenvalues, False otherwise.\n",
      "        \"\"\"\n",
      "        has_negative = np.any(self.eigenvalues < 0)\n",
      "        has_non_negative = np.any(self.eigenvalues >= 0)\n",
      "        return has_negative and has_non_negative\n",
      "\n",
      "    def check_definiteness(self, num_random_vectors: int = 1000) -> Dict[str, bool]:\n",
      "        \"\"\"\n",
      "        Checks the definiteness of the kernel matrix using random vectors.\n",
      "\n",
      "        num_random_vectors: Number of random vectors to use for checking.\n",
      "\n",
      "        Returns:\n",
      "        Dict[str, bool]: A dictionary with the results of the analysis.\n",
      "        \"\"\"\n",
      "        n = self.kernel_matrix.shape[0]\n",
      "        is_positive_definite = True\n",
      "        is_negative_definite = True\n",
      "        for _ in range(num_random_vectors):\n",
      "            x = np.random.randn(n)\n",
      "            xTMx = x.T @ self.kernel_matrix @ x\n",
      "            if xTMx <= 0:\n",
      "                is_positive_definite = False\n",
      "            if xTMx >= 0:\n",
      "                is_negative_definite = False\n",
      "        return {\n",
      "            \"is_positive_definite\": is_positive_definite,\n",
      "            \"is_negative_definite\": is_negative_definite,\n",
      "        }\n",
      "\n",
      "    def run_analysis(self) -> Dict[str, bool]:\n",
      "        return {\n",
      "            \"is_symmetric\": self.check_symmetry(),\n",
      "            \"is_positive_semi_definite\": self.is_positive_semi_definite(),\n",
      "            \"is_negative_semi_definite\": self.is_negative_semi_definite(),\n",
      "            \"is_indefinite\": self.is_indefinite(),\n",
      "            **self.check_definiteness()\n",
      "        }\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: ValueError\n",
      "Function call: ValueError\n",
      "Function call: range\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, embedding_matrix: np.ndarray, kernel_matrix: np.ndarray) -> None:\n",
      "    \"\"\"\n",
      "        Initializes an instance of the EmbeddingAnalysis class.\n",
      "\n",
      "        Parameters:\n",
      "        kernel_matrix (np.ndarray): A square kernel matrix.\n",
      "\n",
      "        Raises:\n",
      "        ValueError: If the input kernel matrix is not square or symmetric.\n",
      "        \"\"\"\n",
      "    if kernel_matrix.shape[0] != kernel_matrix.shape[1]:\n",
      "        raise ValueError(\"The input kernel matrix must be square.\")\n",
      "    if not np.allclose(kernel_matrix, kernel_matrix.T, atol=1e-8):\n",
      "        raise ValueError(\"The input kernel matrix must be symmetric.\")\n",
      "    self.embedding_matrix = embedding_matrix\n",
      "    self.kernel_matrix = kernel_matrix\n",
      "    self.eigenvalues = np.linalg.eigvalsh(kernel_matrix)\n",
      "\n",
      "Function call: ValueError\n",
      "Function call: ValueError\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def mean_center(self, A):\n",
      "    # A.shape = (embedding dim, num of articles)\n",
      "    avg_doc = np.mean(A, axis=1)\n",
      "    # avg_doc.shape = (embedding dim, 1)\n",
      "    # Compute eigenfaces on mean-subtracted training data\n",
      "    X = A - np.tile(avg_doc,(A.shape[1],1)).T\n",
      "    # X.shape = A.shape\n",
      "    return X, avg_doc\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def eigen_topic(self, A, r1 = 5, r2=55):\n",
      "    X, avg_doc = self.mean_center(A)\n",
      "    # SVD on Mean-centered articles\n",
      "    U, _, _ = np.linalg.svd(X)\n",
      "    # U.shape = (embedding dim, embedding dim)\n",
      "    # np.diag(S.shape) = (embedding dim, embedding dim)\n",
      "    # VT.shape = (num of articles, num of articles)\n",
      "    econ_UT = U[:, r1:r2].T\n",
      "    # econ_UT.shape = (r2-r1, emmbedding dim)\n",
      "    transformed_matrix = econ_UT @ X\n",
      "    # transformed_matrix.shape = (r2-r1, num of articles)\n",
      "    return transformed_matrix, avg_doc, econ_UT\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def check_symmetry(self) -> bool:\n",
      "    \"\"\"\n",
      "        Checks if the kernel matrix is symmetric.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if the kernel matrix is symmetric, False otherwise.\n",
      "        \"\"\"\n",
      "    return np.allclose(self.kernel_matrix, self.kernel_matrix.T, atol=1e-8)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def is_positive_definite(self) -> bool:\n",
      "    \"\"\"\n",
      "        Checks if the kernel matrix is positive definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are positive, False otherwise.\n",
      "        \"\"\"\n",
      "    return np.all(self.eigenvalues > 0)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def is_positive_semi_definite(self) -> bool:\n",
      "    \"\"\"\n",
      "        Checks if the kernel matrix is positive semi-definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are non-negative, False otherwise.\n",
      "        \"\"\"\n",
      "    return np.all(self.eigenvalues >= 0)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def is_negative_definite(self) -> bool:\n",
      "    \"\"\"\n",
      "        Checks if the kernel matrix is negative definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are negative, False otherwise.\n",
      "        \"\"\"\n",
      "    return np.all(self.eigenvalues < 0)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def is_negative_semi_definite(self) -> bool:\n",
      "    \"\"\"\n",
      "        Checks if the kernel matrix is negative semi-definite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if all eigenvalues are non-positive, False otherwise.\n",
      "        \"\"\"\n",
      "    return np.all(self.eigenvalues <= 0)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def is_indefinite(self) -> bool:\n",
      "    \"\"\"\n",
      "        Checks if the kernel matrix is indefinite.\n",
      "\n",
      "        Returns:\n",
      "        bool: True if the matrix has both positive and negative eigenvalues, False otherwise.\n",
      "        \"\"\"\n",
      "    has_negative = np.any(self.eigenvalues < 0)\n",
      "    has_non_negative = np.any(self.eigenvalues >= 0)\n",
      "    return has_negative and has_non_negative\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def check_definiteness(self, num_random_vectors: int = 1000) -> Dict[str, bool]:\n",
      "    \"\"\"\n",
      "        Checks the definiteness of the kernel matrix using random vectors.\n",
      "\n",
      "        num_random_vectors: Number of random vectors to use for checking.\n",
      "\n",
      "        Returns:\n",
      "        Dict[str, bool]: A dictionary with the results of the analysis.\n",
      "        \"\"\"\n",
      "    n = self.kernel_matrix.shape[0]\n",
      "    is_positive_definite = True\n",
      "    is_negative_definite = True\n",
      "    for _ in range(num_random_vectors):\n",
      "        x = np.random.randn(n)\n",
      "        xTMx = x.T @ self.kernel_matrix @ x\n",
      "        if xTMx <= 0:\n",
      "            is_positive_definite = False\n",
      "        if xTMx >= 0:\n",
      "            is_negative_definite = False\n",
      "    return {\n",
      "        \"is_positive_definite\": is_positive_definite,\n",
      "        \"is_negative_definite\": is_negative_definite,\n",
      "    }\n",
      "\n",
      "Function call: range\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def run_analysis(self) -> Dict[str, bool]:\n",
      "    return {\n",
      "        \"is_symmetric\": self.check_symmetry(),\n",
      "        \"is_positive_semi_definite\": self.is_positive_semi_definite(),\n",
      "        \"is_negative_semi_definite\": self.is_negative_semi_definite(),\n",
      "        \"is_indefinite\": self.is_indefinite(),\n",
      "        **self.check_definiteness()\n",
      "    }\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class RateLimiter:\n",
      "    def __init__(self, calls_per_minute: int, verbose: bool = False):\n",
      "        self.calls_per_minute = calls_per_minute\n",
      "        self.interval = 60 / calls_per_minute\n",
      "        self.lock = Lock()\n",
      "        self.last_call_time = None\n",
      "        self.verbose = verbose\n",
      "\n",
      "    def __call__(self, func):\n",
      "        @functools.wraps(func)\n",
      "        def wrapper(*args, **kwargs):\n",
      "            with self.lock:\n",
      "                if self.last_call_time is not None:\n",
      "                    time_since_last_call = time.time() - self.last_call_time\n",
      "                    if time_since_last_call < self.interval:\n",
      "                        time_to_wait = self.interval - time_since_last_call\n",
      "                        if self.verbose:\n",
      "                            print(\n",
      "                                f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\n",
      "                            )\n",
      "                        time.sleep(time_to_wait)\n",
      "                    elif self.verbose:\n",
      "                        print(\n",
      "                            f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\n",
      "                        )\n",
      "                else:\n",
      "                    if self.verbose:\n",
      "                        print(\"RateLimiter: This is the first call, no wait required.\")\n",
      "                self.last_call_time = time.time()\n",
      "            return func(*args, **kwargs)\n",
      "\n",
      "        return wrapper\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: Lock\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: func\n",
      "Related codes: ['def len_func(value):\\n    return len(tokenizer.encode(value))\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, calls_per_minute: int, verbose: bool = False):\n",
      "    self.calls_per_minute = calls_per_minute\n",
      "    self.interval = 60 / calls_per_minute\n",
      "    self.lock = Lock()\n",
      "    self.last_call_time = None\n",
      "    self.verbose = verbose\n",
      "\n",
      "Function call: Lock\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __call__(self, func):\n",
      "    @functools.wraps(func)\n",
      "    def wrapper(*args, **kwargs):\n",
      "        with self.lock:\n",
      "            if self.last_call_time is not None:\n",
      "                time_since_last_call = time.time() - self.last_call_time\n",
      "                if time_since_last_call < self.interval:\n",
      "                    time_to_wait = self.interval - time_since_last_call\n",
      "                    if self.verbose:\n",
      "                        print(\n",
      "                            f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\n",
      "                        )\n",
      "                    time.sleep(time_to_wait)\n",
      "                elif self.verbose:\n",
      "                    print(\n",
      "                        f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\n",
      "                    )\n",
      "            else:\n",
      "                if self.verbose:\n",
      "                    print(\"RateLimiter: This is the first call, no wait required.\")\n",
      "            self.last_call_time = time.time()\n",
      "        return func(*args, **kwargs)\n",
      "\n",
      "    return wrapper\n",
      "\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: func\n",
      "Related codes: ['def len_func(value):\\n    return len(tokenizer.encode(value))\\n']\n",
      "\n",
      "********************************************************************************\n",
      "@functools.wraps(func)\n",
      "def wrapper(*args, **kwargs):\n",
      "    with self.lock:\n",
      "        if self.last_call_time is not None:\n",
      "            time_since_last_call = time.time() - self.last_call_time\n",
      "            if time_since_last_call < self.interval:\n",
      "                time_to_wait = self.interval - time_since_last_call\n",
      "                if self.verbose:\n",
      "                    print(\n",
      "                        f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\n",
      "                    )\n",
      "                time.sleep(time_to_wait)\n",
      "            elif self.verbose:\n",
      "                print(\n",
      "                    f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\n",
      "                )\n",
      "        else:\n",
      "            if self.verbose:\n",
      "                print(\"RateLimiter: This is the first call, no wait required.\")\n",
      "        self.last_call_time = time.time()\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: func\n",
      "Related codes: ['def len_func(value):\\n    return len(tokenizer.encode(value))\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class RateLimitedThreadPoolExecutor(ThreadPoolExecutor):\n",
      "    def __init__(self, max_workers=None, *args, **kwargs):\n",
      "        super().__init__(max_workers)\n",
      "        self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\n",
      "\n",
      "    def submit(self, fn, *args, **kwargs):\n",
      "        rate_limited_fn = self.rate_limiter(fn)\n",
      "        return super().submit(rate_limited_fn, *args, **kwargs)\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"ThreadPoolExecutor\"]\n",
      "]\n",
      "Function call: super\n",
      "Function call: RateLimiter\n",
      "Function call: super\n",
      "Related codes: ['\\n\\nclass RateLimiter:\\n    def __init__(self, calls_per_minute: int, verbose: bool = False):\\n        self.calls_per_minute = calls_per_minute\\n        self.interval = 60 / calls_per_minute\\n        self.lock = Lock()\\n        self.last_call_time = None\\n        self.verbose = verbose\\n\\n    def __call__(self, func):\\n        @functools.wraps(func)\\n        def wrapper(*args, **kwargs):\\n            with self.lock:\\n                if self.last_call_time is not None:\\n                    time_since_last_call = time.time() - self.last_call_time\\n                    if time_since_last_call < self.interval:\\n                        time_to_wait = self.interval - time_since_last_call\\n                        if self.verbose:\\n                            print(\\n                                f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\\n                            )\\n                        time.sleep(time_to_wait)\\n                    elif self.verbose:\\n                        print(\\n                            f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\\n                        )\\n                else:\\n                    if self.verbose:\\n                        print(\"RateLimiter: This is the first call, no wait required.\")\\n                self.last_call_time = time.time()\\n            return func(*args, **kwargs)\\n\\n        return wrapper\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, max_workers=None, *args, **kwargs):\n",
      "    super().__init__(max_workers)\n",
      "    self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\n",
      "\n",
      "Function call: super\n",
      "Function call: RateLimiter\n",
      "Related codes: ['\\n\\nclass RateLimiter:\\n    def __init__(self, calls_per_minute: int, verbose: bool = False):\\n        self.calls_per_minute = calls_per_minute\\n        self.interval = 60 / calls_per_minute\\n        self.lock = Lock()\\n        self.last_call_time = None\\n        self.verbose = verbose\\n\\n    def __call__(self, func):\\n        @functools.wraps(func)\\n        def wrapper(*args, **kwargs):\\n            with self.lock:\\n                if self.last_call_time is not None:\\n                    time_since_last_call = time.time() - self.last_call_time\\n                    if time_since_last_call < self.interval:\\n                        time_to_wait = self.interval - time_since_last_call\\n                        if self.verbose:\\n                            print(\\n                                f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\\n                            )\\n                        time.sleep(time_to_wait)\\n                    elif self.verbose:\\n                        print(\\n                            f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\\n                        )\\n                else:\\n                    if self.verbose:\\n                        print(\"RateLimiter: This is the first call, no wait required.\")\\n                self.last_call_time = time.time()\\n            return func(*args, **kwargs)\\n\\n        return wrapper\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def submit(self, fn, *args, **kwargs):\n",
      "    rate_limited_fn = self.rate_limiter(fn)\n",
      "    return super().submit(rate_limited_fn, *args, **kwargs)\n",
      "\n",
      "Function call: super\n",
      "\n",
      "********************************************************************************\n",
      "@dataclass\n",
      "class StatusTracker:\n",
      "    \"\"\"Stores metadata about the script's progress. Only one instance is created.\"\"\"\n",
      "\n",
      "    num_tasks_started: int = 0\n",
      "    num_tasks_in_progress: int = 0  # script ends when this reaches 0\n",
      "    num_tasks_succeeded: int = 0\n",
      "    num_tasks_failed: int = 0\n",
      "    num_rate_limit_errors: int = 0\n",
      "    num_api_errors: int = 0  # excluding rate limit errors, counted above\n",
      "    num_other_errors: int = 0\n",
      "    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class APIRequest:\n",
      "    \"\"\"Stores an API request's inputs, outputs, and other metadata. Contains a method to make an API call.\"\"\"\n",
      "\n",
      "    task_id: int\n",
      "    request_json: dict\n",
      "    token_consumption: int\n",
      "    attempts_left: int\n",
      "    metadata: dict\n",
      "    result: list = field(default_factory=list)\n",
      "\n",
      "    async def call_api(\n",
      "        self,\n",
      "        request_url: str,\n",
      "        request_header: dict,\n",
      "        retry_queue: asyncio.Queue,\n",
      "        save_filepath: str,\n",
      "        status_tracker: StatusTracker,\n",
      "    ):\n",
      "        \"\"\"Calls the OpenAI API and saves results.\"\"\"\n",
      "        logging.info(f\"Starting request #{self.task_id}\")\n",
      "        error = None\n",
      "        try:\n",
      "            async with aiohttp.ClientSession() as session:\n",
      "                async with session.post(\n",
      "                    url=request_url, headers=request_header, json=self.request_json\n",
      "                ) as response:\n",
      "                    response = await response.json()\n",
      "            if \"error\" in response:\n",
      "                logging.warning(\n",
      "                    f\"Request {self.task_id} failed with error {response['error']}\"\n",
      "                )\n",
      "                status_tracker.num_api_errors += 1\n",
      "                error = response\n",
      "                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                    status_tracker.time_of_last_rate_limit_error = time.time()\n",
      "                    status_tracker.num_rate_limit_errors += 1\n",
      "                    status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n",
      "\n",
      "        except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "            logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n",
      "            status_tracker.num_other_errors += 1\n",
      "            error = e\n",
      "        if error:\n",
      "            self.result.append(error)\n",
      "            if self.attempts_left:\n",
      "                retry_queue.put_nowait(self)\n",
      "            else:\n",
      "                logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n",
      "                data = (\n",
      "                    [self.request_json, [str(e) for e in self.result], self.metadata]\n",
      "                    if self.metadata\n",
      "                    else [self.request_json, [str(e) for e in self.result]]\n",
      "                )\n",
      "                append_to_jsonl(data, save_filepath)\n",
      "                status_tracker.num_tasks_in_progress -= 1\n",
      "                status_tracker.num_tasks_failed += 1\n",
      "        else:\n",
      "            data = (\n",
      "                [self.request_json, response, self.metadata]\n",
      "                if self.metadata\n",
      "                else [self.request_json, response]\n",
      "            )\n",
      "            append_to_jsonl(data, save_filepath)\n",
      "            status_tracker.num_tasks_in_progress -= 1\n",
      "            status_tracker.num_tasks_succeeded += 1\n",
      "            logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: field\n",
      "Function call: str\n",
      "Function call: str\n",
      "Function call: append_to_jsonl\n",
      "Function call: append_to_jsonl\n",
      "Related codes: ['\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\n\\ndef append_to_jsonl(data, filename: str) -> None:\\n    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\\n    json_string = json.dumps(data)\\n    with open(filename, \"a\") as f:\\n        f.write(json_string + \"\\\\n\")\\n', '\\n\\ndef append_to_jsonl(data, filename: str) -> None:\\n    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\\n    json_string = json.dumps(data)\\n    with open(filename, \"a\") as f:\\n        f.write(json_string + \"\\\\n\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "async def call_api(\n",
      "    self,\n",
      "    request_url: str,\n",
      "    request_header: dict,\n",
      "    retry_queue: asyncio.Queue,\n",
      "    save_filepath: str,\n",
      "    status_tracker: StatusTracker,\n",
      "):\n",
      "    \"\"\"Calls the OpenAI API and saves results.\"\"\"\n",
      "    logging.info(f\"Starting request #{self.task_id}\")\n",
      "    error = None\n",
      "    try:\n",
      "        async with aiohttp.ClientSession() as session:\n",
      "            async with session.post(\n",
      "                url=request_url, headers=request_header, json=self.request_json\n",
      "            ) as response:\n",
      "                response = await response.json()\n",
      "        if \"error\" in response:\n",
      "            logging.warning(\n",
      "                f\"Request {self.task_id} failed with error {response['error']}\"\n",
      "            )\n",
      "            status_tracker.num_api_errors += 1\n",
      "            error = response\n",
      "            if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                status_tracker.time_of_last_rate_limit_error = time.time()\n",
      "                status_tracker.num_rate_limit_errors += 1\n",
      "                status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n",
      "\n",
      "    except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "        logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n",
      "        status_tracker.num_other_errors += 1\n",
      "        error = e\n",
      "    if error:\n",
      "        self.result.append(error)\n",
      "        if self.attempts_left:\n",
      "            retry_queue.put_nowait(self)\n",
      "        else:\n",
      "            logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n",
      "            data = (\n",
      "                [self.request_json, [str(e) for e in self.result], self.metadata]\n",
      "                if self.metadata\n",
      "                else [self.request_json, [str(e) for e in self.result]]\n",
      "            )\n",
      "            append_to_jsonl(data, save_filepath)\n",
      "            status_tracker.num_tasks_in_progress -= 1\n",
      "            status_tracker.num_tasks_failed += 1\n",
      "    else:\n",
      "        data = (\n",
      "            [self.request_json, response, self.metadata]\n",
      "            if self.metadata\n",
      "            else [self.request_json, response]\n",
      "        )\n",
      "        append_to_jsonl(data, save_filepath)\n",
      "        status_tracker.num_tasks_in_progress -= 1\n",
      "        status_tracker.num_tasks_succeeded += 1\n",
      "        logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n",
      "\n",
      "Function call: str\n",
      "Function call: str\n",
      "Function call: append_to_jsonl\n",
      "Function call: append_to_jsonl\n",
      "Related codes: ['\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\n\\ndef append_to_jsonl(data, filename: str) -> None:\\n    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\\n    json_string = json.dumps(data)\\n    with open(filename, \"a\") as f:\\n        f.write(json_string + \"\\\\n\")\\n', '\\n\\ndef append_to_jsonl(data, filename: str) -> None:\\n    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\\n    json_string = json.dumps(data)\\n    with open(filename, \"a\") as f:\\n        f.write(json_string + \"\\\\n\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# functions\n",
      "\n",
      "\n",
      "def api_endpoint_from_url(request_url):\n",
      "    \"\"\"Extract the API endpoint from the request URL.\"\"\"\n",
      "    match = re.search('^https://[^/]+/v\\\\d+/(.+)$', request_url)\n",
      "    return match[1]\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def append_to_jsonl(data, filename: str) -> None:\n",
      "    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\n",
      "    json_string = json.dumps(data)\n",
      "    with open(filename, \"a\") as f:\n",
      "        f.write(json_string + \"\\n\")\n",
      "\n",
      "Function call: open\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def num_tokens_consumed_from_request(\n",
      "    request_json: dict,\n",
      "    api_endpoint: str,\n",
      "    token_encoding_name: str,\n",
      "):\n",
      "    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\n",
      "    encoding = tiktoken.get_encoding(token_encoding_name)\n",
      "    # if completions request, tokens = prompt + n * max_tokens\n",
      "    if api_endpoint.endswith(\"completions\"):\n",
      "        max_tokens = request_json.get(\"max_tokens\", 15)\n",
      "        n = request_json.get(\"n\", 1)\n",
      "        completion_tokens = n * max_tokens\n",
      "\n",
      "        # chat completions\n",
      "        if api_endpoint.startswith(\"chat/\"):\n",
      "            num_tokens = 0\n",
      "            for message in request_json[\"messages\"]:\n",
      "                num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
      "                for key, value in message.items():\n",
      "                    num_tokens += len(encoding.encode(value))\n",
      "                    if key == \"name\":  # if there's a name, the role is omitted\n",
      "                        num_tokens -= 1  # role is always required and always 1 token\n",
      "            num_tokens += 2  # every reply is primed with <im_start>assistant\n",
      "            return num_tokens + completion_tokens\n",
      "        # normal completions\n",
      "        else:\n",
      "            prompt = request_json[\"prompt\"]\n",
      "            if isinstance(prompt, str):  # single prompt\n",
      "                prompt_tokens = len(encoding.encode(prompt))\n",
      "                num_tokens = prompt_tokens + completion_tokens\n",
      "                return num_tokens\n",
      "            elif isinstance(prompt, list):  # multiple prompts\n",
      "                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\n",
      "                num_tokens = prompt_tokens + completion_tokens * len(prompt)\n",
      "                return num_tokens\n",
      "            else:\n",
      "                raise TypeError('Expecting either string or list of strings for \"prompt\" field in completion request')\n",
      "    # if embeddings request, tokens = input tokens\n",
      "    elif api_endpoint == \"embeddings\":\n",
      "        input = request_json[\"input\"]\n",
      "        if isinstance(input, str):  # single input\n",
      "            num_tokens = len(encoding.encode(input))\n",
      "            return num_tokens\n",
      "        elif isinstance(input, list):  # multiple inputs\n",
      "            num_tokens = sum([len(encoding.encode(i)) for i in input])\n",
      "            return num_tokens\n",
      "        else:\n",
      "            raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n",
      "    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\n",
      "    else:\n",
      "        raise NotImplementedError(f'API endpoint \"{api_endpoint}\" not implemented in this script')\n",
      "\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: sum\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: TypeError\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: sum\n",
      "Function call: len\n",
      "Function call: TypeError\n",
      "Function call: NotImplementedError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def task_id_generator_function():\n",
      "    \"\"\"Generate integers 0, 1, 2, and so on.\"\"\"\n",
      "    task_id = 0\n",
      "    while True:\n",
      "        yield task_id\n",
      "        task_id += 1\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "async def process_chat_requests(\n",
      "    request_data: List[Dict],\n",
      "    save_filepath: str,\n",
      "    request_url: str,\n",
      "    api_key: str,\n",
      "    max_requests_per_minute: float,\n",
      "    max_tokens_per_minute: float,\n",
      "    token_encoding_name: str,\n",
      "    max_attempts: int,\n",
      "    logging_level: int,\n",
      "):\n",
      "    \"\"\"Processes chat requests in parallel, throttling to stay under rate limits.\"\"\"\n",
      "    # constants\n",
      "    seconds_to_pause_after_rate_limit_error = 15\n",
      "    seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n",
      "\n",
      "    # initialize logging\n",
      "    logging.basicConfig(level=logging_level)\n",
      "    logging.debug(f\"Logging initialized at level {logging_level}\")\n",
      "\n",
      "    # infer API endpoint and construct request header\n",
      "    api_endpoint = api_endpoint_from_url(request_url)\n",
      "    request_header = {\"Authorization\": f\"Bearer {api_key}\"}\n",
      "\n",
      "    # initialize trackers\n",
      "    queue_of_requests_to_retry = asyncio.Queue()\n",
      "    task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...\n",
      "    status_tracker = StatusTracker()  # single instance to track a collection of variables\n",
      "    next_request = None  # variable to hold the next request to call\n",
      "\n",
      "    # initialize available capacity counts\n",
      "    available_request_capacity = max_requests_per_minute\n",
      "    available_token_capacity = max_tokens_per_minute\n",
      "    last_update_time = time.time()\n",
      "\n",
      "    # initialize flags\n",
      "    file_not_finished = True  # after file is empty, we'll skip reading it\n",
      "    logging.debug(f\"Initialization complete.\")\n",
      "\n",
      "    # `requests` will provide requests one at a time\n",
      "    requests = iter(request_data)\n",
      "    logging.debug(f\"List initialized. Entering main loop\")\n",
      "\n",
      "    while True:\n",
      "        # get next request (if one is not already waiting for capacity)\n",
      "        if next_request is None:\n",
      "            if not queue_of_requests_to_retry.empty():\n",
      "                next_request = queue_of_requests_to_retry.get_nowait()\n",
      "                logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")\n",
      "            elif file_not_finished:\n",
      "                try:\n",
      "                    # get new request\n",
      "                    request_json = next(requests)\n",
      "                    next_request = APIRequest(\n",
      "                        task_id=next(task_id_generator),\n",
      "                        request_json=request_json,\n",
      "                        token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),\n",
      "                        attempts_left=max_attempts,\n",
      "                        metadata=request_json.pop(\"metadata\", None)\n",
      "                    )\n",
      "                    status_tracker.num_tasks_started += 1\n",
      "                    status_tracker.num_tasks_in_progress += 1\n",
      "                    logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")\n",
      "                except StopIteration:\n",
      "                    # if file runs out, set flag to stop reading it\n",
      "                    logging.debug(\"Read file exhausted\")\n",
      "                    file_not_finished = False\n",
      "\n",
      "        # update available capacity\n",
      "        current_time = time.time()\n",
      "        seconds_since_update = current_time - last_update_time\n",
      "        available_request_capacity = min(\n",
      "            available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,\n",
      "            max_requests_per_minute,\n",
      "        )\n",
      "        available_token_capacity = min(\n",
      "            available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,\n",
      "            max_tokens_per_minute,\n",
      "        )\n",
      "        last_update_time = current_time\n",
      "\n",
      "        # if enough capacity available, call API\n",
      "        if next_request:\n",
      "            next_request_tokens = next_request.token_consumption\n",
      "            if (\n",
      "                available_request_capacity >= 1\n",
      "                and available_token_capacity >= next_request_tokens\n",
      "            ):\n",
      "                # update counters\n",
      "                available_request_capacity -= 1\n",
      "                available_token_capacity -= next_request_tokens\n",
      "                next_request.attempts_left -= 1\n",
      "\n",
      "                # call API\n",
      "                asyncio.create_task(\n",
      "                    next_request.call_api(\n",
      "                        request_url=request_url,\n",
      "                        request_header=request_header,\n",
      "                        retry_queue=queue_of_requests_to_retry,\n",
      "                        save_filepath=save_filepath,\n",
      "                        status_tracker=status_tracker,\n",
      "                    )\n",
      "                )\n",
      "                next_request = None  # reset next_request to empty\n",
      "\n",
      "        # if all tasks are finished, break\n",
      "        if status_tracker.num_tasks_in_progress == 0:\n",
      "            break\n",
      "\n",
      "        # main loop sleeps briefly so concurrent tasks can run\n",
      "        await asyncio.sleep(seconds_to_sleep_each_loop)\n",
      "\n",
      "        # if a rate limit error was hit recently, pause to cool down\n",
      "        seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)\n",
      "        if seconds_since_rate_limit_error < seconds_to_pause_after_rate_limit_error:\n",
      "            remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n",
      "            await asyncio.sleep(remaining_seconds_to_pause)\n",
      "            # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n",
      "            logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")\n",
      "\n",
      "    # after finishing, log final status\n",
      "    logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")\n",
      "    if status_tracker.num_tasks_failed > 0:\n",
      "        logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")\n",
      "    if status_tracker.num_rate_limit_errors > 0:\n",
      "        logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n",
      "\n",
      "Function call: api_endpoint_from_url\n",
      "Function call: task_id_generator_function\n",
      "Function call: StatusTracker\n",
      "Function call: iter\n",
      "Function call: next\n",
      "Function call: APIRequest\n",
      "Function call: next\n",
      "Function call: num_tokens_consumed_from_request\n",
      "Function call: min\n",
      "Function call: min\n",
      "Related codes: ['\\n\\n# functions\\n\\n\\ndef api_endpoint_from_url(request_url):\\n    \"\"\"Extract the API endpoint from the request URL.\"\"\"\\n    match = re.search(\\'^https://[^/]+/v\\\\\\\\d+/(.+)$\\', request_url)\\n    return match[1]\\n', '\\n\\ndef task_id_generator_function():\\n    \"\"\"Generate integers 0, 1, 2, and so on.\"\"\"\\n    task_id = 0\\n    while True:\\n        yield task_id\\n        task_id += 1\\n', '\\n\\n# dataclasses\\n\\n\\n@dataclass\\nclass StatusTracker:\\n    \"\"\"Stores metadata about the script\\'s progress. Only one instance is created.\"\"\"\\n\\n    num_tasks_started: int = 0\\n    num_tasks_in_progress: int = 0  # script ends when this reaches 0\\n    num_tasks_succeeded: int = 0\\n    num_tasks_failed: int = 0\\n    num_rate_limit_errors: int = 0\\n    num_api_errors: int = 0  # excluding rate limit errors, counted above\\n    num_other_errors: int = 0\\n    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\\n', '\\n\\nclass LLMWriter(BaseTask):\\n    def __init__(\\n        self,\\n        index: MemoryIndex,\\n        path: List[List[int]],\\n        chatbot: Chat,\\n        write_func=None,\\n        context=None,\\n        task_name=\"summary\",\\n        max_workers: int = 1,\\n        task_id: str = \"LLMWriteTask\",\\n        calls_per_minute: int = 20,\\n        backup: bool = True,\\n    ):\\n        \"\"\"\\n        Initialize a LLMWriteTask instance.\\n\\n        :param index: List of strings representing the queries.\\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\\n        :param chatbot: Chatbot instance used for executing queries.\\n        :param max_workers: Maximum number of worker threads (default is 4).\\n        \"\"\"\\n        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\\n        self.index = index\\n        self.chatbot = chatbot\\n        self.write_func = write_func if write_func else self.llm_response\\n        self.new_index_name = self.index.name + f\"_{task_name}\"\\n        self.context = context\\n\\n    def llm_response(self, chatbot: Chat, message: str, context=None, id=None):\\n        max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\\n        # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\\n        #     return \"the message is too long to be processed\"\\n        # moved the error catching to multi-threading but custom method could report the error here\\n        return chatbot.reply(message)\\n\\n    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\\n        \"\"\"\\n        Execute a sub-task using a separate copy of the chatbot instance.\\n\\n        :param sub_path: List of indices representing the sub-task\\'s sequence.\\n        :return: List of strings representing the responses for each query in the sub-task.\\n        \"\"\"\\n        if self.parallel:\\n            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\\n            chatbot_instance = copy.deepcopy(self.chatbot)\\n        else:\\n            chatbot_instance = self.chatbot\\n        if isinstance(self.chatbot, BaseThread):\\n            chatbot_instance.reset_memory()\\n\\n        sub_results = {}\\n        for i in sub_path:\\n            current_val = self.index.values[i]\\n            response = self.write_func(\\n                chatbot_instance, current_val, self.context, id=i\\n            )\\n            sub_results[i] = response\\n        return sub_results\\n\\n    def write(self):\\n        content_to_write = self.work()\\n        self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\\n        self.new_index.save()\\n        return self.new_index\\n', '\\n\\n@dataclass\\nclass APIRequest:\\n    \"\"\"Stores an API request\\'s inputs, outputs, and other metadata. Contains a method to make an API call.\"\"\"\\n\\n    task_id: int\\n    request_json: dict\\n    token_consumption: int\\n    attempts_left: int\\n    metadata: dict\\n    result: list = field(default_factory=list)\\n\\n    async def call_api(\\n        self,\\n        request_url: str,\\n        request_header: dict,\\n        retry_queue: asyncio.Queue,\\n        save_filepath: str,\\n        status_tracker: StatusTracker,\\n    ):\\n        \"\"\"Calls the OpenAI API and saves results.\"\"\"\\n        logging.info(f\"Starting request #{self.task_id}\")\\n        error = None\\n        try:\\n            async with aiohttp.ClientSession() as session:\\n                async with session.post(\\n                    url=request_url, headers=request_header, json=self.request_json\\n                ) as response:\\n                    response = await response.json()\\n            if \"error\" in response:\\n                logging.warning(\\n                    f\"Request {self.task_id} failed with error {response[\\'error\\']}\"\\n                )\\n                status_tracker.num_api_errors += 1\\n                error = response\\n                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\\n                    status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \\n                    status_tracker.num_rate_limit_errors += 1\\n                    status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\\n\\n        except Exception as e:  # catching naked exceptions is bad practice, but in this case we\\'ll log & save them\\n            logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\\n            status_tracker.num_other_errors += 1\\n            error = e\\n        if error:\\n            self.result.append(error)\\n            if self.attempts_left:\\n                retry_queue.put_nowait(self)\\n            else:\\n                logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\\n                data = (\\n                    [self.request_json, [str(e) for e in self.result], self.metadata]\\n                    if self.metadata\\n                    else [self.request_json, [str(e) for e in self.result]]\\n                )\\n                append_to_jsonl(data, save_filepath)\\n                status_tracker.num_tasks_in_progress -= 1\\n                status_tracker.num_tasks_failed += 1\\n        else:\\n            data = (\\n                [self.request_json, response, self.metadata]\\n                if self.metadata\\n                else [self.request_json, response]\\n            )\\n            append_to_jsonl(data, save_filepath)\\n            status_tracker.num_tasks_in_progress -= 1\\n            status_tracker.num_tasks_succeeded += 1\\n            logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\\n', '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class OpenAiEmbedder:\n",
      "\n",
      "    def get_embedding_size(self):\n",
      "        return ADA_EMBEDDING_SIZE\n",
      "\n",
      "    def embed(self, data, verbose=False):\n",
      "        if isinstance(data, list) and len(data) > 1:\n",
      "            logger.info(\"Batch embedding\")\n",
      "            return self.batch_embed(data)\n",
      "        elif isinstance(data, list) and len(data) == 1:\n",
      "            logger.info(\"Serial embedding\")\n",
      "            data = data[0]\n",
      "\n",
      "        if isinstance(data, dict) and \"content\" in data:\n",
      "            if verbose:\n",
      "                logger.info(\"Embedding without mark\", data[\"content\"])\n",
      "            out = openai.Embedding.create(\n",
      "                input=data[\"content\"], engine=\"text-embedding-ada-002\"\n",
      "            )\n",
      "        else:\n",
      "            if len(TOKENIZER.encode(data)) > MAX_CONTEXT_LENGTH:\n",
      "                raise ValueError(f\" The input is too long for OpenAI, num tokens is {len(TOKENIZER.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "            if verbose:\n",
      "                logger.info(f\"Embedding without preprocessing the input {data}\")\n",
      "            out = openai.Embedding.create(\n",
      "                input=str(data), engine=\"text-embedding-ada-002\"\n",
      "            )\n",
      "        return out.data[0].embedding\n",
      "\n",
      "    def batch_embed(self, data: List[str], batch_size: int = 1000):\n",
      "        if isinstance(data, dict) and \"content\" in data:\n",
      "            raise ValueError(\"Batch embedding not supported for dictionaries\")\n",
      "        elif isinstance(data, str):\n",
      "            raise ValueError(\"Batch embedding not supported for strings use embed() instead\")\n",
      "        elif isinstance(data, list):\n",
      "            batch = []\n",
      "            embeddings = []\n",
      "            i = 1\n",
      "            total_number_of_batches = len(data)//batch_size + 1 if len(data) % batch_size > 0 else len(data)//batch_size\n",
      "            for value in data:\n",
      "                batch.append(value)\n",
      "                if len(batch) == batch_size:\n",
      "                    start = time.time()\n",
      "                    out = openai.Embedding.create(\n",
      "                        input=batch, engine=\"text-embedding-ada-002\"\n",
      "                    )\n",
      "                    for embedding in out.data:\n",
      "                        embeddings.append(embedding.embedding)\n",
      "                    logger.info(f\"Batch {i} of {total_number_of_batches}\")\n",
      "                    logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\n",
      "                    i += 1\n",
      "                    batch = []\n",
      "            if len(batch) > 0:\n",
      "                start = time.time()\n",
      "                out = openai.Embedding.create(\n",
      "                    input=batch, engine=\"text-embedding-ada-002\"\n",
      "                )\n",
      "                for embedding in out.data:\n",
      "                    embeddings.append(embedding.embedding)\n",
      "                logger.info(f\"Batch {i} of {total_number_of_batches}\")\n",
      "                logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\n",
      "            logger.info(f'Total number of embeddings {len(embeddings)}')\n",
      "\n",
      "            if len(embeddings) != len(data):\n",
      "                raise ValueError(\"The number of embeddings is different from the number of values an error occured in OpenAI API during the embedding process\")\n",
      "            return embeddings\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: len\n",
      "Function call: str\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_embedding_size(self):\n",
      "    return ADA_EMBEDDING_SIZE\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def embed(self, data, verbose=False):\n",
      "    if isinstance(data, list) and len(data) > 1:\n",
      "        logger.info(\"Batch embedding\")\n",
      "        return self.batch_embed(data)\n",
      "    elif isinstance(data, list) and len(data) == 1:\n",
      "        logger.info(\"Serial embedding\")\n",
      "        data = data[0]\n",
      "\n",
      "    if isinstance(data, dict) and \"content\" in data:\n",
      "        if verbose:\n",
      "            logger.info(\"Embedding without mark\", data[\"content\"])\n",
      "        out = openai.Embedding.create(\n",
      "            input=data[\"content\"], engine=\"text-embedding-ada-002\"\n",
      "        )\n",
      "    else:\n",
      "        if len(TOKENIZER.encode(data)) > MAX_CONTEXT_LENGTH:\n",
      "            raise ValueError(f\" The input is too long for OpenAI, num tokens is {len(TOKENIZER.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "        if verbose:\n",
      "            logger.info(f\"Embedding without preprocessing the input {data}\")\n",
      "        out = openai.Embedding.create(\n",
      "            input=str(data), engine=\"text-embedding-ada-002\"\n",
      "        )\n",
      "    return out.data[0].embedding\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: len\n",
      "Function call: str\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def batch_embed(self, data: List[str], batch_size: int = 1000):\n",
      "    if isinstance(data, dict) and \"content\" in data:\n",
      "        raise ValueError(\"Batch embedding not supported for dictionaries\")\n",
      "    elif isinstance(data, str):\n",
      "        raise ValueError(\"Batch embedding not supported for strings use embed() instead\")\n",
      "    elif isinstance(data, list):\n",
      "        batch = []\n",
      "        embeddings = []\n",
      "        i = 1\n",
      "        total_number_of_batches = len(data)//batch_size + 1 if len(data) % batch_size > 0 else len(data)//batch_size\n",
      "        for value in data:\n",
      "            batch.append(value)\n",
      "            if len(batch) == batch_size:\n",
      "                start = time.time()\n",
      "                out = openai.Embedding.create(\n",
      "                    input=batch, engine=\"text-embedding-ada-002\"\n",
      "                )\n",
      "                for embedding in out.data:\n",
      "                    embeddings.append(embedding.embedding)\n",
      "                logger.info(f\"Batch {i} of {total_number_of_batches}\")\n",
      "                logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\n",
      "                i += 1\n",
      "                batch = []\n",
      "        if len(batch) > 0:\n",
      "            start = time.time()\n",
      "            out = openai.Embedding.create(\n",
      "                input=batch, engine=\"text-embedding-ada-002\"\n",
      "            )\n",
      "            for embedding in out.data:\n",
      "                embeddings.append(embedding.embedding)\n",
      "            logger.info(f\"Batch {i} of {total_number_of_batches}\")\n",
      "            logger.info(f\"Embedding batch {i} took {time.time() - start} seconds\")\n",
      "        logger.info(f'Total number of embeddings {len(embeddings)}')\n",
      "\n",
      "        if len(embeddings) != len(data):\n",
      "            raise ValueError(\"The number of embeddings is different from the number of values an error occured in OpenAI API during the embedding process\")\n",
      "        return embeddings\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: ValueError\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "def parse_and_embed_functions(input_str: str) -> List[np.ndarray]:\n",
      "    # Parse the input string with libcst\n",
      "    module = cst.parse_module(input_str)\n",
      "\n",
      "    # Find all the functions in the module and embed them separately\n",
      "    embeddings = []\n",
      "    for node in module.body:\n",
      "\n",
      "        if isinstance(node, cst.FunctionDef) or isinstance(node, cst.ClassDef):\n",
      "            func_str = cst.Module(body=[node]).code\n",
      "            print(\"Function string\", func_str)\n",
      "            embedding = openai.Embedding.create(\n",
      "                input=str(func_str)[:MAX_CONTEXT_LENGTH],\n",
      "                engine=\"text-embedding-ada-002\",\n",
      "            )\n",
      "            if embedding is not None:\n",
      "                embeddings.append(embedding.data[0].embedding)\n",
      "\n",
      "    avg_embedding = avg_embeddings(embeddings)\n",
      "    print(avg_embedding.shape)\n",
      "    return avg_embedding\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "Function call: str\n",
      "Function call: avg_embeddings\n",
      "Function call: print\n",
      "Related codes: ['\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n', '\\n\\ndef avg_embeddings(embeddings: List[np.ndarray]) -> np.ndarray:\\n    print(\"Embeddings len\", len(embeddings))\\n    # convert embeddings to numpy array\\n    embeddings = np.array(embeddings)\\n    print(\"Embedding Matrix Shape\", embeddings.shape)\\n    return np.array([np.sum(embeddings.T, axis=1)]).astype(np.float32)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def avg_embeddings(embeddings: List[np.ndarray]) -> np.ndarray:\n",
      "    print(\"Embeddings len\", len(embeddings))\n",
      "    # convert embeddings to numpy array\n",
      "    embeddings = np.array(embeddings)\n",
      "    print(\"Embedding Matrix Shape\", embeddings.shape)\n",
      "    return np.array([np.sum(embeddings.T, axis=1)]).astype(np.float32)\n",
      "\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class SBERTEmbedder:\n",
      "    def get_embedding_size(self):\n",
      "        return SBERT_EMBEDDING_SIZE\n",
      "\n",
      "    def embed(self,\n",
      "        data,\n",
      "        key=\"content\",\n",
      "        model_name=\"all-MiniLM-L6-v2\",\n",
      "        batch_size=128,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n",
      "        \"\"\"\n",
      "        print(\"Embedding data\")\n",
      "        model = SentenceTransformer(model_name)\n",
      "        print(\"Model loaded\")\n",
      "        if isinstance(data, dict):\n",
      "            sentences = data[key].tolist()\n",
      "            unique_sentences = data[key].unique()\n",
      "        elif isinstance(data, str):\n",
      "            #breal the string into sentences based on . or ? or !\n",
      "            sentences = re.split('[.!?]', data)\n",
      "            sentences = [s.strip() for s in sentences if s.strip()]  #\n",
      "            #filter empty sentences\n",
      "            sentences = list(filter(lambda x: len(x) > 0, sentences))\n",
      "            unique_sentences = list(set(sentences))\n",
      "        else:\n",
      "            raise ValueError(f\"Data must be a dictionary with attribute {key} or a string, but got {type(data)} instead\")\n",
      "        \n",
      "        print(\"Unique sentences\", len(unique_sentences))\n",
      "        self.unique_sentences = unique_sentences\n",
      "        for sentence in unique_sentences:\n",
      "            tokens = tokenizer.encode(sentence)\n",
      "            if len(tokens) > MAX_CONTEXT_LENGTH:\n",
      "                raise ValueError(f\" The input subsentence is too long for SBERT, num tokens is {len(tokens)}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "\n",
      "       \n",
      "        embeddings = model.encode(\n",
      "                unique_sentences, show_progress_bar=True, batch_size=batch_size\n",
      "            )\n",
      "\n",
      "        print(\"Embeddings computed\")\n",
      "\n",
      "        mapping = {\n",
      "            sentence: embedding\n",
      "            for sentence, embedding in zip(unique_sentences, embeddings)\n",
      "        }\n",
      "        embeddings = np.array([mapping[sentence] for sentence in sentences])\n",
      "\n",
      "        return np.mean(embeddings, axis=0).tolist()\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: print\n",
      "Function call: SentenceTransformer\n",
      "Function call: print\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: list\n",
      "Function call: filter\n",
      "Function call: len\n",
      "Function call: list\n",
      "Function call: set\n",
      "Function call: ValueError\n",
      "Function call: type\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: zip\n",
      "Related codes: [\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\ndef filter_col(self, feature: str, filter: str):\\n    try:\\n        return self.memory_thread.lazy().filter(pl.col(feature) == filter).collect()\\n    except Exception as e:\\n        return str(e)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def get_embedding_size(self):\n",
      "    return SBERT_EMBEDDING_SIZE\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def embed(self,\n",
      "    data,\n",
      "    key=\"content\",\n",
      "    model_name=\"all-MiniLM-L6-v2\",\n",
      "    batch_size=128,\n",
      "):\n",
      "    \"\"\"\n",
      "        Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n",
      "        \"\"\"\n",
      "    print(\"Embedding data\")\n",
      "    model = SentenceTransformer(model_name)\n",
      "    print(\"Model loaded\")\n",
      "    if isinstance(data, dict):\n",
      "        sentences = data[key].tolist()\n",
      "        unique_sentences = data[key].unique()\n",
      "    elif isinstance(data, str):\n",
      "        #breal the string into sentences based on . or ? or !\n",
      "        sentences = re.split('[.!?]', data)\n",
      "        sentences = [s.strip() for s in sentences if s.strip()]  #\n",
      "        #filter empty sentences\n",
      "        sentences = list(filter(lambda x: len(x) > 0, sentences))\n",
      "        unique_sentences = list(set(sentences))\n",
      "    else:\n",
      "        raise ValueError(f\"Data must be a dictionary with attribute {key} or a string, but got {type(data)} instead\")\n",
      "    \n",
      "    print(\"Unique sentences\", len(unique_sentences))\n",
      "    self.unique_sentences = unique_sentences\n",
      "    for sentence in unique_sentences:\n",
      "        tokens = tokenizer.encode(sentence)\n",
      "        if len(tokens) > MAX_CONTEXT_LENGTH:\n",
      "            raise ValueError(f\" The input subsentence is too long for SBERT, num tokens is {len(tokens)}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "\n",
      "       \n",
      "    embeddings = model.encode(\n",
      "            unique_sentences, show_progress_bar=True, batch_size=batch_size\n",
      "        )\n",
      "\n",
      "    print(\"Embeddings computed\")\n",
      "\n",
      "    mapping = {\n",
      "        sentence: embedding\n",
      "        for sentence, embedding in zip(unique_sentences, embeddings)\n",
      "    }\n",
      "    embeddings = np.array([mapping[sentence] for sentence in sentences])\n",
      "\n",
      "    return np.mean(embeddings, axis=0).tolist()\n",
      "\n",
      "Function call: print\n",
      "Function call: SentenceTransformer\n",
      "Function call: print\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: list\n",
      "Function call: filter\n",
      "Function call: len\n",
      "Function call: list\n",
      "Function call: set\n",
      "Function call: ValueError\n",
      "Function call: type\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: zip\n",
      "Related codes: [\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\ndef filter_col(self, feature: str, filter: str):\\n    try:\\n        return self.memory_thread.lazy().filter(pl.col(feature) == filter).collect()\\n    except Exception as e:\\n        return str(e)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class CohereEmbedder:\n",
      "    def get_embedding_size(self):\n",
      "        return COHERE_EMBEDDING_SIZE\n",
      "\n",
      "    def embed(self, data, verbose=False):\n",
      "        if type(data) is dict and \"content\" in data:\n",
      "            if verbose is True:\n",
      "                print(\"Embedding from dictionary\", data[\"content\"])\n",
      "                response = co.embed(texts= data[\"content\"],model='multilingual-22-12')\n",
      "        else:\n",
      "            if len(tokenizer.encode(data)) > MAX_CONTEXT_LENGTH:\n",
      "                raise ValueError(f\" The input is too long for Cohere, num tokens is {len(tokenizer.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "            if verbose is True:\n",
      "                print(\"Embedding without preprocessing the input\", data)\n",
      "            response = co.embed(texts=[str(data)],model='multilingual-22-12')\n",
      "        return response.embeddings[0]\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: type\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: str\n",
      "Related codes: ['\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def get_embedding_size(self):\n",
      "    return COHERE_EMBEDDING_SIZE\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def embed(self, data, verbose=False):\n",
      "    if type(data) is dict and \"content\" in data:\n",
      "        if verbose is True:\n",
      "            print(\"Embedding from dictionary\", data[\"content\"])\n",
      "            response = co.embed(texts= data[\"content\"],model='multilingual-22-12')\n",
      "    else:\n",
      "        if len(tokenizer.encode(data)) > MAX_CONTEXT_LENGTH:\n",
      "            raise ValueError(f\" The input is too long for Cohere, num tokens is {len(tokenizer.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\n",
      "        if verbose is True:\n",
      "            print(\"Embedding without preprocessing the input\", data)\n",
      "        response = co.embed(texts=[str(data)],model='multilingual-22-12')\n",
      "    return response.embeddings[0]\n",
      "\n",
      "Function call: type\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: str\n",
      "Related codes: ['\\ndef infer_embeddable_type(column) -> Tuple[EmbeddableType, Callable]:\\n    # Infer the data type of the column\\n    # This will depend on the type of `column` (whether it\\'s a string, Series, etc.)\\n    # Here we\\'ll assume `column` is a pandas Series for simplicity\\n    column_type = str(column.dtype)\\n    print(column_type)\\n    if column_type == \"Utf8\":\\n        # If it\\'s an object, we\\'ll assume it\\'s text\\n        return EmbeddableType.TEXT, OpenAiEmbedder()\\n    elif np.issubdtype(column.dtype, np.number):\\n        # If it\\'s a number, we\\'ll use a different embedding strategy\\n        return EmbeddableType.NUMERIC, numeric_embedder\\n    else:\\n        # For other types, we could throw an error or have a default strategy\\n        raise ValueError(f\"Cannot infer type for column {column.name}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class PolarsGenerator:\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        input_df: Union[pl.DataFrame,str] = 'noinput',\n",
      "        name: str = \"summarizer\",\n",
      "        tokenizer: Optional[Any] = None,\n",
      "        save_path: str = 'batch_generator',\n",
      "        logging_level: int = 10,\n",
      "        api_key: Optional[str] = None,\n",
      "    ) -> None:\n",
      "        #creave save path if it does not exist\n",
      "        if not os.path.exists(save_path):\n",
      "            os.makedirs(save_path)\n",
      "        if isinstance(input_df, pl.DataFrame):\n",
      "            self.load_path = f\"{save_path}/{name}.ndjson\" ## pyright: ignore\n",
      "            input_df.write_ndjson(self.load_path)\n",
      "        elif input_df == 'noinput':\n",
      "            raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n",
      "        elif isinstance(input_df, str):\n",
      "            self.load_path = input_df\n",
      "        else:\n",
      "            raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n",
      "        \n",
      "\n",
      "        # Settings\n",
      "\n",
      "        self.name = name\n",
      "        self.save_path =f\"{save_path}/{self.name}_output.ndjson\"\n",
      "        self.error_path = f\"{save_path}/{self.name}_errors.ndjson\"\n",
      "        self.log_path = f\"{save_path}/{self.name}_log.ndjson\"\n",
      "\n",
      "        self.logging_level = logging_level  \n",
      "        \n",
      "        if tokenizer is None:\n",
      "            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "        else:\n",
      "            self.tokenizer = tokenizer\n",
      "\n",
      "\n",
      "        logging.basicConfig(level=logging_level)\n",
      "        logging.debug(f\"Logging initialized at level {logging_level}\")\n",
      "\n",
      "\n",
      "        # Status Tracker - polars\n",
      "        self.st_model = StatusTrackerModel(name=name)\n",
      "        self.st: pl.DataFrame = pl.DataFrame(self.st_model.dict())\n",
      "        \n",
      "        # loads the frame in advance for usefull checks \n",
      "        self.frame = pl.read_ndjson(self.load_path)\n",
      "\n",
      "        # queues\n",
      "        self.requests_queue = asyncio.Queue()\n",
      "        self.retries_queue = asyncio.Queue()\n",
      "        self.errors_queue = asyncio.Queue()\n",
      "\n",
      "\n",
      "        # api authentication\n",
      "        if api_key is None:\n",
      "            self.api_key =  os.getenv(\"OPENAI_API_KEY\")\n",
      "        else:\n",
      "            self.api_key = api_key\n",
      "        self.request_header = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
      "\n",
      "        logging.debug(f\"Initialization complete.\")\n",
      "\n",
      "    def enqueue_objects(self):\n",
      "        id = 0\n",
      "        with open(self.load_path, 'r') as jsonl_file:\n",
      "            for line in jsonl_file: \n",
      "                line = line.strip()\n",
      "                if not line:\n",
      "                    continue\n",
      "                json_obj = json.loads(line)\n",
      "                request = OpenaiRequest(**json_obj)\n",
      "                self.requests_queue.put_nowait((id,request))\n",
      "                id += 1\n",
      "                self.len_queue = self.requests_queue.qsize()\n",
      "\n",
      "\n",
      "    async def process_objects(self):\n",
      "        while True:\n",
      "                    next_request = None\n",
      "                    if not self.retries_queue.empty():\n",
      "                            next_request = self.retries_queue.get_nowait()\n",
      "\n",
      "                            self.st.replace(\"num_tasks_started\", self.st['num_tasks_started']+1)\n",
      "                            logging.debug(f\"Retrying request: {next_request[0]}\")\n",
      "                            source_queue = self.retries_queue\n",
      "                    elif not self.requests_queue.empty():\n",
      "                            logging.debug(f\"Trying to retrieve next request\")\n",
      "                            next_request = self.requests_queue.get_nowait()\n",
      "\n",
      "                            self.st.replace(\"num_tasks_started\", self.st['num_tasks_started']+1)\n",
      "                            source_queue = self.requests_queue\n",
      "                            logging.info(f'Next request is {next_request[0]} of {self.len_queue}')\n",
      "                            logging.info(f'Respects token limit? {next_request[1].respect_token_limit}')\n",
      "                            if next_request[1].respect_token_limit:\n",
      "                                logging.debug(f\"Reading request: {next_request[0]}\")\n",
      "                            else:\n",
      "                                self.errors_queue.put_nowait(next_request)\n",
      "                    else:\n",
      "                        logging.info(\"Exiting the loop\")\n",
      "                        break\n",
      "\n",
      "                    current_time = time.time()\n",
      "                    seconds_since_update = current_time - self.st['last_update_time'][0]\n",
      "\n",
      "                    if next_request is not None:\n",
      "\n",
      "                        available_request_capacity = pl.Series([min([\n",
      "                            self.st['available_request_capacity'][0] + next_request[1].max_requests_per_minute * seconds_since_update / 60.0,\n",
      "                            next_request[1].max_requests_per_minute\n",
      "                        ])])\n",
      "                        self.st.replace(\"available_request_capacity\", available_request_capacity)\n",
      "                        logging.info(f\"Task number {next_request[0]} available_request_capacity: {self.st['available_request_capacity'][0]}\")\n",
      "\n",
      "\n",
      "                        available_token_capacity = pl.Series([min([\n",
      "                            self.st['available_token_capacity'][0] + next_request[1].max_tokens_per_minute * seconds_since_update / 60.0,\n",
      "                            next_request[1].max_tokens_per_minute\n",
      "                        ])])\n",
      "                        self.st.replace(\"available_token_capacity\", available_token_capacity)\n",
      "                        logging.info(f\"Task number {next_request[0]} available_token_capacity: {self.st['available_token_capacity'][0]}\")\n",
      "\n",
      "                        next_request_tokens = next_request[1].total_tokens\n",
      "\n",
      "                        logging.info(f\"Next request tokens is {next_request_tokens}\")\n",
      "\n",
      "                        if (\n",
      "                            self.st['available_request_capacity'][0] >= 1\n",
      "                            and self.st['available_token_capacity'][0] >= next_request_tokens\n",
      "                        ):\n",
      "                            # update counters\n",
      "                            self.st.replace(\"available_request_capacity\", available_request_capacity-1)\n",
      "                            self.st.replace(\"available_token_capacity\", available_token_capacity-next_request_tokens)\n",
      "                            logging.info(f\"Available_token_capacity changed to  {self.st['available_token_capacity'][0]}\")\n",
      "                  \n",
      "\n",
      "                            # call API\n",
      "                            try:\n",
      "                                logging.info(f\"Calling Api for {next_request[0]}\")\n",
      "                                start_time = int(time.time())\n",
      "                                async with aiohttp.ClientSession() as session:\n",
      "                                    async with session.post(\n",
      "                                    url=next_request[1].url, headers=self.request_header, json=next_request[1].body\n",
      "                                    ) as response:\n",
      "                                        response = await response.json()\n",
      "                                if \"error\" in response:\n",
      "                                    logging.warning(\n",
      "                                        f\"Request {next_request[0]} failed with error {response['error']['message']}\"\n",
      "                                    )\n",
      "                                    \n",
      "                                    if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                                        self.st.replace(\"time_of_last_rate_limit_error\", pl.Series([time.time()]))\n",
      "                                        self.st.replace(\"num_rate_limit_errors\", self.st['num_rate_limit_errors']+1)\n",
      "                                        self.retries_queue.put_nowait(next_request)\n",
      "                                        \n",
      "\n",
      "                                    elif \"currently overloaded\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                                        self.st.replace(\"num_overloaded_errors\", self.st['num_overloaded_errors']+1)\n",
      "                                        self.retries_queue.put_nowait(next_request)\n",
      "                                    else:\n",
      "                                        self.st.replace(\"num_api_errors\", self.st['num_api_errors']+1)\n",
      "                                        json_string = json.dumps(response)\n",
      "                                        with open(self.error_path, \"a\") as f:\n",
      "                                            f.write(json_string + \"\\n\")\n",
      "                                else:\n",
      "                                    output = ''\n",
      "                                    if next_request[1].request_type == 'chat':\n",
      "                                        output = {\n",
      "                                            'id': next_request[0],\n",
      "                                            'start_time': start_time,\n",
      "                                            'output': response['choices'][0]['message']['content'],\n",
      "                                            'prompt_tokens': response['usage']['prompt_tokens'],\n",
      "                                            'completion_tokens': response['usage']['completion_tokens'],\n",
      "                                            'total_tokens': response['usage']['total_tokens'],\n",
      "                                            'end_time': response['created']\n",
      "                                        }\n",
      "                                    elif next_request[1].request_type == 'embedding':\n",
      "\n",
      "                                        output = {\n",
      "                                            'id': next_request[0],\n",
      "                                            'start_time': start_time,\n",
      "                                            'output': response['data'][0]['embedding'],\n",
      "                                            'prompt_tokens': response['usage']['prompt_tokens'],\n",
      "                                            'total_tokens': response['usage']['total_tokens'],\n",
      "                                            'end_time': time.time()\n",
      "                                        }\n",
      "\n",
      "                                    json_string = json.dumps(output)\n",
      "\n",
      "                                    with open(self.save_path, \"a\") as f:\n",
      "                                        f.write(json_string + \"\\n\")\n",
      "\n",
      "                                    total_tokens = response['usage']['total_tokens']\n",
      "                                    self.st.replace(\"available_token_capacity\", available_token_capacity+total_tokens)\n",
      "                                    self.st.replace(\"available_request_capacity\", available_request_capacity+1)\n",
      "\n",
      "                            except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "                                logging.warning(f\"Request {next_request[0]} failed with Exception {e}\")\n",
      "                                self.st.replace(\"num_other_errors\", self.st['num_other_errors']+1)\n",
      "                                self.st.replace(\"available_token_capacity\", available_token_capacity+next_request_tokens)\n",
      "                                self.st.replace(\"available_request_capacity\", available_request_capacity+1)\n",
      "                                json_string = json.dumps(str(e))\n",
      "                                with open(self.error_path, \"a\") as f:\n",
      "                                    f.write(json_string + \"\\n\")\n",
      "\n",
      "                            finally:\n",
      "                                if source_queue is not None:\n",
      "                                    source_queue.task_done()\n",
      "\n",
      "\n",
      "\n",
      "                    await asyncio.sleep(self.st['seconds_to_sleep_each_loop'][0])\n",
      "\n",
      "                    # if a rate limit error was hit recently, pause to cool down\n",
      "                    seconds_since_rate_limit_error = (time.time() - self.st['time_of_last_rate_limit_error'][0])\n",
      "                    if seconds_since_rate_limit_error < self.st['seconds_to_pause_after_rate_limit_error'][0]:\n",
      "                        remaining_seconds_to_pause = (self.st['seconds_to_pause_after_rate_limit_error'][0] - seconds_since_rate_limit_error)\n",
      "                        await asyncio.sleep(remaining_seconds_to_pause)\n",
      "                        # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n",
      "                        logging.warn(f\"Pausing to cool down until {time.ctime(self.st['time_of_last_rate_limit_error'][0] + self.st['seconds_to_pause_after_rate_limit_error'][0])}\")\n",
      "\n",
      "\n",
      "\n",
      "        logging.info(f\"\"\"Parallel processing complete. Results saved to {self.save_path}\"\"\")\n",
      "        if self.st['num_tasks_failed'][0] > 0:\n",
      "            logging.warning(f\"{self.st['num_tasks_failed'][0]} / {self.st['num_tasks_started'][0]} requests failed. Errors logged to {self.log_path}.\")\n",
      "        if self.st['num_rate_limit_errors'][0] > 0:\n",
      "            logging.warning(f\"{self.st['num_rate_limit_errors'][0]} rate limit errors received. Consider running at a lower rate.\")\n",
      "        \n",
      "        self.st.write_ndjson(self.log_path)\n",
      "        print(self.st)\n",
      "\n",
      "                    \n",
      "\n",
      "\n",
      "\n",
      "    async def main(self):\n",
      "        logging.debug(f\"Entering main loop\")\n",
      "        self.enqueue_objects()\n",
      "        consumers = [asyncio.create_task(self.process_objects()) for _ in range(7)]\n",
      "        await self.requests_queue.join()\n",
      "        await self.retries_queue.join()\n",
      "        for consumer in consumers:\n",
      "            consumer.cancel()\n",
      "\n",
      "    def execute(self):\n",
      "        asyncio.run(self.main())\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: TypeError\n",
      "Function call: isinstance\n",
      "Function call: TypeError\n",
      "Function call: StatusTrackerModel\n",
      "Function call: open\n",
      "Function call: OpenaiRequest\n",
      "Function call: min\n",
      "Function call: min\n",
      "Function call: int\n",
      "Function call: open\n",
      "Function call: open\n",
      "Function call: str\n",
      "Function call: open\n",
      "Function call: print\n",
      "Function call: range\n",
      "Related codes: ['\\n\\nclass StatusTrackerModel(BaseModel):\\n    name: str\\n    max_attempts: int = 5\\n    seconds_to_pause_after_rate_limit_error: int = 10\\n    seconds_to_sleep_each_loop: float  = 0.001\\n    available_request_capacity: int = 1500\\n    available_token_capacity: int = 625000\\n    last_update_time: float = now()\\n    num_rate_limit_errors: int = 0\\n    num_overloaded_errors: int = 0\\n    time_of_last_rate_limit_error: float = 0.0\\n    num_tasks_failed: int = 0\\n    num_tasks_started: int = 0\\n    num_api_errors: int = 0\\n    num_other_errors: int = 0 \\n', '\\n\\nclass OpenaiRequest:\\n    def __init__(\\n        self, \\n        **parameters\\n        ) -> None:\\n\\n        if (\\'input\\' in parameters):  \\n            self.request_type = \\'embedding\\'\\n        else:\\n            self.request_type = \\'chat\\'\\n        \\n        self.body = OpenaiRequestModel(\\n                model=parameters.get(\\'model\\',\\'gpt-3.5-turbo\\'),\\n                input=parameters.get(\\'input\\'),\\n                messages=parameters.get(\\'messages\\', [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]) ,\\n                function=parameters.get(\\'function\\'),\\n                function_call=parameters.get(\\'function_call\\'),\\n                temperature=parameters.get(\\'temperature\\'),\\n                top_p=parameters.get(\\'top_p\\'),\\n                n=parameters.get(\\'n\\',1),\\n                stream=parameters.get(\\'stream\\'),\\n                stop=parameters.get(\\'stop\\'),\\n                max_tokens=parameters.get(\\'max_tokens\\',4000),\\n                presence_penalty=parameters.get(\\'presence_penalty\\'),\\n                frequency_penalty=parameters.get(\\'frequency_penalty\\'),\\n                logit_bias=parameters.get(\\'logit_bias\\'),\\n                user=parameters.get(\\'user\\')\\n        )\\n\\n        self.body = {k: v for k, v in self.body.dict().items() if v is not None}\\n        if self.request_type == \\'embedding\\':\\n            keys_to_remove = [\"n\", \"max_tokens\"]\\n            self.body = {k: v for k, v in self.body.items() if k not in keys_to_remove}\\n\\n        \"\"\"Count the number of tokens in the request. Only supports chat and embedding requests.\"\"\"\\n        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n        self.total_tokens = 0\\n        self.respect_token_limit = False\\n        self.max_tokens_per_minute = 0\\n        self.max_requests_per_minute = 0\\n        self.url = \\'\\'\\n\\n        # if completions request, tokens = prompt + n * max_tokens\\n        if self.request_type == \\'chat\\':\\n            max_tokens = self.body[\\'max_tokens\\']  \\n            n = self.body[\\'n\\'] # pyright: ignore\\n            completion_tokens = n * max_tokens # pyright: ignore\\n            for message in self.body[\"messages\"]: # pyright: ignore\\n                self.total_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    self.total_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        self.total_tokens -= 1  # role is always required and always 1 token\\n                    self.total_tokens += 2  # every reply is primed with <im_start>assistant\\n                self.total_tokens =  self.total_tokens + completion_tokens\\n            \\n        # if embeddings request, tokens = input tokens\\n        elif self.request_type == \"embedding\":\\n            if isinstance(self.body[\\'input\\'], str):  # single input # pyright: ignore\\n                self.total_tokens = len(encoding.encode(self.body[\"input\"]))  # pyright: ignore\\n            elif isinstance(input, list):  # multiple inputs\\n                self.total_tokens = sum([len(encoding.encode(i)) for i in self.body[\"input\"]]) # pyright: ignore\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n\\n\\n        match self.body[\\'model\\']: # pyright: ignore\\n            case \\'gpt-3.5-turbo\\':\\n                if self.total_tokens < 10000:\\n                    self.respect_token_limit = True\\n                    self.max_requests_per_minute = 3000\\n                    self.max_tokens_per_minute= 1000000\\n                    self.url = \\'https://api.openai.com/v1/chat/completions\\'\\n                else:\\n                    self.respect_token_limit = False\\n                    self.max_requests_per_minute = 0\\n                    self.max_tokens_per_minute= 0\\n\\n            case \\'gpt-3.5-turbo-16k-0613\\':\\n                if self.total_tokens < 16000:\\n                    self.respect_token_limit = True\\n                    self.max_requests_per_minute = 3000\\n                    self.max_tokens_per_minute= 1000000\\n                    self.url = \\'https://api.openai.com/v1/chat/completions\\'\\n                else:\\n                    self.respect_token_limit = False\\n                    self.max_requests_per_minute = 0\\n                    self.max_tokens_per_minute= 0\\n\\n            case \\'gpt-4\\':\\n                if self.total_tokens < 16000:\\n                    self.respect_token_limit = True\\n                    self.max_requests_per_minute = 3000\\n                    self.max_tokens_per_minute= 1000000\\n                    self.url = \\'https://api.openai.com/v1/chat/completions\\'\\n                else:\\n                    self.respect_token_limit = False\\n                    self.max_requests_per_minute = 0\\n                    self.max_tokens_per_minute= 0\\n\\n\\n            case \\'text-embedding-ada-002\\':\\n                if self.total_tokens < 10000:\\n                    self.respect_token_limit = True\\n                    self.max_tokens_per_minute = 1000000\\n                    self.max_requests_per_minute = 3000\\n                    self.url = \\'https://api.openai.com/v1/embeddings\\'\\n                else:\\n                    self.respect_token_limit = False              \\n                    self.max_requests_per_minute = 0\\n                    self.max_tokens_per_minute= 0\\n\\n\\n                \\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    input_df: Union[pl.DataFrame,str] = 'noinput',\n",
      "    name: str = \"summarizer\",\n",
      "    tokenizer: Optional[Any] = None,\n",
      "    save_path: str = 'batch_generator',\n",
      "    logging_level: int = 10,\n",
      "    api_key: Optional[str] = None,\n",
      ") -> None:\n",
      "    #creave save path if it does not exist\n",
      "    if not os.path.exists(save_path):\n",
      "        os.makedirs(save_path)\n",
      "    if isinstance(input_df, pl.DataFrame):\n",
      "        self.load_path = f\"{save_path}/{name}.ndjson\" ## pyright: ignore\n",
      "        input_df.write_ndjson(self.load_path)\n",
      "    elif input_df == 'noinput':\n",
      "        raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n",
      "    elif isinstance(input_df, str):\n",
      "        self.load_path = input_df\n",
      "    else:\n",
      "        raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n",
      "    \n",
      "\n",
      "    # Settings\n",
      "\n",
      "    self.name = name\n",
      "    self.save_path =f\"{save_path}/{self.name}_output.ndjson\"\n",
      "    self.error_path = f\"{save_path}/{self.name}_errors.ndjson\"\n",
      "    self.log_path = f\"{save_path}/{self.name}_log.ndjson\"\n",
      "\n",
      "    self.logging_level = logging_level  \n",
      "    \n",
      "    if tokenizer is None:\n",
      "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "    else:\n",
      "        self.tokenizer = tokenizer\n",
      "\n",
      "\n",
      "    logging.basicConfig(level=logging_level)\n",
      "    logging.debug(f\"Logging initialized at level {logging_level}\")\n",
      "\n",
      "\n",
      "    # Status Tracker - polars\n",
      "    self.st_model = StatusTrackerModel(name=name)\n",
      "    self.st: pl.DataFrame = pl.DataFrame(self.st_model.dict())\n",
      "    \n",
      "    # loads the frame in advance for usefull checks \n",
      "    self.frame = pl.read_ndjson(self.load_path)\n",
      "\n",
      "    # queues\n",
      "    self.requests_queue = asyncio.Queue()\n",
      "    self.retries_queue = asyncio.Queue()\n",
      "    self.errors_queue = asyncio.Queue()\n",
      "\n",
      "\n",
      "    # api authentication\n",
      "    if api_key is None:\n",
      "        self.api_key =  os.getenv(\"OPENAI_API_KEY\")\n",
      "    else:\n",
      "        self.api_key = api_key\n",
      "    self.request_header = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
      "\n",
      "    logging.debug(f\"Initialization complete.\")\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: TypeError\n",
      "Function call: isinstance\n",
      "Function call: TypeError\n",
      "Function call: StatusTrackerModel\n",
      "Related codes: ['\\n\\nclass StatusTrackerModel(BaseModel):\\n    name: str\\n    max_attempts: int = 5\\n    seconds_to_pause_after_rate_limit_error: int = 10\\n    seconds_to_sleep_each_loop: float  = 0.001\\n    available_request_capacity: int = 1500\\n    available_token_capacity: int = 625000\\n    last_update_time: float = now()\\n    num_rate_limit_errors: int = 0\\n    num_overloaded_errors: int = 0\\n    time_of_last_rate_limit_error: float = 0.0\\n    num_tasks_failed: int = 0\\n    num_tasks_started: int = 0\\n    num_api_errors: int = 0\\n    num_other_errors: int = 0 \\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def enqueue_objects(self):\n",
      "    id = 0\n",
      "    with open(self.load_path, 'r') as jsonl_file:\n",
      "        for line in jsonl_file: \n",
      "            line = line.strip()\n",
      "            if not line:\n",
      "                continue\n",
      "            json_obj = json.loads(line)\n",
      "            request = OpenaiRequest(**json_obj)\n",
      "            self.requests_queue.put_nowait((id,request))\n",
      "            id += 1\n",
      "            self.len_queue = self.requests_queue.qsize()\n",
      "\n",
      "Function call: open\n",
      "Function call: OpenaiRequest\n",
      "Related codes: ['\\n\\nclass OpenaiRequest:\\n    def __init__(\\n        self, \\n        **parameters\\n        ) -> None:\\n\\n        if (\\'input\\' in parameters):  \\n            self.request_type = \\'embedding\\'\\n        else:\\n            self.request_type = \\'chat\\'\\n        \\n        self.body = OpenaiRequestModel(\\n                model=parameters.get(\\'model\\',\\'gpt-3.5-turbo\\'),\\n                input=parameters.get(\\'input\\'),\\n                messages=parameters.get(\\'messages\\', [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]) ,\\n                function=parameters.get(\\'function\\'),\\n                function_call=parameters.get(\\'function_call\\'),\\n                temperature=parameters.get(\\'temperature\\'),\\n                top_p=parameters.get(\\'top_p\\'),\\n                n=parameters.get(\\'n\\',1),\\n                stream=parameters.get(\\'stream\\'),\\n                stop=parameters.get(\\'stop\\'),\\n                max_tokens=parameters.get(\\'max_tokens\\',4000),\\n                presence_penalty=parameters.get(\\'presence_penalty\\'),\\n                frequency_penalty=parameters.get(\\'frequency_penalty\\'),\\n                logit_bias=parameters.get(\\'logit_bias\\'),\\n                user=parameters.get(\\'user\\')\\n        )\\n\\n        self.body = {k: v for k, v in self.body.dict().items() if v is not None}\\n        if self.request_type == \\'embedding\\':\\n            keys_to_remove = [\"n\", \"max_tokens\"]\\n            self.body = {k: v for k, v in self.body.items() if k not in keys_to_remove}\\n\\n        \"\"\"Count the number of tokens in the request. Only supports chat and embedding requests.\"\"\"\\n        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n        self.total_tokens = 0\\n        self.respect_token_limit = False\\n        self.max_tokens_per_minute = 0\\n        self.max_requests_per_minute = 0\\n        self.url = \\'\\'\\n\\n        # if completions request, tokens = prompt + n * max_tokens\\n        if self.request_type == \\'chat\\':\\n            max_tokens = self.body[\\'max_tokens\\']  \\n            n = self.body[\\'n\\'] # pyright: ignore\\n            completion_tokens = n * max_tokens # pyright: ignore\\n            for message in self.body[\"messages\"]: # pyright: ignore\\n                self.total_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    self.total_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        self.total_tokens -= 1  # role is always required and always 1 token\\n                    self.total_tokens += 2  # every reply is primed with <im_start>assistant\\n                self.total_tokens =  self.total_tokens + completion_tokens\\n            \\n        # if embeddings request, tokens = input tokens\\n        elif self.request_type == \"embedding\":\\n            if isinstance(self.body[\\'input\\'], str):  # single input # pyright: ignore\\n                self.total_tokens = len(encoding.encode(self.body[\"input\"]))  # pyright: ignore\\n            elif isinstance(input, list):  # multiple inputs\\n                self.total_tokens = sum([len(encoding.encode(i)) for i in self.body[\"input\"]]) # pyright: ignore\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n\\n\\n        match self.body[\\'model\\']: # pyright: ignore\\n            case \\'gpt-3.5-turbo\\':\\n                if self.total_tokens < 10000:\\n                    self.respect_token_limit = True\\n                    self.max_requests_per_minute = 3000\\n                    self.max_tokens_per_minute= 1000000\\n                    self.url = \\'https://api.openai.com/v1/chat/completions\\'\\n                else:\\n                    self.respect_token_limit = False\\n                    self.max_requests_per_minute = 0\\n                    self.max_tokens_per_minute= 0\\n\\n            case \\'gpt-3.5-turbo-16k-0613\\':\\n                if self.total_tokens < 16000:\\n                    self.respect_token_limit = True\\n                    self.max_requests_per_minute = 3000\\n                    self.max_tokens_per_minute= 1000000\\n                    self.url = \\'https://api.openai.com/v1/chat/completions\\'\\n                else:\\n                    self.respect_token_limit = False\\n                    self.max_requests_per_minute = 0\\n                    self.max_tokens_per_minute= 0\\n\\n            case \\'gpt-4\\':\\n                if self.total_tokens < 16000:\\n                    self.respect_token_limit = True\\n                    self.max_requests_per_minute = 3000\\n                    self.max_tokens_per_minute= 1000000\\n                    self.url = \\'https://api.openai.com/v1/chat/completions\\'\\n                else:\\n                    self.respect_token_limit = False\\n                    self.max_requests_per_minute = 0\\n                    self.max_tokens_per_minute= 0\\n\\n\\n            case \\'text-embedding-ada-002\\':\\n                if self.total_tokens < 10000:\\n                    self.respect_token_limit = True\\n                    self.max_tokens_per_minute = 1000000\\n                    self.max_requests_per_minute = 3000\\n                    self.url = \\'https://api.openai.com/v1/embeddings\\'\\n                else:\\n                    self.respect_token_limit = False              \\n                    self.max_requests_per_minute = 0\\n                    self.max_tokens_per_minute= 0\\n\\n\\n                \\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "async def process_objects(self):\n",
      "    while True:\n",
      "                next_request = None\n",
      "                if not self.retries_queue.empty():\n",
      "                        next_request = self.retries_queue.get_nowait()\n",
      "\n",
      "                        self.st.replace(\"num_tasks_started\", self.st['num_tasks_started']+1)\n",
      "                        logging.debug(f\"Retrying request: {next_request[0]}\")\n",
      "                        source_queue = self.retries_queue\n",
      "                elif not self.requests_queue.empty():\n",
      "                        logging.debug(f\"Trying to retrieve next request\")\n",
      "                        next_request = self.requests_queue.get_nowait()\n",
      "\n",
      "                        self.st.replace(\"num_tasks_started\", self.st['num_tasks_started']+1)\n",
      "                        source_queue = self.requests_queue\n",
      "                        logging.info(f'Next request is {next_request[0]} of {self.len_queue}')\n",
      "                        logging.info(f'Respects token limit? {next_request[1].respect_token_limit}')\n",
      "                        if next_request[1].respect_token_limit:\n",
      "                            logging.debug(f\"Reading request: {next_request[0]}\")\n",
      "                        else:\n",
      "                            self.errors_queue.put_nowait(next_request)\n",
      "                else:\n",
      "                    logging.info(\"Exiting the loop\")\n",
      "                    break\n",
      "\n",
      "                current_time = time.time()\n",
      "                seconds_since_update = current_time - self.st['last_update_time'][0]\n",
      "\n",
      "                if next_request is not None:\n",
      "\n",
      "                    available_request_capacity = pl.Series([min([\n",
      "                        self.st['available_request_capacity'][0] + next_request[1].max_requests_per_minute * seconds_since_update / 60.0,\n",
      "                        next_request[1].max_requests_per_minute\n",
      "                    ])])\n",
      "                    self.st.replace(\"available_request_capacity\", available_request_capacity)\n",
      "                    logging.info(f\"Task number {next_request[0]} available_request_capacity: {self.st['available_request_capacity'][0]}\")\n",
      "\n",
      "\n",
      "                    available_token_capacity = pl.Series([min([\n",
      "                        self.st['available_token_capacity'][0] + next_request[1].max_tokens_per_minute * seconds_since_update / 60.0,\n",
      "                        next_request[1].max_tokens_per_minute\n",
      "                    ])])\n",
      "                    self.st.replace(\"available_token_capacity\", available_token_capacity)\n",
      "                    logging.info(f\"Task number {next_request[0]} available_token_capacity: {self.st['available_token_capacity'][0]}\")\n",
      "\n",
      "                    next_request_tokens = next_request[1].total_tokens\n",
      "\n",
      "                    logging.info(f\"Next request tokens is {next_request_tokens}\")\n",
      "\n",
      "                    if (\n",
      "                        self.st['available_request_capacity'][0] >= 1\n",
      "                        and self.st['available_token_capacity'][0] >= next_request_tokens\n",
      "                    ):\n",
      "                        # update counters\n",
      "                        self.st.replace(\"available_request_capacity\", available_request_capacity-1)\n",
      "                        self.st.replace(\"available_token_capacity\", available_token_capacity-next_request_tokens)\n",
      "                        logging.info(f\"Available_token_capacity changed to  {self.st['available_token_capacity'][0]}\")\n",
      "                  \n",
      "\n",
      "                        # call API\n",
      "                        try:\n",
      "                            logging.info(f\"Calling Api for {next_request[0]}\")\n",
      "                            start_time = int(time.time())\n",
      "                            async with aiohttp.ClientSession() as session:\n",
      "                                async with session.post(\n",
      "                                url=next_request[1].url, headers=self.request_header, json=next_request[1].body\n",
      "                                ) as response:\n",
      "                                    response = await response.json()\n",
      "                            if \"error\" in response:\n",
      "                                logging.warning(\n",
      "                                    f\"Request {next_request[0]} failed with error {response['error']['message']}\"\n",
      "                                )\n",
      "                                \n",
      "                                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                                    self.st.replace(\"time_of_last_rate_limit_error\", pl.Series([time.time()]))\n",
      "                                    self.st.replace(\"num_rate_limit_errors\", self.st['num_rate_limit_errors']+1)\n",
      "                                    self.retries_queue.put_nowait(next_request)\n",
      "                                    \n",
      "\n",
      "                                elif \"currently overloaded\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                                    self.st.replace(\"num_overloaded_errors\", self.st['num_overloaded_errors']+1)\n",
      "                                    self.retries_queue.put_nowait(next_request)\n",
      "                                else:\n",
      "                                    self.st.replace(\"num_api_errors\", self.st['num_api_errors']+1)\n",
      "                                    json_string = json.dumps(response)\n",
      "                                    with open(self.error_path, \"a\") as f:\n",
      "                                        f.write(json_string + \"\\n\")\n",
      "                            else:\n",
      "                                output = ''\n",
      "                                if next_request[1].request_type == 'chat':\n",
      "                                    output = {\n",
      "                                        'id': next_request[0],\n",
      "                                        'start_time': start_time,\n",
      "                                        'output': response['choices'][0]['message']['content'],\n",
      "                                        'prompt_tokens': response['usage']['prompt_tokens'],\n",
      "                                        'completion_tokens': response['usage']['completion_tokens'],\n",
      "                                        'total_tokens': response['usage']['total_tokens'],\n",
      "                                        'end_time': response['created']\n",
      "                                    }\n",
      "                                elif next_request[1].request_type == 'embedding':\n",
      "\n",
      "                                    output = {\n",
      "                                        'id': next_request[0],\n",
      "                                        'start_time': start_time,\n",
      "                                        'output': response['data'][0]['embedding'],\n",
      "                                        'prompt_tokens': response['usage']['prompt_tokens'],\n",
      "                                        'total_tokens': response['usage']['total_tokens'],\n",
      "                                        'end_time': time.time()\n",
      "                                    }\n",
      "\n",
      "                                json_string = json.dumps(output)\n",
      "\n",
      "                                with open(self.save_path, \"a\") as f:\n",
      "                                    f.write(json_string + \"\\n\")\n",
      "\n",
      "                                total_tokens = response['usage']['total_tokens']\n",
      "                                self.st.replace(\"available_token_capacity\", available_token_capacity+total_tokens)\n",
      "                                self.st.replace(\"available_request_capacity\", available_request_capacity+1)\n",
      "\n",
      "                        except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "                            logging.warning(f\"Request {next_request[0]} failed with Exception {e}\")\n",
      "                            self.st.replace(\"num_other_errors\", self.st['num_other_errors']+1)\n",
      "                            self.st.replace(\"available_token_capacity\", available_token_capacity+next_request_tokens)\n",
      "                            self.st.replace(\"available_request_capacity\", available_request_capacity+1)\n",
      "                            json_string = json.dumps(str(e))\n",
      "                            with open(self.error_path, \"a\") as f:\n",
      "                                f.write(json_string + \"\\n\")\n",
      "\n",
      "                        finally:\n",
      "                            if source_queue is not None:\n",
      "                                source_queue.task_done()\n",
      "\n",
      "\n",
      "\n",
      "                await asyncio.sleep(self.st['seconds_to_sleep_each_loop'][0])\n",
      "\n",
      "                # if a rate limit error was hit recently, pause to cool down\n",
      "                seconds_since_rate_limit_error = (time.time() - self.st['time_of_last_rate_limit_error'][0])\n",
      "                if seconds_since_rate_limit_error < self.st['seconds_to_pause_after_rate_limit_error'][0]:\n",
      "                    remaining_seconds_to_pause = (self.st['seconds_to_pause_after_rate_limit_error'][0] - seconds_since_rate_limit_error)\n",
      "                    await asyncio.sleep(remaining_seconds_to_pause)\n",
      "                    # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n",
      "                    logging.warn(f\"Pausing to cool down until {time.ctime(self.st['time_of_last_rate_limit_error'][0] + self.st['seconds_to_pause_after_rate_limit_error'][0])}\")\n",
      "\n",
      "\n",
      "\n",
      "    logging.info(f\"\"\"Parallel processing complete. Results saved to {self.save_path}\"\"\")\n",
      "    if self.st['num_tasks_failed'][0] > 0:\n",
      "        logging.warning(f\"{self.st['num_tasks_failed'][0]} / {self.st['num_tasks_started'][0]} requests failed. Errors logged to {self.log_path}.\")\n",
      "    if self.st['num_rate_limit_errors'][0] > 0:\n",
      "        logging.warning(f\"{self.st['num_rate_limit_errors'][0]} rate limit errors received. Consider running at a lower rate.\")\n",
      "    \n",
      "    self.st.write_ndjson(self.log_path)\n",
      "    print(self.st)\n",
      "\n",
      "                \n",
      "\n",
      "Function call: min\n",
      "Function call: min\n",
      "Function call: int\n",
      "Function call: open\n",
      "Function call: open\n",
      "Function call: str\n",
      "Function call: open\n",
      "Function call: print\n",
      "Related codes: ['\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\n@field_validator(\\'joint_alphabet\\')\\ndef check_joint_alphabet(cls, v, info):\\n    if v is not None and \"values\" in info.data:\\n        expected_tuple_length = len(info.data[\"values\"][0].value)\\n        for item in v:\\n            if not isinstance(item, tuple) or len(item) != expected_tuple_length:\\n                raise ValueError(f\"Each element in \\'joint_alphabet\\' should be a tuple of length {expected_tuple_length}.\")\\n            for dim_value, dim_alphabet in zip(item, [value.alphabet for value in info.data[\"values\"][0].value]):\\n                if dim_alphabet is not None and dim_value not in dim_alphabet:\\n                    raise ValueError(f\"Value {dim_value} is not in the alphabet for its dimension.\")\\n    return v\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "async def main(self):\n",
      "    logging.debug(f\"Entering main loop\")\n",
      "    self.enqueue_objects()\n",
      "    consumers = [asyncio.create_task(self.process_objects()) for _ in range(7)]\n",
      "    await self.requests_queue.join()\n",
      "    await self.retries_queue.join()\n",
      "    for consumer in consumers:\n",
      "        consumer.cancel()\n",
      "\n",
      "Function call: range\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def execute(self):\n",
      "    asyncio.run(self.main())\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class OpenaiRequest:\n",
      "    def __init__(\n",
      "        self, \n",
      "        **parameters\n",
      "        ) -> None:\n",
      "\n",
      "        if ('input' in parameters):  \n",
      "            self.request_type = 'embedding'\n",
      "        else:\n",
      "            self.request_type = 'chat'\n",
      "        \n",
      "        self.body = OpenaiRequestModel(\n",
      "                model=parameters.get('model','gpt-3.5-turbo'),\n",
      "                input=parameters.get('input'),\n",
      "                messages=parameters.get('messages', [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]) ,\n",
      "                function=parameters.get('function'),\n",
      "                function_call=parameters.get('function_call'),\n",
      "                temperature=parameters.get('temperature'),\n",
      "                top_p=parameters.get('top_p'),\n",
      "                n=parameters.get('n',1),\n",
      "                stream=parameters.get('stream'),\n",
      "                stop=parameters.get('stop'),\n",
      "                max_tokens=parameters.get('max_tokens',4000),\n",
      "                presence_penalty=parameters.get('presence_penalty'),\n",
      "                frequency_penalty=parameters.get('frequency_penalty'),\n",
      "                logit_bias=parameters.get('logit_bias'),\n",
      "                user=parameters.get('user')\n",
      "        )\n",
      "\n",
      "        self.body = {k: v for k, v in self.body.dict().items() if v is not None}\n",
      "        if self.request_type == 'embedding':\n",
      "            keys_to_remove = [\"n\", \"max_tokens\"]\n",
      "            self.body = {k: v for k, v in self.body.items() if k not in keys_to_remove}\n",
      "\n",
      "        \"\"\"Count the number of tokens in the request. Only supports chat and embedding requests.\"\"\"\n",
      "        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "        self.total_tokens = 0\n",
      "        self.respect_token_limit = False\n",
      "        self.max_tokens_per_minute = 0\n",
      "        self.max_requests_per_minute = 0\n",
      "        self.url = ''\n",
      "\n",
      "        # if completions request, tokens = prompt + n * max_tokens\n",
      "        if self.request_type == 'chat':\n",
      "            max_tokens = self.body['max_tokens']  \n",
      "            n = self.body['n'] # pyright: ignore\n",
      "            completion_tokens = n * max_tokens # pyright: ignore\n",
      "            for message in self.body[\"messages\"]: # pyright: ignore\n",
      "                self.total_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
      "                for key, value in message.items():\n",
      "                    self.total_tokens += len(encoding.encode(value))\n",
      "                    if key == \"name\":  # if there's a name, the role is omitted\n",
      "                        self.total_tokens -= 1  # role is always required and always 1 token\n",
      "                    self.total_tokens += 2  # every reply is primed with <im_start>assistant\n",
      "                self.total_tokens =  self.total_tokens + completion_tokens\n",
      "            \n",
      "        # if embeddings request, tokens = input tokens\n",
      "        elif self.request_type == \"embedding\":\n",
      "            if isinstance(self.body['input'], str):  # single input # pyright: ignore\n",
      "                self.total_tokens = len(encoding.encode(self.body[\"input\"]))  # pyright: ignore\n",
      "            elif isinstance(input, list):  # multiple inputs\n",
      "                self.total_tokens = sum([len(encoding.encode(i)) for i in self.body[\"input\"]]) # pyright: ignore\n",
      "            else:\n",
      "                raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n",
      "\n",
      "\n",
      "        match self.body['model']: # pyright: ignore\n",
      "            case 'gpt-3.5-turbo':\n",
      "                if self.total_tokens < 10000:\n",
      "                    self.respect_token_limit = True\n",
      "                    self.max_requests_per_minute = 3000\n",
      "                    self.max_tokens_per_minute= 1000000\n",
      "                    self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "                else:\n",
      "                    self.respect_token_limit = False\n",
      "                    self.max_requests_per_minute = 0\n",
      "                    self.max_tokens_per_minute= 0\n",
      "\n",
      "            case 'gpt-3.5-turbo-16k-0613':\n",
      "                if self.total_tokens < 16000:\n",
      "                    self.respect_token_limit = True\n",
      "                    self.max_requests_per_minute = 3000\n",
      "                    self.max_tokens_per_minute= 1000000\n",
      "                    self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "                else:\n",
      "                    self.respect_token_limit = False\n",
      "                    self.max_requests_per_minute = 0\n",
      "                    self.max_tokens_per_minute= 0\n",
      "\n",
      "            case 'gpt-4':\n",
      "                if self.total_tokens < 16000:\n",
      "                    self.respect_token_limit = True\n",
      "                    self.max_requests_per_minute = 3000\n",
      "                    self.max_tokens_per_minute= 1000000\n",
      "                    self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "                else:\n",
      "                    self.respect_token_limit = False\n",
      "                    self.max_requests_per_minute = 0\n",
      "                    self.max_tokens_per_minute= 0\n",
      "\n",
      "\n",
      "            case 'text-embedding-ada-002':\n",
      "                if self.total_tokens < 10000:\n",
      "                    self.respect_token_limit = True\n",
      "                    self.max_tokens_per_minute = 1000000\n",
      "                    self.max_requests_per_minute = 3000\n",
      "                    self.url = 'https://api.openai.com/v1/embeddings'\n",
      "                else:\n",
      "                    self.respect_token_limit = False              \n",
      "                    self.max_requests_per_minute = 0\n",
      "                    self.max_tokens_per_minute= 0\n",
      "\n",
      "\n",
      "                \n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: OpenaiRequestModel\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: sum\n",
      "Function call: len\n",
      "Function call: TypeError\n",
      "Related codes: ['\\n\\n\\nclass OpenaiRequestModel(BaseModel):\\n    model: str\\n    input: Union[str,List,None]\\n    messages: Union[List,None]\\n    function: Union[List,None]\\n    function_call: Union[str,Any,None]\\n    temperature: Union[float,None]\\n    top_p: Union[float,None]\\n    n: Union[int,None]\\n    stream: Union[bool,None]\\n    stop: Union[str,List,None]\\n    max_tokens: Union[int,None]\\n    presence_penalty: Union[float,None]\\n    frequency_penalty: Union[float,None]\\n    logit_bias: Union[Mapping,None]\\n    user: Union[str,None]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self, \n",
      "    **parameters\n",
      "    ) -> None:\n",
      "\n",
      "    if ('input' in parameters):  \n",
      "        self.request_type = 'embedding'\n",
      "    else:\n",
      "        self.request_type = 'chat'\n",
      "    \n",
      "    self.body = OpenaiRequestModel(\n",
      "            model=parameters.get('model','gpt-3.5-turbo'),\n",
      "            input=parameters.get('input'),\n",
      "            messages=parameters.get('messages', [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]) ,\n",
      "            function=parameters.get('function'),\n",
      "            function_call=parameters.get('function_call'),\n",
      "            temperature=parameters.get('temperature'),\n",
      "            top_p=parameters.get('top_p'),\n",
      "            n=parameters.get('n',1),\n",
      "            stream=parameters.get('stream'),\n",
      "            stop=parameters.get('stop'),\n",
      "            max_tokens=parameters.get('max_tokens',4000),\n",
      "            presence_penalty=parameters.get('presence_penalty'),\n",
      "            frequency_penalty=parameters.get('frequency_penalty'),\n",
      "            logit_bias=parameters.get('logit_bias'),\n",
      "            user=parameters.get('user')\n",
      "    )\n",
      "\n",
      "    self.body = {k: v for k, v in self.body.dict().items() if v is not None}\n",
      "    if self.request_type == 'embedding':\n",
      "        keys_to_remove = [\"n\", \"max_tokens\"]\n",
      "        self.body = {k: v for k, v in self.body.items() if k not in keys_to_remove}\n",
      "\n",
      "    \"\"\"Count the number of tokens in the request. Only supports chat and embedding requests.\"\"\"\n",
      "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "    self.total_tokens = 0\n",
      "    self.respect_token_limit = False\n",
      "    self.max_tokens_per_minute = 0\n",
      "    self.max_requests_per_minute = 0\n",
      "    self.url = ''\n",
      "\n",
      "    # if completions request, tokens = prompt + n * max_tokens\n",
      "    if self.request_type == 'chat':\n",
      "        max_tokens = self.body['max_tokens']  \n",
      "        n = self.body['n'] # pyright: ignore\n",
      "        completion_tokens = n * max_tokens # pyright: ignore\n",
      "        for message in self.body[\"messages\"]: # pyright: ignore\n",
      "            self.total_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
      "            for key, value in message.items():\n",
      "                self.total_tokens += len(encoding.encode(value))\n",
      "                if key == \"name\":  # if there's a name, the role is omitted\n",
      "                    self.total_tokens -= 1  # role is always required and always 1 token\n",
      "                self.total_tokens += 2  # every reply is primed with <im_start>assistant\n",
      "            self.total_tokens =  self.total_tokens + completion_tokens\n",
      "        \n",
      "    # if embeddings request, tokens = input tokens\n",
      "    elif self.request_type == \"embedding\":\n",
      "        if isinstance(self.body['input'], str):  # single input # pyright: ignore\n",
      "            self.total_tokens = len(encoding.encode(self.body[\"input\"]))  # pyright: ignore\n",
      "        elif isinstance(input, list):  # multiple inputs\n",
      "            self.total_tokens = sum([len(encoding.encode(i)) for i in self.body[\"input\"]]) # pyright: ignore\n",
      "        else:\n",
      "            raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n",
      "\n",
      "\n",
      "    match self.body['model']: # pyright: ignore\n",
      "        case 'gpt-3.5-turbo':\n",
      "            if self.total_tokens < 10000:\n",
      "                self.respect_token_limit = True\n",
      "                self.max_requests_per_minute = 3000\n",
      "                self.max_tokens_per_minute= 1000000\n",
      "                self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "            else:\n",
      "                self.respect_token_limit = False\n",
      "                self.max_requests_per_minute = 0\n",
      "                self.max_tokens_per_minute= 0\n",
      "\n",
      "        case 'gpt-3.5-turbo-16k-0613':\n",
      "            if self.total_tokens < 16000:\n",
      "                self.respect_token_limit = True\n",
      "                self.max_requests_per_minute = 3000\n",
      "                self.max_tokens_per_minute= 1000000\n",
      "                self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "            else:\n",
      "                self.respect_token_limit = False\n",
      "                self.max_requests_per_minute = 0\n",
      "                self.max_tokens_per_minute= 0\n",
      "\n",
      "        case 'gpt-4':\n",
      "            if self.total_tokens < 16000:\n",
      "                self.respect_token_limit = True\n",
      "                self.max_requests_per_minute = 3000\n",
      "                self.max_tokens_per_minute= 1000000\n",
      "                self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "            else:\n",
      "                self.respect_token_limit = False\n",
      "                self.max_requests_per_minute = 0\n",
      "                self.max_tokens_per_minute= 0\n",
      "\n",
      "\n",
      "        case 'text-embedding-ada-002':\n",
      "            if self.total_tokens < 10000:\n",
      "                self.respect_token_limit = True\n",
      "                self.max_tokens_per_minute = 1000000\n",
      "                self.max_requests_per_minute = 3000\n",
      "                self.url = 'https://api.openai.com/v1/embeddings'\n",
      "            else:\n",
      "                self.respect_token_limit = False              \n",
      "                self.max_requests_per_minute = 0\n",
      "                self.max_tokens_per_minute= 0\n",
      "\n",
      "\n",
      "            \n",
      "\n",
      "Function call: OpenaiRequestModel\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: sum\n",
      "Function call: len\n",
      "Function call: TypeError\n",
      "Related codes: ['\\n\\n\\nclass OpenaiRequestModel(BaseModel):\\n    model: str\\n    input: Union[str,List,None]\\n    messages: Union[List,None]\\n    function: Union[List,None]\\n    function_call: Union[str,Any,None]\\n    temperature: Union[float,None]\\n    top_p: Union[float,None]\\n    n: Union[int,None]\\n    stream: Union[bool,None]\\n    stop: Union[str,List,None]\\n    max_tokens: Union[int,None]\\n    presence_penalty: Union[float,None]\\n    frequency_penalty: Union[float,None]\\n    logit_bias: Union[Mapping,None]\\n    user: Union[str,None]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class BatchGenerator:\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        input_df: Union[pl.DataFrame,str] = 'noinput',\n",
      "        task: str = 'chat',\n",
      "        name: str = \"summarizer\",\n",
      "        tokenizer: Optional[Any] = None,\n",
      "        save_path: str = 'batch_generator',\n",
      "        logging_level: int = 10,\n",
      "    ) -> None:\n",
      "\n",
      "        if isinstance(input_df, pl.DataFrame):\n",
      "            self.load_path = f\"{save_path}/{name}.ndjson\" ## pyright: ignore\n",
      "            input_df.write_ndjson(self.load_path)\n",
      "        elif input_df == 'noinput':\n",
      "            raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n",
      "        else:\n",
      "            self.load_path = input_df\n",
      "\n",
      "        self.name = name\n",
      "        self.task = task\n",
      "        self.save_path = save_path\n",
      "        self.logging_level = logging_level    \n",
      "\n",
      "        if tokenizer is None:\n",
      "            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "        else:\n",
      "            self.tokenizer = tokenizer\n",
      "\n",
      "        # debug loads the frame in advance for usefull checks \n",
      "        self.frame = pl.read_ndjson(self.load_path)\n",
      "\n",
      "        # queues\n",
      "        self.requests_queue = asyncio.Queue()\n",
      "        self.retries_queue = asyncio.Queue()\n",
      "        self.errors_queue = asyncio.Queue()\n",
      "\n",
      "        # constants\n",
      "        self.max_attempts = 5\n",
      "        self.seconds_to_pause_after_rate_limit_error = 15\n",
      "        self.seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n",
      "\n",
      "        # initialize logging\n",
      "        logging.basicConfig(level=logging_level)\n",
      "        logging.debug(f\"Logging initialized at level {logging_level}\")\n",
      "        \n",
      "        # initialize available capacity counts\n",
      "        self.available_request_capacity = 1500\n",
      "        self.available_token_capacity = 6250000\n",
      "        self.last_update_time = time.time()\n",
      "\n",
      "        # api authentication\n",
      "        self.api_key =  os.getenv(\"OPENAI_API_KEY\")\n",
      "        self.request_header = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
      "    \n",
      "        @dataclass\n",
      "        class StatusTracker:\n",
      "            num_tasks_started: int = 0\n",
      "            num_tasks_in_progress: int = 0  # script ends when this reaches 0\n",
      "            num_tasks_succeeded: int = 0\n",
      "            num_tasks_failed: int = 0\n",
      "            num_rate_limit_errors: int = 0\n",
      "            num_api_errors: int = 0  # excluding rate limit errors, counted above\n",
      "            num_other_errors: int = 0\n",
      "            time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n",
      "\n",
      "        self.status_tracker = StatusTracker()\n",
      "\n",
      "        self.finished = False  # after file is empty, we'll skip reading it\n",
      "        logging.debug(f\"Initialization complete.\")\n",
      "\n",
      "    def enqueue_objects(self):\n",
      "        with open(self.load_path, 'r') as jsonl_file:\n",
      "            for line in jsonl_file:\n",
      "                line = line.strip()\n",
      "                if not line:\n",
      "                    continue\n",
      "                json_obj = json.loads(line)                \n",
      "                self.requests_queue.put_nowait(json_obj)  \n",
      "\n",
      "\n",
      "    async def process_objects(self, queue):\n",
      "        next_request = None  # variable to hold the next request to call\n",
      "        while True:\n",
      "                    # get next request (if one is not already waiting for capacity)\n",
      "                    if not self.retries_queue.empty():\n",
      "                            next_request = self.retries_queue.get_nowait()\n",
      "                            logging.debug(f\"Retrying request: {next_request}\")\n",
      "                    elif not self.requests_queue.empty():\n",
      "                            try:\n",
      "                                logging.debug(f\"Trying to retrieve next request\")\n",
      "                                next_request = self.requests_queue.get_nowait()     \n",
      "                                logging.info(f'Next request is {next_request}')\n",
      "                                if next_request.respect_token_limit:\n",
      "                                    self.status_tracker.num_tasks_started += 1\n",
      "                                    self.status_tracker.num_tasks_in_progress += 1\n",
      "                                    logging.debug(f\"Reading request: {next_request}\")\n",
      "                                else:\n",
      "                                    self.errors_queue.put_nowait(next_request)\n",
      "                            except StopIteration:\n",
      "                                # if file runs out, set flag to stop reading it\n",
      "                                logging.debug(\"Read file exhausted\")\n",
      "                                self.finished = True\n",
      "\n",
      "                    # update available capacity\n",
      "                    current_time = time.time()\n",
      "                    seconds_since_update = current_time - self.last_update_time\n",
      "                    self.available_request_capacity = min(\n",
      "                    self.available_request_capacity + next_request.max_requests_per_minute * seconds_since_update / 60.0, ## pyright: ignore\n",
      "                        next_request.max_requests_per_minute, ## pyright: ignore\n",
      "                    )\n",
      "                    self.available_token_capacity = min(\n",
      "                        self.available_token_capacity + next_request.max_tokens_per_minute * seconds_since_update / 60.0, ## pyright: ignore\n",
      "                        next_request.max_tokens_per_minute,  ## pyright: ignore\n",
      "                    )\n",
      "                    self.last_update_time = current_time\n",
      "\n",
      "                    # if enough capacity available, call API\n",
      "                    if next_request:\n",
      "                        next_request_tokens = next_request.total_tokens\n",
      "                        if (\n",
      "                            self.available_request_capacity >= 1\n",
      "                            and self.available_token_capacity >= next_request_tokens\n",
      "                        ):\n",
      "                            # update counters\n",
      "                            self.available_request_capacity -= 1\n",
      "                            self.available_token_capacity -= next_request_tokens\n",
      "                            \n",
      "\n",
      "                            # call API\n",
      "                            try:\n",
      "                                logging.info(f\"Calling Api for {next_request.body}\")\n",
      "                                async with aiohttp.ClientSession() as session:\n",
      "                                    async with session.post(\n",
      "                                    url=next_request.url, headers=self.request_header, json=next_request.body\n",
      "                                    ) as response:\n",
      "                                        response = await response.json()\n",
      "                                if \"error\" in response:\n",
      "                                    logging.warning(\n",
      "                                        \"\"\" f\"Request {self.task_id} failed with error {response['error']}\" \"\"\"\n",
      "                                    )\n",
      "                                    self.status_tracker.num_api_errors += 1\n",
      "                                    if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                                        self.status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \n",
      "                                        self.status_tracker.num_rate_limit_errors += 1\n",
      "                                        self.status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n",
      "                                        self.retries_queue.put_nowait(next_request)\n",
      "                                    else:\n",
      "                                        self.errors_queue.put_nowait(next_request)\n",
      "                                else:\n",
      "                                        print(response)\n",
      "                                        \n",
      "\n",
      "                            except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "                                logging.warning(f\"Request {next_request} failed with Exception {e}\")\n",
      "                                self.status_tracker.num_other_errors += 1\n",
      "                            queue.task_done() \n",
      "                            next_request = None  # reset next_request to empty\n",
      "\n",
      "                    # if all tasks are finished, break\n",
      "                    if self.status_tracker.num_tasks_in_progress == 0:\n",
      "                        break\n",
      "\n",
      "                    # main loop sleeps briefly so concurrent tasks can run\n",
      "                    await asyncio.sleep(self.seconds_to_sleep_each_loop)\n",
      "\n",
      "                    # if a rate limit error was hit recently, pause to cool down\n",
      "                    seconds_since_rate_limit_error = (time.time() - self.status_tracker.time_of_last_rate_limit_error)\n",
      "                    if seconds_since_rate_limit_error < self.seconds_to_pause_after_rate_limit_error:\n",
      "                        remaining_seconds_to_pause = (self.seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n",
      "                        await asyncio.sleep(remaining_seconds_to_pause)\n",
      "                        # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n",
      "                        logging.warn(f\"Pausing to cool down until {time.ctime(self.status_tracker.time_of_last_rate_limit_error + self.seconds_to_pause_after_rate_limit_error)}\")\n",
      "\n",
      "        # after finishing, log final status\n",
      "        logging.info(f\"\"\"Parallel processing complete. Results saved to {self.save_path}\"\"\")\n",
      "        if self.status_tracker.num_tasks_failed > 0:\n",
      "            logging.warning(f\"{self.status_tracker.num_tasks_failed} / {self.status_tracker.num_tasks_started} requests failed. Errors logged to {self.save_path}.\")\n",
      "        if self.status_tracker.num_rate_limit_errors > 0:\n",
      "            logging.warning(f\"{self.status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n",
      "\n",
      "\n",
      "\n",
      "    async def main(self):\n",
      "        logging.debug(f\"Entering main loop\")\n",
      "        self.enqueue_objects()\n",
      "        consumers = [asyncio.create_task(self.process_objects(self.requests_queue)) for _ in range(5)]\n",
      "        await self.requests_queue.join()\n",
      "        await self.retries_queue.join()\n",
      "        for consumer in consumers:\n",
      "            consumer.cancel()\n",
      "\n",
      "    def execute(self):\n",
      "        asyncio.run(self.main())\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: TypeError\n",
      "Function call: StatusTracker\n",
      "Function call: open\n",
      "Function call: min\n",
      "Function call: min\n",
      "Function call: print\n",
      "Function call: range\n",
      "Related codes: ['\\n\\n# dataclasses\\n\\n\\n@dataclass\\nclass StatusTracker:\\n    \"\"\"Stores metadata about the script\\'s progress. Only one instance is created.\"\"\"\\n\\n    num_tasks_started: int = 0\\n    num_tasks_in_progress: int = 0  # script ends when this reaches 0\\n    num_tasks_succeeded: int = 0\\n    num_tasks_failed: int = 0\\n    num_rate_limit_errors: int = 0\\n    num_api_errors: int = 0  # excluding rate limit errors, counted above\\n    num_other_errors: int = 0\\n    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    input_df: Union[pl.DataFrame,str] = 'noinput',\n",
      "    task: str = 'chat',\n",
      "    name: str = \"summarizer\",\n",
      "    tokenizer: Optional[Any] = None,\n",
      "    save_path: str = 'batch_generator',\n",
      "    logging_level: int = 10,\n",
      ") -> None:\n",
      "\n",
      "    if isinstance(input_df, pl.DataFrame):\n",
      "        self.load_path = f\"{save_path}/{name}.ndjson\" ## pyright: ignore\n",
      "        input_df.write_ndjson(self.load_path)\n",
      "    elif input_df == 'noinput':\n",
      "        raise TypeError('Constructor requires either a pl.Dataframe or a path to a ndjson')\n",
      "    else:\n",
      "        self.load_path = input_df\n",
      "\n",
      "    self.name = name\n",
      "    self.task = task\n",
      "    self.save_path = save_path\n",
      "    self.logging_level = logging_level    \n",
      "\n",
      "    if tokenizer is None:\n",
      "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "    else:\n",
      "        self.tokenizer = tokenizer\n",
      "\n",
      "    # debug loads the frame in advance for usefull checks \n",
      "    self.frame = pl.read_ndjson(self.load_path)\n",
      "\n",
      "    # queues\n",
      "    self.requests_queue = asyncio.Queue()\n",
      "    self.retries_queue = asyncio.Queue()\n",
      "    self.errors_queue = asyncio.Queue()\n",
      "\n",
      "    # constants\n",
      "    self.max_attempts = 5\n",
      "    self.seconds_to_pause_after_rate_limit_error = 15\n",
      "    self.seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n",
      "\n",
      "    # initialize logging\n",
      "    logging.basicConfig(level=logging_level)\n",
      "    logging.debug(f\"Logging initialized at level {logging_level}\")\n",
      "    \n",
      "    # initialize available capacity counts\n",
      "    self.available_request_capacity = 1500\n",
      "    self.available_token_capacity = 6250000\n",
      "    self.last_update_time = time.time()\n",
      "\n",
      "    # api authentication\n",
      "    self.api_key =  os.getenv(\"OPENAI_API_KEY\")\n",
      "    self.request_header = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
      "    \n",
      "    @dataclass\n",
      "    class StatusTracker:\n",
      "        num_tasks_started: int = 0\n",
      "        num_tasks_in_progress: int = 0  # script ends when this reaches 0\n",
      "        num_tasks_succeeded: int = 0\n",
      "        num_tasks_failed: int = 0\n",
      "        num_rate_limit_errors: int = 0\n",
      "        num_api_errors: int = 0  # excluding rate limit errors, counted above\n",
      "        num_other_errors: int = 0\n",
      "        time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n",
      "\n",
      "    self.status_tracker = StatusTracker()\n",
      "\n",
      "    self.finished = False  # after file is empty, we'll skip reading it\n",
      "    logging.debug(f\"Initialization complete.\")\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: TypeError\n",
      "Function call: StatusTracker\n",
      "Related codes: ['\\n\\n# dataclasses\\n\\n\\n@dataclass\\nclass StatusTracker:\\n    \"\"\"Stores metadata about the script\\'s progress. Only one instance is created.\"\"\"\\n\\n    num_tasks_started: int = 0\\n    num_tasks_in_progress: int = 0  # script ends when this reaches 0\\n    num_tasks_succeeded: int = 0\\n    num_tasks_failed: int = 0\\n    num_rate_limit_errors: int = 0\\n    num_api_errors: int = 0  # excluding rate limit errors, counted above\\n    num_other_errors: int = 0\\n    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\\n']\n",
      "\n",
      "********************************************************************************\n",
      "    \n",
      "@dataclass\n",
      "class StatusTracker:\n",
      "    num_tasks_started: int = 0\n",
      "    num_tasks_in_progress: int = 0  # script ends when this reaches 0\n",
      "    num_tasks_succeeded: int = 0\n",
      "    num_tasks_failed: int = 0\n",
      "    num_rate_limit_errors: int = 0\n",
      "    num_api_errors: int = 0  # excluding rate limit errors, counted above\n",
      "    num_other_errors: int = 0\n",
      "    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def enqueue_objects(self):\n",
      "    with open(self.load_path, 'r') as jsonl_file:\n",
      "        for line in jsonl_file:\n",
      "            line = line.strip()\n",
      "            if not line:\n",
      "                continue\n",
      "            json_obj = json.loads(line)                \n",
      "            self.requests_queue.put_nowait(json_obj)  \n",
      "\n",
      "Function call: open\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "async def process_objects(self, queue):\n",
      "    next_request = None  # variable to hold the next request to call\n",
      "    while True:\n",
      "                # get next request (if one is not already waiting for capacity)\n",
      "                if not self.retries_queue.empty():\n",
      "                        next_request = self.retries_queue.get_nowait()\n",
      "                        logging.debug(f\"Retrying request: {next_request}\")\n",
      "                elif not self.requests_queue.empty():\n",
      "                        try:\n",
      "                            logging.debug(f\"Trying to retrieve next request\")\n",
      "                            next_request = self.requests_queue.get_nowait()     \n",
      "                            logging.info(f'Next request is {next_request}')\n",
      "                            if next_request.respect_token_limit:\n",
      "                                self.status_tracker.num_tasks_started += 1\n",
      "                                self.status_tracker.num_tasks_in_progress += 1\n",
      "                                logging.debug(f\"Reading request: {next_request}\")\n",
      "                            else:\n",
      "                                self.errors_queue.put_nowait(next_request)\n",
      "                        except StopIteration:\n",
      "                            # if file runs out, set flag to stop reading it\n",
      "                            logging.debug(\"Read file exhausted\")\n",
      "                            self.finished = True\n",
      "\n",
      "                # update available capacity\n",
      "                current_time = time.time()\n",
      "                seconds_since_update = current_time - self.last_update_time\n",
      "                self.available_request_capacity = min(\n",
      "                self.available_request_capacity + next_request.max_requests_per_minute * seconds_since_update / 60.0, ## pyright: ignore\n",
      "                    next_request.max_requests_per_minute, ## pyright: ignore\n",
      "                )\n",
      "                self.available_token_capacity = min(\n",
      "                    self.available_token_capacity + next_request.max_tokens_per_minute * seconds_since_update / 60.0, ## pyright: ignore\n",
      "                    next_request.max_tokens_per_minute,  ## pyright: ignore\n",
      "                )\n",
      "                self.last_update_time = current_time\n",
      "\n",
      "                # if enough capacity available, call API\n",
      "                if next_request:\n",
      "                    next_request_tokens = next_request.total_tokens\n",
      "                    if (\n",
      "                        self.available_request_capacity >= 1\n",
      "                        and self.available_token_capacity >= next_request_tokens\n",
      "                    ):\n",
      "                        # update counters\n",
      "                        self.available_request_capacity -= 1\n",
      "                        self.available_token_capacity -= next_request_tokens\n",
      "                        \n",
      "\n",
      "                        # call API\n",
      "                        try:\n",
      "                            logging.info(f\"Calling Api for {next_request.body}\")\n",
      "                            async with aiohttp.ClientSession() as session:\n",
      "                                async with session.post(\n",
      "                                url=next_request.url, headers=self.request_header, json=next_request.body\n",
      "                                ) as response:\n",
      "                                    response = await response.json()\n",
      "                            if \"error\" in response:\n",
      "                                logging.warning(\n",
      "                                    \"\"\" f\"Request {self.task_id} failed with error {response['error']}\" \"\"\"\n",
      "                                )\n",
      "                                self.status_tracker.num_api_errors += 1\n",
      "                                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
      "                                    self.status_tracker.time_of_last_rate_limit_error = time.time() # pyright: ignore \n",
      "                                    self.status_tracker.num_rate_limit_errors += 1\n",
      "                                    self.status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n",
      "                                    self.retries_queue.put_nowait(next_request)\n",
      "                                else:\n",
      "                                    self.errors_queue.put_nowait(next_request)\n",
      "                            else:\n",
      "                                    print(response)\n",
      "                                    \n",
      "\n",
      "                        except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
      "                            logging.warning(f\"Request {next_request} failed with Exception {e}\")\n",
      "                            self.status_tracker.num_other_errors += 1\n",
      "                        queue.task_done() \n",
      "                        next_request = None  # reset next_request to empty\n",
      "\n",
      "                # if all tasks are finished, break\n",
      "                if self.status_tracker.num_tasks_in_progress == 0:\n",
      "                    break\n",
      "\n",
      "                # main loop sleeps briefly so concurrent tasks can run\n",
      "                await asyncio.sleep(self.seconds_to_sleep_each_loop)\n",
      "\n",
      "                # if a rate limit error was hit recently, pause to cool down\n",
      "                seconds_since_rate_limit_error = (time.time() - self.status_tracker.time_of_last_rate_limit_error)\n",
      "                if seconds_since_rate_limit_error < self.seconds_to_pause_after_rate_limit_error:\n",
      "                    remaining_seconds_to_pause = (self.seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n",
      "                    await asyncio.sleep(remaining_seconds_to_pause)\n",
      "                    # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n",
      "                    logging.warn(f\"Pausing to cool down until {time.ctime(self.status_tracker.time_of_last_rate_limit_error + self.seconds_to_pause_after_rate_limit_error)}\")\n",
      "\n",
      "    # after finishing, log final status\n",
      "    logging.info(f\"\"\"Parallel processing complete. Results saved to {self.save_path}\"\"\")\n",
      "    if self.status_tracker.num_tasks_failed > 0:\n",
      "        logging.warning(f\"{self.status_tracker.num_tasks_failed} / {self.status_tracker.num_tasks_started} requests failed. Errors logged to {self.save_path}.\")\n",
      "    if self.status_tracker.num_rate_limit_errors > 0:\n",
      "        logging.warning(f\"{self.status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n",
      "\n",
      "Function call: min\n",
      "Function call: min\n",
      "Function call: print\n",
      "Related codes: ['\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "async def main(self):\n",
      "    logging.debug(f\"Entering main loop\")\n",
      "    self.enqueue_objects()\n",
      "    consumers = [asyncio.create_task(self.process_objects(self.requests_queue)) for _ in range(5)]\n",
      "    await self.requests_queue.join()\n",
      "    await self.retries_queue.join()\n",
      "    for consumer in consumers:\n",
      "        consumer.cancel()\n",
      "\n",
      "Function call: range\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def execute(self):\n",
      "    asyncio.run(self.main())\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class ApiRequest:\n",
      "    def __init__(\n",
      "        self, \n",
      "        **parameters\n",
      "        ) -> None:\n",
      "\n",
      "        if ('input' in parameters) or ('input' in parameters.get('body', {})):  \n",
      "            self.request_type = 'embedding'\n",
      "        else:\n",
      "            self.request_type = 'chat'\n",
      "        \n",
      "\n",
      "        if parameters.get('body') is not None:\n",
      "            self.body = parameters.get('body')\n",
      "\n",
      "        else:\n",
      "            self.body = {\n",
      "                \"model\": parameters.get('model'),\n",
      "                \"input\": parameters.get('input'),\n",
      "                \"messages\": parameters.get('messages'),\n",
      "                \"function\": parameters.get('function'),\n",
      "                \"function_call\": parameters.get('function_call'),\n",
      "                \"temperature\": parameters.get('temperature'),\n",
      "                \"top_p\": parameters.get('top_p'),\n",
      "                \"n\": parameters.get('n'),\n",
      "                \"stream\": parameters.get('stream'),\n",
      "                \"stop\": parameters.get('stop'),\n",
      "                \"max_tokens\": parameters.get('max_tokens'),\n",
      "                \"presence_penalty\": parameters.get('presence_penalty'),\n",
      "                \"frequency_penalty\": parameters.get('frequency_penalty'),\n",
      "                \"logit_bias\": parameters.get('logit_bias'),\n",
      "                \"user\": parameters.get('user')\n",
      "            }\n",
      "\n",
      "        # Remove keys with None values\n",
      "        if self.body is not None:\n",
      "            self.body = {k: v for k, v in self.body.items() if v is not None}\n",
      "\n",
      "        \"\"\"Count the number of tokens in the request. Only supports chat and embedding requests.\"\"\"\n",
      "        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "\n",
      "        # if completions request, tokens = prompt + n * max_tokens\n",
      "        if self.request_type == 'chat':\n",
      "            max_tokens = self.body['max_tokens'] # pyright: ignore\n",
      "            n = self.body['n'] # pyright: ignore\n",
      "            completion_tokens = n * max_tokens # pyright: ignore\n",
      "            for message in self.body[\"messages\"]: # pyright: ignore\n",
      "                self.total_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
      "                for key, value in message.items():\n",
      "                    self.total_tokens += len(encoding.encode(value))\n",
      "                    if key == \"name\":  # if there's a name, the role is omitted\n",
      "                        self.total_tokens -= 1  # role is always required and always 1 token\n",
      "                    self.total_tokens += 2  # every reply is primed with <im_start>assistant\n",
      "                self.total_tokens =  self.total_tokens + completion_tokens\n",
      "            \n",
      "        # if embeddings request, tokens = input tokens\n",
      "        elif self.request_type == \"embedding\":\n",
      "            if isinstance(self.body['input'], str):  # single input # pyright: ignore\n",
      "                self.total_tokens = len(encoding.encode(self.body[\"input\"]))  # pyright: ignore\n",
      "            elif isinstance(input, list):  # multiple inputs\n",
      "                self.total_tokens = sum([len(encoding.encode(i)) for i in self.body[\"input\"]]) # pyright: ignore\n",
      "            else:\n",
      "                raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n",
      "\n",
      "        match self.body['model']: # pyright: ignore\n",
      "            case 'gpt-3.5-turbo':\n",
      "                if self.total_tokens < 4000:\n",
      "                    self.respect_token_limit = True\n",
      "                    self.max_requests_per_minute = 3000\n",
      "                    self.max_tokens_per_minute= 1000000\n",
      "                    self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "                else:\n",
      "                    self.respect_token_limit = False\n",
      "            case 'gpt-4':\n",
      "                if self.total_tokens < 16000:\n",
      "                    self.respect_token_limit = True\n",
      "                    self.max_requests_per_minute = 3000\n",
      "                    self.max_tokens_per_minute= 1000000\n",
      "                    self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "                else:\n",
      "                    self.respect_token_limit = False\n",
      "\n",
      "            case 'ext-embedding-ada-002':\n",
      "                if self.total_tokens < 4000:\n",
      "                    self.respect_token_limit = True\n",
      "                    self.max_tokens_per_minute = 1000000\n",
      "                    self.max_requests_per_minute = 3000\n",
      "                    self.url = 'https://api.openai.com/v1/embeddings'\n",
      "                else:\n",
      "                    self.respect_token_limit = False                \n",
      "\n",
      "                \n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: sum\n",
      "Function call: len\n",
      "Function call: TypeError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self, \n",
      "    **parameters\n",
      "    ) -> None:\n",
      "\n",
      "    if ('input' in parameters) or ('input' in parameters.get('body', {})):  \n",
      "        self.request_type = 'embedding'\n",
      "    else:\n",
      "        self.request_type = 'chat'\n",
      "    \n",
      "\n",
      "    if parameters.get('body') is not None:\n",
      "        self.body = parameters.get('body')\n",
      "\n",
      "    else:\n",
      "        self.body = {\n",
      "            \"model\": parameters.get('model'),\n",
      "            \"input\": parameters.get('input'),\n",
      "            \"messages\": parameters.get('messages'),\n",
      "            \"function\": parameters.get('function'),\n",
      "            \"function_call\": parameters.get('function_call'),\n",
      "            \"temperature\": parameters.get('temperature'),\n",
      "            \"top_p\": parameters.get('top_p'),\n",
      "            \"n\": parameters.get('n'),\n",
      "            \"stream\": parameters.get('stream'),\n",
      "            \"stop\": parameters.get('stop'),\n",
      "            \"max_tokens\": parameters.get('max_tokens'),\n",
      "            \"presence_penalty\": parameters.get('presence_penalty'),\n",
      "            \"frequency_penalty\": parameters.get('frequency_penalty'),\n",
      "            \"logit_bias\": parameters.get('logit_bias'),\n",
      "            \"user\": parameters.get('user')\n",
      "        }\n",
      "\n",
      "    # Remove keys with None values\n",
      "    if self.body is not None:\n",
      "        self.body = {k: v for k, v in self.body.items() if v is not None}\n",
      "\n",
      "    \"\"\"Count the number of tokens in the request. Only supports chat and embedding requests.\"\"\"\n",
      "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "\n",
      "    # if completions request, tokens = prompt + n * max_tokens\n",
      "    if self.request_type == 'chat':\n",
      "        max_tokens = self.body['max_tokens'] # pyright: ignore\n",
      "        n = self.body['n'] # pyright: ignore\n",
      "        completion_tokens = n * max_tokens # pyright: ignore\n",
      "        for message in self.body[\"messages\"]: # pyright: ignore\n",
      "            self.total_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
      "            for key, value in message.items():\n",
      "                self.total_tokens += len(encoding.encode(value))\n",
      "                if key == \"name\":  # if there's a name, the role is omitted\n",
      "                    self.total_tokens -= 1  # role is always required and always 1 token\n",
      "                self.total_tokens += 2  # every reply is primed with <im_start>assistant\n",
      "            self.total_tokens =  self.total_tokens + completion_tokens\n",
      "        \n",
      "    # if embeddings request, tokens = input tokens\n",
      "    elif self.request_type == \"embedding\":\n",
      "        if isinstance(self.body['input'], str):  # single input # pyright: ignore\n",
      "            self.total_tokens = len(encoding.encode(self.body[\"input\"]))  # pyright: ignore\n",
      "        elif isinstance(input, list):  # multiple inputs\n",
      "            self.total_tokens = sum([len(encoding.encode(i)) for i in self.body[\"input\"]]) # pyright: ignore\n",
      "        else:\n",
      "            raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n",
      "\n",
      "    match self.body['model']: # pyright: ignore\n",
      "        case 'gpt-3.5-turbo':\n",
      "            if self.total_tokens < 4000:\n",
      "                self.respect_token_limit = True\n",
      "                self.max_requests_per_minute = 3000\n",
      "                self.max_tokens_per_minute= 1000000\n",
      "                self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "            else:\n",
      "                self.respect_token_limit = False\n",
      "        case 'gpt-4':\n",
      "            if self.total_tokens < 16000:\n",
      "                self.respect_token_limit = True\n",
      "                self.max_requests_per_minute = 3000\n",
      "                self.max_tokens_per_minute= 1000000\n",
      "                self.url = 'https://api.openai.com/v1/chat/completions'\n",
      "            else:\n",
      "                self.respect_token_limit = False\n",
      "\n",
      "        case 'ext-embedding-ada-002':\n",
      "            if self.total_tokens < 4000:\n",
      "                self.respect_token_limit = True\n",
      "                self.max_tokens_per_minute = 1000000\n",
      "                self.max_requests_per_minute = 3000\n",
      "                self.url = 'https://api.openai.com/v1/embeddings'\n",
      "            else:\n",
      "                self.respect_token_limit = False                \n",
      "\n",
      "            \n",
      "\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: len\n",
      "Function call: isinstance\n",
      "Function call: sum\n",
      "Function call: len\n",
      "Function call: TypeError\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef num_tokens_consumed_from_request(\\n    request_json: dict,\\n    api_endpoint: str,\\n    token_encoding_name: str,\\n):\\n    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\\n    encoding = tiktoken.get_encoding(token_encoding_name)\\n    # if completions request, tokens = prompt + n * max_tokens\\n    if api_endpoint.endswith(\"completions\"):\\n        max_tokens = request_json.get(\"max_tokens\", 15)\\n        n = request_json.get(\"n\", 1)\\n        completion_tokens = n * max_tokens\\n\\n        # chat completions\\n        if api_endpoint.startswith(\"chat/\"):\\n            num_tokens = 0\\n            for message in request_json[\"messages\"]:\\n                num_tokens += 4  # every message follows <im_start>{role/name}\\\\n{content}<im_end>\\\\n\\n                for key, value in message.items():\\n                    num_tokens += len(encoding.encode(value))\\n                    if key == \"name\":  # if there\\'s a name, the role is omitted\\n                        num_tokens -= 1  # role is always required and always 1 token\\n            num_tokens += 2  # every reply is primed with <im_start>assistant\\n            return num_tokens + completion_tokens\\n        # normal completions\\n        else:\\n            prompt = request_json[\"prompt\"]\\n            if isinstance(prompt, str):  # single prompt\\n                prompt_tokens = len(encoding.encode(prompt))\\n                num_tokens = prompt_tokens + completion_tokens\\n                return num_tokens\\n            elif isinstance(prompt, list):  # multiple prompts\\n                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\\n                num_tokens = prompt_tokens + completion_tokens * len(prompt)\\n                return num_tokens\\n            else:\\n                raise TypeError(\\'Expecting either string or list of strings for \"prompt\" field in completion request\\')\\n    # if embeddings request, tokens = input tokens\\n    elif api_endpoint == \"embeddings\":\\n        input = request_json[\"input\"]\\n        if isinstance(input, str):  # single input\\n            num_tokens = len(encoding.encode(input))\\n            return num_tokens\\n        elif isinstance(input, list):  # multiple inputs\\n            num_tokens = sum([len(encoding.encode(i)) for i in input])\\n            return num_tokens\\n        else:\\n            raise TypeError(\\'Expecting either string or list of strings for \"inputs\" field in embedding request\\')\\n    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\\n    else:\\n        raise NotImplementedError(f\\'API endpoint \"{api_endpoint}\" not implemented in this script\\')\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def chatgpt_response(\n",
      "     \n",
      "    prompt: List[Dict[str, Union[str, Dict[str, str]]]], \n",
      "    model: str = \"gpt-3.5-turbo\", \n",
      "    max_tokens: int = 1000, \n",
      "    stream: bool = False\n",
      ") -> Union[Generator,Tuple]:\n",
      "\n",
      "    try:\n",
      "        print(\"Trying to call OpenAI API...\")\n",
      "        response = openai.ChatCompletion.create(\n",
      "            model=model,\n",
      "            messages=prompt,\n",
      "            max_tokens=max_tokens,\n",
      "            stream=stream\n",
      "        )\n",
      "        return response, True\n",
      "\n",
      "    except openai.APIError as e:\n",
      "        logging.error(f\"Unexpected error in openai call: {e}\") \n",
      "        fail_response = {\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"message\": {\n",
      "                        \"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "        return fail_response, False\n",
      "\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class StatusTrackerModel(BaseModel):\n",
      "    name: str\n",
      "    max_attempts: int = 5\n",
      "    seconds_to_pause_after_rate_limit_error: int = 10\n",
      "    seconds_to_sleep_each_loop: float  = 0.001\n",
      "    available_request_capacity: int = 1500\n",
      "    available_token_capacity: int = 625000\n",
      "    last_update_time: float = now()\n",
      "    num_rate_limit_errors: int = 0\n",
      "    num_overloaded_errors: int = 0\n",
      "    time_of_last_rate_limit_error: float = 0.0\n",
      "    num_tasks_failed: int = 0\n",
      "    num_tasks_started: int = 0\n",
      "    num_api_errors: int = 0\n",
      "    num_other_errors: int = 0 \n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseModel\"]\n",
      "]\n",
      "Function call: now\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "class OpenaiRequestModel(BaseModel):\n",
      "    model: str\n",
      "    input: Union[str,List,None]\n",
      "    messages: Union[List,None]\n",
      "    function: Union[List,None]\n",
      "    function_call: Union[str,Any,None]\n",
      "    temperature: Union[float,None]\n",
      "    top_p: Union[float,None]\n",
      "    n: Union[int,None]\n",
      "    stream: Union[bool,None]\n",
      "    stop: Union[str,List,None]\n",
      "    max_tokens: Union[int,None]\n",
      "    presence_penalty: Union[float,None]\n",
      "    frequency_penalty: Union[float,None]\n",
      "    logit_bias: Union[Mapping,None]\n",
      "    user: Union[str,None]\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"BaseModel\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "def cohere_response(prompt: List[dict],model: str = \"command\", max_tokens: int = 1000\n",
      "    ) -> Tuple[Dict, bool]:\n",
      "    \"\"\"\n",
      "        Call the Cohere API with the given prompt and maximum number of output tokens.\n",
      "        \n",
      "\n",
      "        :param prompt: A list of strings representing the prompt to send to the API.\n",
      "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
      "        :param model: A string representing the model to use. either command or command-nightly\n",
      "        :return: A tuple containing the API response cohere object and a boolean indicating success.\n",
      "        \"\"\"             \n",
      "    try:\n",
      "        prompt= convert_mark_to_str_prompt(prompt)\n",
      "        print(\"Trying to call Cohere API... using model:\", model)\n",
      "        response = co.generate(\n",
      "        model= model,\n",
      "        prompt= prompt,\n",
      "        max_tokens=max_tokens,\n",
      "        #end_sequences=['#SYSTEM:', '#USER:'],\n",
      "        )\n",
      "        return response, True\n",
      "\n",
      "    except:\n",
      "          return None, False\n",
      "    \n",
      "\n",
      "Function call: convert_mark_to_str_prompt\n",
      "Function call: print\n",
      "Related codes: ['\\ndef convert_mark_to_str_prompt(messages: List[dict], prompt: str = \"\") -> str:\\n    prompt = \"\"\\n    for message in messages:\\n        role = message[\"role\"].upper()\\n        content = message[\"content\"]\\n        prompt += f\" #{role}: {content}\"\\n\\n    return prompt\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def cohere_summarize(prompt: str, model: str = \"summarize-xlarge\", length: str = \"medium\", extractiveness: str = \"medium\", format: str = \"bullets\", additional_command: str = None) -> str:\n",
      "    response = co.summarize( \n",
      "    text=prompt,model=model, \n",
      "    length=length,\n",
      "    extractiveness=extractiveness,\n",
      "    format=format,\n",
      "    additional_command=additional_command\n",
      "    )\n",
      "\n",
      "    summary = response.summary\n",
      "    return summary\n",
      "              \n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class OsProcessor:\n",
      "    def __init__(self, directory_path: str):\n",
      "        self.directory_path = directory_path\n",
      "\n",
      "    def get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\n",
      "        \"\"\"Returns a list of all files in a directory\"\"\"\n",
      "        if directory_path is None:\n",
      "            directory_path = self.directory_path\n",
      "\n",
      "        all_files = []\n",
      "        for root, _, files in os.walk(directory_path):\n",
      "            for file in files:\n",
      "                all_files.append(os.path.join(root, file))\n",
      "\n",
      "        return all_files\n",
      "\n",
      "    def get_files_with_extension(\n",
      "        self, extension: str, directory_path: Optional[str] = None\n",
      "    ) -> List[str]:\n",
      "        \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\n",
      "        if directory_path is None:\n",
      "            directory_path = self.directory_path\n",
      "\n",
      "        all_files = self.get_all_files(directory_path)\n",
      "        files_with_extension = [file for file in all_files if file.endswith(extension)]\n",
      "\n",
      "        return files_with_extension\n",
      "\n",
      "    def get_file_extension(self, file_path: str) -> str:\n",
      "        \"\"\"Returns the extension of a file\"\"\"\n",
      "        return Path(file_path).suffix\n",
      "\n",
      "    def get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\n",
      "        \"\"\"Returns a list of all subdirectories in a directory\"\"\"\n",
      "        if directory_path is None:\n",
      "            directory_path = self.directory_path\n",
      "\n",
      "        subdirectories = [\n",
      "            os.path.join(directory_path, d)\n",
      "            for d in os.listdir(directory_path)\n",
      "            if os.path.isdir(os.path.join(directory_path, d))\n",
      "        ]\n",
      "\n",
      "        return subdirectories\n",
      "\n",
      "    def create_directory(self, directory_path: str) -> None:\n",
      "        \"\"\"Creates a directory if it does not exist\"\"\"\n",
      "        if not os.path.exists(directory_path):\n",
      "            os.makedirs(directory_path)\n",
      "\n",
      "    def delete_directory(self, directory_path: str) -> None:\n",
      "        \"\"\"Deletes a directory if it exists\"\"\"\n",
      "        if os.path.exists(directory_path):\n",
      "            shutil.rmtree(directory_path)\n",
      "\n",
      "    def copy_file(self, source_path: str, destination_path: str) -> None:\n",
      "        \"\"\"Copies a file from one location to another\"\"\"\n",
      "        shutil.copy2(source_path, destination_path)\n",
      "\n",
      "    def move_file(self, source_path: str, destination_path: str) -> None:\n",
      "        \"\"\"Moves a file from one location to another\"\"\"\n",
      "        shutil.move(source_path, destination_path)\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: Path\n",
      "Related codes: ['\\n\\nclass ClusterPaths:\\n    def create_paths(\\n        self, embeddings: np.ndarray, num_clusters: int\\n    ) -> List[List[int]]:\\n        raise NotImplementedError\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, directory_path: str):\n",
      "    self.directory_path = directory_path\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\n",
      "    \"\"\"Returns a list of all files in a directory\"\"\"\n",
      "    if directory_path is None:\n",
      "        directory_path = self.directory_path\n",
      "\n",
      "    all_files = []\n",
      "    for root, _, files in os.walk(directory_path):\n",
      "        for file in files:\n",
      "            all_files.append(os.path.join(root, file))\n",
      "\n",
      "    return all_files\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_files_with_extension(\n",
      "    self, extension: str, directory_path: Optional[str] = None\n",
      ") -> List[str]:\n",
      "    \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\n",
      "    if directory_path is None:\n",
      "        directory_path = self.directory_path\n",
      "\n",
      "    all_files = self.get_all_files(directory_path)\n",
      "    files_with_extension = [file for file in all_files if file.endswith(extension)]\n",
      "\n",
      "    return files_with_extension\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_file_extension(self, file_path: str) -> str:\n",
      "    \"\"\"Returns the extension of a file\"\"\"\n",
      "    return Path(file_path).suffix\n",
      "\n",
      "Function call: Path\n",
      "Related codes: ['\\n\\nclass ClusterPaths:\\n    def create_paths(\\n        self, embeddings: np.ndarray, num_clusters: int\\n    ) -> List[List[int]]:\\n        raise NotImplementedError\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\n",
      "    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\n",
      "    if directory_path is None:\n",
      "        directory_path = self.directory_path\n",
      "\n",
      "    subdirectories = [\n",
      "        os.path.join(directory_path, d)\n",
      "        for d in os.listdir(directory_path)\n",
      "        if os.path.isdir(os.path.join(directory_path, d))\n",
      "    ]\n",
      "\n",
      "    return subdirectories\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def create_directory(self, directory_path: str) -> None:\n",
      "    \"\"\"Creates a directory if it does not exist\"\"\"\n",
      "    if not os.path.exists(directory_path):\n",
      "        os.makedirs(directory_path)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def delete_directory(self, directory_path: str) -> None:\n",
      "    \"\"\"Deletes a directory if it exists\"\"\"\n",
      "    if os.path.exists(directory_path):\n",
      "        shutil.rmtree(directory_path)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def copy_file(self, source_path: str, destination_path: str) -> None:\n",
      "    \"\"\"Copies a file from one location to another\"\"\"\n",
      "    shutil.copy2(source_path, destination_path)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def move_file(self, source_path: str, destination_path: str) -> None:\n",
      "    \"\"\"Moves a file from one location to another\"\"\"\n",
      "    shutil.move(source_path, destination_path)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class GithubProcessor(OsProcessor):\n",
      "    def __init__(\n",
      "        self,\n",
      "        base_directory: str,\n",
      "        username=None,\n",
      "        repo_name=None,\n",
      "        code_parsers=None,\n",
      "        minify_code: bool = False,\n",
      "        remove_docstrings: bool = False,\n",
      "    ):\n",
      "        self.username = username\n",
      "        self.repo_name = repo_name\n",
      "        self.base_directory = base_directory\n",
      "        self.github = Github()\n",
      "        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\n",
      "        repo_path = self.clone_repo(self.repo.clone_url)\n",
      "\n",
      "        OsProcessor.__init__(self, repo_path)\n",
      "        self.code_parsers = code_parsers or [\n",
      "            PythonParser(\n",
      "                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\n",
      "            )\n",
      "        ]\n",
      "\n",
      "    def get_public_repos(self):\n",
      "        \"\"\"Returns a list of all public repos for the user.\"\"\"\n",
      "        user = self.github.get_user(self.username)\n",
      "        return user.get_repos()\n",
      "\n",
      "    def clone_repo(self, repo_url: str):\n",
      "        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\n",
      "        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
      "        target_directory = os.path.join(self.base_directory, repo_name)\n",
      "\n",
      "        if os.path.exists(target_directory):\n",
      "            shutil.rmtree(target_directory)\n",
      "\n",
      "        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
      "\n",
      "        return target_directory\n",
      "\n",
      "    def process_repo(self, repo_path=None):\n",
      "        \"\"\"Processes the repo at the specified path.\n",
      "        If no path is specified, the repo at self.directory_path is processed.\n",
      "        Returns the list of parsed functions and classes.\"\"\"\n",
      "        if repo_path is None:\n",
      "            repo_path = self.directory_path\n",
      "\n",
      "        for code_parser in self.code_parsers:\n",
      "            code_parser.directory_path = repo_path\n",
      "            code_parser.process_directory(repo_path)\n",
      "\n",
      "    def process_repos(self):\n",
      "        \"\"\"Processes all public repos for the user.\"\"\"\n",
      "        for repo in self.get_public_repos():\n",
      "            if not repo.private:\n",
      "                print(f\"Processing repo: {repo.name}\")\n",
      "                repo_path = self.clone_repo(repo.clone_url)\n",
      "                self.process_repo(repo_path)\n",
      "                shutil.rmtree(repo_path)\n",
      "\n",
      "    def get_repo(self, repo_name):\n",
      "        \"\"\"Returns the repo with the specified name.\"\"\"\n",
      "        user = self.github.get_user(self.username)\n",
      "        return user.get_repo(repo_name)\n",
      "\n",
      "    def process_single_repo(self):\n",
      "\n",
      "        repo = self.get_repo(self.repo_name)\n",
      "        print(f\"Processing repo: {self.repo_name}\")\n",
      "        repo_path = self.clone_repo(repo.clone_url)\n",
      "        self.process_repo(repo_path)\n",
      "        shutil.rmtree(repo_path)\n",
      "\n",
      "    def get_issues(self, state=\"open\"):\n",
      "        \"\"\"\n",
      "        Returns a list of all issues in the repo with the specified state.\n",
      "        \"\"\"\n",
      "        issues = []\n",
      "        for issue in self.repo.get_issues(state=state):\n",
      "            issues.append(issue)\n",
      "        return issues\n",
      "\n",
      "    def parse_issues(self, state=\"open\"):\n",
      "        \"\"\"\n",
      "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
      "        Each dict contains the issue number, title, body, and labels.\n",
      "        \"\"\"\n",
      "        parsed_issues = []\n",
      "        issues = self.get_issues(state=state)\n",
      "        for issue in issues:\n",
      "            parsed_issue = {\n",
      "                \"number\": issue.number,\n",
      "                \"title\": issue.title,\n",
      "                \"body\": issue.body,\n",
      "                \"labels\": [label.name for label in issue.labels],\n",
      "            }\n",
      "            parsed_issues.append(parsed_issue)\n",
      "        return parsed_issues\n",
      "\n",
      "    def get_commits(self):\n",
      "        \"\"\"\n",
      "        Returns a list of all commits in the main branch of the repository.\n",
      "        \"\"\"\n",
      "        commits = []\n",
      "        branch = self.repo.get_branch(\"main\")\n",
      "        for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
      "            commits.append(commit)\n",
      "        return commits\n",
      "\n",
      "    def parse_commits(self):\n",
      "        \"\"\"\n",
      "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
      "        Each dict contains the commit sha, commit message, and author information.\n",
      "        \"\"\"\n",
      "        parsed_commits = []\n",
      "        commits = self.get_commits()\n",
      "        for commit in commits:\n",
      "            parsed_commit = {\n",
      "                \"sha\": commit.sha,\n",
      "                \"message\": commit.commit.message,\n",
      "                \"author\": {\n",
      "                    \"name\": commit.commit.author.name,\n",
      "                    \"email\": commit.commit.author.email,\n",
      "                    \"date\": commit.commit.author.date,\n",
      "                },\n",
      "            }\n",
      "            parsed_commits.append(parsed_commit)\n",
      "        return parsed_commits\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"OsProcessor\"]\n",
      "]\n",
      "Function call: Github\n",
      "Function call: PythonParser\n",
      "Function call: print\n",
      "Function call: print\n",
      "Related codes: ['\\n\\nclass GithubProcessor(OsProcessor):\\n    def __init__(\\n        self,\\n        base_directory: str,\\n        username=None,\\n        repo_name=None,\\n        code_parsers=None,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n    ):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.base_directory = base_directory\\n        self.github = Github()\\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n        repo_path = self.clone_repo(self.repo.clone_url)\\n\\n        OsProcessor.__init__(self, repo_path)\\n        self.code_parsers = code_parsers or [\\n            PythonParser(\\n                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\\n            )\\n        ]\\n\\n    def get_public_repos(self):\\n        \"\"\"Returns a list of all public repos for the user.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def clone_repo(self, repo_url: str):\\n        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.base_directory, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n    def process_repo(self, repo_path=None):\\n        \"\"\"Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n        if repo_path is None:\\n            repo_path = self.directory_path\\n\\n        for code_parser in self.code_parsers:\\n            code_parser.directory_path = repo_path\\n            code_parser.process_directory(repo_path)\\n\\n    def process_repos(self):\\n        \"\"\"Processes all public repos for the user.\"\"\"\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.clone_repo(repo.clone_url)\\n                self.process_repo(repo_path)\\n                shutil.rmtree(repo_path)\\n\\n    def get_repo(self, repo_name):\\n        \"\"\"Returns the repo with the specified name.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_single_repo(self):\\n\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.clone_repo(repo.clone_url)\\n        self.process_repo(repo_path)\\n        shutil.rmtree(repo_path)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n', '\\n\\nclass PythonParser(OsProcessor):\\n    def __init__(\\n        self,\\n        directory_path: str,\\n        visitor: Optional[FunctionAndClassVisitor] = None,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n    ):\\n        super().__init__(directory_path)\\n        self.visitor = visitor if visitor else FunctionAndClassVisitor()\\n        self.minify_code = minify_code\\n        self.remove_docstrings = remove_docstrings\\n\\n    def remove_docstring(self, tree: cst.Module) -> str:\\n        \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\\n\\n        # Remove docstrings using a transformer\\n        class DocstringRemover(cst.CSTTransformer):\\n            def leave_FunctionDef(\\n                self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\\n            ) -> cst.FunctionDef:\\n                docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n                if docstring.startswith(\"No docstring\"):\\n                    return updated_node\\n\\n                return updated_node.with_changes(\\n                    body=updated_node.body.with_changes(\\n                        body=[\\n                            stmt\\n                            for stmt in updated_node.body.body\\n                            if not (\\n                                isinstance(stmt, cst.SimpleStatementLine)\\n                                and any(\\n                                    isinstance(expr, cst.Expr)\\n                                    and isinstance(expr.value, cst.SimpleString)\\n                                    for expr in stmt.body\\n                                )\\n                            )\\n                        ]\\n                    )\\n                )\\n\\n        tree = tree.visit(DocstringRemover())\\n        return tree.code\\n\\n    def _process_file(self, file_path: str):\\n        \"\"\"This method is called for every file in the directory.\\n        It does the following:\\n        1. Reads the file\\n        2. Parses the file\\n        3. Visits the file with the visitor\\n        \"\"\"\\n        #get current number of nodes in visitor\\n        current_node_count = self.visitor.function_count + self.visitor.class_count\\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\\n            source_code = file.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n        except cst.ParserSyntaxError:\\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n            return\\n\\n        tree.visit(self.visitor)\\n        #calculate how many new nodes were added\\n        new_node_counter = self.visitor.function_count + self.visitor.class_count - current_node_count\\n        self.visitor.filename_map.extend([file_path]*new_node_counter)\\n        # Remove docstrings if specified\\n        if self.remove_docstrings:\\n            source_code = self.remove_docstring(source_code, tree)\\n\\n        # Minify the code if specified\\n        if self.minify_code:\\n            minifier = PythonMinifier(source_code)\\n            source_code = minifier.get_minified_code()\\n\\n        # Add the processed code to the corresponding list in the visitor\\n        self.visitor.function_source_codes.append(source_code)\\n\\n    def process_file(self, file_path: str):\\n        \"\"\"This method is called for every file in the directory.\\n        It does the following:\\n        1. Runs flake8 on the file\\n        if flake8 returns a non-zero exit code, it means the file has a syntax error\\n        2. Reads the file\\n        3. Parses the file\\n        4. Visits the file with the visitor\\n\\n        \"\"\"\\n        result = subprocess.run(\\n            [\"flake8\", \"--select=E999\", file_path], capture_output=True\\n        )\\n\\n        if result.returncode != 0:\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n            print(result.stderr.decode(\"utf-8\"))\\n            return\\n\\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\\n            source_code = f.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n            tree.visit(self.visitor)\\n        except cst.ParserSyntaxError as e:\\n            print(f\"Syntax error: {e}\")\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n\\n    def process_directory(\\n        self,\\n    ) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\\n        \"\"\"This method is called for every directory.\\n        It does the following:\\n        1. Gets all the python files in the directory\\n        2. Processes each file\\n        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\\n        \"\"\"\\n\\n        python_files = self.get_files_with_extension(\".py\")\\n\\n        for file_path in python_files:\\n            self._process_file(file_path)\\n\\n        result_dict = {\\n            \\'function_source_codes\\': self.visitor.function_source_codes,\\n            \\'function_nodes\\': self.visitor.function_nodes,\\n            \\'class_source_codes\\': self.visitor.class_source_codes,\\n            \\'class_nodes\\': self.visitor.class_nodes,\\n            \\'file_map\\': self.visitor.filename_map,\\n            \\'full_nodes\\': self.visitor.full_node_list,\\n            \\'full_source\\': self.visitor.full_source_list\\n        }\\n\\n\\n        return result_dict\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    base_directory: str,\n",
      "    username=None,\n",
      "    repo_name=None,\n",
      "    code_parsers=None,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "):\n",
      "    self.username = username\n",
      "    self.repo_name = repo_name\n",
      "    self.base_directory = base_directory\n",
      "    self.github = Github()\n",
      "    self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\n",
      "    repo_path = self.clone_repo(self.repo.clone_url)\n",
      "\n",
      "    OsProcessor.__init__(self, repo_path)\n",
      "    self.code_parsers = code_parsers or [\n",
      "        PythonParser(\n",
      "            repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\n",
      "        )\n",
      "    ]\n",
      "\n",
      "Function call: Github\n",
      "Function call: PythonParser\n",
      "Related codes: ['\\n\\nclass GithubProcessor(OsProcessor):\\n    def __init__(\\n        self,\\n        base_directory: str,\\n        username=None,\\n        repo_name=None,\\n        code_parsers=None,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n    ):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.base_directory = base_directory\\n        self.github = Github()\\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n        repo_path = self.clone_repo(self.repo.clone_url)\\n\\n        OsProcessor.__init__(self, repo_path)\\n        self.code_parsers = code_parsers or [\\n            PythonParser(\\n                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\\n            )\\n        ]\\n\\n    def get_public_repos(self):\\n        \"\"\"Returns a list of all public repos for the user.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def clone_repo(self, repo_url: str):\\n        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.base_directory, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n    def process_repo(self, repo_path=None):\\n        \"\"\"Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n        if repo_path is None:\\n            repo_path = self.directory_path\\n\\n        for code_parser in self.code_parsers:\\n            code_parser.directory_path = repo_path\\n            code_parser.process_directory(repo_path)\\n\\n    def process_repos(self):\\n        \"\"\"Processes all public repos for the user.\"\"\"\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.clone_repo(repo.clone_url)\\n                self.process_repo(repo_path)\\n                shutil.rmtree(repo_path)\\n\\n    def get_repo(self, repo_name):\\n        \"\"\"Returns the repo with the specified name.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_single_repo(self):\\n\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.clone_repo(repo.clone_url)\\n        self.process_repo(repo_path)\\n        shutil.rmtree(repo_path)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n', '\\n\\nclass PythonParser(OsProcessor):\\n    def __init__(\\n        self,\\n        directory_path: str,\\n        visitor: Optional[FunctionAndClassVisitor] = None,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n    ):\\n        super().__init__(directory_path)\\n        self.visitor = visitor if visitor else FunctionAndClassVisitor()\\n        self.minify_code = minify_code\\n        self.remove_docstrings = remove_docstrings\\n\\n    def remove_docstring(self, tree: cst.Module) -> str:\\n        \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\\n\\n        # Remove docstrings using a transformer\\n        class DocstringRemover(cst.CSTTransformer):\\n            def leave_FunctionDef(\\n                self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\\n            ) -> cst.FunctionDef:\\n                docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n                if docstring.startswith(\"No docstring\"):\\n                    return updated_node\\n\\n                return updated_node.with_changes(\\n                    body=updated_node.body.with_changes(\\n                        body=[\\n                            stmt\\n                            for stmt in updated_node.body.body\\n                            if not (\\n                                isinstance(stmt, cst.SimpleStatementLine)\\n                                and any(\\n                                    isinstance(expr, cst.Expr)\\n                                    and isinstance(expr.value, cst.SimpleString)\\n                                    for expr in stmt.body\\n                                )\\n                            )\\n                        ]\\n                    )\\n                )\\n\\n        tree = tree.visit(DocstringRemover())\\n        return tree.code\\n\\n    def _process_file(self, file_path: str):\\n        \"\"\"This method is called for every file in the directory.\\n        It does the following:\\n        1. Reads the file\\n        2. Parses the file\\n        3. Visits the file with the visitor\\n        \"\"\"\\n        #get current number of nodes in visitor\\n        current_node_count = self.visitor.function_count + self.visitor.class_count\\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\\n            source_code = file.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n        except cst.ParserSyntaxError:\\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n            return\\n\\n        tree.visit(self.visitor)\\n        #calculate how many new nodes were added\\n        new_node_counter = self.visitor.function_count + self.visitor.class_count - current_node_count\\n        self.visitor.filename_map.extend([file_path]*new_node_counter)\\n        # Remove docstrings if specified\\n        if self.remove_docstrings:\\n            source_code = self.remove_docstring(source_code, tree)\\n\\n        # Minify the code if specified\\n        if self.minify_code:\\n            minifier = PythonMinifier(source_code)\\n            source_code = minifier.get_minified_code()\\n\\n        # Add the processed code to the corresponding list in the visitor\\n        self.visitor.function_source_codes.append(source_code)\\n\\n    def process_file(self, file_path: str):\\n        \"\"\"This method is called for every file in the directory.\\n        It does the following:\\n        1. Runs flake8 on the file\\n        if flake8 returns a non-zero exit code, it means the file has a syntax error\\n        2. Reads the file\\n        3. Parses the file\\n        4. Visits the file with the visitor\\n\\n        \"\"\"\\n        result = subprocess.run(\\n            [\"flake8\", \"--select=E999\", file_path], capture_output=True\\n        )\\n\\n        if result.returncode != 0:\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n            print(result.stderr.decode(\"utf-8\"))\\n            return\\n\\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\\n            source_code = f.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n            tree.visit(self.visitor)\\n        except cst.ParserSyntaxError as e:\\n            print(f\"Syntax error: {e}\")\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n\\n    def process_directory(\\n        self,\\n    ) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\\n        \"\"\"This method is called for every directory.\\n        It does the following:\\n        1. Gets all the python files in the directory\\n        2. Processes each file\\n        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\\n        \"\"\"\\n\\n        python_files = self.get_files_with_extension(\".py\")\\n\\n        for file_path in python_files:\\n            self._process_file(file_path)\\n\\n        result_dict = {\\n            \\'function_source_codes\\': self.visitor.function_source_codes,\\n            \\'function_nodes\\': self.visitor.function_nodes,\\n            \\'class_source_codes\\': self.visitor.class_source_codes,\\n            \\'class_nodes\\': self.visitor.class_nodes,\\n            \\'file_map\\': self.visitor.filename_map,\\n            \\'full_nodes\\': self.visitor.full_node_list,\\n            \\'full_source\\': self.visitor.full_source_list\\n        }\\n\\n\\n        return result_dict\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_public_repos(self):\n",
      "    \"\"\"Returns a list of all public repos for the user.\"\"\"\n",
      "    user = self.github.get_user(self.username)\n",
      "    return user.get_repos()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def clone_repo(self, repo_url: str):\n",
      "    \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\n",
      "    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
      "    target_directory = os.path.join(self.base_directory, repo_name)\n",
      "\n",
      "    if os.path.exists(target_directory):\n",
      "        shutil.rmtree(target_directory)\n",
      "\n",
      "    subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
      "\n",
      "    return target_directory\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def process_repo(self, repo_path=None):\n",
      "    \"\"\"Processes the repo at the specified path.\n",
      "        If no path is specified, the repo at self.directory_path is processed.\n",
      "        Returns the list of parsed functions and classes.\"\"\"\n",
      "    if repo_path is None:\n",
      "        repo_path = self.directory_path\n",
      "\n",
      "    for code_parser in self.code_parsers:\n",
      "        code_parser.directory_path = repo_path\n",
      "        code_parser.process_directory(repo_path)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def process_repos(self):\n",
      "    \"\"\"Processes all public repos for the user.\"\"\"\n",
      "    for repo in self.get_public_repos():\n",
      "        if not repo.private:\n",
      "            print(f\"Processing repo: {repo.name}\")\n",
      "            repo_path = self.clone_repo(repo.clone_url)\n",
      "            self.process_repo(repo_path)\n",
      "            shutil.rmtree(repo_path)\n",
      "\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_repo(self, repo_name):\n",
      "    \"\"\"Returns the repo with the specified name.\"\"\"\n",
      "    user = self.github.get_user(self.username)\n",
      "    return user.get_repo(repo_name)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def process_single_repo(self):\n",
      "\n",
      "    repo = self.get_repo(self.repo_name)\n",
      "    print(f\"Processing repo: {self.repo_name}\")\n",
      "    repo_path = self.clone_repo(repo.clone_url)\n",
      "    self.process_repo(repo_path)\n",
      "    shutil.rmtree(repo_path)\n",
      "\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_issues(self, state=\"open\"):\n",
      "    \"\"\"\n",
      "        Returns a list of all issues in the repo with the specified state.\n",
      "        \"\"\"\n",
      "    issues = []\n",
      "    for issue in self.repo.get_issues(state=state):\n",
      "        issues.append(issue)\n",
      "    return issues\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def parse_issues(self, state=\"open\"):\n",
      "    \"\"\"\n",
      "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
      "        Each dict contains the issue number, title, body, and labels.\n",
      "        \"\"\"\n",
      "    parsed_issues = []\n",
      "    issues = self.get_issues(state=state)\n",
      "    for issue in issues:\n",
      "        parsed_issue = {\n",
      "            \"number\": issue.number,\n",
      "            \"title\": issue.title,\n",
      "            \"body\": issue.body,\n",
      "            \"labels\": [label.name for label in issue.labels],\n",
      "        }\n",
      "        parsed_issues.append(parsed_issue)\n",
      "    return parsed_issues\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_commits(self):\n",
      "    \"\"\"\n",
      "        Returns a list of all commits in the main branch of the repository.\n",
      "        \"\"\"\n",
      "    commits = []\n",
      "    branch = self.repo.get_branch(\"main\")\n",
      "    for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
      "        commits.append(commit)\n",
      "    return commits\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def parse_commits(self):\n",
      "    \"\"\"\n",
      "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
      "        Each dict contains the commit sha, commit message, and author information.\n",
      "        \"\"\"\n",
      "    parsed_commits = []\n",
      "    commits = self.get_commits()\n",
      "    for commit in commits:\n",
      "        parsed_commit = {\n",
      "            \"sha\": commit.sha,\n",
      "            \"message\": commit.commit.message,\n",
      "            \"author\": {\n",
      "                \"name\": commit.commit.author.name,\n",
      "                \"email\": commit.commit.author.email,\n",
      "                \"date\": commit.commit.author.date,\n",
      "            },\n",
      "        }\n",
      "        parsed_commits.append(parsed_commit)\n",
      "    return parsed_commits\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "# A custom visitor to find function calls and their arguments\n",
      "class FunctionCallFinder(cst.CSTVisitor):\n",
      "    METADATA_DEPENDENCIES = (PositionProvider,)\n",
      "\n",
      "    def visit_Call(self, node: cst.Call) -> None:\n",
      "        function_name = None\n",
      "        if isinstance(node.func, cst.Name):\n",
      "            function_name = node.func.value\n",
      "\n",
      "        if function_name:\n",
      "            pos = self.get_metadata(PositionProvider, node).start\n",
      "            print(\n",
      "                f\"Function '{function_name}' called at line {pos.line}, column {pos.column} with arguments:\"\n",
      "            )\n",
      "\n",
      "            for arg in node.args:\n",
      "                arg_start_pos = self.get_metadata(PositionProvider, arg).start\n",
      "                arg_value = arg.value\n",
      "                if isinstance(arg_value, cst.SimpleString):\n",
      "                    arg_value = arg_value.evaluated_value\n",
      "                print(\n",
      "                    f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\n",
      "                )\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Call(self, node: cst.Call) -> None:\n",
      "    function_name = None\n",
      "    if isinstance(node.func, cst.Name):\n",
      "        function_name = node.func.value\n",
      "\n",
      "    if function_name:\n",
      "        pos = self.get_metadata(PositionProvider, node).start\n",
      "        print(\n",
      "            f\"Function '{function_name}' called at line {pos.line}, column {pos.column} with arguments:\"\n",
      "        )\n",
      "\n",
      "        for arg in node.args:\n",
      "            arg_start_pos = self.get_metadata(PositionProvider, arg).start\n",
      "            arg_value = arg.value\n",
      "            if isinstance(arg_value, cst.SimpleString):\n",
      "                arg_value = arg_value.evaluated_value\n",
      "            print(\n",
      "                f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\n",
      "            )\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "Function call: isinstance\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class MultiplicationCounterVisitor(cst.CSTVisitor):\n",
      "    def __init__(self):\n",
      "        self.count = 0\n",
      "        self.functions_with_operation_dict = {}\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "        self.current_function = node\n",
      "        self.functions_with_operation_dict[node.name] = []\n",
      "\n",
      "    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "        self.current_function = None\n",
      "\n",
      "    def visit_BinaryOperation(self, node: cst.BinaryOperation) -> None:\n",
      "        if isinstance(node.operator, cst.Multiply) or isinstance(\n",
      "            node.operator, cst.BitAnd\n",
      "        ):\n",
      "            self.count += 1\n",
      "            if self.current_function:\n",
      "                self.functions_with_operation_dict[self.current_function.name].append(\n",
      "                    cst.Module([]).code_for_node(node)\n",
      "                )\n",
      "\n",
      "    def visit_Call(self, node: cst.Call) -> None:\n",
      "        if m.matches(node, m.Call(func=m.Attribute(attr=m.Name(\"dot\")))) or m.matches(\n",
      "            node, m.Call(func=m.Name(\"dot\"), args=[m.Arg(), m.Arg()])\n",
      "        ):\n",
      "            self.count += 1\n",
      "            if self.current_function:\n",
      "                self.functions_with_operation_dict[self.current_function.name].append(\n",
      "                    cst.Module([]).code_for_node(node)\n",
      "                )\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self):\n",
      "    self.count = 0\n",
      "    self.functions_with_operation_dict = {}\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "    self.current_function = node\n",
      "    self.functions_with_operation_dict[node.name] = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "    self.current_function = None\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_BinaryOperation(self, node: cst.BinaryOperation) -> None:\n",
      "    if isinstance(node.operator, cst.Multiply) or isinstance(\n",
      "        node.operator, cst.BitAnd\n",
      "    ):\n",
      "        self.count += 1\n",
      "        if self.current_function:\n",
      "            self.functions_with_operation_dict[self.current_function.name].append(\n",
      "                cst.Module([]).code_for_node(node)\n",
      "            )\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_Call(self, node: cst.Call) -> None:\n",
      "    if m.matches(node, m.Call(func=m.Attribute(attr=m.Name(\"dot\")))) or m.matches(\n",
      "        node, m.Call(func=m.Name(\"dot\"), args=[m.Arg(), m.Arg()])\n",
      "    ):\n",
      "        self.count += 1\n",
      "        if self.current_function:\n",
      "            self.functions_with_operation_dict[self.current_function.name].append(\n",
      "                cst.Module([]).code_for_node(node)\n",
      "            )\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class FunctionAndClassVisitor(cst.CSTVisitor):\n",
      "    def __init__(self):\n",
      "        self.function_source_codes = []\n",
      "        self.function_nodes = []\n",
      "        self.class_source_codes = []\n",
      "        self.class_nodes = []\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "        function_source_code = cst.Module([]).code_for_node(node)\n",
      "        # add in place summary and code mod\n",
      "        self.function_nodes.append(node)\n",
      "        self.function_source_codes.append(function_source_code)\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
      "        class_source_code = cst.Module([]).code_for_node(node)\n",
      "        # add in place summary and code mod\n",
      "        self.class_nodes.append(node)\n",
      "        self.class_source_codes.append(class_source_code)\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self):\n",
      "    self.function_source_codes = []\n",
      "    self.function_nodes = []\n",
      "    self.class_source_codes = []\n",
      "    self.class_nodes = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "    function_source_code = cst.Module([]).code_for_node(node)\n",
      "    # add in place summary and code mod\n",
      "    self.function_nodes.append(node)\n",
      "    self.function_source_codes.append(function_source_code)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
      "    class_source_code = cst.Module([]).code_for_node(node)\n",
      "    # add in place summary and code mod\n",
      "    self.class_nodes.append(node)\n",
      "    self.class_source_codes.append(class_source_code)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class TypingCollector(cst.CSTVisitor):\n",
      "    def __init__(self):\n",
      "        # stack for storing the canonical name of the current function\n",
      "        self.stack: List[Tuple[str, ...]] = []\n",
      "        # store the annotations\n",
      "        self.annotations: Dict[\n",
      "            Tuple[str, ...],  # key: tuple of canonical class/function name\n",
      "            Tuple[cst.Parameters, Optional[cst.Annotation]],  # value: (params, returns)\n",
      "        ] = {}\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef) -> Optional[bool]:\n",
      "        self.stack.append(node.name.value)\n",
      "\n",
      "    def leave_ClassDef(self, node: cst.ClassDef) -> None:\n",
      "        self.stack.pop()\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> Optional[bool]:\n",
      "        self.stack.append(node.name.value)\n",
      "        self.annotations[tuple(self.stack)] = (node.params, node.returns)\n",
      "        return False  # pyi files don't support inner functions, return False to stop the traversal.\n",
      "\n",
      "    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "        self.stack.pop()\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "Function call: tuple\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self):\n",
      "    # stack for storing the canonical name of the current function\n",
      "    self.stack: List[Tuple[str, ...]] = []\n",
      "    # store the annotations\n",
      "    self.annotations: Dict[\n",
      "        Tuple[str, ...],  # key: tuple of canonical class/function name\n",
      "        Tuple[cst.Parameters, Optional[cst.Annotation]],  # value: (params, returns)\n",
      "    ] = {}\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef) -> Optional[bool]:\n",
      "    self.stack.append(node.name.value)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def leave_ClassDef(self, node: cst.ClassDef) -> None:\n",
      "    self.stack.pop()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> Optional[bool]:\n",
      "    self.stack.append(node.name.value)\n",
      "    self.annotations[tuple(self.stack)] = (node.params, node.returns)\n",
      "    return False  # pyi files don't support inner functions, return False to stop the traversal.\n",
      "\n",
      "Function call: tuple\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "    self.stack.pop()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class PythonMinifier:\n",
      "    def __init__(self, code: str = None):\n",
      "\n",
      "        self.code = code\n",
      "        self.output_code = None\n",
      "\n",
      "    def minify(self):\n",
      "        if self.code:\n",
      "            self.output_code = self.minify_code(self.code)\n",
      "\n",
      "    def get_minified_code(self):\n",
      "        if not self.output_code:\n",
      "            self.minify()\n",
      "        return self.output_code\n",
      "\n",
      "    @staticmethod\n",
      "    def minify_code(code: str) -> str:\n",
      "        return minify(code)\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: minify\n",
      "Related codes: ['\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, code: str = None):\n",
      "\n",
      "    self.code = code\n",
      "    self.output_code = None\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def minify(self):\n",
      "    if self.code:\n",
      "        self.output_code = self.minify_code(self.code)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_minified_code(self):\n",
      "    if not self.output_code:\n",
      "        self.minify()\n",
      "    return self.output_code\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "@staticmethod\n",
      "def minify_code(code: str) -> str:\n",
      "    return minify(code)\n",
      "\n",
      "Function call: minify\n",
      "Related codes: ['\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class PythonDocstringExtractor:\n",
      "    @staticmethod\n",
      "    def extract_docstring(function_def: cst.FunctionDef) -> str:\n",
      "        docstring = None\n",
      "\n",
      "        for stmt in function_def.body.body:\n",
      "            if isinstance(stmt, cst.SimpleStatementLine):\n",
      "                for expr in stmt.body:\n",
      "                    if isinstance(expr, cst.Expr) and isinstance(\n",
      "                        expr.value, cst.SimpleString\n",
      "                    ):\n",
      "                        docstring = expr.value.value.strip('\"').strip(\"'\")\n",
      "                        break\n",
      "            if docstring is not None:\n",
      "                break\n",
      "\n",
      "        if docstring is not None:\n",
      "            return docstring.strip()\n",
      "        else:\n",
      "            function_name = function_def.name.value\n",
      "            return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "@staticmethod\n",
      "def extract_docstring(function_def: cst.FunctionDef) -> str:\n",
      "    docstring = None\n",
      "\n",
      "    for stmt in function_def.body.body:\n",
      "        if isinstance(stmt, cst.SimpleStatementLine):\n",
      "            for expr in stmt.body:\n",
      "                if isinstance(expr, cst.Expr) and isinstance(\n",
      "                    expr.value, cst.SimpleString\n",
      "                ):\n",
      "                    docstring = expr.value.value.strip('\"').strip(\"'\")\n",
      "                    break\n",
      "        if docstring is not None:\n",
      "            break\n",
      "\n",
      "    if docstring is not None:\n",
      "        return docstring.strip()\n",
      "    else:\n",
      "        function_name = function_def.name.value\n",
      "        return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class FunctionAndClassVisitor(cst.CSTVisitor):\n",
      "    def __init__(self):\n",
      "        self.function_source_codes = []\n",
      "        self.function_nodes = []\n",
      "        self.function_count = 0\n",
      "        self.class_source_codes = []\n",
      "        self.class_nodes = []\n",
      "        self.class_count = 0\n",
      "        self.filename_map = []\n",
      "        self.full_source_list = []\n",
      "        self.full_node_list = []\n",
      "\n",
      "\n",
      "    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "        \"\"\"This method is called for every FunctionDef node in the tree.\n",
      "        and it does the following:\n",
      "        1. Gets the source code for the node\n",
      "        2. Adds the node to the list of function nodes\n",
      "        3. Adds the source code to the list of function source codes\n",
      "        4. Increments the function count\n",
      "        \"\"\"\n",
      "        function_source_code = cst.Module([]).code_for_node(node)\n",
      "        self.function_nodes.append(node)\n",
      "        self.function_source_codes.append(function_source_code)\n",
      "        self.full_node_list.append(node)\n",
      "        self.full_source_list.append(function_source_code)\n",
      "        self.function_count += 1\n",
      "\n",
      "    def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
      "        \"\"\"This method is called for every ClassDef node in the tree.\n",
      "        and it does the following:\n",
      "        1. Gets the source code for the node\n",
      "        2. Adds the node to the list of class nodes\n",
      "        3. Adds the source code to the list of class source codes\n",
      "        4. Increments the class count\n",
      "        \"\"\"\n",
      "        class_source_code = cst.Module([]).code_for_node(node)\n",
      "        self.class_nodes.append(node)\n",
      "        self.class_source_codes.append(class_source_code)\n",
      "        self.full_node_list.append(node)\n",
      "        self.full_source_list.append(class_source_code)\n",
      "        self.class_count += 1\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTVisitor\"]\n",
      "]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self):\n",
      "    self.function_source_codes = []\n",
      "    self.function_nodes = []\n",
      "    self.function_count = 0\n",
      "    self.class_source_codes = []\n",
      "    self.class_nodes = []\n",
      "    self.class_count = 0\n",
      "    self.filename_map = []\n",
      "    self.full_source_list = []\n",
      "    self.full_node_list = []\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
      "    \"\"\"This method is called for every FunctionDef node in the tree.\n",
      "        and it does the following:\n",
      "        1. Gets the source code for the node\n",
      "        2. Adds the node to the list of function nodes\n",
      "        3. Adds the source code to the list of function source codes\n",
      "        4. Increments the function count\n",
      "        \"\"\"\n",
      "    function_source_code = cst.Module([]).code_for_node(node)\n",
      "    self.function_nodes.append(node)\n",
      "    self.function_source_codes.append(function_source_code)\n",
      "    self.full_node_list.append(node)\n",
      "    self.full_source_list.append(function_source_code)\n",
      "    self.function_count += 1\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
      "    \"\"\"This method is called for every ClassDef node in the tree.\n",
      "        and it does the following:\n",
      "        1. Gets the source code for the node\n",
      "        2. Adds the node to the list of class nodes\n",
      "        3. Adds the source code to the list of class source codes\n",
      "        4. Increments the class count\n",
      "        \"\"\"\n",
      "    class_source_code = cst.Module([]).code_for_node(node)\n",
      "    self.class_nodes.append(node)\n",
      "    self.class_source_codes.append(class_source_code)\n",
      "    self.full_node_list.append(node)\n",
      "    self.full_source_list.append(class_source_code)\n",
      "    self.class_count += 1\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class PythonParser(OsProcessor):\n",
      "    def __init__(\n",
      "        self,\n",
      "        directory_path: str,\n",
      "        visitor: Optional[FunctionAndClassVisitor] = None,\n",
      "        minify_code: bool = False,\n",
      "        remove_docstrings: bool = False,\n",
      "    ):\n",
      "        super().__init__(directory_path)\n",
      "        self.visitor = visitor if visitor else FunctionAndClassVisitor()\n",
      "        self.minify_code = minify_code\n",
      "        self.remove_docstrings = remove_docstrings\n",
      "\n",
      "    def remove_docstring(self, tree: cst.Module) -> str:\n",
      "        \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\n",
      "\n",
      "        # Remove docstrings using a transformer\n",
      "        class DocstringRemover(cst.CSTTransformer):\n",
      "            def leave_FunctionDef(\n",
      "                self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
      "            ) -> cst.FunctionDef:\n",
      "                docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
      "                if docstring.startswith(\"No docstring\"):\n",
      "                    return updated_node\n",
      "\n",
      "                return updated_node.with_changes(\n",
      "                    body=updated_node.body.with_changes(\n",
      "                        body=[\n",
      "                            stmt\n",
      "                            for stmt in updated_node.body.body\n",
      "                            if not (\n",
      "                                isinstance(stmt, cst.SimpleStatementLine)\n",
      "                                and any(\n",
      "                                    isinstance(expr, cst.Expr)\n",
      "                                    and isinstance(expr.value, cst.SimpleString)\n",
      "                                    for expr in stmt.body\n",
      "                                )\n",
      "                            )\n",
      "                        ]\n",
      "                    )\n",
      "                )\n",
      "\n",
      "        tree = tree.visit(DocstringRemover())\n",
      "        return tree.code\n",
      "\n",
      "    def _process_file(self, file_path: str):\n",
      "        \"\"\"This method is called for every file in the directory.\n",
      "        It does the following:\n",
      "        1. Reads the file\n",
      "        2. Parses the file\n",
      "        3. Visits the file with the visitor\n",
      "        \"\"\"\n",
      "        #get current number of nodes in visitor\n",
      "        current_node_count = self.visitor.function_count + self.visitor.class_count\n",
      "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
      "            source_code = file.read()\n",
      "\n",
      "        try:\n",
      "            tree = cst.parse_module(source_code)\n",
      "        except cst.ParserSyntaxError:\n",
      "            print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
      "            return\n",
      "\n",
      "        tree.visit(self.visitor)\n",
      "        #calculate how many new nodes were added\n",
      "        new_node_counter = self.visitor.function_count + self.visitor.class_count - current_node_count\n",
      "        self.visitor.filename_map.extend([file_path]*new_node_counter)\n",
      "        # Remove docstrings if specified\n",
      "        if self.remove_docstrings:\n",
      "            source_code = self.remove_docstring(source_code, tree)\n",
      "\n",
      "        # Minify the code if specified\n",
      "        if self.minify_code:\n",
      "            minifier = PythonMinifier(source_code)\n",
      "            source_code = minifier.get_minified_code()\n",
      "\n",
      "        # Add the processed code to the corresponding list in the visitor\n",
      "        self.visitor.function_source_codes.append(source_code)\n",
      "\n",
      "    def process_file(self, file_path: str):\n",
      "        \"\"\"This method is called for every file in the directory.\n",
      "        It does the following:\n",
      "        1. Runs flake8 on the file\n",
      "        if flake8 returns a non-zero exit code, it means the file has a syntax error\n",
      "        2. Reads the file\n",
      "        3. Parses the file\n",
      "        4. Visits the file with the visitor\n",
      "\n",
      "        \"\"\"\n",
      "        result = subprocess.run(\n",
      "            [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
      "        )\n",
      "\n",
      "        if result.returncode != 0:\n",
      "            print(f\"Skipping file with syntax error: {file_path}\")\n",
      "            print(result.stderr.decode(\"utf-8\"))\n",
      "            return\n",
      "\n",
      "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
      "            source_code = f.read()\n",
      "\n",
      "        try:\n",
      "            tree = cst.parse_module(source_code)\n",
      "            tree.visit(self.visitor)\n",
      "        except cst.ParserSyntaxError as e:\n",
      "            print(f\"Syntax error: {e}\")\n",
      "            print(f\"Skipping file with syntax error: {file_path}\")\n",
      "\n",
      "    def process_directory(\n",
      "        self,\n",
      "    ) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\n",
      "        \"\"\"This method is called for every directory.\n",
      "        It does the following:\n",
      "        1. Gets all the python files in the directory\n",
      "        2. Processes each file\n",
      "        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\n",
      "        \"\"\"\n",
      "\n",
      "        python_files = self.get_files_with_extension(\".py\")\n",
      "\n",
      "        for file_path in python_files:\n",
      "            self._process_file(file_path)\n",
      "\n",
      "        result_dict = {\n",
      "            'function_source_codes': self.visitor.function_source_codes,\n",
      "            'function_nodes': self.visitor.function_nodes,\n",
      "            'class_source_codes': self.visitor.class_source_codes,\n",
      "            'class_nodes': self.visitor.class_nodes,\n",
      "            'file_map': self.visitor.filename_map,\n",
      "            'full_nodes': self.visitor.full_node_list,\n",
      "            'full_source': self.visitor.full_source_list\n",
      "        }\n",
      "\n",
      "\n",
      "        return result_dict\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"OsProcessor\"]\n",
      "]\n",
      "Function call: super\n",
      "Function call: FunctionAndClassVisitor\n",
      "Function call: isinstance\n",
      "Function call: any\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: DocstringRemover\n",
      "Function call: open\n",
      "Function call: print\n",
      "Function call: PythonMinifier\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: open\n",
      "Function call: print\n",
      "Function call: print\n",
      "Related codes: ['\\n\\nclass FunctionAndClassVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        function_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.function_nodes.append(node)\\n        self.function_source_codes.append(function_source_code)\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\\n        class_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.class_nodes.append(node)\\n        self.class_source_codes.append(class_source_code)\\n', '\\n# Remove docstrings using a transformer\\nclass DocstringRemover(cst.CSTTransformer):\\n    def leave_FunctionDef(\\n        self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\\n    ) -> cst.FunctionDef:\\n        docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n        if docstring.startswith(\"No docstring\"):\\n            return updated_node\\n\\n        return updated_node.with_changes(\\n            body=updated_node.body.with_changes(\\n                body=[\\n                    stmt\\n                    for stmt in updated_node.body.body\\n                    if not (\\n                        isinstance(stmt, cst.SimpleStatementLine)\\n                        and any(\\n                            isinstance(expr, cst.Expr)\\n                            and isinstance(expr.value, cst.SimpleString)\\n                            for expr in stmt.body\\n                        )\\n                    )\\n                ]\\n            )\\n        )\\n', '\\n\\nclass PythonMinifier:\\n    def __init__(self, code: str = None):\\n\\n        self.code = code\\n        self.output_code = None\\n\\n    def minify(self):\\n        if self.code:\\n            self.output_code = self.minify_code(self.code)\\n\\n    def get_minified_code(self):\\n        if not self.output_code:\\n            self.minify()\\n        return self.output_code\\n\\n    @staticmethod\\n    def minify_code(code: str) -> str:\\n        return minify(code)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self,\n",
      "    directory_path: str,\n",
      "    visitor: Optional[FunctionAndClassVisitor] = None,\n",
      "    minify_code: bool = False,\n",
      "    remove_docstrings: bool = False,\n",
      "):\n",
      "    super().__init__(directory_path)\n",
      "    self.visitor = visitor if visitor else FunctionAndClassVisitor()\n",
      "    self.minify_code = minify_code\n",
      "    self.remove_docstrings = remove_docstrings\n",
      "\n",
      "Function call: super\n",
      "Function call: FunctionAndClassVisitor\n",
      "Related codes: ['\\n\\nclass FunctionAndClassVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        function_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.function_nodes.append(node)\\n        self.function_source_codes.append(function_source_code)\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\\n        class_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.class_nodes.append(node)\\n        self.class_source_codes.append(class_source_code)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def remove_docstring(self, tree: cst.Module) -> str:\n",
      "    \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\n",
      "\n",
      "    # Remove docstrings using a transformer\n",
      "    class DocstringRemover(cst.CSTTransformer):\n",
      "        def leave_FunctionDef(\n",
      "            self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
      "        ) -> cst.FunctionDef:\n",
      "            docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
      "            if docstring.startswith(\"No docstring\"):\n",
      "                return updated_node\n",
      "\n",
      "            return updated_node.with_changes(\n",
      "                body=updated_node.body.with_changes(\n",
      "                    body=[\n",
      "                        stmt\n",
      "                        for stmt in updated_node.body.body\n",
      "                        if not (\n",
      "                            isinstance(stmt, cst.SimpleStatementLine)\n",
      "                            and any(\n",
      "                                isinstance(expr, cst.Expr)\n",
      "                                and isinstance(expr.value, cst.SimpleString)\n",
      "                                for expr in stmt.body\n",
      "                            )\n",
      "                        )\n",
      "                    ]\n",
      "                )\n",
      "            )\n",
      "\n",
      "    tree = tree.visit(DocstringRemover())\n",
      "    return tree.code\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTTransformer\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: any\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "Function call: DocstringRemover\n",
      "Related codes: ['\\n# Remove docstrings using a transformer\\nclass DocstringRemover(cst.CSTTransformer):\\n    def leave_FunctionDef(\\n        self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\\n    ) -> cst.FunctionDef:\\n        docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n        if docstring.startswith(\"No docstring\"):\\n            return updated_node\\n\\n        return updated_node.with_changes(\\n            body=updated_node.body.with_changes(\\n                body=[\\n                    stmt\\n                    for stmt in updated_node.body.body\\n                    if not (\\n                        isinstance(stmt, cst.SimpleStatementLine)\\n                        and any(\\n                            isinstance(expr, cst.Expr)\\n                            and isinstance(expr.value, cst.SimpleString)\\n                            for expr in stmt.body\\n                        )\\n                    )\\n                ]\\n            )\\n        )\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "# Remove docstrings using a transformer\n",
      "class DocstringRemover(cst.CSTTransformer):\n",
      "    def leave_FunctionDef(\n",
      "        self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
      "    ) -> cst.FunctionDef:\n",
      "        docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
      "        if docstring.startswith(\"No docstring\"):\n",
      "            return updated_node\n",
      "\n",
      "        return updated_node.with_changes(\n",
      "            body=updated_node.body.with_changes(\n",
      "                body=[\n",
      "                    stmt\n",
      "                    for stmt in updated_node.body.body\n",
      "                    if not (\n",
      "                        isinstance(stmt, cst.SimpleStatementLine)\n",
      "                        and any(\n",
      "                            isinstance(expr, cst.Expr)\n",
      "                            and isinstance(expr.value, cst.SimpleString)\n",
      "                            for expr in stmt.body\n",
      "                        )\n",
      "                    )\n",
      "                ]\n",
      "            )\n",
      "        )\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[\"cst.CSTTransformer\"]\n",
      "]\n",
      "Function call: isinstance\n",
      "Function call: any\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "def leave_FunctionDef(\n",
      "    self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
      ") -> cst.FunctionDef:\n",
      "    docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
      "    if docstring.startswith(\"No docstring\"):\n",
      "        return updated_node\n",
      "\n",
      "    return updated_node.with_changes(\n",
      "        body=updated_node.body.with_changes(\n",
      "            body=[\n",
      "                stmt\n",
      "                for stmt in updated_node.body.body\n",
      "                if not (\n",
      "                    isinstance(stmt, cst.SimpleStatementLine)\n",
      "                    and any(\n",
      "                        isinstance(expr, cst.Expr)\n",
      "                        and isinstance(expr.value, cst.SimpleString)\n",
      "                        for expr in stmt.body\n",
      "                    )\n",
      "                )\n",
      "            ]\n",
      "        )\n",
      "    )\n",
      "\n",
      "Function call: isinstance\n",
      "Function call: any\n",
      "Function call: isinstance\n",
      "Function call: isinstance\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _process_file(self, file_path: str):\n",
      "    \"\"\"This method is called for every file in the directory.\n",
      "        It does the following:\n",
      "        1. Reads the file\n",
      "        2. Parses the file\n",
      "        3. Visits the file with the visitor\n",
      "        \"\"\"\n",
      "    #get current number of nodes in visitor\n",
      "    current_node_count = self.visitor.function_count + self.visitor.class_count\n",
      "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
      "        source_code = file.read()\n",
      "\n",
      "    try:\n",
      "        tree = cst.parse_module(source_code)\n",
      "    except cst.ParserSyntaxError:\n",
      "        print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
      "        return\n",
      "\n",
      "    tree.visit(self.visitor)\n",
      "    #calculate how many new nodes were added\n",
      "    new_node_counter = self.visitor.function_count + self.visitor.class_count - current_node_count\n",
      "    self.visitor.filename_map.extend([file_path]*new_node_counter)\n",
      "    # Remove docstrings if specified\n",
      "    if self.remove_docstrings:\n",
      "        source_code = self.remove_docstring(source_code, tree)\n",
      "\n",
      "    # Minify the code if specified\n",
      "    if self.minify_code:\n",
      "        minifier = PythonMinifier(source_code)\n",
      "        source_code = minifier.get_minified_code()\n",
      "\n",
      "    # Add the processed code to the corresponding list in the visitor\n",
      "    self.visitor.function_source_codes.append(source_code)\n",
      "\n",
      "Function call: open\n",
      "Function call: print\n",
      "Function call: PythonMinifier\n",
      "Related codes: ['\\n\\nclass PythonMinifier:\\n    def __init__(self, code: str = None):\\n\\n        self.code = code\\n        self.output_code = None\\n\\n    def minify(self):\\n        if self.code:\\n            self.output_code = self.minify_code(self.code)\\n\\n    def get_minified_code(self):\\n        if not self.output_code:\\n            self.minify()\\n        return self.output_code\\n\\n    @staticmethod\\n    def minify_code(code: str) -> str:\\n        return minify(code)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def process_file(self, file_path: str):\n",
      "    \"\"\"This method is called for every file in the directory.\n",
      "        It does the following:\n",
      "        1. Runs flake8 on the file\n",
      "        if flake8 returns a non-zero exit code, it means the file has a syntax error\n",
      "        2. Reads the file\n",
      "        3. Parses the file\n",
      "        4. Visits the file with the visitor\n",
      "\n",
      "        \"\"\"\n",
      "    result = subprocess.run(\n",
      "        [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
      "    )\n",
      "\n",
      "    if result.returncode != 0:\n",
      "        print(f\"Skipping file with syntax error: {file_path}\")\n",
      "        print(result.stderr.decode(\"utf-8\"))\n",
      "        return\n",
      "\n",
      "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
      "        source_code = f.read()\n",
      "\n",
      "    try:\n",
      "        tree = cst.parse_module(source_code)\n",
      "        tree.visit(self.visitor)\n",
      "    except cst.ParserSyntaxError as e:\n",
      "        print(f\"Syntax error: {e}\")\n",
      "        print(f\"Skipping file with syntax error: {file_path}\")\n",
      "\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: open\n",
      "Function call: print\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def process_directory(\n",
      "    self,\n",
      ") -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\n",
      "    \"\"\"This method is called for every directory.\n",
      "        It does the following:\n",
      "        1. Gets all the python files in the directory\n",
      "        2. Processes each file\n",
      "        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\n",
      "        \"\"\"\n",
      "\n",
      "    python_files = self.get_files_with_extension(\".py\")\n",
      "\n",
      "    for file_path in python_files:\n",
      "        self._process_file(file_path)\n",
      "\n",
      "    result_dict = {\n",
      "        'function_source_codes': self.visitor.function_source_codes,\n",
      "        'function_nodes': self.visitor.function_nodes,\n",
      "        'class_source_codes': self.visitor.class_source_codes,\n",
      "        'class_nodes': self.visitor.class_nodes,\n",
      "        'file_map': self.visitor.filename_map,\n",
      "        'full_nodes': self.visitor.full_node_list,\n",
      "        'full_source': self.visitor.full_source_list\n",
      "    }\n",
      "\n",
      "\n",
      "    return result_dict\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class IssueParser:\n",
      "    def __init__(self, repo_name):\n",
      "        self.g = Github()\n",
      "        self.repo = self.g.get_repo(repo_name)\n",
      "\n",
      "    def get_issues(self, state=\"open\"):\n",
      "        \"\"\"\n",
      "        Returns a list of all issues in the repo with the specified state.\n",
      "        \"\"\"\n",
      "        issues = []\n",
      "        for issue in self.repo.get_issues(state=state):\n",
      "            issues.append(issue)\n",
      "        return issues\n",
      "\n",
      "    def parse_issues(self, state=\"open\"):\n",
      "        \"\"\"\n",
      "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
      "        Each dict contains the issue number, title, body, and labels.\n",
      "        \"\"\"\n",
      "        parsed_issues = []\n",
      "        issues = self.get_issues(state=state)\n",
      "        for issue in issues:\n",
      "            parsed_issue = {\n",
      "                \"number\": issue.number,\n",
      "                \"title\": issue.title,\n",
      "                \"body\": issue.body,\n",
      "                \"labels\": [label.name for label in issue.labels],\n",
      "            }\n",
      "            parsed_issues.append(parsed_issue)\n",
      "        return parsed_issues\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: Github\n",
      "Related codes: ['\\n\\nclass GithubProcessor(OsProcessor):\\n    def __init__(\\n        self,\\n        base_directory: str,\\n        username=None,\\n        repo_name=None,\\n        code_parsers=None,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n    ):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.base_directory = base_directory\\n        self.github = Github()\\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n        repo_path = self.clone_repo(self.repo.clone_url)\\n\\n        OsProcessor.__init__(self, repo_path)\\n        self.code_parsers = code_parsers or [\\n            PythonParser(\\n                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\\n            )\\n        ]\\n\\n    def get_public_repos(self):\\n        \"\"\"Returns a list of all public repos for the user.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def clone_repo(self, repo_url: str):\\n        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.base_directory, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n    def process_repo(self, repo_path=None):\\n        \"\"\"Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n        if repo_path is None:\\n            repo_path = self.directory_path\\n\\n        for code_parser in self.code_parsers:\\n            code_parser.directory_path = repo_path\\n            code_parser.process_directory(repo_path)\\n\\n    def process_repos(self):\\n        \"\"\"Processes all public repos for the user.\"\"\"\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.clone_repo(repo.clone_url)\\n                self.process_repo(repo_path)\\n                shutil.rmtree(repo_path)\\n\\n    def get_repo(self, repo_name):\\n        \"\"\"Returns the repo with the specified name.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_single_repo(self):\\n\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.clone_repo(repo.clone_url)\\n        self.process_repo(repo_path)\\n        shutil.rmtree(repo_path)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, repo_name):\n",
      "    self.g = Github()\n",
      "    self.repo = self.g.get_repo(repo_name)\n",
      "\n",
      "Function call: Github\n",
      "Related codes: ['\\n\\nclass GithubProcessor(OsProcessor):\\n    def __init__(\\n        self,\\n        base_directory: str,\\n        username=None,\\n        repo_name=None,\\n        code_parsers=None,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n    ):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.base_directory = base_directory\\n        self.github = Github()\\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n        repo_path = self.clone_repo(self.repo.clone_url)\\n\\n        OsProcessor.__init__(self, repo_path)\\n        self.code_parsers = code_parsers or [\\n            PythonParser(\\n                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\\n            )\\n        ]\\n\\n    def get_public_repos(self):\\n        \"\"\"Returns a list of all public repos for the user.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def clone_repo(self, repo_url: str):\\n        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.base_directory, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n    def process_repo(self, repo_path=None):\\n        \"\"\"Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n        if repo_path is None:\\n            repo_path = self.directory_path\\n\\n        for code_parser in self.code_parsers:\\n            code_parser.directory_path = repo_path\\n            code_parser.process_directory(repo_path)\\n\\n    def process_repos(self):\\n        \"\"\"Processes all public repos for the user.\"\"\"\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.clone_repo(repo.clone_url)\\n                self.process_repo(repo_path)\\n                shutil.rmtree(repo_path)\\n\\n    def get_repo(self, repo_name):\\n        \"\"\"Returns the repo with the specified name.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_single_repo(self):\\n\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.clone_repo(repo.clone_url)\\n        self.process_repo(repo_path)\\n        shutil.rmtree(repo_path)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_issues(self, state=\"open\"):\n",
      "    \"\"\"\n",
      "        Returns a list of all issues in the repo with the specified state.\n",
      "        \"\"\"\n",
      "    issues = []\n",
      "    for issue in self.repo.get_issues(state=state):\n",
      "        issues.append(issue)\n",
      "    return issues\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def parse_issues(self, state=\"open\"):\n",
      "    \"\"\"\n",
      "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
      "        Each dict contains the issue number, title, body, and labels.\n",
      "        \"\"\"\n",
      "    parsed_issues = []\n",
      "    issues = self.get_issues(state=state)\n",
      "    for issue in issues:\n",
      "        parsed_issue = {\n",
      "            \"number\": issue.number,\n",
      "            \"title\": issue.title,\n",
      "            \"body\": issue.body,\n",
      "            \"labels\": [label.name for label in issue.labels],\n",
      "        }\n",
      "        parsed_issues.append(parsed_issue)\n",
      "    return parsed_issues\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class CommitParser:\n",
      "    def __init__(self, repo_name):\n",
      "        self.g = Github()\n",
      "        self.repo = self.g.get_repo(repo_name)\n",
      "\n",
      "    def get_commits(self):\n",
      "        \"\"\"\n",
      "        Returns a list of all commits in the main branch of the repository.\n",
      "        \"\"\"\n",
      "        commits = []\n",
      "        branch = self.repo.get_branch(\"main\")\n",
      "        for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
      "            commits.append(commit)\n",
      "        return commits\n",
      "\n",
      "    def parse_commits(self):\n",
      "        \"\"\"\n",
      "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
      "        Each dict contains the commit sha, commit message, and author information.\n",
      "        \"\"\"\n",
      "        parsed_commits = []\n",
      "        commits = self.get_commits()\n",
      "        for commit in commits:\n",
      "            parsed_commit = {\n",
      "                \"sha\": commit.sha,\n",
      "                \"message\": commit.commit.message,\n",
      "                \"author\": {\n",
      "                    \"name\": commit.commit.author.name,\n",
      "                    \"email\": commit.commit.author.email,\n",
      "                    \"date\": commit.commit.author.date,\n",
      "                },\n",
      "            }\n",
      "            parsed_commits.append(parsed_commit)\n",
      "        return parsed_commits\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: Github\n",
      "Related codes: ['\\n\\nclass GithubProcessor(OsProcessor):\\n    def __init__(\\n        self,\\n        base_directory: str,\\n        username=None,\\n        repo_name=None,\\n        code_parsers=None,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n    ):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.base_directory = base_directory\\n        self.github = Github()\\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n        repo_path = self.clone_repo(self.repo.clone_url)\\n\\n        OsProcessor.__init__(self, repo_path)\\n        self.code_parsers = code_parsers or [\\n            PythonParser(\\n                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\\n            )\\n        ]\\n\\n    def get_public_repos(self):\\n        \"\"\"Returns a list of all public repos for the user.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def clone_repo(self, repo_url: str):\\n        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.base_directory, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n    def process_repo(self, repo_path=None):\\n        \"\"\"Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n        if repo_path is None:\\n            repo_path = self.directory_path\\n\\n        for code_parser in self.code_parsers:\\n            code_parser.directory_path = repo_path\\n            code_parser.process_directory(repo_path)\\n\\n    def process_repos(self):\\n        \"\"\"Processes all public repos for the user.\"\"\"\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.clone_repo(repo.clone_url)\\n                self.process_repo(repo_path)\\n                shutil.rmtree(repo_path)\\n\\n    def get_repo(self, repo_name):\\n        \"\"\"Returns the repo with the specified name.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_single_repo(self):\\n\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.clone_repo(repo.clone_url)\\n        self.process_repo(repo_path)\\n        shutil.rmtree(repo_path)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, repo_name):\n",
      "    self.g = Github()\n",
      "    self.repo = self.g.get_repo(repo_name)\n",
      "\n",
      "Function call: Github\n",
      "Related codes: ['\\n\\nclass GithubProcessor(OsProcessor):\\n    def __init__(\\n        self,\\n        base_directory: str,\\n        username=None,\\n        repo_name=None,\\n        code_parsers=None,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n    ):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.base_directory = base_directory\\n        self.github = Github()\\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n        repo_path = self.clone_repo(self.repo.clone_url)\\n\\n        OsProcessor.__init__(self, repo_path)\\n        self.code_parsers = code_parsers or [\\n            PythonParser(\\n                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\\n            )\\n        ]\\n\\n    def get_public_repos(self):\\n        \"\"\"Returns a list of all public repos for the user.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def clone_repo(self, repo_url: str):\\n        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.base_directory, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n    def process_repo(self, repo_path=None):\\n        \"\"\"Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n        if repo_path is None:\\n            repo_path = self.directory_path\\n\\n        for code_parser in self.code_parsers:\\n            code_parser.directory_path = repo_path\\n            code_parser.process_directory(repo_path)\\n\\n    def process_repos(self):\\n        \"\"\"Processes all public repos for the user.\"\"\"\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.clone_repo(repo.clone_url)\\n                self.process_repo(repo_path)\\n                shutil.rmtree(repo_path)\\n\\n    def get_repo(self, repo_name):\\n        \"\"\"Returns the repo with the specified name.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_single_repo(self):\\n\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.clone_repo(repo.clone_url)\\n        self.process_repo(repo_path)\\n        shutil.rmtree(repo_path)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_commits(self):\n",
      "    \"\"\"\n",
      "        Returns a list of all commits in the main branch of the repository.\n",
      "        \"\"\"\n",
      "    commits = []\n",
      "    branch = self.repo.get_branch(\"main\")\n",
      "    for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
      "        commits.append(commit)\n",
      "    return commits\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def parse_commits(self):\n",
      "    \"\"\"\n",
      "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
      "        Each dict contains the commit sha, commit message, and author information.\n",
      "        \"\"\"\n",
      "    parsed_commits = []\n",
      "    commits = self.get_commits()\n",
      "    for commit in commits:\n",
      "        parsed_commit = {\n",
      "            \"sha\": commit.sha,\n",
      "            \"message\": commit.commit.message,\n",
      "            \"author\": {\n",
      "                \"name\": commit.commit.author.name,\n",
      "                \"email\": commit.commit.author.email,\n",
      "                \"date\": commit.commit.author.date,\n",
      "            },\n",
      "        }\n",
      "        parsed_commits.append(parsed_commit)\n",
      "    return parsed_commits\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class DirectoryProcessor:\n",
      "    def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\n",
      "        self.directory_path = directory_path\n",
      "        self.visitor = visitor\n",
      "\n",
      "    def _process_file(self, file_path: str):\n",
      "        with open(file_path, \"r\") as file:\n",
      "            source_code = file.read()\n",
      "\n",
      "        try:\n",
      "            tree = cst.parse_module(source_code)\n",
      "        except cst.ParserSyntaxError:\n",
      "            print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
      "            return\n",
      "\n",
      "        tree.visit(self.visitor)\n",
      "\n",
      "    def process_file(self, file_path: str):\n",
      "        # Run flake8 on the file\n",
      "        result = subprocess.run(\n",
      "            [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
      "        )\n",
      "\n",
      "        if result.returncode != 0:\n",
      "            print(f\"Skipping file with syntax error: {file_path}\")\n",
      "            print(result.stderr.decode(\"utf-8\"))\n",
      "            return\n",
      "\n",
      "        with open(file_path, \"r\") as f:\n",
      "            source_code = f.read()\n",
      "\n",
      "        try:\n",
      "            tree = cst.parse_module(source_code)\n",
      "            tree.visit(self.visitor)\n",
      "        except cst.ParserSyntaxError as e:\n",
      "            print(f\"Syntax error: {e}\")\n",
      "            print(f\"Skipping file with syntax error: {file_path}\")\n",
      "\n",
      "    def process_directory(self) -> List[str]:\n",
      "        function_source_codes = []\n",
      "        class_source_codes = []\n",
      "\n",
      "        for root, _, files in os.walk(self.directory_path):\n",
      "            for file in files:\n",
      "                if file.endswith(\".py\"):\n",
      "                    file_path = os.path.join(root, file)\n",
      "                    self._process_file(file_path)\n",
      "\n",
      "        function_source_codes = self.visitor.function_source_codes\n",
      "        function_nodes = self.visitor.function_nodes\n",
      "        class_source_codes = self.visitor.class_source_codes\n",
      "        class_nodes = self.visitor.class_nodes\n",
      "\n",
      "        return function_source_codes, class_source_codes, function_nodes, class_nodes\n",
      "\n",
      "    def clone_repo(self, repo_url):\n",
      "        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
      "        target_directory = os.path.join(self.directory_path, repo_name)\n",
      "\n",
      "        if os.path.exists(target_directory):\n",
      "            shutil.rmtree(target_directory)\n",
      "\n",
      "        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
      "\n",
      "        return target_directory\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: FunctionAndClassVisitor\n",
      "Function call: open\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: open\n",
      "Function call: print\n",
      "Function call: print\n",
      "Related codes: ['\\n\\nclass FunctionAndClassVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        function_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.function_nodes.append(node)\\n        self.function_source_codes.append(function_source_code)\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\\n        class_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.class_nodes.append(node)\\n        self.class_source_codes.append(class_source_code)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\n",
      "    self.directory_path = directory_path\n",
      "    self.visitor = visitor\n",
      "\n",
      "Function call: FunctionAndClassVisitor\n",
      "Related codes: ['\\n\\nclass FunctionAndClassVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        function_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.function_nodes.append(node)\\n        self.function_source_codes.append(function_source_code)\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\\n        class_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.class_nodes.append(node)\\n        self.class_source_codes.append(class_source_code)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _process_file(self, file_path: str):\n",
      "    with open(file_path, \"r\") as file:\n",
      "        source_code = file.read()\n",
      "\n",
      "    try:\n",
      "        tree = cst.parse_module(source_code)\n",
      "    except cst.ParserSyntaxError:\n",
      "        print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
      "        return\n",
      "\n",
      "    tree.visit(self.visitor)\n",
      "\n",
      "Function call: open\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def process_file(self, file_path: str):\n",
      "    # Run flake8 on the file\n",
      "    result = subprocess.run(\n",
      "        [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
      "    )\n",
      "\n",
      "    if result.returncode != 0:\n",
      "        print(f\"Skipping file with syntax error: {file_path}\")\n",
      "        print(result.stderr.decode(\"utf-8\"))\n",
      "        return\n",
      "\n",
      "    with open(file_path, \"r\") as f:\n",
      "        source_code = f.read()\n",
      "\n",
      "    try:\n",
      "        tree = cst.parse_module(source_code)\n",
      "        tree.visit(self.visitor)\n",
      "    except cst.ParserSyntaxError as e:\n",
      "        print(f\"Syntax error: {e}\")\n",
      "        print(f\"Skipping file with syntax error: {file_path}\")\n",
      "\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: open\n",
      "Function call: print\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def process_directory(self) -> List[str]:\n",
      "    function_source_codes = []\n",
      "    class_source_codes = []\n",
      "\n",
      "    for root, _, files in os.walk(self.directory_path):\n",
      "        for file in files:\n",
      "            if file.endswith(\".py\"):\n",
      "                file_path = os.path.join(root, file)\n",
      "                self._process_file(file_path)\n",
      "\n",
      "    function_source_codes = self.visitor.function_source_codes\n",
      "    function_nodes = self.visitor.function_nodes\n",
      "    class_source_codes = self.visitor.class_source_codes\n",
      "    class_nodes = self.visitor.class_nodes\n",
      "\n",
      "    return function_source_codes, class_source_codes, function_nodes, class_nodes\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def clone_repo(self, repo_url):\n",
      "    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
      "    target_directory = os.path.join(self.directory_path, repo_name)\n",
      "\n",
      "    if os.path.exists(target_directory):\n",
      "        shutil.rmtree(target_directory)\n",
      "\n",
      "    subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
      "\n",
      "    return target_directory\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class GitHubUserProcessor:\n",
      "    def __init__(\n",
      "        self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
      "    ):\n",
      "        self.username = username\n",
      "        self.repo_name = repo_name\n",
      "        self.github = Github()\n",
      "        self.directory_processor = None\n",
      "        self.function_source_codes = []\n",
      "        self.class_source_codes = []\n",
      "        self.visitor = visitor\n",
      "\n",
      "    def get_public_repos(self):\n",
      "        user = self.github.get_user(self.username)\n",
      "        return user.get_repos()\n",
      "\n",
      "    def process_repos(self, base_directory):\n",
      "        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
      "        for repo in self.get_public_repos():\n",
      "            if not repo.private:\n",
      "                print(f\"Processing repo: {repo.name}\")\n",
      "                repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
      "                (\n",
      "                    function_source_codes,\n",
      "                    class_source_codes,\n",
      "                ) = self.directory_processor.process_directory()\n",
      "                self.function_source_codes.extend(function_source_codes)\n",
      "                self.class_source_codes.extend(class_source_codes)\n",
      "                shutil.rmtree(repo_path)\n",
      "\n",
      "        return self.directory_processor\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: FunctionAndClassVisitor\n",
      "Function call: Github\n",
      "Function call: DirectoryProcessor\n",
      "Function call: print\n",
      "Related codes: ['\\n\\nclass FunctionAndClassVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        function_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.function_nodes.append(node)\\n        self.function_source_codes.append(function_source_code)\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\\n        class_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.class_nodes.append(node)\\n        self.class_source_codes.append(class_source_code)\\n', '\\n\\nclass GithubProcessor(OsProcessor):\\n    def __init__(\\n        self,\\n        base_directory: str,\\n        username=None,\\n        repo_name=None,\\n        code_parsers=None,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n    ):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.base_directory = base_directory\\n        self.github = Github()\\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n        repo_path = self.clone_repo(self.repo.clone_url)\\n\\n        OsProcessor.__init__(self, repo_path)\\n        self.code_parsers = code_parsers or [\\n            PythonParser(\\n                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\\n            )\\n        ]\\n\\n    def get_public_repos(self):\\n        \"\"\"Returns a list of all public repos for the user.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def clone_repo(self, repo_url: str):\\n        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.base_directory, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n    def process_repo(self, repo_path=None):\\n        \"\"\"Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n        if repo_path is None:\\n            repo_path = self.directory_path\\n\\n        for code_parser in self.code_parsers:\\n            code_parser.directory_path = repo_path\\n            code_parser.process_directory(repo_path)\\n\\n    def process_repos(self):\\n        \"\"\"Processes all public repos for the user.\"\"\"\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.clone_repo(repo.clone_url)\\n                self.process_repo(repo_path)\\n                shutil.rmtree(repo_path)\\n\\n    def get_repo(self, repo_name):\\n        \"\"\"Returns the repo with the specified name.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_single_repo(self):\\n\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.clone_repo(repo.clone_url)\\n        self.process_repo(repo_path)\\n        shutil.rmtree(repo_path)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n', '\\n\\nclass DirectoryProcessor:\\n    def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\\n        self.directory_path = directory_path\\n        self.visitor = visitor\\n\\n    def _process_file(self, file_path: str):\\n        with open(file_path, \"r\") as file:\\n            source_code = file.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n        except cst.ParserSyntaxError:\\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n            return\\n\\n        tree.visit(self.visitor)\\n\\n    def process_file(self, file_path: str):\\n        # Run flake8 on the file\\n        result = subprocess.run(\\n            [\"flake8\", \"--select=E999\", file_path], capture_output=True\\n        )\\n\\n        if result.returncode != 0:\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n            print(result.stderr.decode(\"utf-8\"))\\n            return\\n\\n        with open(file_path, \"r\") as f:\\n            source_code = f.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n            tree.visit(self.visitor)\\n        except cst.ParserSyntaxError as e:\\n            print(f\"Syntax error: {e}\")\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n\\n    def process_directory(self) -> List[str]:\\n        function_source_codes = []\\n        class_source_codes = []\\n\\n        for root, _, files in os.walk(self.directory_path):\\n            for file in files:\\n                if file.endswith(\".py\"):\\n                    file_path = os.path.join(root, file)\\n                    self._process_file(file_path)\\n\\n        function_source_codes = self.visitor.function_source_codes\\n        function_nodes = self.visitor.function_nodes\\n        class_source_codes = self.visitor.class_source_codes\\n        class_nodes = self.visitor.class_nodes\\n\\n        return function_source_codes, class_source_codes, function_nodes, class_nodes\\n\\n    def clone_repo(self, repo_url):\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.directory_path, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
      "):\n",
      "    self.username = username\n",
      "    self.repo_name = repo_name\n",
      "    self.github = Github()\n",
      "    self.directory_processor = None\n",
      "    self.function_source_codes = []\n",
      "    self.class_source_codes = []\n",
      "    self.visitor = visitor\n",
      "\n",
      "Function call: FunctionAndClassVisitor\n",
      "Function call: Github\n",
      "Related codes: ['\\n\\nclass FunctionAndClassVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        function_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.function_nodes.append(node)\\n        self.function_source_codes.append(function_source_code)\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\\n        class_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.class_nodes.append(node)\\n        self.class_source_codes.append(class_source_code)\\n', '\\n\\nclass GithubProcessor(OsProcessor):\\n    def __init__(\\n        self,\\n        base_directory: str,\\n        username=None,\\n        repo_name=None,\\n        code_parsers=None,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n    ):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.base_directory = base_directory\\n        self.github = Github()\\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n        repo_path = self.clone_repo(self.repo.clone_url)\\n\\n        OsProcessor.__init__(self, repo_path)\\n        self.code_parsers = code_parsers or [\\n            PythonParser(\\n                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\\n            )\\n        ]\\n\\n    def get_public_repos(self):\\n        \"\"\"Returns a list of all public repos for the user.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def clone_repo(self, repo_url: str):\\n        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.base_directory, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n    def process_repo(self, repo_path=None):\\n        \"\"\"Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n        if repo_path is None:\\n            repo_path = self.directory_path\\n\\n        for code_parser in self.code_parsers:\\n            code_parser.directory_path = repo_path\\n            code_parser.process_directory(repo_path)\\n\\n    def process_repos(self):\\n        \"\"\"Processes all public repos for the user.\"\"\"\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.clone_repo(repo.clone_url)\\n                self.process_repo(repo_path)\\n                shutil.rmtree(repo_path)\\n\\n    def get_repo(self, repo_name):\\n        \"\"\"Returns the repo with the specified name.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_single_repo(self):\\n\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.clone_repo(repo.clone_url)\\n        self.process_repo(repo_path)\\n        shutil.rmtree(repo_path)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_public_repos(self):\n",
      "    user = self.github.get_user(self.username)\n",
      "    return user.get_repos()\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def process_repos(self, base_directory):\n",
      "    self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
      "    for repo in self.get_public_repos():\n",
      "        if not repo.private:\n",
      "            print(f\"Processing repo: {repo.name}\")\n",
      "            repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
      "            (\n",
      "                function_source_codes,\n",
      "                class_source_codes,\n",
      "            ) = self.directory_processor.process_directory()\n",
      "            self.function_source_codes.extend(function_source_codes)\n",
      "            self.class_source_codes.extend(class_source_codes)\n",
      "            shutil.rmtree(repo_path)\n",
      "\n",
      "    return self.directory_processor\n",
      "\n",
      "Function call: DirectoryProcessor\n",
      "Function call: print\n",
      "Related codes: ['\\n\\nclass DirectoryProcessor:\\n    def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\\n        self.directory_path = directory_path\\n        self.visitor = visitor\\n\\n    def _process_file(self, file_path: str):\\n        with open(file_path, \"r\") as file:\\n            source_code = file.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n        except cst.ParserSyntaxError:\\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n            return\\n\\n        tree.visit(self.visitor)\\n\\n    def process_file(self, file_path: str):\\n        # Run flake8 on the file\\n        result = subprocess.run(\\n            [\"flake8\", \"--select=E999\", file_path], capture_output=True\\n        )\\n\\n        if result.returncode != 0:\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n            print(result.stderr.decode(\"utf-8\"))\\n            return\\n\\n        with open(file_path, \"r\") as f:\\n            source_code = f.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n            tree.visit(self.visitor)\\n        except cst.ParserSyntaxError as e:\\n            print(f\"Syntax error: {e}\")\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n\\n    def process_directory(self) -> List[str]:\\n        function_source_codes = []\\n        class_source_codes = []\\n\\n        for root, _, files in os.walk(self.directory_path):\\n            for file in files:\\n                if file.endswith(\".py\"):\\n                    file_path = os.path.join(root, file)\\n                    self._process_file(file_path)\\n\\n        function_source_codes = self.visitor.function_source_codes\\n        function_nodes = self.visitor.function_nodes\\n        class_source_codes = self.visitor.class_source_codes\\n        class_nodes = self.visitor.class_nodes\\n\\n        return function_source_codes, class_source_codes, function_nodes, class_nodes\\n\\n    def clone_repo(self, repo_url):\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.directory_path, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class GitHubRepoProcessor:\n",
      "    def __init__(\n",
      "        self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
      "    ):\n",
      "        self.username = username\n",
      "        self.repo_name = repo_name\n",
      "        self.github = Github()\n",
      "        self.directory_processor = None\n",
      "        self.function_source_codes = []\n",
      "        self.function_nodes = []\n",
      "        self.class_source_codes = []\n",
      "        self.class_nodes = []\n",
      "        self.visitor = visitor\n",
      "\n",
      "    def get_repo(self, repo_name):\n",
      "        user = self.github.get_user(self.username)\n",
      "        return user.get_repo(repo_name)\n",
      "\n",
      "    def process_repo(self, base_directory):\n",
      "        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
      "        repo = self.get_repo(self.repo_name)\n",
      "        print(f\"Processing repo: {self.repo_name}\")\n",
      "        repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
      "        (\n",
      "            function_source_codes,\n",
      "            class_source_codes,\n",
      "            function_nodes,\n",
      "            class_nodes,\n",
      "        ) = self.directory_processor.process_directory()\n",
      "        self.function_source_codes.extend(function_source_codes)\n",
      "        self.function_nodes.extend(function_nodes)\n",
      "        self.class_source_codes.extend(class_source_codes)\n",
      "        self.class_nodes.extend(class_nodes)\n",
      "        shutil.rmtree(repo_path)\n",
      "        return self.directory_processor\n",
      "\n",
      "    def get_values(self):\n",
      "        # concatenate the function and class source codes\n",
      "        self.function_source_codes.extend(self.class_source_codes)\n",
      "        self.function_nodes.extend(self.class_nodes)\n",
      "        return self.function_source_codes, self.function_nodes\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: FunctionAndClassVisitor\n",
      "Function call: Github\n",
      "Function call: DirectoryProcessor\n",
      "Function call: print\n",
      "Related codes: ['\\n\\nclass FunctionAndClassVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        function_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.function_nodes.append(node)\\n        self.function_source_codes.append(function_source_code)\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\\n        class_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.class_nodes.append(node)\\n        self.class_source_codes.append(class_source_code)\\n', '\\n\\nclass GithubProcessor(OsProcessor):\\n    def __init__(\\n        self,\\n        base_directory: str,\\n        username=None,\\n        repo_name=None,\\n        code_parsers=None,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n    ):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.base_directory = base_directory\\n        self.github = Github()\\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n        repo_path = self.clone_repo(self.repo.clone_url)\\n\\n        OsProcessor.__init__(self, repo_path)\\n        self.code_parsers = code_parsers or [\\n            PythonParser(\\n                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\\n            )\\n        ]\\n\\n    def get_public_repos(self):\\n        \"\"\"Returns a list of all public repos for the user.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def clone_repo(self, repo_url: str):\\n        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.base_directory, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n    def process_repo(self, repo_path=None):\\n        \"\"\"Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n        if repo_path is None:\\n            repo_path = self.directory_path\\n\\n        for code_parser in self.code_parsers:\\n            code_parser.directory_path = repo_path\\n            code_parser.process_directory(repo_path)\\n\\n    def process_repos(self):\\n        \"\"\"Processes all public repos for the user.\"\"\"\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.clone_repo(repo.clone_url)\\n                self.process_repo(repo_path)\\n                shutil.rmtree(repo_path)\\n\\n    def get_repo(self, repo_name):\\n        \"\"\"Returns the repo with the specified name.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_single_repo(self):\\n\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.clone_repo(repo.clone_url)\\n        self.process_repo(repo_path)\\n        shutil.rmtree(repo_path)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n', '\\n\\nclass DirectoryProcessor:\\n    def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\\n        self.directory_path = directory_path\\n        self.visitor = visitor\\n\\n    def _process_file(self, file_path: str):\\n        with open(file_path, \"r\") as file:\\n            source_code = file.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n        except cst.ParserSyntaxError:\\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n            return\\n\\n        tree.visit(self.visitor)\\n\\n    def process_file(self, file_path: str):\\n        # Run flake8 on the file\\n        result = subprocess.run(\\n            [\"flake8\", \"--select=E999\", file_path], capture_output=True\\n        )\\n\\n        if result.returncode != 0:\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n            print(result.stderr.decode(\"utf-8\"))\\n            return\\n\\n        with open(file_path, \"r\") as f:\\n            source_code = f.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n            tree.visit(self.visitor)\\n        except cst.ParserSyntaxError as e:\\n            print(f\"Syntax error: {e}\")\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n\\n    def process_directory(self) -> List[str]:\\n        function_source_codes = []\\n        class_source_codes = []\\n\\n        for root, _, files in os.walk(self.directory_path):\\n            for file in files:\\n                if file.endswith(\".py\"):\\n                    file_path = os.path.join(root, file)\\n                    self._process_file(file_path)\\n\\n        function_source_codes = self.visitor.function_source_codes\\n        function_nodes = self.visitor.function_nodes\\n        class_source_codes = self.visitor.class_source_codes\\n        class_nodes = self.visitor.class_nodes\\n\\n        return function_source_codes, class_source_codes, function_nodes, class_nodes\\n\\n    def clone_repo(self, repo_url):\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.directory_path, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(\n",
      "    self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
      "):\n",
      "    self.username = username\n",
      "    self.repo_name = repo_name\n",
      "    self.github = Github()\n",
      "    self.directory_processor = None\n",
      "    self.function_source_codes = []\n",
      "    self.function_nodes = []\n",
      "    self.class_source_codes = []\n",
      "    self.class_nodes = []\n",
      "    self.visitor = visitor\n",
      "\n",
      "Function call: FunctionAndClassVisitor\n",
      "Function call: Github\n",
      "Related codes: ['\\n\\nclass FunctionAndClassVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        function_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.function_nodes.append(node)\\n        self.function_source_codes.append(function_source_code)\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\\n        class_source_code = cst.Module([]).code_for_node(node)\\n        # add in place summary and code mod\\n        self.class_nodes.append(node)\\n        self.class_source_codes.append(class_source_code)\\n', '\\n\\nclass GithubProcessor(OsProcessor):\\n    def __init__(\\n        self,\\n        base_directory: str,\\n        username=None,\\n        repo_name=None,\\n        code_parsers=None,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n    ):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.base_directory = base_directory\\n        self.github = Github()\\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n        repo_path = self.clone_repo(self.repo.clone_url)\\n\\n        OsProcessor.__init__(self, repo_path)\\n        self.code_parsers = code_parsers or [\\n            PythonParser(\\n                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\\n            )\\n        ]\\n\\n    def get_public_repos(self):\\n        \"\"\"Returns a list of all public repos for the user.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def clone_repo(self, repo_url: str):\\n        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.base_directory, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n    def process_repo(self, repo_path=None):\\n        \"\"\"Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n        if repo_path is None:\\n            repo_path = self.directory_path\\n\\n        for code_parser in self.code_parsers:\\n            code_parser.directory_path = repo_path\\n            code_parser.process_directory(repo_path)\\n\\n    def process_repos(self):\\n        \"\"\"Processes all public repos for the user.\"\"\"\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.clone_repo(repo.clone_url)\\n                self.process_repo(repo_path)\\n                shutil.rmtree(repo_path)\\n\\n    def get_repo(self, repo_name):\\n        \"\"\"Returns the repo with the specified name.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_single_repo(self):\\n\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.clone_repo(repo.clone_url)\\n        self.process_repo(repo_path)\\n        shutil.rmtree(repo_path)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_repo(self, repo_name):\n",
      "    user = self.github.get_user(self.username)\n",
      "    return user.get_repo(repo_name)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def process_repo(self, base_directory):\n",
      "    self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
      "    repo = self.get_repo(self.repo_name)\n",
      "    print(f\"Processing repo: {self.repo_name}\")\n",
      "    repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
      "    (\n",
      "        function_source_codes,\n",
      "        class_source_codes,\n",
      "        function_nodes,\n",
      "        class_nodes,\n",
      "    ) = self.directory_processor.process_directory()\n",
      "    self.function_source_codes.extend(function_source_codes)\n",
      "    self.function_nodes.extend(function_nodes)\n",
      "    self.class_source_codes.extend(class_source_codes)\n",
      "    self.class_nodes.extend(class_nodes)\n",
      "    shutil.rmtree(repo_path)\n",
      "    return self.directory_processor\n",
      "\n",
      "Function call: DirectoryProcessor\n",
      "Function call: print\n",
      "Related codes: ['\\n\\nclass DirectoryProcessor:\\n    def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\\n        self.directory_path = directory_path\\n        self.visitor = visitor\\n\\n    def _process_file(self, file_path: str):\\n        with open(file_path, \"r\") as file:\\n            source_code = file.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n        except cst.ParserSyntaxError:\\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n            return\\n\\n        tree.visit(self.visitor)\\n\\n    def process_file(self, file_path: str):\\n        # Run flake8 on the file\\n        result = subprocess.run(\\n            [\"flake8\", \"--select=E999\", file_path], capture_output=True\\n        )\\n\\n        if result.returncode != 0:\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n            print(result.stderr.decode(\"utf-8\"))\\n            return\\n\\n        with open(file_path, \"r\") as f:\\n            source_code = f.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n            tree.visit(self.visitor)\\n        except cst.ParserSyntaxError as e:\\n            print(f\"Syntax error: {e}\")\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n\\n    def process_directory(self) -> List[str]:\\n        function_source_codes = []\\n        class_source_codes = []\\n\\n        for root, _, files in os.walk(self.directory_path):\\n            for file in files:\\n                if file.endswith(\".py\"):\\n                    file_path = os.path.join(root, file)\\n                    self._process_file(file_path)\\n\\n        function_source_codes = self.visitor.function_source_codes\\n        function_nodes = self.visitor.function_nodes\\n        class_source_codes = self.visitor.class_source_codes\\n        class_nodes = self.visitor.class_nodes\\n\\n        return function_source_codes, class_source_codes, function_nodes, class_nodes\\n\\n    def clone_repo(self, repo_url):\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.directory_path, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def get_values(self):\n",
      "    # concatenate the function and class source codes\n",
      "    self.function_source_codes.extend(self.class_source_codes)\n",
      "    self.function_nodes.extend(self.class_nodes)\n",
      "    return self.function_source_codes, self.function_nodes\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class PubmedAPI:\n",
      "    def __init__(self):\n",
      "        self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
      "\n",
      "    def search(self, query, max_results=10):\n",
      "        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
      "        record = Entrez.read(handle)\n",
      "        handle.close()\n",
      "        return record[\"IdList\"]\n",
      "\n",
      "    def fetch_abstract(self, pubmed_id):\n",
      "        handle = Entrez.efetch(\n",
      "            db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\"\n",
      "        )\n",
      "        abstract = handle.read()\n",
      "        handle.close()\n",
      "        return abstract\n",
      "\n",
      "    def fetch_pmc_full_text(self, pubmed_id):\n",
      "        # Get the PMC ID for the PubMed ID\n",
      "        handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\n",
      "        record = Entrez.read(handle)\n",
      "        handle.close()\n",
      "        pmc_id = None\n",
      "        for link in record[0][\"LinkSetDb\"]:\n",
      "            if link[\"DbTo\"] == \"pmc\":\n",
      "                pmc_id = link[\"Link\"][0][\"Id\"]\n",
      "                break\n",
      "\n",
      "        if not pmc_id:\n",
      "            return None\n",
      "\n",
      "        # Fetch the PMC article XML\n",
      "        handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\n",
      "        xml_content = handle.read()\n",
      "        handle.close()\n",
      "\n",
      "        # Parse the XML and extract the full text\n",
      "        soup = BeautifulSoup(xml_content, \"xml\")\n",
      "        full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
      "\n",
      "        return full_text\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: BeautifulSoup\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self):\n",
      "    self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def search(self, query, max_results=10):\n",
      "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
      "    record = Entrez.read(handle)\n",
      "    handle.close()\n",
      "    return record[\"IdList\"]\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def fetch_abstract(self, pubmed_id):\n",
      "    handle = Entrez.efetch(\n",
      "        db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\"\n",
      "    )\n",
      "    abstract = handle.read()\n",
      "    handle.close()\n",
      "    return abstract\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def fetch_pmc_full_text(self, pubmed_id):\n",
      "    # Get the PMC ID for the PubMed ID\n",
      "    handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\n",
      "    record = Entrez.read(handle)\n",
      "    handle.close()\n",
      "    pmc_id = None\n",
      "    for link in record[0][\"LinkSetDb\"]:\n",
      "        if link[\"DbTo\"] == \"pmc\":\n",
      "            pmc_id = link[\"Link\"][0][\"Id\"]\n",
      "            break\n",
      "\n",
      "    if not pmc_id:\n",
      "        return None\n",
      "\n",
      "    # Fetch the PMC article XML\n",
      "    handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\n",
      "    xml_content = handle.read()\n",
      "    handle.close()\n",
      "\n",
      "    # Parse the XML and extract the full text\n",
      "    soup = BeautifulSoup(xml_content, \"xml\")\n",
      "    full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
      "\n",
      "    return full_text\n",
      "\n",
      "Function call: BeautifulSoup\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class PubmedParser:\n",
      "    def __init__(self):\n",
      "        self.api = PubmedAPI()\n",
      "\n",
      "    def parse_papers(self, query, max_results=10):\n",
      "        pubmed_ids = self.api.search(query, max_results)\n",
      "        paper_list = []\n",
      "        for pubmed_id in pubmed_ids:\n",
      "            paper_dict = {}\n",
      "            paper_dict[\"pubmed_id\"] = pubmed_id\n",
      "            paper_dict[\"abstract\"] = self.api.fetch_abstract(pubmed_id)\n",
      "            paper_dict[\"content\"] = self.api.fetch_pmc_full_text(pubmed_id)\n",
      "            paper_list.append(paper_dict)\n",
      "        return paper_list\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: PubmedAPI\n",
      "Related codes: ['\\n\\nclass PubmedAPI:\\n    def __init__(self):\\n        self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\\n\\n    def search(self, query, max_results=10):\\n        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\\n        record = Entrez.read(handle)\\n        handle.close()\\n        return record[\"IdList\"]\\n\\n    def fetch_abstract(self, pubmed_id):\\n        handle = Entrez.efetch(\\n            db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\"\\n        )\\n        abstract = handle.read()\\n        handle.close()\\n        return abstract\\n\\n    def fetch_pmc_full_text(self, pubmed_id):\\n        # Get the PMC ID for the PubMed ID\\n        handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\\n        record = Entrez.read(handle)\\n        handle.close()\\n        pmc_id = None\\n        for link in record[0][\"LinkSetDb\"]:\\n            if link[\"DbTo\"] == \"pmc\":\\n                pmc_id = link[\"Link\"][0][\"Id\"]\\n                break\\n\\n        if not pmc_id:\\n            return None\\n\\n        # Fetch the PMC article XML\\n        handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\\n        xml_content = handle.read()\\n        handle.close()\\n\\n        # Parse the XML and extract the full text\\n        soup = BeautifulSoup(xml_content, \"xml\")\\n        full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\\n\\n        return full_text\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self):\n",
      "    self.api = PubmedAPI()\n",
      "\n",
      "Function call: PubmedAPI\n",
      "Related codes: ['\\n\\nclass PubmedAPI:\\n    def __init__(self):\\n        self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\\n\\n    def search(self, query, max_results=10):\\n        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\\n        record = Entrez.read(handle)\\n        handle.close()\\n        return record[\"IdList\"]\\n\\n    def fetch_abstract(self, pubmed_id):\\n        handle = Entrez.efetch(\\n            db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\"\\n        )\\n        abstract = handle.read()\\n        handle.close()\\n        return abstract\\n\\n    def fetch_pmc_full_text(self, pubmed_id):\\n        # Get the PMC ID for the PubMed ID\\n        handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\\n        record = Entrez.read(handle)\\n        handle.close()\\n        pmc_id = None\\n        for link in record[0][\"LinkSetDb\"]:\\n            if link[\"DbTo\"] == \"pmc\":\\n                pmc_id = link[\"Link\"][0][\"Id\"]\\n                break\\n\\n        if not pmc_id:\\n            return None\\n\\n        # Fetch the PMC article XML\\n        handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\\n        xml_content = handle.read()\\n        handle.close()\\n\\n        # Parse the XML and extract the full text\\n        soup = BeautifulSoup(xml_content, \"xml\")\\n        full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\\n\\n        return full_text\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def parse_papers(self, query, max_results=10):\n",
      "    pubmed_ids = self.api.search(query, max_results)\n",
      "    paper_list = []\n",
      "    for pubmed_id in pubmed_ids:\n",
      "        paper_dict = {}\n",
      "        paper_dict[\"pubmed_id\"] = pubmed_id\n",
      "        paper_dict[\"abstract\"] = self.api.fetch_abstract(pubmed_id)\n",
      "        paper_dict[\"content\"] = self.api.fetch_pmc_full_text(pubmed_id)\n",
      "        paper_list.append(paper_dict)\n",
      "    return paper_list\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class ArxivVanityParser:\n",
      "    def __init__(self):\n",
      "        self.base_url = \"https://www.arxiv-vanity.com/\"\n",
      "\n",
      "    def _get_vanity_url(self, arxiv_id):\n",
      "        return urljoin(self.base_url, \"papers/\" + arxiv_id)\n",
      "\n",
      "    def _fetch_html(self, url):\n",
      "        response = requests.get(url)\n",
      "        if response.status_code == 200:\n",
      "            return response.text\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    def _extract_main_content(self, html):\n",
      "        soup = BeautifulSoup(html, \"html.parser\")\n",
      "        paragraphs = soup.find_all(\"div\", {\"class\": \"ltx_para\"})\n",
      "        content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\n",
      "        return content\n",
      "\n",
      "    def parse_paper(self, arxiv_id):\n",
      "        vanity_url = self._get_vanity_url(arxiv_id)\n",
      "        html = self._fetch_html(vanity_url)\n",
      "        if html is not None:\n",
      "            return self._extract_main_content(html)\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: urljoin\n",
      "Function call: BeautifulSoup\n",
      "Function call: enumerate\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self):\n",
      "    self.base_url = \"https://www.arxiv-vanity.com/\"\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _get_vanity_url(self, arxiv_id):\n",
      "    return urljoin(self.base_url, \"papers/\" + arxiv_id)\n",
      "\n",
      "Function call: urljoin\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _fetch_html(self, url):\n",
      "    response = requests.get(url)\n",
      "    if response.status_code == 200:\n",
      "        return response.text\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _extract_main_content(self, html):\n",
      "    soup = BeautifulSoup(html, \"html.parser\")\n",
      "    paragraphs = soup.find_all(\"div\", {\"class\": \"ltx_para\"})\n",
      "    content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\n",
      "    return content\n",
      "\n",
      "Function call: BeautifulSoup\n",
      "Function call: enumerate\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def parse_paper(self, arxiv_id):\n",
      "    vanity_url = self._get_vanity_url(arxiv_id)\n",
      "    html = self._fetch_html(vanity_url)\n",
      "    if html is not None:\n",
      "        return self._extract_main_content(html)\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class ArxivAPI:\n",
      "    def __init__(self):\n",
      "        self.base_url = \"http://export.arxiv.org/api/query?\"\n",
      "        self.pdf_download_url = \"https://arxiv.org/pdf/\"\n",
      "\n",
      "    def search(self, query, max_results=10):\n",
      "        url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\n",
      "        response = requests.get(url)\n",
      "        if response.status_code == 200:\n",
      "            return response.text\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "    def download_pdf(self, paper_key, save_directory=\"./\"):\n",
      "        pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\n",
      "        response = requests.get(pdf_url)\n",
      "        if response.status_code == 200:\n",
      "            with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\n",
      "                f.write(response.content)\n",
      "            print(f\"PDF for {paper_key} downloaded successfully.\")\n",
      "        else:\n",
      "            print(f\"Error downloading PDF for {paper_key}.\")\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: open\n",
      "Function call: print\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self):\n",
      "    self.base_url = \"http://export.arxiv.org/api/query?\"\n",
      "    self.pdf_download_url = \"https://arxiv.org/pdf/\"\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def search(self, query, max_results=10):\n",
      "    url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\n",
      "    response = requests.get(url)\n",
      "    if response.status_code == 200:\n",
      "        return response.text\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def download_pdf(self, paper_key, save_directory=\"./\"):\n",
      "    pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\n",
      "    response = requests.get(pdf_url)\n",
      "    if response.status_code == 200:\n",
      "        with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\n",
      "            f.write(response.content)\n",
      "        print(f\"PDF for {paper_key} downloaded successfully.\")\n",
      "    else:\n",
      "        print(f\"Error downloading PDF for {paper_key}.\")\n",
      "\n",
      "Function call: open\n",
      "Function call: print\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class ArxivParser:\n",
      "    def __init__(self):\n",
      "        self.api = ArxivAPI()\n",
      "        self.vanity_parser = ArxivVanityParser()\n",
      "\n",
      "    def _parse_arxiv_id(self, url):\n",
      "        return url.split(\"/\")[-1]\n",
      "\n",
      "    def parse_papers(self, query, max_results=10):\n",
      "        search_results = self.api.search(query, max_results)\n",
      "        if search_results is not None:\n",
      "            soup = BeautifulSoup(search_results, \"html.parser\")\n",
      "            entries = soup.find_all(\"entry\")\n",
      "            paper_list = []\n",
      "            for entry in entries:\n",
      "                paper_dict = {}\n",
      "                arxiv_id = self._parse_arxiv_id(entry.id.string)\n",
      "                paper_dict[\"arxiv_id\"] = arxiv_id\n",
      "                paper_dict[\"title\"] = entry.title.string\n",
      "                paper_dict[\"summary\"] = entry.summary.string\n",
      "                paper_dict[\"content\"] = self.vanity_parser.parse_paper(str(arxiv_id))\n",
      "                if paper_dict[\"content\"] == None:\n",
      "                    continue\n",
      "                paper_list.append(paper_dict)\n",
      "            return paper_list\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: ArxivAPI\n",
      "Function call: ArxivVanityParser\n",
      "Function call: BeautifulSoup\n",
      "Function call: str\n",
      "Related codes: ['\\n\\nclass ArxivAPI:\\n    def __init__(self):\\n        self.base_url = \"http://export.arxiv.org/api/query?\"\\n        self.pdf_download_url = \"https://arxiv.org/pdf/\"\\n\\n    def search(self, query, max_results=10):\\n        url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\\n        response = requests.get(url)\\n        if response.status_code == 200:\\n            return response.text\\n        else:\\n            return None\\n\\n    def download_pdf(self, paper_key, save_directory=\"./\"):\\n        pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\\n        response = requests.get(pdf_url)\\n        if response.status_code == 200:\\n            with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\\n                f.write(response.content)\\n            print(f\"PDF for {paper_key} downloaded successfully.\")\\n        else:\\n            print(f\"Error downloading PDF for {paper_key}.\")\\n', '\\n\\nclass ArxivVanityParser:\\n    def __init__(self):\\n        self.base_url = \"https://www.arxiv-vanity.com/\"\\n\\n    def _get_vanity_url(self, arxiv_id):\\n        return urljoin(self.base_url, \"papers/\" + arxiv_id)\\n\\n    def _fetch_html(self, url):\\n        response = requests.get(url)\\n        if response.status_code == 200:\\n            return response.text\\n        else:\\n            return None\\n\\n    def _extract_main_content(self, html):\\n        soup = BeautifulSoup(html, \"html.parser\")\\n        paragraphs = soup.find_all(\"div\", {\"class\": \"ltx_para\"})\\n        content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\\n        return content\\n\\n    def parse_paper(self, arxiv_id):\\n        vanity_url = self._get_vanity_url(arxiv_id)\\n        html = self._fetch_html(vanity_url)\\n        if html is not None:\\n            return self._extract_main_content(html)\\n        else:\\n            return None\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self):\n",
      "    self.api = ArxivAPI()\n",
      "    self.vanity_parser = ArxivVanityParser()\n",
      "\n",
      "Function call: ArxivAPI\n",
      "Function call: ArxivVanityParser\n",
      "Related codes: ['\\n\\nclass ArxivAPI:\\n    def __init__(self):\\n        self.base_url = \"http://export.arxiv.org/api/query?\"\\n        self.pdf_download_url = \"https://arxiv.org/pdf/\"\\n\\n    def search(self, query, max_results=10):\\n        url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\\n        response = requests.get(url)\\n        if response.status_code == 200:\\n            return response.text\\n        else:\\n            return None\\n\\n    def download_pdf(self, paper_key, save_directory=\"./\"):\\n        pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\\n        response = requests.get(pdf_url)\\n        if response.status_code == 200:\\n            with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\\n                f.write(response.content)\\n            print(f\"PDF for {paper_key} downloaded successfully.\")\\n        else:\\n            print(f\"Error downloading PDF for {paper_key}.\")\\n', '\\n\\nclass ArxivVanityParser:\\n    def __init__(self):\\n        self.base_url = \"https://www.arxiv-vanity.com/\"\\n\\n    def _get_vanity_url(self, arxiv_id):\\n        return urljoin(self.base_url, \"papers/\" + arxiv_id)\\n\\n    def _fetch_html(self, url):\\n        response = requests.get(url)\\n        if response.status_code == 200:\\n            return response.text\\n        else:\\n            return None\\n\\n    def _extract_main_content(self, html):\\n        soup = BeautifulSoup(html, \"html.parser\")\\n        paragraphs = soup.find_all(\"div\", {\"class\": \"ltx_para\"})\\n        content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\\n        return content\\n\\n    def parse_paper(self, arxiv_id):\\n        vanity_url = self._get_vanity_url(arxiv_id)\\n        html = self._fetch_html(vanity_url)\\n        if html is not None:\\n            return self._extract_main_content(html)\\n        else:\\n            return None\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _parse_arxiv_id(self, url):\n",
      "    return url.split(\"/\")[-1]\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def parse_papers(self, query, max_results=10):\n",
      "    search_results = self.api.search(query, max_results)\n",
      "    if search_results is not None:\n",
      "        soup = BeautifulSoup(search_results, \"html.parser\")\n",
      "        entries = soup.find_all(\"entry\")\n",
      "        paper_list = []\n",
      "        for entry in entries:\n",
      "            paper_dict = {}\n",
      "            arxiv_id = self._parse_arxiv_id(entry.id.string)\n",
      "            paper_dict[\"arxiv_id\"] = arxiv_id\n",
      "            paper_dict[\"title\"] = entry.title.string\n",
      "            paper_dict[\"summary\"] = entry.summary.string\n",
      "            paper_dict[\"content\"] = self.vanity_parser.parse_paper(str(arxiv_id))\n",
      "            if paper_dict[\"content\"] == None:\n",
      "                continue\n",
      "            paper_list.append(paper_dict)\n",
      "        return paper_list\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "Function call: BeautifulSoup\n",
      "Function call: str\n",
      "Related codes: ['\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class SubjectPerspectiveAnalyzer:\n",
      "    def __init__(self, chatbot: 'Chat'):\n",
      "        self.chatbot = chatbot\n",
      "\n",
      "    def analyze_subject_perspective(self, user_subject: str, user_perspective: str) -> dict:\n",
      "        prompts = [\n",
      "            f\"Generate ideas and concepts that explore the connection between {user_subject} and {user_perspective}, considering both traditional and unconventional approaches.\",\n",
      "            f\"List key concepts or topics that would help analyze {user_subject} through the lens of {user_perspective}, including relevant principles, theories, or models.\",\n",
      "            f\"Identify potential areas or research topics where {user_subject} and {user_perspective} intersect, highlighting intriguing or innovative perspectives.\"\n",
      "        ]\n",
      "\n",
      "        output = {}\n",
      "\n",
      "        with RateLimitedThreadPoolExecutor(max_workers=3, calls_per_minute=20, verbose = False) as executor:\n",
      "            future_to_prompt = {executor.submit(self._analyze_prompt, prompt): prompt for prompt in prompts}\n",
      "            for future in as_completed(future_to_prompt):\n",
      "                prompt = future_to_prompt[future]\n",
      "                try:\n",
      "                    output[prompt] = future.result()\n",
      "                except Exception as exc:\n",
      "                    counter = 0\n",
      "                    print(f'An exception occurred while analyzing prompt \"{prompt}\": {exc}')\n",
      "                    while counter < 3:\n",
      "                        try:\n",
      "                            output[prompt] = self._analyze_prompt(prompt)\n",
      "                            break\n",
      "                        except Exception as exc:\n",
      "                            counter += 1\n",
      "                            print(f'An exception occurred while analyzing prompt \"{prompt}\": {exc}')\n",
      "                    if counter == 3:\n",
      "                        output[prompt] = []\n",
      "\n",
      "        return output\n",
      "\n",
      "    def _analyze_prompt(self, prompt: str) -> list:\n",
      "        response = self.chatbot.reply(prompt, verbose=False)\n",
      "        return self._format_response(response)\n",
      "\n",
      "    def _format_response(self, response: str) -> list:\n",
      "        formatted_response = response.strip().split('\\n')\n",
      "        return formatted_response\n",
      "    \n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: RateLimitedThreadPoolExecutor\n",
      "Function call: as_completed\n",
      "Function call: print\n",
      "Function call: print\n",
      "Related codes: ['\\n\\nclass RateLimitedThreadPoolExecutor(ThreadPoolExecutor):\\n    def __init__(self, max_workers=None, *args, **kwargs):\\n        super().__init__(max_workers)\\n        self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\\n\\n    def submit(self, fn, *args, **kwargs):\\n        rate_limited_fn = self.rate_limiter(fn)\\n        return super().submit(rate_limited_fn, *args, **kwargs)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, chatbot: 'Chat'):\n",
      "    self.chatbot = chatbot\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def analyze_subject_perspective(self, user_subject: str, user_perspective: str) -> dict:\n",
      "    prompts = [\n",
      "        f\"Generate ideas and concepts that explore the connection between {user_subject} and {user_perspective}, considering both traditional and unconventional approaches.\",\n",
      "        f\"List key concepts or topics that would help analyze {user_subject} through the lens of {user_perspective}, including relevant principles, theories, or models.\",\n",
      "        f\"Identify potential areas or research topics where {user_subject} and {user_perspective} intersect, highlighting intriguing or innovative perspectives.\"\n",
      "    ]\n",
      "\n",
      "    output = {}\n",
      "\n",
      "    with RateLimitedThreadPoolExecutor(max_workers=3, calls_per_minute=20, verbose = False) as executor:\n",
      "        future_to_prompt = {executor.submit(self._analyze_prompt, prompt): prompt for prompt in prompts}\n",
      "        for future in as_completed(future_to_prompt):\n",
      "            prompt = future_to_prompt[future]\n",
      "            try:\n",
      "                output[prompt] = future.result()\n",
      "            except Exception as exc:\n",
      "                counter = 0\n",
      "                print(f'An exception occurred while analyzing prompt \"{prompt}\": {exc}')\n",
      "                while counter < 3:\n",
      "                    try:\n",
      "                        output[prompt] = self._analyze_prompt(prompt)\n",
      "                        break\n",
      "                    except Exception as exc:\n",
      "                        counter += 1\n",
      "                        print(f'An exception occurred while analyzing prompt \"{prompt}\": {exc}')\n",
      "                if counter == 3:\n",
      "                    output[prompt] = []\n",
      "\n",
      "    return output\n",
      "\n",
      "Function call: RateLimitedThreadPoolExecutor\n",
      "Function call: as_completed\n",
      "Function call: print\n",
      "Function call: print\n",
      "Related codes: ['\\n\\nclass RateLimitedThreadPoolExecutor(ThreadPoolExecutor):\\n    def __init__(self, max_workers=None, *args, **kwargs):\\n        super().__init__(max_workers)\\n        self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\\n\\n    def submit(self, fn, *args, **kwargs):\\n        rate_limited_fn = self.rate_limiter(fn)\\n        return super().submit(rate_limited_fn, *args, **kwargs)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _analyze_prompt(self, prompt: str) -> list:\n",
      "    response = self.chatbot.reply(prompt, verbose=False)\n",
      "    return self._format_response(response)\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _format_response(self, response: str) -> list:\n",
      "    formatted_response = response.strip().split('\\n')\n",
      "    return formatted_response\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class Ideation:\n",
      "    def __init__(self, memory_index: MemoryIndex):\n",
      "        self.memory_index = memory_index\n",
      "\n",
      "    def retrieve_ideas(self, queries: Dict, k: int = 30, max_tokens: int = 10000):\n",
      "        \"\"\"\n",
      "        Generate ideas based on the given list of queries.\n",
      "\n",
      "        Args:\n",
      "            queries: The list of queries for generating ideas.\n",
      "            k: The number of top search results to consider.\n",
      "            max_tokens: The maximum number of tokens to return.\n",
      "\n",
      "        Returns:\n",
      "            A list of ideas generated based on the queries.\n",
      "        \"\"\"\n",
      "        ideas = []\n",
      "        for key, queries in queries.items():\n",
      "            for query in queries:\n",
      "                if query is None or len(query) < 10:\n",
      "                    continue\n",
      "                top_k_hints, scores, indices = self.memory_index.token_bound_query(\n",
      "                    query, k=k, max_tokens=max_tokens\n",
      "                )\n",
      "                last_query = self.memory_index.query_history[-1]\n",
      "                hints_tokens = last_query[\"hints_tokens\"]\n",
      "                returned_tokens = last_query[\"returned_tokens\"]\n",
      "\n",
      "                ideas.append({\"key_task\": key, \"query\": query, \"hints\": top_k_hints, \"scores\": scores, \"hints_tokens\": hints_tokens, \"returned_tokens\": returned_tokens})\n",
      "\n",
      "        return ideas\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, memory_index: MemoryIndex):\n",
      "    self.memory_index = memory_index\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def retrieve_ideas(self, queries: Dict, k: int = 30, max_tokens: int = 10000):\n",
      "    \"\"\"\n",
      "        Generate ideas based on the given list of queries.\n",
      "\n",
      "        Args:\n",
      "            queries: The list of queries for generating ideas.\n",
      "            k: The number of top search results to consider.\n",
      "            max_tokens: The maximum number of tokens to return.\n",
      "\n",
      "        Returns:\n",
      "            A list of ideas generated based on the queries.\n",
      "        \"\"\"\n",
      "    ideas = []\n",
      "    for key, queries in queries.items():\n",
      "        for query in queries:\n",
      "            if query is None or len(query) < 10:\n",
      "                continue\n",
      "            top_k_hints, scores, indices = self.memory_index.token_bound_query(\n",
      "                query, k=k, max_tokens=max_tokens\n",
      "            )\n",
      "            last_query = self.memory_index.query_history[-1]\n",
      "            hints_tokens = last_query[\"hints_tokens\"]\n",
      "            returned_tokens = last_query[\"returned_tokens\"]\n",
      "\n",
      "            ideas.append({\"key_task\": key, \"query\": query, \"hints\": top_k_hints, \"scores\": scores, \"hints_tokens\": hints_tokens, \"returned_tokens\": returned_tokens})\n",
      "\n",
      "    return ideas\n",
      "\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class IdeaCluster:\n",
      "    def __init__(self, ideas: list, max_tokens_per_cluster: int):\n",
      "        self.ideas = ideas\n",
      "        self.max_tokens_per_cluster = max_tokens_per_cluster\n",
      "        self.idea_index = self.create_idea_index()\n",
      "        self.cluster_labels = None\n",
      "\n",
      "    def create_idea_index(self):\n",
      "        gathered_docs = []\n",
      "        for idea in self.ideas:\n",
      "            for hint in idea[\"hints\"]:\n",
      "                gathered_docs.append(hint)\n",
      "        self.gathered_docs = set(gathered_docs)\n",
      "        idea_index = MemoryIndex(values=self.gathered_docs, is_batched=True, name=\"ideas\")\n",
      "        return idea_index\n",
      "\n",
      "    def cluster_embeddings(self, n_neighbors: int = 10, min_cluster_size: int = 5):\n",
      "        reducer = umap.UMAP(n_neighbors=n_neighbors)\n",
      "        reduced_embeddings = reducer.fit_transform(self.idea_index.get_all_embeddings())\n",
      "\n",
      "        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
      "        labels = clusterer.fit_predict(reduced_embeddings)\n",
      "\n",
      "        token_count_per_cluster = self.count_tokens_per_cluster(labels)\n",
      "        print(token_count_per_cluster)\n",
      "        if max(token_count_per_cluster.values()) <= self.max_tokens_per_cluster:\n",
      "            self.cluster_labels = labels\n",
      "            print(\"Clusters created successfully.\")\n",
      "        else:\n",
      "            print(\"Clusters exceed the maximum token count.\")\n",
      "\n",
      "    def count_tokens_per_cluster(self, labels):\n",
      "        token_count_per_cluster = {}\n",
      "\n",
      "        for label, doc in zip(labels, self.gathered_docs):\n",
      "            if label not in token_count_per_cluster:\n",
      "                token_count_per_cluster[label] = len(tokenizer.encode(doc))\n",
      "            else:\n",
      "                token_count_per_cluster[label] += len(tokenizer.encode(doc))\n",
      "        return token_count_per_cluster\n",
      "\n",
      "    def create_minimum_spanning_paths(self):\n",
      "        if self.cluster_labels is None:\n",
      "            raise ValueError(\"You must run cluster_embeddings() before creating minimum spanning paths.\")\n",
      "\n",
      "        unique_labels = np.unique(self.cluster_labels)\n",
      "        min_span_paths = []\n",
      "\n",
      "        for label in unique_labels:\n",
      "\n",
      "\n",
      "            # Get the indices of the current cluster\n",
      "            cluster_indices = np.where(self.cluster_labels == label)[0]\n",
      "\n",
      "            # Calculate the pairwise distances between embeddings in the cluster\n",
      "            cluster_embeddings = self.idea_index.embeddings[cluster_indices]\n",
      "            dist_matrix = squareform(pdist(cluster_embeddings))\n",
      "\n",
      "            # Create a graph from the distance matrix\n",
      "            graph = nx.from_numpy_array(dist_matrix)\n",
      "\n",
      "            # Compute the minimum spanning tree of the graph\n",
      "            min_span_tree = nx.minimum_spanning_tree(graph)\n",
      "\n",
      "            # Get the minimum spanning paths\n",
      "            min_span_paths_cluster = []\n",
      "            visited = set()\n",
      "            for u, v in min_span_tree.edges():\n",
      "                if u not in visited and v not in visited:\n",
      "                    orig_u = cluster_indices[u]\n",
      "                    orig_v = cluster_indices[v]\n",
      "                    min_span_paths_cluster.append(orig_u)\n",
      "                    visited.add(u)\n",
      "                    visited.add(v)\n",
      "            # Add the last node to complete the path\n",
      "            min_span_paths_cluster.append(orig_v)\n",
      "\n",
      "            min_span_paths.append(min_span_paths_cluster)\n",
      "\n",
      "        self.min_span_paths = min_span_paths\n",
      "\n",
      "    \n",
      "    def plot_embeddings_with_path(self):\n",
      "        paths = self.min_span_paths\n",
      "        embeddings = self.idea_index.embeddings\n",
      "        title = \"Minimum Spanning Paths\"\n",
      "        tsne = TSNE(n_components=2, random_state=42)\n",
      "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
      "\n",
      "        plt.figure(figsize=(10, 8))\n",
      "        colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n",
      "        for i, path in enumerate(paths):\n",
      "            path_embeddings = reduced_embeddings[path]\n",
      "            plt.scatter(\n",
      "                path_embeddings[:, 0],\n",
      "                path_embeddings[:, 1],\n",
      "                color=colors[i],\n",
      "                label=f\"Cluster {i}\",\n",
      "            )\n",
      "            for j in range(len(path) - 1):\n",
      "                plt.plot(\n",
      "                    [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n",
      "                    [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n",
      "                    color=colors[i],\n",
      "                )\n",
      "        plt.title(title)\n",
      "        plt.legend()\n",
      "        plt.show()\n",
      "    def get_clustered_ideas(self):\n",
      "        if self.cluster_labels is None:\n",
      "            raise ValueError(\"You must run cluster_embeddings() before getting clustered ideas.\")\n",
      "        \n",
      "        clustered_ideas = {}\n",
      "        for label, idea in zip(self.cluster_labels, self.idea_index.values):\n",
      "            if label not in clustered_ideas:\n",
      "                clustered_ideas[label] = [idea]\n",
      "            else:\n",
      "                clustered_ideas[label].append(idea)\n",
      "        \n",
      "        # Convert the dictionary to a list of lists (each list corresponds to a cluster)\n",
      "        return list(clustered_ideas.values())\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: set\n",
      "Function call: MemoryIndex\n",
      "Function call: print\n",
      "Function call: max\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: zip\n",
      "Function call: len\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: squareform\n",
      "Function call: pdist\n",
      "Function call: set\n",
      "Function call: TSNE\n",
      "Function call: len\n",
      "Function call: enumerate\n",
      "Function call: range\n",
      "Function call: len\n",
      "Function call: ValueError\n",
      "Function call: zip\n",
      "Function call: list\n",
      "Related codes: ['\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\n\\n\\nclass MemoryIndex(NpIndex):\\n    \"\"\"\\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \\n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\\n    \"\"\"\\n    @staticmethod\\n    def check_uniform_context_type(context: List[Any]) -> None:\\n        \"\"\"Check if all context elements are of the same type.\"\"\"\\n        if not all(isinstance(x, type(context[0])) for x in context):\\n            raise ValueError(\"All context elements must be of the same type.\")\\n\\n    def __init__(\\n        self,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        context: Optional[List[Any]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ):\\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\\n        if context is not None and len(context) != len(values):\\n            raise ValueError(\"The context must have the same length as the values\")\\n\\n\\n        self.markdown = markdown\\n        if context is not None and values is not None:\\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\\n\\n        if context is not None:\\n            self.check_uniform_context_type(context)\\n            self.context_type = type(context[0])\\n\\n\\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\\n        if isinstance(identifier, list):\\n            return [self.get_context(value) for value in identifier]\\n        else:\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n            return self.context[value]\\n\\n    def clean_context(self):\\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\\n        self.context = {value: self.context[value] for value in self.values}\\n\\n    def add_to_context(self, value: str, context: Any):\\n        \"\"\" add a context to a value \"\"\"\\n        if not isinstance(context, self.context_type):\\n            raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if value in self.values:\\n            if value not in self.context:\\n                self.context[value] = []\\n            self.context[value].append(context)\\n\\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\\n        if isinstance(values, str):\\n            values = [values]\\n        NpIndex.add(self, values, embedding)\\n        if context is not None:\\n            for value, cont in zip(values, context):\\n                self.add_to_context(value, cont)\\n\\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\\n        if not isinstance(identifier, list):\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n        NpIndex.remove(self, identifier)\\n        if not isinstance(identifier, list):\\n            self.context.pop(value)\\n\\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\\n        #recover value from old_identifier\\n        if new_context is not None:\\n            if not isinstance(new_context, self.context_type):\\n                raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if not isinstance(old_identifier, list):\\n            old_id = self.identify_input(old_identifier)\\n            old_value = self.values[old_id]\\n            # Only perform the update if the old_value is not the same as the new_value.\\n            if old_value != new_value:\\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\\n\\n            if new_context is not None:\\n                self.context[new_value] = [new_context]\\n            else:\\n                self.context[new_value] = self.context.pop(old_value)\\n        else:\\n            self.context[new_value] = self.context.pop(old_value)\\n\\n    @classmethod\\n    def from_pandas(\\n        cls,\\n        data_frame: Union[pd.DataFrame, str],\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        if (\\n            isinstance(data_frame, str)\\n            and data_frame.endswith(\".csv\")\\n            and os.path.isfile(data_frame)\\n        ):\\n            logger.info(\"Loading the CSV file\")\\n            data_frame = pd.read_csv(data_frame)\\n            name = os.path.basename(data_frame).split(\".\")[0]\\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\\n            logger.info(\"Loading the pandas DataFrame\")\\n        else:\\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\\n\\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_pandas(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_hf_dataset(\\n        cls,\\n        dataset_url: str,\\n        value_column: str,\\n        data_split: str = \"train\",\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        dataset = load_dataset(dataset_url)[data_split]\\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_hf(dataset, context_columns)\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_polars(\\n        cls,\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        print(\"Loading the Polars DataFrame\")\\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_polars(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n\\n    @classmethod\\n    def from_python(\\n        cls,\\n        directory_path: str,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        markdown: str = \"python/markdown\",\\n        resolution: str = \"both\",\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', \"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\"]\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, ideas: list, max_tokens_per_cluster: int):\n",
      "    self.ideas = ideas\n",
      "    self.max_tokens_per_cluster = max_tokens_per_cluster\n",
      "    self.idea_index = self.create_idea_index()\n",
      "    self.cluster_labels = None\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def create_idea_index(self):\n",
      "    gathered_docs = []\n",
      "    for idea in self.ideas:\n",
      "        for hint in idea[\"hints\"]:\n",
      "            gathered_docs.append(hint)\n",
      "    self.gathered_docs = set(gathered_docs)\n",
      "    idea_index = MemoryIndex(values=self.gathered_docs, is_batched=True, name=\"ideas\")\n",
      "    return idea_index\n",
      "\n",
      "Function call: set\n",
      "Function call: MemoryIndex\n",
      "Related codes: ['\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\n\\n\\nclass MemoryIndex(NpIndex):\\n    \"\"\"\\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \\n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\\n    \"\"\"\\n    @staticmethod\\n    def check_uniform_context_type(context: List[Any]) -> None:\\n        \"\"\"Check if all context elements are of the same type.\"\"\"\\n        if not all(isinstance(x, type(context[0])) for x in context):\\n            raise ValueError(\"All context elements must be of the same type.\")\\n\\n    def __init__(\\n        self,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        context: Optional[List[Any]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ):\\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\\n        if context is not None and len(context) != len(values):\\n            raise ValueError(\"The context must have the same length as the values\")\\n\\n\\n        self.markdown = markdown\\n        if context is not None and values is not None:\\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\\n\\n        if context is not None:\\n            self.check_uniform_context_type(context)\\n            self.context_type = type(context[0])\\n\\n\\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\\n        if isinstance(identifier, list):\\n            return [self.get_context(value) for value in identifier]\\n        else:\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n            return self.context[value]\\n\\n    def clean_context(self):\\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\\n        self.context = {value: self.context[value] for value in self.values}\\n\\n    def add_to_context(self, value: str, context: Any):\\n        \"\"\" add a context to a value \"\"\"\\n        if not isinstance(context, self.context_type):\\n            raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if value in self.values:\\n            if value not in self.context:\\n                self.context[value] = []\\n            self.context[value].append(context)\\n\\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\\n        if isinstance(values, str):\\n            values = [values]\\n        NpIndex.add(self, values, embedding)\\n        if context is not None:\\n            for value, cont in zip(values, context):\\n                self.add_to_context(value, cont)\\n\\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\\n        if not isinstance(identifier, list):\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n        NpIndex.remove(self, identifier)\\n        if not isinstance(identifier, list):\\n            self.context.pop(value)\\n\\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\\n        #recover value from old_identifier\\n        if new_context is not None:\\n            if not isinstance(new_context, self.context_type):\\n                raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if not isinstance(old_identifier, list):\\n            old_id = self.identify_input(old_identifier)\\n            old_value = self.values[old_id]\\n            # Only perform the update if the old_value is not the same as the new_value.\\n            if old_value != new_value:\\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\\n\\n            if new_context is not None:\\n                self.context[new_value] = [new_context]\\n            else:\\n                self.context[new_value] = self.context.pop(old_value)\\n        else:\\n            self.context[new_value] = self.context.pop(old_value)\\n\\n    @classmethod\\n    def from_pandas(\\n        cls,\\n        data_frame: Union[pd.DataFrame, str],\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        if (\\n            isinstance(data_frame, str)\\n            and data_frame.endswith(\".csv\")\\n            and os.path.isfile(data_frame)\\n        ):\\n            logger.info(\"Loading the CSV file\")\\n            data_frame = pd.read_csv(data_frame)\\n            name = os.path.basename(data_frame).split(\".\")[0]\\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\\n            logger.info(\"Loading the pandas DataFrame\")\\n        else:\\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\\n\\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_pandas(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_hf_dataset(\\n        cls,\\n        dataset_url: str,\\n        value_column: str,\\n        data_split: str = \"train\",\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        dataset = load_dataset(dataset_url)[data_split]\\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_hf(dataset, context_columns)\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_polars(\\n        cls,\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        print(\"Loading the Polars DataFrame\")\\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_polars(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n\\n    @classmethod\\n    def from_python(\\n        cls,\\n        directory_path: str,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        markdown: str = \"python/markdown\",\\n        resolution: str = \"both\",\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def cluster_embeddings(self, n_neighbors: int = 10, min_cluster_size: int = 5):\n",
      "    reducer = umap.UMAP(n_neighbors=n_neighbors)\n",
      "    reduced_embeddings = reducer.fit_transform(self.idea_index.get_all_embeddings())\n",
      "\n",
      "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
      "    labels = clusterer.fit_predict(reduced_embeddings)\n",
      "\n",
      "    token_count_per_cluster = self.count_tokens_per_cluster(labels)\n",
      "    print(token_count_per_cluster)\n",
      "    if max(token_count_per_cluster.values()) <= self.max_tokens_per_cluster:\n",
      "        self.cluster_labels = labels\n",
      "        print(\"Clusters created successfully.\")\n",
      "    else:\n",
      "        print(\"Clusters exceed the maximum token count.\")\n",
      "\n",
      "Function call: print\n",
      "Function call: max\n",
      "Function call: print\n",
      "Function call: print\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def count_tokens_per_cluster(self, labels):\n",
      "    token_count_per_cluster = {}\n",
      "\n",
      "    for label, doc in zip(labels, self.gathered_docs):\n",
      "        if label not in token_count_per_cluster:\n",
      "            token_count_per_cluster[label] = len(tokenizer.encode(doc))\n",
      "        else:\n",
      "            token_count_per_cluster[label] += len(tokenizer.encode(doc))\n",
      "    return token_count_per_cluster\n",
      "\n",
      "Function call: zip\n",
      "Function call: len\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def create_minimum_spanning_paths(self):\n",
      "    if self.cluster_labels is None:\n",
      "        raise ValueError(\"You must run cluster_embeddings() before creating minimum spanning paths.\")\n",
      "\n",
      "    unique_labels = np.unique(self.cluster_labels)\n",
      "    min_span_paths = []\n",
      "\n",
      "    for label in unique_labels:\n",
      "\n",
      "\n",
      "        # Get the indices of the current cluster\n",
      "        cluster_indices = np.where(self.cluster_labels == label)[0]\n",
      "\n",
      "        # Calculate the pairwise distances between embeddings in the cluster\n",
      "        cluster_embeddings = self.idea_index.embeddings[cluster_indices]\n",
      "        dist_matrix = squareform(pdist(cluster_embeddings))\n",
      "\n",
      "        # Create a graph from the distance matrix\n",
      "        graph = nx.from_numpy_array(dist_matrix)\n",
      "\n",
      "        # Compute the minimum spanning tree of the graph\n",
      "        min_span_tree = nx.minimum_spanning_tree(graph)\n",
      "\n",
      "        # Get the minimum spanning paths\n",
      "        min_span_paths_cluster = []\n",
      "        visited = set()\n",
      "        for u, v in min_span_tree.edges():\n",
      "            if u not in visited and v not in visited:\n",
      "                orig_u = cluster_indices[u]\n",
      "                orig_v = cluster_indices[v]\n",
      "                min_span_paths_cluster.append(orig_u)\n",
      "                visited.add(u)\n",
      "                visited.add(v)\n",
      "        # Add the last node to complete the path\n",
      "        min_span_paths_cluster.append(orig_v)\n",
      "\n",
      "        min_span_paths.append(min_span_paths_cluster)\n",
      "\n",
      "    self.min_span_paths = min_span_paths\n",
      "\n",
      "Function call: ValueError\n",
      "Function call: squareform\n",
      "Function call: pdist\n",
      "Function call: set\n",
      "Related codes: ['\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def plot_embeddings_with_path(self):\n",
      "    paths = self.min_span_paths\n",
      "    embeddings = self.idea_index.embeddings\n",
      "    title = \"Minimum Spanning Paths\"\n",
      "    tsne = TSNE(n_components=2, random_state=42)\n",
      "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
      "\n",
      "    plt.figure(figsize=(10, 8))\n",
      "    colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n",
      "    for i, path in enumerate(paths):\n",
      "        path_embeddings = reduced_embeddings[path]\n",
      "        plt.scatter(\n",
      "            path_embeddings[:, 0],\n",
      "            path_embeddings[:, 1],\n",
      "            color=colors[i],\n",
      "            label=f\"Cluster {i}\",\n",
      "        )\n",
      "        for j in range(len(path) - 1):\n",
      "            plt.plot(\n",
      "                [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n",
      "                [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n",
      "                color=colors[i],\n",
      "            )\n",
      "    plt.title(title)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "Function call: TSNE\n",
      "Function call: len\n",
      "Function call: enumerate\n",
      "Function call: range\n",
      "Function call: len\n",
      "Related codes: ['\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def get_clustered_ideas(self):\n",
      "    if self.cluster_labels is None:\n",
      "        raise ValueError(\"You must run cluster_embeddings() before getting clustered ideas.\")\n",
      "    \n",
      "    clustered_ideas = {}\n",
      "    for label, idea in zip(self.cluster_labels, self.idea_index.values):\n",
      "        if label not in clustered_ideas:\n",
      "            clustered_ideas[label] = [idea]\n",
      "        else:\n",
      "            clustered_ideas[label].append(idea)\n",
      "    \n",
      "    # Convert the dictionary to a list of lists (each list corresponds to a cluster)\n",
      "    return list(clustered_ideas.values())\n",
      "\n",
      "Function call: ValueError\n",
      "Function call: zip\n",
      "Function call: list\n",
      "Related codes: [\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\"]\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "class Summarizer:\n",
      "    def __init__(self, texts: list):\n",
      "        self.texts = texts\n",
      "\n",
      "    def summarize_texts(self) -> dict:\n",
      "        output = {}\n",
      "\n",
      "        with RateLimitedThreadPoolExecutor(max_workers=12, calls_per_minute=200, verbose = False) as executor:\n",
      "            future_to_text = {executor.submit(self._summarize_text, text): text for text in self.texts}\n",
      "            for future in as_completed(future_to_text):\n",
      "                text = future_to_text[future]\n",
      "                try:\n",
      "                    output[text] = future.result()\n",
      "                except Exception as exc:\n",
      "                    print(f'An exception occurred while summarizing text: {exc}')\n",
      "\n",
      "        return output\n",
      "\n",
      "    def _summarize_text(self, text: str) -> str:\n",
      "        summary = cohere_summarize(text, model=\"summarize-xlarge\", length=\"auto\", extractiveness=\"low\", format=\"auto\")\n",
      "        return summary\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: RateLimitedThreadPoolExecutor\n",
      "Function call: as_completed\n",
      "Function call: print\n",
      "Function call: cohere_summarize\n",
      "Related codes: ['\\n\\nclass RateLimitedThreadPoolExecutor(ThreadPoolExecutor):\\n    def __init__(self, max_workers=None, *args, **kwargs):\\n        super().__init__(max_workers)\\n        self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\\n\\n    def submit(self, fn, *args, **kwargs):\\n        rate_limited_fn = self.rate_limiter(fn)\\n        return super().submit(rate_limited_fn, *args, **kwargs)\\n', '\\ndef cohere_summarize(prompt: str, model: str = \"summarize-xlarge\", length: str = \"medium\", extractiveness: str = \"medium\", format: str = \"bullets\", additional_command: str = None) -> str:\\n    response = co.summarize( \\n    text=prompt,model=model, \\n    length=length,\\n    extractiveness=extractiveness,\\n    format=format,\\n    additional_command=additional_command\\n    )\\n\\n    summary = response.summary\\n    return summary\\n              \\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, texts: list):\n",
      "    self.texts = texts\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def summarize_texts(self) -> dict:\n",
      "    output = {}\n",
      "\n",
      "    with RateLimitedThreadPoolExecutor(max_workers=12, calls_per_minute=200, verbose = False) as executor:\n",
      "        future_to_text = {executor.submit(self._summarize_text, text): text for text in self.texts}\n",
      "        for future in as_completed(future_to_text):\n",
      "            text = future_to_text[future]\n",
      "            try:\n",
      "                output[text] = future.result()\n",
      "            except Exception as exc:\n",
      "                print(f'An exception occurred while summarizing text: {exc}')\n",
      "\n",
      "    return output\n",
      "\n",
      "Function call: RateLimitedThreadPoolExecutor\n",
      "Function call: as_completed\n",
      "Function call: print\n",
      "Related codes: ['\\n\\nclass RateLimitedThreadPoolExecutor(ThreadPoolExecutor):\\n    def __init__(self, max_workers=None, *args, **kwargs):\\n        super().__init__(max_workers)\\n        self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\\n\\n    def submit(self, fn, *args, **kwargs):\\n        rate_limited_fn = self.rate_limiter(fn)\\n        return super().submit(rate_limited_fn, *args, **kwargs)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def _summarize_text(self, text: str) -> str:\n",
      "    summary = cohere_summarize(text, model=\"summarize-xlarge\", length=\"auto\", extractiveness=\"low\", format=\"auto\")\n",
      "    return summary\n",
      "\n",
      "Function call: cohere_summarize\n",
      "Related codes: ['\\ndef cohere_summarize(prompt: str, model: str = \"summarize-xlarge\", length: str = \"medium\", extractiveness: str = \"medium\", format: str = \"bullets\", additional_command: str = None) -> str:\\n    response = co.summarize( \\n    text=prompt,model=model, \\n    length=length,\\n    extractiveness=extractiveness,\\n    format=format,\\n    additional_command=additional_command\\n    )\\n\\n    summary = response.summary\\n    return summary\\n              \\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "def generate_perspective_prompt(user_subject, user_perspective, seed_model = \"gpt-3.5-turbo\"):\n",
      "    start = perf_counter()\n",
      "    chat_instance = Chat(model=seed_model)\n",
      "    analyzer = SubjectPerspectiveAnalyzer(chat_instance)\n",
      "    output = analyzer.analyze_subject_perspective(user_subject, user_perspective)\n",
      "    end = perf_counter()\n",
      "    print(\"Time to analyze_perspective: \", end - start)\n",
      "\n",
      "    dataset_url = \"Cohere/wikipedia-22-12-simple-embeddings\"\n",
      "    start = perf_counter()\n",
      "    index = MemoryIndex(name=\"wiki_index\", load=True, is_batched=True,embedder=CohereEmbedder)\n",
      "    if len(index.values)>0:\n",
      "        loaded = True\n",
      "    else:\n",
      "        loaded = False\n",
      "\n",
      "    if not loaded:\n",
      "        print(\"Index not found, creating new index\")\n",
      "        index = MemoryIndex.from_hf_dataset(dataset_url, [\"title\", \"text\"],embeddings_column= \"emb\", name=\"wiki_index\", is_batched=True,embedder=CohereEmbedder)\n",
      "    end = perf_counter()\n",
      "    print(\"Time to index: \", end - start)\n",
      "\n",
      "    start = perf_counter()\n",
      "    ideation = Ideation(memory_index=index)\n",
      "    ideas = ideation.retrieve_ideas(output, k=40, max_tokens=10000)\n",
      "    token_count = 0\n",
      "    for idea in ideas:\n",
      "        token_count += idea[\"returned_tokens\"]\n",
      "    end = perf_counter()\n",
      "    print(\"Time to retrieve ideas: \", end - start)\n",
      "    print(\"Number of tokens: \", token_count)\n",
      "\n",
      "    start = perf_counter()\n",
      "    max_tokens_per_cluster = 20000\n",
      "    idea_cluster = IdeaCluster(ideas, max_tokens_per_cluster)\n",
      "    idea_cluster.cluster_embeddings()\n",
      "    time.sleep(0.5)\n",
      "    ideas = idea_cluster.get_clustered_ideas()\n",
      "    end = perf_counter()\n",
      "    combined_idea = []\n",
      "    ideas_tokens = []\n",
      "    for idea in ideas:\n",
      "        combined_idea.append(\"\\n\".join(idea))\n",
      "        ideas_tokens.append(tokenizer.encode(combined_idea[-1]))\n",
      "    print(\"Time to cluster ideas: \", end - start)\n",
      "    print(\"Number of clusters: \", len(ideas))\n",
      "    print(\"Number of tokens in each cluster: \", [len(tokens) for tokens in ideas_tokens])\n",
      "    start = perf_counter()\n",
      "    summarizer = Summarizer(combined_idea)\n",
      "    summaries = summarizer.summarize_texts()\n",
      "    end = perf_counter()\n",
      "    print(\"Time to summarize: \", end - start)\n",
      "    print(\"Number of summaries: \", len(summaries))\n",
      "    print(\"Number of tokens in each summary: \", [len(tokenizer.encode(summary)) for summary in summaries.values()])\n",
      "    start = perf_counter()\n",
      "    system_prompt = f\"\"\"With the summarized information from Cohere about the essential ideas, concenpts, priciples, and intersection points between {user_subject} and {user_perspective}, construct an appealing and context-aware chatbot prompt that impels the chatbot to respond with insights and perspectives born from the synergy of these two domains. Use the tips below to help you create an effective chatbot prompt: 1. Begin with a concise introduction: Initiate the chatbot prompt by setting the context, encompassing the user's specified subject and perspective. 2. Accentuate the intersection: Secure that the chatbot prompt underlines the connection between the user_subject and user_perspective, leading to more pertinent and perceptive responses. 3. Foster exploration: Ensure the chatbot prompt provokes the chatbot to delve into the main ideas, principles, and concepts from the summaries with a thoughtful and reflective approach. 4. Pose open-ended questions: Incorporate open-ended queries in the chatbot prompt, stimulating the chatbot to contemplate beyond the summaries and offer comprehensive responses. 5. Prioritize simplicity: Maintain the chatbot prompt's clarity and brevity, assisting the chatbot in comprehending the context and reacting suitably. 6. Steer the conversation: Craft the chatbot prompt in a manner that subtly directs the chatbot's answers, confirming they consistently focus on the user_subject and user_perspective. By leveraging these tips, develop a chatbot prompt that generates responses illustrative of the user's specified subject and perspective, culminating in a customized and significant interaction. Remember to conclude the prompt with 7 content pillars that will help the chatbot use the perspective at the best of its capacity. \"\"\".format(user_subject=user_subject, user_perspective=user_perspective)\n",
      "    for i, summary in enumerate(summaries.values()):\n",
      "        system_prompt += summary + \"\\n\\n\"\n",
      "    prompt_generator = Chat(name= \"prompt generator\",system_prompt=system_prompt, max_output_tokens= 2000, model = \"gpt-4\")\n",
      "    perspective_prompt = prompt_generator.reply(\"\", verbose=False)\n",
      "    additional_prompt =  \"\"\" \\n\\n Bear in mind that while engaging with the user, questions may arise that seem unrelated to the initial prompt. It's crucial to maintain flexibility and creativity, interpreting and addressing these topics through the unique prism provided by the initial prompt.\"\"\"\n",
      "    perspective_prompt += additional_prompt\n",
      "    end = perf_counter()\n",
      "    print(\"Time to generate prompt: \", end - start)\n",
      "    return perspective_prompt\n",
      "\n",
      "Function call: perf_counter\n",
      "Function call: Chat\n",
      "Function call: SubjectPerspectiveAnalyzer\n",
      "Function call: perf_counter\n",
      "Function call: print\n",
      "Function call: perf_counter\n",
      "Function call: MemoryIndex\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: perf_counter\n",
      "Function call: print\n",
      "Function call: perf_counter\n",
      "Function call: Ideation\n",
      "Function call: perf_counter\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: perf_counter\n",
      "Function call: IdeaCluster\n",
      "Function call: perf_counter\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: perf_counter\n",
      "Function call: Summarizer\n",
      "Function call: perf_counter\n",
      "Function call: print\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: print\n",
      "Function call: len\n",
      "Function call: perf_counter\n",
      "Function call: enumerate\n",
      "Function call: Chat\n",
      "Function call: perf_counter\n",
      "Function call: print\n",
      "Related codes: ['class FifoChat(FifoThread, Chat):\\n    \"\"\"\\n    A chatbot class that combines FIFO Memory Thread, BaseChat, and Prompter. The oldest messages are removed first\\n    when reaching the max_memory limit. The memory is defined in terms of tokens, and outs are passed to the\\n    longterm_memory. The lucid_memory is a redundant memory that stores all the messages.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        model: Optional[str] = None,\\n        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\\n        system_prompt: Optional[str] = None,\\n        user_prompt: Optional[str] = None,\\n        name: str = \"fifo_memory\",\\n        max_index_memory: int = 400,\\n        max_fifo_memory: int = 2048,\\n        max_output_tokens: int = 1000,\\n        longterm_thread: Optional[BaseThread] = None,\\n    ):\\n\\n        FifoThread.__init__(\\n            self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread\\n        )\\n        Chat.__init__(\\n            self,\\n            model=model,\\n            index_dict=index_dict,\\n            max_output_tokens=max_output_tokens,\\n            max_index_memory=max_index_memory,\\n            system_prompt=system_prompt,\\n            user_prompt=user_prompt,\\n            name=name,\\n        )\\n\\n        self.prompt_func = self.fifo_memory_prompt\\n\\n    def fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\\n        \"\"\"\\n        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n        marked_question = mark_question(self.user_prompt(message))\\n        prompt = (\\n            [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\\n        )\\n        return prompt, marked_question\\n\\n    def query(self, question: str, verbose: bool = True, stream: bool = False) -> Union[Generator,str]:\\n        \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n        marked_question = mark_question(question)\\n        self.add_message(marked_question)\\n        answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\\n        if stream:\\n            return answer\\n        else:\\n            self.add_message(answer)\\n            return answer\\n', '\\n\\nclass SubjectPerspectiveAnalyzer:\\n    def __init__(self, chatbot: \\'Chat\\'):\\n        self.chatbot = chatbot\\n\\n    def analyze_subject_perspective(self, user_subject: str, user_perspective: str) -> dict:\\n        prompts = [\\n            f\"Generate ideas and concepts that explore the connection between {user_subject} and {user_perspective}, considering both traditional and unconventional approaches.\",\\n            f\"List key concepts or topics that would help analyze {user_subject} through the lens of {user_perspective}, including relevant principles, theories, or models.\",\\n            f\"Identify potential areas or research topics where {user_subject} and {user_perspective} intersect, highlighting intriguing or innovative perspectives.\"\\n        ]\\n\\n        output = {}\\n\\n        with RateLimitedThreadPoolExecutor(max_workers=3, calls_per_minute=20, verbose = False) as executor:\\n            future_to_prompt = {executor.submit(self._analyze_prompt, prompt): prompt for prompt in prompts}\\n            for future in as_completed(future_to_prompt):\\n                prompt = future_to_prompt[future]\\n                try:\\n                    output[prompt] = future.result()\\n                except Exception as exc:\\n                    counter = 0\\n                    print(f\\'An exception occurred while analyzing prompt \"{prompt}\": {exc}\\')\\n                    while counter < 3:\\n                        try:\\n                            output[prompt] = self._analyze_prompt(prompt)\\n                            break\\n                        except Exception as exc:\\n                            counter += 1\\n                            print(f\\'An exception occurred while analyzing prompt \"{prompt}\": {exc}\\')\\n                    if counter == 3:\\n                        output[prompt] = []\\n\\n        return output\\n\\n    def _analyze_prompt(self, prompt: str) -> list:\\n        response = self.chatbot.reply(prompt, verbose=False)\\n        return self._format_response(response)\\n\\n    def _format_response(self, response: str) -> list:\\n        formatted_response = response.strip().split(\\'\\\\n\\')\\n        return formatted_response\\n    \\n', '\\n\\n\\nclass MemoryIndex(NpIndex):\\n    \"\"\"\\n    this class is a wrapper for a Np index, it contains information about the format of the index the index itself\\n    ways to load it from: python lists, pandas dataframe, huggingface dataset, polars dataframe or local python package with libcst pre-processing\\n    a concept of context that can be used to store information about the values, there is a one to many relationship between values and context, \\n    when loading from a dataframe or dataset the context is automatically extracted from the dataframe/dataset if context_columns are provided\\n    \"\"\"\\n    @staticmethod\\n    def check_uniform_context_type(context: List[Any]) -> None:\\n        \"\"\"Check if all context elements are of the same type.\"\"\"\\n        if not all(isinstance(x, type(context[0])) for x in context):\\n            raise ValueError(\"All context elements must be of the same type.\")\\n\\n    def __init__(\\n        self,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        context: Optional[List[Any]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ):\\n        NpIndex.__init__(self, values=values, embeddings=embeddings, name=name, save_path=save_path, load=load, embedder=embedder, token_overflow_strategy=token_overflow_strategy)\\n        if context is not None and len(context) != len(values):\\n            raise ValueError(\"The context must have the same length as the values\")\\n\\n\\n        self.markdown = markdown\\n        if context is not None and values is not None:\\n            self.context = {value: [context[old_id] for old_id in self.old_ids[value]] for value in self.values}\\n\\n        if context is not None:\\n            self.check_uniform_context_type(context)\\n            self.context_type = type(context[0])\\n\\n\\n    def get_context(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> Optional[Any]:\\n        \"\"\" get the context of a value id or embedding or a list of \"\"\"\\n        if isinstance(identifier, list):\\n            return [self.get_context(value) for value in identifier]\\n        else:\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n            return self.context[value]\\n\\n    def clean_context(self):\\n        \"\"\" method to be called after parent modifications with add/remove/update remove from context all the values that are not in the index anymore \"\"\"\\n        self.context = {value: self.context[value] for value in self.values}\\n\\n    def add_to_context(self, value: str, context: Any):\\n        \"\"\" add a context to a value \"\"\"\\n        if not isinstance(context, self.context_type):\\n            raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if value in self.values:\\n            if value not in self.context:\\n                self.context[value] = []\\n            self.context[value].append(context)\\n\\n    def add(self, values: List[str], embedding: Optional[Union[List[float], np.ndarray]] = None, context: Optional[Any] = None):\\n        \"\"\" add a value to the index, if the value is already in the index it will be updated \"\"\"\\n        if isinstance(values, str):\\n            values = [values]\\n        NpIndex.add(self, values, embedding)\\n        if context is not None:\\n            for value, cont in zip(values, context):\\n                self.add_to_context(value, cont)\\n\\n    def remove(self, identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]]) -> None:\\n        if not isinstance(identifier, list):\\n            id = self.identify_input(identifier)\\n            value = self.values[id]\\n        NpIndex.remove(self, identifier)\\n        if not isinstance(identifier, list):\\n            self.context.pop(value)\\n\\n    def update(self, old_identifier: Union[int, str, np.ndarray, List[Union[int, str, np.ndarray]]], new_value: Union[str, List[str]], new_context: Optional[Any] = None, new_embedding: Optional[Union[List[float], np.ndarray, List[List[float]], List[np.ndarray]]] = None) -> None:\\n        #recover value from old_identifier\\n        if new_context is not None:\\n            if not isinstance(new_context, self.context_type):\\n                raise ValueError(\"The context must be of the same type as the other contexts\")\\n        if not isinstance(old_identifier, list):\\n            old_id = self.identify_input(old_identifier)\\n            old_value = self.values[old_id]\\n            # Only perform the update if the old_value is not the same as the new_value.\\n            if old_value != new_value:\\n                NpIndex.update(self, old_identifier, new_value, new_embedding)\\n\\n            if new_context is not None:\\n                self.context[new_value] = [new_context]\\n            else:\\n                self.context[new_value] = self.context.pop(old_value)\\n        else:\\n            self.context[new_value] = self.context.pop(old_value)\\n\\n    @classmethod\\n    def from_pandas(\\n        cls,\\n        data_frame: Union[pd.DataFrame, str],\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        if (\\n            isinstance(data_frame, str)\\n            and data_frame.endswith(\".csv\")\\n            and os.path.isfile(data_frame)\\n        ):\\n            logger.info(\"Loading the CSV file\")\\n            data_frame = pd.read_csv(data_frame)\\n            name = os.path.basename(data_frame).split(\".\")[0]\\n        elif isinstance(data_frame, pd.core.frame.DataFrame):\\n            logger.info(\"Loading the pandas DataFrame\")\\n        else:\\n            raise ValueError(\"The data_frame is not a valid pandas dataframe or the path is not valid\")\\n\\n        values, embeddings = extract_values_and_embeddings_pd(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_pandas(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_hf_dataset(\\n        cls,\\n        dataset_url: str,\\n        value_column: str,\\n        data_split: str = \"train\",\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        dataset = load_dataset(dataset_url)[data_split]\\n        values, embeddings = extract_values_and_embeddings_hf(dataset, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_hf(dataset, context_columns)\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, markdown=markdown, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n    @classmethod\\n    def from_polars(\\n        cls,\\n        data_frame: pl.DataFrame,\\n        value_column: str,\\n        embeddings_column: Optional[str] = None,\\n        context_columns: Optional[List[str]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        markdown: str = \"text/markdown\",\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        print(\"Loading the Polars DataFrame\")\\n        values, embeddings = extract_values_and_embeddings_polars(data_frame, value_column, embeddings_column)\\n        if context_columns is not None:\\n            context = get_context_from_polars(data_frame, context_columns)\\n\\n        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n\\n\\n    @classmethod\\n    def from_python(\\n        cls,\\n        directory_path: str,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        markdown: str = \"python/markdown\",\\n        resolution: str = \"both\",\\n        embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= OpenAiEmbedder,\\n        token_overflow_strategy: str = \"ignore\",\\n    ) -> \"MemoryIndex\":\\n        values, context = extract_values_and_embeddings_python(directory_path, minify_code, remove_docstrings, resolution)\\n        logger.info(f\"Found {len(values)} values in the directory {directory_path}\")\\n        return cls(values=values, embeddings=None, name=name, save_path=save_path,markdown=markdown, embedder=embedder, context = context, token_overflow_strategy=token_overflow_strategy)\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\nclass Ideation:\\n    def __init__(self, memory_index: MemoryIndex):\\n        self.memory_index = memory_index\\n\\n    def retrieve_ideas(self, queries: Dict, k: int = 30, max_tokens: int = 10000):\\n        \"\"\"\\n        Generate ideas based on the given list of queries.\\n\\n        Args:\\n            queries: The list of queries for generating ideas.\\n            k: The number of top search results to consider.\\n            max_tokens: The maximum number of tokens to return.\\n\\n        Returns:\\n            A list of ideas generated based on the queries.\\n        \"\"\"\\n        ideas = []\\n        for key, queries in queries.items():\\n            for query in queries:\\n                if query is None or len(query) < 10:\\n                    continue\\n                top_k_hints, scores, indices = self.memory_index.token_bound_query(\\n                    query, k=k, max_tokens=max_tokens\\n                )\\n                last_query = self.memory_index.query_history[-1]\\n                hints_tokens = last_query[\"hints_tokens\"]\\n                returned_tokens = last_query[\"returned_tokens\"]\\n\\n                ideas.append({\"key_task\": key, \"query\": query, \"hints\": top_k_hints, \"scores\": scores, \"hints_tokens\": hints_tokens, \"returned_tokens\": returned_tokens})\\n\\n        return ideas\\n', '\\nclass IdeaCluster:\\n    def __init__(self, ideas: list, max_tokens_per_cluster: int):\\n        self.ideas = ideas\\n        self.max_tokens_per_cluster = max_tokens_per_cluster\\n        self.idea_index = self.create_idea_index()\\n        self.cluster_labels = None\\n\\n    def create_idea_index(self):\\n        gathered_docs = []\\n        for idea in self.ideas:\\n            for hint in idea[\"hints\"]:\\n                gathered_docs.append(hint)\\n        self.gathered_docs = set(gathered_docs)\\n        idea_index = MemoryIndex(values=self.gathered_docs, is_batched=True, name=\"ideas\")\\n        return idea_index\\n\\n    def cluster_embeddings(self, n_neighbors: int = 10, min_cluster_size: int = 5):\\n        reducer = umap.UMAP(n_neighbors=n_neighbors)\\n        reduced_embeddings = reducer.fit_transform(self.idea_index.get_all_embeddings())\\n\\n        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\\n        labels = clusterer.fit_predict(reduced_embeddings)\\n\\n        token_count_per_cluster = self.count_tokens_per_cluster(labels)\\n        print(token_count_per_cluster)\\n        if max(token_count_per_cluster.values()) <= self.max_tokens_per_cluster:\\n            self.cluster_labels = labels\\n            print(\"Clusters created successfully.\")\\n        else:\\n            print(\"Clusters exceed the maximum token count.\")\\n\\n    def count_tokens_per_cluster(self, labels):\\n        token_count_per_cluster = {}\\n\\n        for label, doc in zip(labels, self.gathered_docs):\\n            if label not in token_count_per_cluster:\\n                token_count_per_cluster[label] = len(tokenizer.encode(doc))\\n            else:\\n                token_count_per_cluster[label] += len(tokenizer.encode(doc))\\n        return token_count_per_cluster\\n\\n    def create_minimum_spanning_paths(self):\\n        if self.cluster_labels is None:\\n            raise ValueError(\"You must run cluster_embeddings() before creating minimum spanning paths.\")\\n\\n        unique_labels = np.unique(self.cluster_labels)\\n        min_span_paths = []\\n\\n        for label in unique_labels:\\n\\n\\n            # Get the indices of the current cluster\\n            cluster_indices = np.where(self.cluster_labels == label)[0]\\n\\n            # Calculate the pairwise distances between embeddings in the cluster\\n            cluster_embeddings = self.idea_index.embeddings[cluster_indices]\\n            dist_matrix = squareform(pdist(cluster_embeddings))\\n\\n            # Create a graph from the distance matrix\\n            graph = nx.from_numpy_array(dist_matrix)\\n\\n            # Compute the minimum spanning tree of the graph\\n            min_span_tree = nx.minimum_spanning_tree(graph)\\n\\n            # Get the minimum spanning paths\\n            min_span_paths_cluster = []\\n            visited = set()\\n            for u, v in min_span_tree.edges():\\n                if u not in visited and v not in visited:\\n                    orig_u = cluster_indices[u]\\n                    orig_v = cluster_indices[v]\\n                    min_span_paths_cluster.append(orig_u)\\n                    visited.add(u)\\n                    visited.add(v)\\n            # Add the last node to complete the path\\n            min_span_paths_cluster.append(orig_v)\\n\\n            min_span_paths.append(min_span_paths_cluster)\\n\\n        self.min_span_paths = min_span_paths\\n\\n    \\n    def plot_embeddings_with_path(self):\\n        paths = self.min_span_paths\\n        embeddings = self.idea_index.embeddings\\n        title = \"Minimum Spanning Paths\"\\n        tsne = TSNE(n_components=2, random_state=42)\\n        reduced_embeddings = tsne.fit_transform(embeddings)\\n\\n        plt.figure(figsize=(10, 8))\\n        colors = cm.rainbow(np.linspace(0, 1, len(paths)))\\n        for i, path in enumerate(paths):\\n            path_embeddings = reduced_embeddings[path]\\n            plt.scatter(\\n                path_embeddings[:, 0],\\n                path_embeddings[:, 1],\\n                color=colors[i],\\n                label=f\"Cluster {i}\",\\n            )\\n            for j in range(len(path) - 1):\\n                plt.plot(\\n                    [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\\n                    [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\\n                    color=colors[i],\\n                )\\n        plt.title(title)\\n        plt.legend()\\n        plt.show()\\n    def get_clustered_ideas(self):\\n        if self.cluster_labels is None:\\n            raise ValueError(\"You must run cluster_embeddings() before getting clustered ideas.\")\\n        \\n        clustered_ideas = {}\\n        for label, idea in zip(self.cluster_labels, self.idea_index.values):\\n            if label not in clustered_ideas:\\n                clustered_ideas[label] = [idea]\\n            else:\\n                clustered_ideas[label].append(idea)\\n        \\n        # Convert the dictionary to a list of lists (each list corresponds to a cluster)\\n        return list(clustered_ideas.values())\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\n\\nclass Summarizer:\\n    def __init__(self, texts: list):\\n        self.texts = texts\\n\\n    def summarize_texts(self) -> dict:\\n        output = {}\\n\\n        with RateLimitedThreadPoolExecutor(max_workers=12, calls_per_minute=200, verbose = False) as executor:\\n            future_to_text = {executor.submit(self._summarize_text, text): text for text in self.texts}\\n            for future in as_completed(future_to_text):\\n                text = future_to_text[future]\\n                try:\\n                    output[text] = future.result()\\n                except Exception as exc:\\n                    print(f\\'An exception occurred while summarizing text: {exc}\\')\\n\\n        return output\\n\\n    def _summarize_text(self, text: str) -> str:\\n        summary = cohere_summarize(text, model=\"summarize-xlarge\", length=\"auto\", extractiveness=\"low\", format=\"auto\")\\n        return summary\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', '\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n', 'class FifoChat(FifoThread, Chat):\\n    \"\"\"\\n    A chatbot class that combines FIFO Memory Thread, BaseChat, and Prompter. The oldest messages are removed first\\n    when reaching the max_memory limit. The memory is defined in terms of tokens, and outs are passed to the\\n    longterm_memory. The lucid_memory is a redundant memory that stores all the messages.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        model: Optional[str] = None,\\n        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\\n        system_prompt: Optional[str] = None,\\n        user_prompt: Optional[str] = None,\\n        name: str = \"fifo_memory\",\\n        max_index_memory: int = 400,\\n        max_fifo_memory: int = 2048,\\n        max_output_tokens: int = 1000,\\n        longterm_thread: Optional[BaseThread] = None,\\n    ):\\n\\n        FifoThread.__init__(\\n            self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread\\n        )\\n        Chat.__init__(\\n            self,\\n            model=model,\\n            index_dict=index_dict,\\n            max_output_tokens=max_output_tokens,\\n            max_index_memory=max_index_memory,\\n            system_prompt=system_prompt,\\n            user_prompt=user_prompt,\\n            name=name,\\n        )\\n\\n        self.prompt_func = self.fifo_memory_prompt\\n\\n    def fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\\n        \"\"\"\\n        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n        marked_question = mark_question(self.user_prompt(message))\\n        prompt = (\\n            [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\\n        )\\n        return prompt, marked_question\\n\\n    def query(self, question: str, verbose: bool = True, stream: bool = False) -> Union[Generator,str]:\\n        \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n        marked_question = mark_question(question)\\n        self.add_message(marked_question)\\n        answer = BaseChat.query(self, message=question, verbose=verbose, stream=stream)\\n        if stream:\\n            return answer\\n        else:\\n            self.add_message(answer)\\n            return answer\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "class PerspectivePromptGenerator:\n",
      "    def __init__(self, subjects, perspectives, max_workers=10, calls_per_minute=20, base_filename=\"prompt_\"):\n",
      "        self.subjects = subjects\n",
      "        self.perspectives = perspectives\n",
      "        self.executor = RateLimitedThreadPoolExecutor(\n",
      "            max_workers=max_workers, \n",
      "            calls_per_minute=calls_per_minute\n",
      "        )\n",
      "        self.prompts = []\n",
      "        self.base_filename = base_filename\n",
      "        self.lock = threading.Lock()  # create a lock\n",
      "\n",
      "    def handle_future(self, future):\n",
      "        try:\n",
      "            result = future.result()\n",
      "            self.prompts.append(result)\n",
      "            complete_filename = self.base_filename + \"results.json\"\n",
      "            with self.lock:  # acquire the lock before writing to the file\n",
      "                self.save_prompts_to_json(complete_filename)\n",
      "        except Exception as e:\n",
      "            error_report = {\"error\": str(e), \"traceback\": traceback.format_exc()}\n",
      "            self.prompts.append(error_report)\n",
      "            complete_filename = self.base_filename + \"errors.json\"\n",
      "            with self.lock:  # acquire the lock before writing to the file\n",
      "                self.save_prompts_to_json(complete_filename)\n",
      "    \n",
      "    def generate_prompts(self):\n",
      "        for subject in self.subjects:\n",
      "            for perspective in self.perspectives:\n",
      "                future = self.executor.submit(\n",
      "                    generate_perspective_prompt, \n",
      "                    subject, \n",
      "                    perspective\n",
      "                )\n",
      "                future.add_done_callback(self.handle_future)\n",
      "        self.executor.shutdown(wait=True)\n",
      "        return self.prompts\n",
      "\n",
      "    def save_prompts_to_json(self, filename):\n",
      "        with open(filename, 'w') as f:\n",
      "            json.dump(self.prompts, f)\n",
      "\n",
      "Parent classes: shape: (1,)\n",
      "Series: '' [list[str]]\n",
      "[\n",
      "\t[]\n",
      "]\n",
      "Function call: RateLimitedThreadPoolExecutor\n",
      "Function call: str\n",
      "Function call: open\n",
      "Related codes: ['\\n\\nclass RateLimitedThreadPoolExecutor(ThreadPoolExecutor):\\n    def __init__(self, max_workers=None, *args, **kwargs):\\n        super().__init__(max_workers)\\n        self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\\n\\n    def submit(self, fn, *args, **kwargs):\\n        rate_limited_fn = self.rate_limiter(fn)\\n        return super().submit(rate_limited_fn, *args, **kwargs)\\n', '\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "def __init__(self, subjects, perspectives, max_workers=10, calls_per_minute=20, base_filename=\"prompt_\"):\n",
      "    self.subjects = subjects\n",
      "    self.perspectives = perspectives\n",
      "    self.executor = RateLimitedThreadPoolExecutor(\n",
      "        max_workers=max_workers, \n",
      "        calls_per_minute=calls_per_minute\n",
      "    )\n",
      "    self.prompts = []\n",
      "    self.base_filename = base_filename\n",
      "    self.lock = threading.Lock()  # create a lock\n",
      "\n",
      "Function call: RateLimitedThreadPoolExecutor\n",
      "Related codes: ['\\n\\nclass RateLimitedThreadPoolExecutor(ThreadPoolExecutor):\\n    def __init__(self, max_workers=None, *args, **kwargs):\\n        super().__init__(max_workers)\\n        self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\\n\\n    def submit(self, fn, *args, **kwargs):\\n        rate_limited_fn = self.rate_limiter(fn)\\n        return super().submit(rate_limited_fn, *args, **kwargs)\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def handle_future(self, future):\n",
      "    try:\n",
      "        result = future.result()\n",
      "        self.prompts.append(result)\n",
      "        complete_filename = self.base_filename + \"results.json\"\n",
      "        with self.lock:  # acquire the lock before writing to the file\n",
      "            self.save_prompts_to_json(complete_filename)\n",
      "    except Exception as e:\n",
      "        error_report = {\"error\": str(e), \"traceback\": traceback.format_exc()}\n",
      "        self.prompts.append(error_report)\n",
      "        complete_filename = self.base_filename + \"errors.json\"\n",
      "        with self.lock:  # acquire the lock before writing to the file\n",
      "            self.save_prompts_to_json(complete_filename)\n",
      "\n",
      "Function call: str\n",
      "Related codes: ['\\ndef create_stratas(self):\\n    \"\"\"\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        \"\"\"\\n    pass\\n']\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def generate_prompts(self):\n",
      "    for subject in self.subjects:\n",
      "        for perspective in self.perspectives:\n",
      "            future = self.executor.submit(\n",
      "                generate_perspective_prompt, \n",
      "                subject, \n",
      "                perspective\n",
      "            )\n",
      "            future.add_done_callback(self.handle_future)\n",
      "    self.executor.shutdown(wait=True)\n",
      "    return self.prompts\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "def save_prompts_to_json(self, filename):\n",
      "    with open(filename, 'w') as f:\n",
      "        json.dump(self.prompts, f)\n",
      "\n",
      "Function call: open\n",
      "\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#for row in column search through inheritance structure and print code search for parent class in df and print code also look for function calls in df and print code\n",
    "new_columns = []\n",
    "for code, parent_classes, function_calls in zip(mfp.df['code'], mfp.df['code_parent_classes|ClassInheritanceVisitor'], mfp.df['code_function_calls|FunctionCallCollector']):\n",
    "    print(code)\n",
    "    related_codes = []\n",
    "    if parent_classes.shape[0] > 0:\n",
    "        print(f\"Parent classes: {parent_classes}\")\n",
    "    if function_calls.shape[0] > 0:\n",
    "        for function_call in function_calls:\n",
    "            print(f\"Function call: {function_call}\")\n",
    "            #search row using code_main_entity|MainEntityVisitor\n",
    "            mask = mfp.df.filter(pl.col('code_main_entity|MainEntityVisitor').str.contains(function_call))\n",
    "            if mask.shape[0] > 0:\n",
    "                related_codes.append(mask['code'].to_list()[0])\n",
    "            \n",
    "    if len(related_codes) > 0:\n",
    "        print(f\"Related codes: {related_codes}\")\n",
    "        new_columns.append(f'{code}\\n\\n Related Code: {related_codes}')\n",
    "    else:\n",
    "        new_columns.append(f'{code}')\n",
    "        # create a new column with the related codes map it to the code column\n",
    "        \n",
    "    print()\n",
    "                \n",
    "    print(\"*\" * 80)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th><th>code_main_entity|MainEntityVisitor</th><th>code_function_calls|FunctionCallCollector</th><th>code_parent_classes|ClassInheritanceVisitor</th><th>code_with_related_prompt</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td><td>str</td><td>list[str]</td><td>list[list[str]]</td><td>str</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012073, -0.004771, … -0.043605]</td><td>&quot;EmbeddableType…</td><td>[]</td><td>[[&quot;Enum&quot;]]</td><td>&quot;\n",
       "class Embedda…</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.03026, 0.011673, … -0.039312]</td><td>&quot;infer_embeddab…</td><td>[&quot;str&quot;, &quot;print&quot;, … &quot;ValueError&quot;]</td><td>[]</td><td>&quot;\n",
       "def infer_emb…</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012823, 0.010932, … -0.027359]</td><td>&quot;numeric_embedd…</td><td>[]</td><td>[]</td><td>&quot;\n",
       "def numeric_e…</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025804, -0.008756, … -0.042009]</td><td>&quot;EmbeddingTask&quot;</td><td>[&quot;len&quot;, &quot;ValueError&quot;]</td><td>[[&quot;BaseTask&quot;]]</td><td>&quot;\n",
       "\n",
       "class Embedd…</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 198]</td><td>102</td><td>[-0.018768, -0.018833, … -0.046555]</td><td>&quot;__init__&quot;</td><td>[]</td><td>[]</td><td>&quot;def __init__(\n",
       "…</td></tr><tr><td>&quot;\n",
       "def _execute_…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>101</td><td>[-0.037655, -0.008194, … -0.016726]</td><td>&quot;_execute_sub_t…</td><td>[&quot;len&quot;, &quot;ValueError&quot;]</td><td>[]</td><td>&quot;\n",
       "def _execute_…</td></tr><tr><td>&quot;\n",
       "def parallel_…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>140</td><td>[-0.041914, -0.0109, … -0.027008]</td><td>&quot;parallel_embed…</td><td>[&quot;print&quot;, &quot;len&quot;, … &quot;sorted&quot;]</td><td>[]</td><td>&quot;\n",
       "def parallel_…</td></tr><tr><td>&quot;\n",
       "\n",
       "class TopicT…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>992</td><td>[-0.023198, -0.002724, … -0.053914]</td><td>&quot;TopicTreeTask&quot;</td><td>[&quot;super&quot;, &quot;print&quot;, … &quot;MemoryIndex&quot;]</td><td>[[&quot;BaseTask&quot;]]</td><td>&quot;\n",
       "\n",
       "class TopicT…</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 340]</td><td>220</td><td>[-0.004693, 0.001029, … -0.048643]</td><td>&quot;__init__&quot;</td><td>[&quot;super&quot;]</td><td>[]</td><td>&quot;def __init__(\n",
       "…</td></tr><tr><td>&quot;\n",
       "\n",
       "def _setup_m…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[271, 755, … 14790]</td><td>104</td><td>[-0.01272, 0.016058, … -0.001598]</td><td>&quot;_setup_memory_…</td><td>[&quot;print&quot;, &quot;HDBSCANMultiKernel&quot;, … &quot;ValueError&quot;]</td><td>[]</td><td>&quot;\n",
       "\n",
       "def _setup_m…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 10)\n",
       "┌─────────┬───────────┬────────┬───────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ code    ┆ libcst    ┆ filena ┆ tokens|co ┆ … ┆ code_main_ ┆ code_funct ┆ code_paren ┆ code_with_ │\n",
       "│ ---     ┆ tree      ┆ me     ┆ de        ┆   ┆ entity|Mai ┆ ion_calls| ┆ t_classes| ┆ related_pr │\n",
       "│ str     ┆ ---       ┆ ---    ┆ ---       ┆   ┆ nEntityVis ┆ FunctionCa ┆ ClassInher ┆ ompt       │\n",
       "│         ┆ str       ┆ str    ┆ list[i64] ┆   ┆ it…        ┆ ll…        ┆ it…        ┆ ---        │\n",
       "│         ┆           ┆        ┆           ┆   ┆ ---        ┆ ---        ┆ ---        ┆ str        │\n",
       "│         ┆           ┆        ┆           ┆   ┆ str        ┆ list[str]  ┆ list[list[ ┆            │\n",
       "│         ┆           ┆        ┆           ┆   ┆            ┆            ┆ str]]      ┆            │\n",
       "╞═════════╪═══════════╪════════╪═══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [198,     ┆ … ┆ Embeddable ┆ []         ┆ [[\"Enum\"]] ┆            │\n",
       "│ class   ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆ Type       ┆            ┆            ┆ class Embe │\n",
       "│ Embedda ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆            ┆            ┆            ┆ ddableType │\n",
       "│ bleType ┆        …  ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆ (Enum):    │\n",
       "│ (Enum): ┆           ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆    …       │\n",
       "│    …    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ infer_embe ┆ [\"str\",    ┆ []         ┆            │\n",
       "│ def inf ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆ ddable_typ ┆ \"print\", … ┆            ┆ def infer_ │\n",
       "│ er_embe ┆ name=Name ┆ lhug/n ┆ 14790]    ┆   ┆ e          ┆ \"ValueErro ┆            ┆ embeddable │\n",
       "│ ddable_ ┆ (         ┆ eurald ┆           ┆   ┆            ┆ r\"]        ┆            ┆ _type(colu │\n",
       "│ type(co ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆ m…         │\n",
       "│ lum…    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ numeric_em ┆ []         ┆ []         ┆            │\n",
       "│ def num ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆ bedder     ┆            ┆            ┆ def numeri │\n",
       "│ eric_em ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆            ┆            ┆            ┆ c_embedder │\n",
       "│ bedder( ┆ (         ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆ (column):  │\n",
       "│ column) ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆  …         │\n",
       "│ :       ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│  …      ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [271,     ┆ … ┆ EmbeddingT ┆ [\"len\",    ┆ [[\"BaseTas ┆            │\n",
       "│         ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆ ask        ┆ \"ValueErro ┆ k\"]]       ┆            │\n",
       "│ class   ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆            ┆ r\"]        ┆            ┆ class Embe │\n",
       "│ Embeddi ┆        …  ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆ ddingTask( │\n",
       "│ ngTask( ┆           ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆ BaseTask): │\n",
       "│ BaseTas ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆ …          │\n",
       "│ k):…    ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ …       ┆ …         ┆ …      ┆ …         ┆ … ┆ …          ┆ …          ┆ …          ┆ …          │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ parallel_e ┆ [\"print\",  ┆ []         ┆            │\n",
       "│ def par ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆ mbeddings  ┆ \"len\", …   ┆            ┆ def parall │\n",
       "│ allel_e ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆            ┆ \"sorted\"]  ┆            ┆ el_embeddi │\n",
       "│ mbeddin ┆ (         ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆ ngs(embedd │\n",
       "│ gs(embe ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆ e…         │\n",
       "│ dde…    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [271,     ┆ … ┆ TopicTreeT ┆ [\"super\",  ┆ [[\"BaseTas ┆            │\n",
       "│         ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆ ask        ┆ \"print\", … ┆ k\"]]       ┆            │\n",
       "│ class   ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆            ┆ \"MemoryInd ┆            ┆ class Topi │\n",
       "│ TopicTr ┆        …  ┆ eurald ┆           ┆   ┆            ┆ e…         ┆            ┆ cTreeTask( │\n",
       "│ eeTask( ┆           ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆ BaseTask): │\n",
       "│ BaseTas ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆ …          │\n",
       "│ k):…    ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ def __i ┆ FunctionD ┆ /Users ┆ [755,     ┆ … ┆ __init__   ┆ [\"super\"]  ┆ []         ┆ def        │\n",
       "│ nit__(  ┆ ef(       ┆ /danie ┆ 1328, …   ┆   ┆            ┆            ┆            ┆ __init__(  │\n",
       "│ self,   ┆ name=Name ┆ lhug/n ┆ 340]      ┆   ┆            ┆            ┆            ┆     self,  │\n",
       "│ memo…   ┆ (         ┆ eurald ┆           ┆   ┆            ┆            ┆            ┆     memo…  │\n",
       "│         ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [271,     ┆ … ┆ _setup_mem ┆ [\"print\",  ┆ []         ┆            │\n",
       "│         ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆ ory_kernel ┆ \"HDBSCANMu ┆            ┆            │\n",
       "│ def _se ┆ name=Name ┆ lhug/n ┆ 14790]    ┆   ┆ _group     ┆ ltiKernel\" ┆            ┆ def _setup │\n",
       "│ tup_mem ┆ (         ┆ eurald ┆           ┆   ┆            ┆ , …        ┆            ┆ _memory_ke │\n",
       "│ ory_ker ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆            ┆            ┆ rnel_group │\n",
       "│ nel_gro ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆ …          │\n",
       "│ up…     ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "└─────────┴───────────┴────────┴───────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add new column using .with_columns\n",
    "new_df = pl.DataFrame({\"code_with_related_prompt\": new_columns })\n",
    "mfp.df = mfp.df.with_columns(new_df)\n",
    "\n",
    "mfp.df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all rows with code_main_entity|MainEntityVisitor == '__init__'\n",
    "mfp.df = mfp.df.filter(pl.col('code_main_entity|MainEntityVisitor') != '__init__')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (963, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th><th>code_main_entity|MainEntityVisitor</th><th>code_function_calls|FunctionCallCollector</th><th>code_parent_classes|ClassInheritanceVisitor</th><th>code_with_related_prompt</th><th>tokens|code_with_related_prompt</th><th>tokens_len|code_with_related_prompt</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td><td>str</td><td>list[str]</td><td>list[list[str]]</td><td>str</td><td>list[i64]</td><td>i64</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012073, -0.004771, … -0.043605]</td><td>&quot;EmbeddableType…</td><td>[]</td><td>[[&quot;Enum&quot;]]</td><td>&quot;\n",
       "class Embedda…</td><td>[198, 1058, … 198]</td><td>40</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.03026, 0.011673, … -0.039312]</td><td>&quot;infer_embeddab…</td><td>[&quot;str&quot;, &quot;print&quot;, … &quot;ValueError&quot;]</td><td>[]</td><td>&quot;\n",
       "def infer_emb…</td><td>[198, 755, … 663]</td><td>953</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012823, 0.010932, … -0.027359]</td><td>&quot;numeric_embedd…</td><td>[]</td><td>[]</td><td>&quot;\n",
       "def numeric_e…</td><td>[198, 755, … 198]</td><td>59</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025804, -0.008756, … -0.042009]</td><td>&quot;EmbeddingTask&quot;</td><td>[&quot;len&quot;, &quot;ValueError&quot;]</td><td>[[&quot;BaseTask&quot;]]</td><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>[271, 1058, … 663]</td><td>236</td></tr><tr><td>&quot;\n",
       "def _execute_…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>101</td><td>[-0.037655, -0.008194, … -0.016726]</td><td>&quot;_execute_sub_t…</td><td>[&quot;len&quot;, &quot;ValueError&quot;]</td><td>[]</td><td>&quot;\n",
       "def _execute_…</td><td>[198, 755, … 663]</td><td>124</td></tr><tr><td>&quot;\n",
       "def parallel_…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>140</td><td>[-0.041914, -0.0109, … -0.027008]</td><td>&quot;parallel_embed…</td><td>[&quot;print&quot;, &quot;len&quot;, … &quot;sorted&quot;]</td><td>[]</td><td>&quot;\n",
       "def parallel_…</td><td>[198, 755, … 663]</td><td>820</td></tr><tr><td>&quot;\n",
       "\n",
       "class TopicT…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>992</td><td>[-0.023198, -0.002724, … -0.053914]</td><td>&quot;TopicTreeTask&quot;</td><td>[&quot;super&quot;, &quot;print&quot;, … &quot;MemoryIndex&quot;]</td><td>[[&quot;BaseTask&quot;]]</td><td>&quot;\n",
       "\n",
       "class TopicT…</td><td>[271, 1058, … 663]</td><td>3801</td></tr><tr><td>&quot;\n",
       "\n",
       "def _setup_m…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[271, 755, … 14790]</td><td>104</td><td>[-0.01272, 0.016058, … -0.001598]</td><td>&quot;_setup_memory_…</td><td>[&quot;print&quot;, &quot;HDBSCANMultiKernel&quot;, … &quot;ValueError&quot;]</td><td>[]</td><td>&quot;\n",
       "\n",
       "def _setup_m…</td><td>[271, 755, … 663]</td><td>466</td></tr><tr><td>&quot;\n",
       "def generate_…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 746]</td><td>23</td><td>[-0.030834, 0.009467, … -0.015321]</td><td>&quot;generate_task_…</td><td>[&quot;print&quot;]</td><td>[]</td><td>&quot;\n",
       "def generate_…</td><td>[198, 755, … 746]</td><td>23</td></tr><tr><td>&quot;\n",
       "def llm_respo…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 340]</td><td>53</td><td>[-0.013516, -0.007049, … -0.063983]</td><td>&quot;llm_response&quot;</td><td>[]</td><td>[]</td><td>&quot;\n",
       "def llm_respo…</td><td>[198, 755, … 340]</td><td>53</td></tr><tr><td>&quot;\n",
       "def _execute_…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>293</td><td>[-0.017594, 0.009393, … -0.020512]</td><td>&quot;_execute_sub_t…</td><td>[&quot;print&quot;, &quot;len&quot;, … &quot;print&quot;]</td><td>[]</td><td>&quot;\n",
       "def _execute_…</td><td>[198, 755, … 663]</td><td>316</td></tr><tr><td>&quot;\n",
       "def execute_t…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 1158]</td><td>276</td><td>[-0.036908, 0.010786, … -0.028764]</td><td>&quot;execute_task&quot;</td><td>[&quot;isinstance&quot;, &quot;int&quot;, … &quot;MemoryIndex&quot;]</td><td>[]</td><td>&quot;\n",
       "def execute_t…</td><td>[198, 755, … 663]</td><td>2709</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;\n",
       "def count_tok…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>80</td><td>[-0.019238, 0.005321, … -0.034412]</td><td>&quot;count_tokens_p…</td><td>[&quot;zip&quot;, &quot;len&quot;, &quot;len&quot;]</td><td>[]</td><td>&quot;\n",
       "def count_tok…</td><td>[198, 755, … 663]</td><td>122</td></tr><tr><td>&quot;\n",
       "def create_mi…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>277</td><td>[-0.017391, 0.002888, … -0.037907]</td><td>&quot;create_minimum…</td><td>[&quot;ValueError&quot;, &quot;squareform&quot;, … &quot;set&quot;]</td><td>[]</td><td>&quot;\n",
       "def create_mi…</td><td>[198, 755, … 663]</td><td>402</td></tr><tr><td>&quot;\n",
       "\n",
       "def plot_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[271, 755, … 746]</td><td>215</td><td>[-0.036079, 0.014086, … -0.051123]</td><td>&quot;plot_embedding…</td><td>[&quot;TSNE&quot;, &quot;len&quot;, … &quot;len&quot;]</td><td>[]</td><td>&quot;\n",
       "\n",
       "def plot_emb…</td><td>[271, 755, … 663]</td><td>257</td></tr><tr><td>&quot;def get_cluste…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 636, … 2455]</td><td>114</td><td>[-0.036263, -0.000354, … -0.057481]</td><td>&quot;get_clustered_…</td><td>[&quot;ValueError&quot;, &quot;zip&quot;, &quot;list&quot;]</td><td>[]</td><td>&quot;def get_cluste…</td><td>[755, 636, … 1365]</td><td>215</td></tr><tr><td>&quot;\n",
       "\n",
       "class Summar…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>193</td><td>[-0.029496, 0.013841, … -0.046939]</td><td>&quot;Summarizer&quot;</td><td>[&quot;RateLimitedThreadPoolExecutor&quot;, &quot;as_completed&quot;, … &quot;cohere_summarize&quot;]</td><td>[[]]</td><td>&quot;\n",
       "\n",
       "class Summar…</td><td>[271, 1058, … 663]</td><td>431</td></tr><tr><td>&quot;\n",
       "def summarize…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>118</td><td>[-0.043786, 0.011305, … -0.059675]</td><td>&quot;summarize_text…</td><td>[&quot;RateLimitedThreadPoolExecutor&quot;, &quot;as_completed&quot;, &quot;print&quot;]</td><td>[]</td><td>&quot;\n",
       "def summarize…</td><td>[198, 755, … 663]</td><td>237</td></tr><tr><td>&quot;\n",
       "def _summariz…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>51</td><td>[-0.011196, 0.028116, … -0.031305]</td><td>&quot;_summarize_tex…</td><td>[&quot;cohere_summarize&quot;]</td><td>[]</td><td>&quot;\n",
       "def _summariz…</td><td>[198, 755, … 663]</td><td>174</td></tr><tr><td>&quot;\n",
       "\n",
       "def generate…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[271, 755, … 198]</td><td>1078</td><td>[-0.015097, 0.004607, … -0.048512]</td><td>&quot;generate_persp…</td><td>[&quot;perf_counter&quot;, &quot;Chat&quot;, … &quot;print&quot;]</td><td>[]</td><td>&quot;\n",
       "\n",
       "def generate…</td><td>[271, 755, … 663]</td><td>6780</td></tr><tr><td>&quot;\n",
       "class Perspec…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 340]</td><td>338</td><td>[-0.036914, -0.021619, … -0.061177]</td><td>&quot;PerspectivePro…</td><td>[&quot;RateLimitedThreadPoolExecutor&quot;, &quot;str&quot;, &quot;open&quot;]</td><td>[[]]</td><td>&quot;\n",
       "class Perspec…</td><td>[198, 1058, … 663]</td><td>498</td></tr><tr><td>&quot;\n",
       "def handle_fu…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 340]</td><td>134</td><td>[-0.0293, -0.012099, … -0.05066]</td><td>&quot;handle_future&quot;</td><td>[&quot;str&quot;]</td><td>[]</td><td>&quot;\n",
       "def handle_fu…</td><td>[198, 755, … 663]</td><td>179</td></tr><tr><td>&quot;\n",
       "def generate_…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>69</td><td>[-0.036225, -0.012628, … -0.043738]</td><td>&quot;generate_promp…</td><td>[]</td><td>[]</td><td>&quot;\n",
       "def generate_…</td><td>[198, 755, … 198]</td><td>69</td></tr><tr><td>&quot;\n",
       "def save_prom…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 340]</td><td>31</td><td>[-0.020762, -0.00763, … -0.053758]</td><td>&quot;save_prompts_t…</td><td>[&quot;open&quot;]</td><td>[]</td><td>&quot;\n",
       "def save_prom…</td><td>[198, 755, … 340]</td><td>31</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (963, 12)\n",
       "┌─────────┬───────────┬────────┬───────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ code    ┆ libcst    ┆ filena ┆ tokens|co ┆ … ┆ code_paren ┆ code_with_ ┆ tokens|cod ┆ tokens_len │\n",
       "│ ---     ┆ tree      ┆ me     ┆ de        ┆   ┆ t_classes| ┆ related_pr ┆ e_with_rel ┆ |code_with │\n",
       "│ str     ┆ ---       ┆ ---    ┆ ---       ┆   ┆ ClassInher ┆ ompt       ┆ ated_promp ┆ _related_p │\n",
       "│         ┆ str       ┆ str    ┆ list[i64] ┆   ┆ it…        ┆ ---        ┆ t          ┆ ro…        │\n",
       "│         ┆           ┆        ┆           ┆   ┆ ---        ┆ str        ┆ ---        ┆ ---        │\n",
       "│         ┆           ┆        ┆           ┆   ┆ list[list[ ┆            ┆ list[i64]  ┆ i64        │\n",
       "│         ┆           ┆        ┆           ┆   ┆ str]]      ┆            ┆            ┆            │\n",
       "╞═════════╪═══════════╪════════╪═══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [198,     ┆ … ┆ [[\"Enum\"]] ┆            ┆ [198,      ┆ 40         │\n",
       "│ class   ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆            ┆ class Embe ┆ 1058, …    ┆            │\n",
       "│ Embedda ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆            ┆ ddableType ┆ 198]       ┆            │\n",
       "│ bleType ┆        …  ┆ eurald ┆           ┆   ┆            ┆ (Enum):    ┆            ┆            │\n",
       "│ (Enum): ┆           ┆ ragon/ ┆           ┆   ┆            ┆    …       ┆            ┆            │\n",
       "│    …    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ []         ┆            ┆ [198, 755, ┆ 953        │\n",
       "│ def inf ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆            ┆ def infer_ ┆ … 663]     ┆            │\n",
       "│ er_embe ┆ name=Name ┆ lhug/n ┆ 14790]    ┆   ┆            ┆ embeddable ┆            ┆            │\n",
       "│ ddable_ ┆ (         ┆ eurald ┆           ┆   ┆            ┆ _type(colu ┆            ┆            │\n",
       "│ type(co ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆ m…         ┆            ┆            │\n",
       "│ lum…    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ []         ┆            ┆ [198, 755, ┆ 59         │\n",
       "│ def num ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆            ┆ def numeri ┆ … 198]     ┆            │\n",
       "│ eric_em ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆            ┆ c_embedder ┆            ┆            │\n",
       "│ bedder( ┆ (         ┆ eurald ┆           ┆   ┆            ┆ (column):  ┆            ┆            │\n",
       "│ column) ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆  …         ┆            ┆            │\n",
       "│ :       ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│  …      ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [271,     ┆ … ┆ [[\"BaseTas ┆            ┆ [271,      ┆ 236        │\n",
       "│         ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆ k\"]]       ┆            ┆ 1058, …    ┆            │\n",
       "│ class   ┆ (         ┆ lhug/n ┆ 198]      ┆   ┆            ┆ class Embe ┆ 663]       ┆            │\n",
       "│ Embeddi ┆        …  ┆ eurald ┆           ┆   ┆            ┆ ddingTask( ┆            ┆            │\n",
       "│ ngTask( ┆           ┆ ragon/ ┆           ┆   ┆            ┆ BaseTask): ┆            ┆            │\n",
       "│ BaseTas ┆           ┆ gi…    ┆           ┆   ┆            ┆ …          ┆            ┆            │\n",
       "│ k):…    ┆           ┆        ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│ …       ┆ …         ┆ …      ┆ …         ┆ … ┆ …          ┆ …          ┆ …          ┆ …          │\n",
       "│         ┆ ClassDef( ┆ /Users ┆ [198,     ┆ … ┆ [[]]       ┆            ┆ [198,      ┆ 498        │\n",
       "│ class   ┆ name=Name ┆ /danie ┆ 1058, …   ┆   ┆            ┆ class Pers ┆ 1058, …    ┆            │\n",
       "│ Perspec ┆ (         ┆ lhug/n ┆ 340]      ┆   ┆            ┆ pectivePro ┆ 663]       ┆            │\n",
       "│ tivePro ┆        …  ┆ eurald ┆           ┆   ┆            ┆ mptGenerat ┆            ┆            │\n",
       "│ mptGene ┆           ┆ ragon/ ┆           ┆   ┆            ┆ o…         ┆            ┆            │\n",
       "│ rato…   ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ []         ┆            ┆ [198, 755, ┆ 179        │\n",
       "│ def han ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆            ┆ def handle ┆ … 663]     ┆            │\n",
       "│ dle_fut ┆ name=Name ┆ lhug/n ┆ 340]      ┆   ┆            ┆ _future(se ┆            ┆            │\n",
       "│ ure(sel ┆ (         ┆ eurald ┆           ┆   ┆            ┆ lf,        ┆            ┆            │\n",
       "│ f, futu ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆ future)…   ┆            ┆            │\n",
       "│ re)…    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ []         ┆            ┆ [198, 755, ┆ 69         │\n",
       "│ def gen ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆            ┆ def genera ┆ … 198]     ┆            │\n",
       "│ erate_p ┆ name=Name ┆ lhug/n ┆ 198]      ┆   ┆            ┆ te_prompts ┆            ┆            │\n",
       "│ rompts( ┆ (         ┆ eurald ┆           ┆   ┆            ┆ (self):    ┆            ┆            │\n",
       "│ self):  ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆    …       ┆            ┆            │\n",
       "│    …    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "│         ┆ FunctionD ┆ /Users ┆ [198,     ┆ … ┆ []         ┆            ┆ [198, 755, ┆ 31         │\n",
       "│ def sav ┆ ef(       ┆ /danie ┆ 755, …    ┆   ┆            ┆ def save_p ┆ … 340]     ┆            │\n",
       "│ e_promp ┆ name=Name ┆ lhug/n ┆ 340]      ┆   ┆            ┆ rompts_to_ ┆            ┆            │\n",
       "│ ts_to_j ┆ (         ┆ eurald ┆           ┆   ┆            ┆ json(self, ┆            ┆            │\n",
       "│ son(sel ┆     …     ┆ ragon/ ┆           ┆   ┆            ┆ …          ┆            ┆            │\n",
       "│ f, …    ┆           ┆ gi…    ┆           ┆   ┆            ┆            ┆            ┆            │\n",
       "└─────────┴───────────┴────────┴───────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfp = mfp.tokenize_column(\"code_with_related_prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>value</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>963.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>285.559709</td></tr><tr><td>&quot;std&quot;</td><td>773.576245</td></tr><tr><td>&quot;min&quot;</td><td>11.0</td></tr><tr><td>&quot;max&quot;</td><td>8453.0</td></tr><tr><td>&quot;median&quot;</td><td>82.0</td></tr><tr><td>&quot;25%&quot;</td><td>31.0</td></tr><tr><td>&quot;75%&quot;</td><td>164.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 2)\n",
       "┌────────────┬────────────┐\n",
       "│ statistic  ┆ value      │\n",
       "│ ---        ┆ ---        │\n",
       "│ str        ┆ f64        │\n",
       "╞════════════╪════════════╡\n",
       "│ count      ┆ 963.0      │\n",
       "│ null_count ┆ 0.0        │\n",
       "│ mean       ┆ 285.559709 │\n",
       "│ std        ┆ 773.576245 │\n",
       "│ min        ┆ 11.0       │\n",
       "│ max        ┆ 8453.0     │\n",
       "│ median     ┆ 82.0       │\n",
       "│ 25%        ┆ 31.0       │\n",
       "│ 75%        ┆ 164.0      │\n",
       "└────────────┴────────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print stats on tokens_len|code\n",
    "mfp.df['tokens_len|code_with_related_prompt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfp.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate column for summaries of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.dates as mdates\n",
    "from babydragon.models.generators.PolarsGenerator import PolarsGenerator\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_df = prepare_input_df(df=mfp.df,\\n                            messages_col=\\'code_with_related_prompt\\', \\n                            system_prompt=\"You are a Python Code Summarizer. Please summarize the meaning of the code in a rigorous and clear manner. \\n\\nCode: \")\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_input_df(df, messages_col, system_prompt):\n",
    "\n",
    "    df = df.select(messages_col).with_columns(pl.lit(\"gpt-3.5-turbo-16k\").alias(\"model\"))\n",
    "\n",
    "    def create_content(value):\n",
    "        return ([{\"role\": \"system\", \"content\":system_prompt}, \n",
    "                       {\"role\": \"user\", \"content\": f\"{value}\"}])\n",
    "\n",
    "    input_df = df.with_columns(df[messages_col].apply(create_content, return_dtype=pl.List).alias('messages')).drop(messages_col)\n",
    "\n",
    "    return(input_df)\n",
    "'''\n",
    "input_df = prepare_input_df(df=mfp.df,\n",
    "                            messages_col='code_with_related_prompt', \n",
    "                            system_prompt=\"You are a Python Code Summarizer. Please summarize the meaning of the code in a rigorous and clear manner. \\n\\nCode: \")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>messages</th></tr><tr><td>str</td><td>list[struct[2]]</td></tr></thead><tbody><tr><td>&quot;gpt-3.5-turbo-…</td><td>[{&quot;system&quot;,&quot;You are a Python Code Summarizer. Please summarize the meaning of the code in a rigorous and clear manner. \n",
       "\n",
       "Code: &quot;}, {&quot;user&quot;,&quot;\n",
       "class EmbeddableType(Enum):\n",
       "    TEXT = &quot;text&quot;\n",
       "    NUMERIC = &quot;numeric&quot;\n",
       "    CATEGORICAL = &quot;categorical&quot;\n",
       "    # Add more data types as required\n",
       "&quot;}]</td></tr><tr><td>&quot;gpt-3.5-turbo-…</td><td>[{&quot;system&quot;,&quot;You are a Python Code Summarizer. Please summarize the meaning of the code in a rigorous and clear manner. \n",
       "\n",
       "Code: &quot;}, {&quot;user&quot;,&quot;\n",
       "def infer_embeddable_type(column) -&gt; Tuple[EmbeddableType, Callable]:\n",
       "    # Infer the data type of the column\n",
       "    # This will depend on the type of `column` (whether it&#x27;s a string, Series, etc.)\n",
       "    # Here we&#x27;ll assume `column` is a pandas Series for simplicity\n",
       "    column_type = str(column.dtype)\n",
       "    print(column_type)\n",
       "    if column_type == &quot;Utf8&quot;:\n",
       "        # If it&#x27;s an object, we&#x27;ll assume it&#x27;s text\n",
       "        return EmbeddableType.TEXT, OpenAiEmbedder()\n",
       "    elif np.issubdtype(column.dtype, np.number):\n",
       "        # If it&#x27;s a number, we&#x27;ll use a different embedding strategy\n",
       "        return EmbeddableType.NUMERIC, numeric_embedder\n",
       "    else:\n",
       "        # For other types, we could throw an error or have a default strategy\n",
       "        raise ValueError(f&quot;Cannot infer type for column {column.name}&quot;)\n",
       "\n",
       "\n",
       " Related Code: [&#x27;\\ndef create_stratas(self):\\n    &quot;&quot;&quot;\\n        Creates stratas for all columns in the DataFrame by calling _create_strata on each column.\\n        &quot;&quot;&quot;\\n    pass\\n&#x27;, &#x27;\\nclass OpenAiEmbedder:\\n\\n    def get_embedding_size(self):\\n        return ADA_EMBEDDING_SIZE\\n\\n    def embed(self, data, verbose=False):\\n        if isinstance(data, list) and len(data) &gt; 1:\\n            logger.info(&quot;Batch embedding&quot;)\\n            return self.batch_embed(data)\\n        elif isinstance(data, list) and len(data) == 1:\\n            logger.info(&quot;Serial embedding&quot;)\\n            data = data[0]\\n\\n        if isinstance(data, dict) and &quot;content&quot; in data:\\n            if verbose:\\n                logger.info(&quot;Embedding without mark&quot;, data[&quot;content&quot;])\\n            out = openai.Embedding.create(\\n                input=data[&quot;content&quot;], engine=&quot;text-embedding-ada-002&quot;\\n            )\\n        else:\\n            if len(TOKENIZER.encode(data)) &gt; MAX_CONTEXT_LENGTH:\\n                raise ValueError(f&quot; The input is too long for OpenAI, num tokens is {len(TOKENIZER.encode(data))}, instead of {MAX_CONTEXT_LENGTH}&quot;)\\n            if verbose:\\n                logger.info(f&quot;Embedding without preprocessing the input {data}&quot;)\\n            out = openai.Embedding.create(\\n                input=str(data), engine=&quot;text-embedding-ada-002&quot;\\n            )\\n        return out.data[0].embedding\\n\\n    def batch_embed(self, data: List[str], batch_size: int = 1000):\\n        if isinstance(data, dict) and &quot;content&quot; in data:\\n            raise ValueError(&quot;Batch embedding not supported for dictionaries&quot;)\\n        elif isinstance(data, str):\\n            raise ValueError(&quot;Batch embedding not supported for strings use embed() instead&quot;)\\n        elif isinstance(data, list):\\n            batch = []\\n            embeddings = []\\n            i = 1\\n            total_number_of_batches = len(data)//batch_size + 1 if len(data) % batch_size &gt; 0 else len(data)//batch_size\\n            for value in data:\\n                batch.append(value)\\n                if len(batch) == batch_size:\\n                    start = time.time()\\n                    out = openai.Embedding.create(\\n                        input=batch, engine=&quot;text-embedding-ada-002&quot;\\n                    )\\n                    for embedding in out.data:\\n                        embeddings.append(embedding.embedding)\\n                    logger.info(f&quot;Batch {i} of {total_number_of_batches}&quot;)\\n                    logger.info(f&quot;Embedding batch {i} took {time.time() - start} seconds&quot;)\\n                    i += 1\\n                    batch = []\\n            if len(batch) &gt; 0:\\n                start = time.time()\\n                out = openai.Embedding.create(\\n                    input=batch, engine=&quot;text-embedding-ada-002&quot;\\n                )\\n                for embedding in out.data:\\n                    embeddings.append(embedding.embedding)\\n                logger.info(f&quot;Batch {i} of {total_number_of_batches}&quot;)\\n                logger.info(f&quot;Embedding batch {i} took {time.time() - start} seconds&quot;)\\n            logger.info(f\\&#x27;Total number of embeddings {len(embeddings)}\\&#x27;)\\n\\n            if len(embeddings) != len(data):\\n                raise ValueError(&quot;The number of embeddings is different from the number of values an error occured in OpenAI API during the embedding process&quot;)\\n            return embeddings\\n&#x27;]&quot;}]</td></tr><tr><td>&quot;gpt-3.5-turbo-…</td><td>[{&quot;system&quot;,&quot;You are a Python Code Summarizer. Please summarize the meaning of the code in a rigorous and clear manner. \n",
       "\n",
       "Code: &quot;}, {&quot;user&quot;,&quot;\n",
       "def numeric_embedder(column):\n",
       "    # Implement the numeric embedding strategy\n",
       "    # This will depend on the type of `column` (whether it&#x27;s a string, Series, etc.)\n",
       "    # Here we&#x27;ll assume `column` is a pandas Series for simplicity\n",
       "    return column.values\n",
       "&quot;}]</td></tr><tr><td>&quot;gpt-3.5-turbo-…</td><td>[{&quot;system&quot;,&quot;You are a Python Code Summarizer. Please summarize the meaning of the code in a rigorous and clear manner. \n",
       "\n",
       "Code: &quot;}, {&quot;user&quot;,&quot;\n",
       "\n",
       "class EmbeddingTask(BaseTask):\n",
       "    def __init__(\n",
       "        self,\n",
       "        embedder: OpenAiEmbedder,\n",
       "        values: List[Any],\n",
       "        path: List[List[int]],\n",
       "        max_workers: int = 1,\n",
       "        task_id: str = &quot;task&quot;,\n",
       "        calls_per_minute: int = 1500,\n",
       "        backup: bool = True,\n",
       "    ):\n",
       "        BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup)\n",
       "        self.embedder = embedder\n",
       "        self.values = values\n",
       "\n",
       "    def _execute_sub_task(self, sub_path: List[int]) -&gt; List[str]:\n",
       "        # expected to work with a lig of a single element\n",
       "        if len(sub_path) != 1:\n",
       "            raise ValueError(\n",
       "                &quot;Embedding task expected to work with a list of a single element&quot;\n",
       "            )\n",
       "        sub_results = {}\n",
       "        for i in sub_path:\n",
       "            embedded_value = self.embedder.embed(self.values[i])\n",
       "            sub_results[i] = embedded_value\n",
       "        return sub_results\n",
       "\n",
       "\n",
       " Related Code: [&#x27;\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n&#x27;]&quot;}]</td></tr><tr><td>&quot;gpt-3.5-turbo-…</td><td>[{&quot;system&quot;,&quot;You are a Python Code Summarizer. Please summarize the meaning of the code in a rigorous and clear manner. \n",
       "\n",
       "Code: &quot;}, {&quot;user&quot;,&quot;\n",
       "def _execute_sub_task(self, sub_path: List[int]) -&gt; List[str]:\n",
       "    # expected to work with a lig of a single element\n",
       "    if len(sub_path) != 1:\n",
       "        raise ValueError(\n",
       "            &quot;Embedding task expected to work with a list of a single element&quot;\n",
       "        )\n",
       "    sub_results = {}\n",
       "    for i in sub_path:\n",
       "        embedded_value = self.embedder.embed(self.values[i])\n",
       "        sub_results[i] = embedded_value\n",
       "    return sub_results\n",
       "\n",
       "\n",
       " Related Code: [&#x27;\\ndef __len__(self):\\n    return self.memory_thread.shape[0]\\n&#x27;]&quot;}]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌────────────────────────┬───────────────────────────────────┐\n",
       "│ model                  ┆ messages                          │\n",
       "│ ---                    ┆ ---                               │\n",
       "│ str                    ┆ list[struct[2]]                   │\n",
       "╞════════════════════════╪═══════════════════════════════════╡\n",
       "│ gpt-3.5-turbo-16k-0613 ┆ [{\"system\",\"You are a Python Cod… │\n",
       "│ gpt-3.5-turbo-16k-0613 ┆ [{\"system\",\"You are a Python Cod… │\n",
       "│ gpt-3.5-turbo-16k-0613 ┆ [{\"system\",\"You are a Python Cod… │\n",
       "│ gpt-3.5-turbo-16k-0613 ┆ [{\"system\",\"You are a Python Cod… │\n",
       "│ gpt-3.5-turbo-16k-0613 ┆ [{\"system\",\"You are a Python Cod… │\n",
       "└────────────────────────┴───────────────────────────────────┘"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The following input is a workaround to let work the asyncio functions in a jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "generator = PolarsGenerator( input_df = input_df, name = 'babydragon_code')\n",
    "\n",
    "generator.execute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation parsing and mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering of the code and creating columns with the cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst tree</th><th>filename</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌──────────────────────────────────┬────────────────┬───────────────────────────────────┐\n",
       "│ code                             ┆ libcst tree    ┆ filename                          │\n",
       "│ ---                              ┆ ---            ┆ ---                               │\n",
       "│ str                              ┆ str            ┆ str                               │\n",
       "╞══════════════════════════════════╪════════════════╪═══════════════════════════════════╡\n",
       "│                                  ┆ ClassDef(      ┆ /Users/danielhug/neuraldragon/gi… │\n",
       "│ class EmbeddableType(Enum):      ┆     name=Name( ┆                                   │\n",
       "│    …                             ┆        …       ┆                                   │\n",
       "│                                  ┆ FunctionDef(   ┆ /Users/danielhug/neuraldragon/gi… │\n",
       "│ def infer_embeddable_type(colum… ┆     name=Name( ┆                                   │\n",
       "│                                  ┆     …          ┆                                   │\n",
       "│                                  ┆ FunctionDef(   ┆ /Users/danielhug/neuraldragon/gi… │\n",
       "│ def numeric_embedder(column):    ┆     name=Name( ┆                                   │\n",
       "│  …                               ┆     …          ┆                                   │\n",
       "│                                  ┆ ClassDef(      ┆ /Users/danielhug/neuraldragon/gi… │\n",
       "│                                  ┆     name=Name( ┆                                   │\n",
       "│ class EmbeddingTask(BaseTask):…  ┆        …       ┆                                   │\n",
       "│ def __init__(                    ┆ FunctionDef(   ┆ /Users/danielhug/neuraldragon/gi… │\n",
       "│     self,                        ┆     name=Name( ┆                                   │\n",
       "│     embe…                        ┆     …          ┆                                   │\n",
       "└──────────────────────────────────┴────────────────┴───────────────────────────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "babydragon.utils.main_logger - INFO - Batch embedding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utf8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "babydragon.utils.main_logger - INFO - Batch 1 of 2\n",
      "babydragon.utils.main_logger - INFO - Embedding batch 1 took 3.0889389514923096 seconds\n",
      "babydragon.utils.main_logger - INFO - Batch 2 of 2\n",
      "babydragon.utils.main_logger - INFO - Embedding batch 2 took 1.2139949798583984 seconds\n",
      "babydragon.utils.main_logger - INFO - Total number of embeddings 1184\n"
     ]
    }
   ],
   "source": [
    "mfp.embed_columns([\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import hdbscan\n",
    "dim_reduction_model = umap.UMAP(n_neighbors=10)\n",
    "cluster_model = hdbscan.HDBSCAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>NAIVE QUERY PLAN</h4><p>run <b>LazyFrame.show_graph()</b> to see the optimized version</p><?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 7.0.5 (20221223.1930)\n",
       " -->\n",
       "<!-- Title: polars_query Pages: 1 -->\n",
       "<svg width=\"73pt\" height=\"61pt\"\n",
       " viewBox=\"0.00 0.00 73.00 61.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 57)\">\n",
       "<title>polars_query</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-57 69,-57 69,4 -4,4\"/>\n",
       "<!-- [TABLE\n",
       "π */7;\n",
       "σ &#45;;] -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>[TABLE\n",
       "π */7;\n",
       "σ &#45;;]</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"65,-53 0,-53 0,0 65,0 65,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"32.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">[TABLE</text>\n",
       "<text text-anchor=\"middle\" x=\"32.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">π */7;</text>\n",
       "<text text-anchor=\"middle\" x=\"32.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">σ &#45;;]</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<babydragon.memory.frames.code_frame.CodeFrame at 0x107285ea0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.cluster_embeddings(column_name=\"embedding|code\", dim_reduction_model=dim_reduction_model, cluster_model=cluster_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th><th>cluster|embedding|code</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td><td>i64</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012073, -0.004771, … -0.043605]</td><td>15</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.030298, 0.011617, … -0.039327]</td><td>15</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012823, 0.010932, … -0.027359]</td><td>15</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025782, -0.008832, … -0.04199]</td><td>72</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 198]</td><td>102</td><td>[-0.018791, -0.018855, … -0.04652]</td><td>72</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌──────────────┬─────────────┬─────────────┬─────────────┬─────────────┬─────────────┬─────────────┐\n",
       "│ code         ┆ libcst tree ┆ filename    ┆ tokens|code ┆ tokens_len| ┆ embedding|c ┆ cluster|emb │\n",
       "│ ---          ┆ ---         ┆ ---         ┆ ---         ┆ code        ┆ ode         ┆ edding|code │\n",
       "│ str          ┆ str         ┆ str         ┆ list[i64]   ┆ ---         ┆ ---         ┆ ---         │\n",
       "│              ┆             ┆             ┆             ┆ i64         ┆ list[f64]   ┆ i64         │\n",
       "╞══════════════╪═════════════╪═════════════╪═════════════╪═════════════╪═════════════╪═════════════╡\n",
       "│              ┆ ClassDef(   ┆ /Users/dani ┆ [198, 1058, ┆ 40          ┆ [-0.012073, ┆ 15          │\n",
       "│ class Embedd ┆ name=Name(  ┆ elhug/neura ┆ … 198]      ┆             ┆ -0.004771,  ┆             │\n",
       "│ ableType(Enu ┆        …    ┆ ldragon/gi… ┆             ┆             ┆ … -0.0436…  ┆             │\n",
       "│ m):          ┆             ┆             ┆             ┆             ┆             ┆             │\n",
       "│    …         ┆             ┆             ┆             ┆             ┆             ┆             │\n",
       "│              ┆ FunctionDef ┆ /Users/dani ┆ [198, 755,  ┆ 194         ┆ [0.030298,  ┆ 15          │\n",
       "│ def infer_em ┆ (           ┆ elhug/neura ┆ … 14790]    ┆             ┆ 0.011617, … ┆             │\n",
       "│ beddable_typ ┆ name=Name(  ┆ ldragon/gi… ┆             ┆             ┆ -0.039327…  ┆             │\n",
       "│ e(colum…     ┆     …       ┆             ┆             ┆             ┆             ┆             │\n",
       "│              ┆ FunctionDef ┆ /Users/dani ┆ [198, 755,  ┆ 59          ┆ [0.012823,  ┆ 15          │\n",
       "│ def numeric_ ┆ (           ┆ elhug/neura ┆ … 198]      ┆             ┆ 0.010932, … ┆             │\n",
       "│ embedder(col ┆ name=Name(  ┆ ldragon/gi… ┆             ┆             ┆ -0.027359…  ┆             │\n",
       "│ umn):        ┆     …       ┆             ┆             ┆             ┆             ┆             │\n",
       "│  …           ┆             ┆             ┆             ┆             ┆             ┆             │\n",
       "│              ┆ ClassDef(   ┆ /Users/dani ┆ [271, 1058, ┆ 213         ┆ [-0.025782, ┆ 72          │\n",
       "│              ┆ name=Name(  ┆ elhug/neura ┆ … 198]      ┆             ┆ -0.008832,  ┆             │\n",
       "│ class Embedd ┆        …    ┆ ldragon/gi… ┆             ┆             ┆ … -0.0419…  ┆             │\n",
       "│ ingTask(Base ┆             ┆             ┆             ┆             ┆             ┆             │\n",
       "│ Task):…      ┆             ┆             ┆             ┆             ┆             ┆             │\n",
       "│ def          ┆ FunctionDef ┆ /Users/dani ┆ [755, 1328, ┆ 102         ┆ [-0.018791, ┆ 72          │\n",
       "│ __init__(    ┆ (           ┆ elhug/neura ┆ … 198]      ┆             ┆ -0.018855,  ┆             │\n",
       "│     self,    ┆ name=Name(  ┆ ldragon/gi… ┆             ┆             ┆ … -0.0465…  ┆             │\n",
       "│     embe…    ┆     …       ┆             ┆             ┆             ┆             ┆             │\n",
       "└──────────────┴─────────────┴─────────────┴─────────────┴─────────────┴─────────────┴─────────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>describe</th><th>cluster|embedding|code</th><th>sum_cluster_tokens_len|code</th></tr><tr><td>str</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>86.0</td><td>86.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>41.5</td><td>1796.523256</td></tr><tr><td>&quot;std&quot;</td><td>24.969982</td><td>3400.03621</td></tr><tr><td>&quot;min&quot;</td><td>-1.0</td><td>110.0</td></tr><tr><td>&quot;max&quot;</td><td>84.0</td><td>24751.0</td></tr><tr><td>&quot;median&quot;</td><td>41.5</td><td>925.0</td></tr><tr><td>&quot;25%&quot;</td><td>20.0</td><td>305.0</td></tr><tr><td>&quot;75%&quot;</td><td>63.0</td><td>1807.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 3)\n",
       "┌────────────┬────────────────────────┬─────────────────────────────┐\n",
       "│ describe   ┆ cluster|embedding|code ┆ sum_cluster_tokens_len|code │\n",
       "│ ---        ┆ ---                    ┆ ---                         │\n",
       "│ str        ┆ f64                    ┆ f64                         │\n",
       "╞════════════╪════════════════════════╪═════════════════════════════╡\n",
       "│ count      ┆ 86.0                   ┆ 86.0                        │\n",
       "│ null_count ┆ 0.0                    ┆ 0.0                         │\n",
       "│ mean       ┆ 41.5                   ┆ 1796.523256                 │\n",
       "│ std        ┆ 24.969982              ┆ 3400.03621                  │\n",
       "│ min        ┆ -1.0                   ┆ 110.0                       │\n",
       "│ max        ┆ 84.0                   ┆ 24751.0                     │\n",
       "│ median     ┆ 41.5                   ┆ 925.0                       │\n",
       "│ 25%        ┆ 20.0                   ┆ 305.0                       │\n",
       "│ 75%        ┆ 63.0                   ┆ 1807.0                      │\n",
       "└────────────┴────────────────────────┴─────────────────────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#group rows by cluster|embedding|code id and sum tokens_len|code\n",
    "mfp.df.groupby('cluster|embedding|code').agg([pl.col(\"tokens_len|code\").sum().alias(\"sum_cluster_tokens_len|code\")]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>prompts</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Generate ideas…</td></tr><tr><td>&quot;List key conce…</td></tr><tr><td>&quot;Identify poten…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 1)\n",
       "┌───────────────────────────────────┐\n",
       "│ prompts                           │\n",
       "│ ---                               │\n",
       "│ str                               │\n",
       "╞═══════════════════════════════════╡\n",
       "│ Generate ideas and concepts that… │\n",
       "│ List key concepts or topics that… │\n",
       "│ Identify potential areas or rese… │\n",
       "└───────────────────────────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_subject = \"Python Code Summarizer\"\n",
    "user_perspective = \"a machine learning model\"\n",
    "prompts = [\n",
    "            f\"Generate ideas and concepts that explore the connection between {user_subject} and {user_perspective}, considering both traditional and unconventional approaches.\",\n",
    "            f\"List key concepts or topics that would help analyze {user_subject} through the lens of {user_perspective}, including relevant principles, theories, or models.\",\n",
    "            f\"Identify potential areas or research topics where {user_subject} and {user_perspective} intersect, highlighting intriguing or innovative perspectives.\"\n",
    "        ]\n",
    "\n",
    "#create dataframe with prompts\n",
    "prompts_df = pl.DataFrame({\"prompts\": prompts})\n",
    "prompts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to parse out the python files in a directory tree structure given a root directory folder\n",
    "def parse_python_files(root_directory):\n",
    "    python_files = []\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                python_files.append(os.path.join(root, file))\n",
    "    return python_files\n",
    "\n",
    "python_files = parse_python_files(\"/Users/danielhug/neuraldragon/frames_arc/BabyDragon/babydragon\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_df = prepare_input_df(df=prompts_df,\n",
    "                            messages_col='prompts',\n",
    "                            system_prompt=f\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Logging initialized at level 10\n",
      "DEBUG:root:Initialization complete.\n",
      "DEBUG:root:Entering main loop\n",
      "INFO:root:Next request is 0 of 3\n",
      "INFO:root:Calling Api for 0...\n",
      "INFO:root:Next request is 1 of 3\n",
      "INFO:root:Calling Api for 1...\n",
      "INFO:root:Next request is 2 of 3\n",
      "INFO:root:Calling Api for 2...\n",
      "INFO:root:Exiting the loop\n",
      "INFO:root:Exiting the loop\n",
      "INFO:root:Exiting the loop\n",
      "INFO:root:Exiting the loop\n",
      "INFO:root:From Headers: Available_token_capacity changed to 171952 for request with id 2\n",
      "INFO:root:Exiting the loop\n",
      "INFO:root:From Headers: Available_token_capacity changed to 175953 for request with id 0\n",
      "INFO:root:Exiting the loop\n",
      "INFO:root:From Headers: Available_token_capacity changed to 168142 for request with id 1\n",
      "INFO:root:Exiting the loop\n",
      "INFO:root:Parallel processing complete. Results saved to batch_generator/babydragon_summary_output.ndjson\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 6)\n",
      "┌────────────────┬────────────────┬────────────────┬───────────────┬───────────────┬───────────────┐\n",
      "│ name           ┆ num_rate_limit ┆ num_overloaded ┆ num_tasks_sta ┆ num_api_error ┆ num_other_err │\n",
      "│ ---            ┆ _errors        ┆ _errors        ┆ rted          ┆ s             ┆ ors           │\n",
      "│ str            ┆ ---            ┆ ---            ┆ ---           ┆ ---           ┆ ---           │\n",
      "│                ┆ i64            ┆ i64            ┆ i64           ┆ i64           ┆ i64           │\n",
      "╞════════════════╪════════════════╪════════════════╪═══════════════╪═══════════════╪═══════════════╡\n",
      "│ babydragon_sum ┆ 0              ┆ 0              ┆ 3             ┆ 0             ┆ 0             │\n",
      "│ mary           ┆                ┆                ┆               ┆               ┆               │\n",
      "└────────────────┴────────────────┴────────────────┴───────────────┴───────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "## The following input is a workaround to let work the asyncio functions in a jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "generator = PolarsGenerator( input_df = input_df, name = 'babydragon_summary')\n",
    "\n",
    "generator.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfp = CodeFrame.load(frame_path='./storage/babydragon_frame', name='babydragon_frame')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012073, -0.004771, … -0.043605]</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.030298, 0.011617, … -0.039327]</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012848, 0.010885, … -0.027489]</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025782, -0.008832, … -0.04199]</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 198]</td><td>102</td><td>[-0.018791, -0.018855, … -0.04652]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌────────────────┬────────────────┬────────────────┬───────────────┬───────────────┬───────────────┐\n",
       "│ code           ┆ libcst tree    ┆ filename       ┆ tokens|code   ┆ tokens_len|co ┆ embedding|cod │\n",
       "│ ---            ┆ ---            ┆ ---            ┆ ---           ┆ de            ┆ e             │\n",
       "│ str            ┆ str            ┆ str            ┆ list[i64]     ┆ ---           ┆ ---           │\n",
       "│                ┆                ┆                ┆               ┆ i64           ┆ list[f64]     │\n",
       "╞════════════════╪════════════════╪════════════════╪═══════════════╪═══════════════╪═══════════════╡\n",
       "│                ┆ ClassDef(      ┆ /Users/danielh ┆ [198, 1058, … ┆ 40            ┆ [-0.012073,   │\n",
       "│ class Embeddab ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ -0.004771, …  │\n",
       "│ leType(Enum):  ┆        …       ┆ n/gi…          ┆               ┆               ┆ -0.0436…      │\n",
       "│    …           ┆                ┆                ┆               ┆               ┆               │\n",
       "│                ┆ FunctionDef(   ┆ /Users/danielh ┆ [198, 755, …  ┆ 194           ┆ [0.030298,    │\n",
       "│ def infer_embe ┆     name=Name( ┆ ug/neuraldrago ┆ 14790]        ┆               ┆ 0.011617, …   │\n",
       "│ ddable_type(co ┆     …          ┆ n/gi…          ┆               ┆               ┆ -0.039327…    │\n",
       "│ lum…           ┆                ┆                ┆               ┆               ┆               │\n",
       "│                ┆ FunctionDef(   ┆ /Users/danielh ┆ [198, 755, …  ┆ 59            ┆ [0.012848,    │\n",
       "│ def numeric_em ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ 0.010885, …   │\n",
       "│ bedder(column) ┆     …          ┆ n/gi…          ┆               ┆               ┆ -0.027489…    │\n",
       "│ :              ┆                ┆                ┆               ┆               ┆               │\n",
       "│  …             ┆                ┆                ┆               ┆               ┆               │\n",
       "│                ┆ ClassDef(      ┆ /Users/danielh ┆ [271, 1058, … ┆ 213           ┆ [-0.025782,   │\n",
       "│                ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ -0.008832, …  │\n",
       "│ class Embeddin ┆        …       ┆ n/gi…          ┆               ┆               ┆ -0.0419…      │\n",
       "│ gTask(BaseTask ┆                ┆                ┆               ┆               ┆               │\n",
       "│ ):…            ┆                ┆                ┆               ┆               ┆               │\n",
       "│ def __init__(  ┆ FunctionDef(   ┆ /Users/danielh ┆ [755, 1328, … ┆ 102           ┆ [-0.018791,   │\n",
       "│     self,      ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ -0.018855, …  │\n",
       "│     embe…      ┆     …          ┆ n/gi…          ┆               ┆               ┆ -0.0465…      │\n",
       "└────────────────┴────────────────┴────────────────┴───────────────┴───────────────┴───────────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (27, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_0</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Python Code Su…</td></tr><tr><td>&quot;1. Automatic c…</td></tr><tr><td>&quot;2. Transfer le…</td></tr><tr><td>&quot;3. Multi-modal…</td></tr><tr><td>&quot;4. Context-awa…</td></tr><tr><td>&quot;5. Evaluating …</td></tr><tr><td>&quot;6. Code summar…</td></tr><tr><td>&quot;7. Explainabil…</td></tr><tr><td>&quot;These areas of…</td></tr><tr><td>&quot;1. Traditional…</td></tr><tr><td>&quot;2. Unconventio…</td></tr><tr><td>&quot;3. Traditional…</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;7. Traditional…</td></tr><tr><td>&quot;8. Unconventio…</td></tr><tr><td>&quot;1. Natural Lan…</td></tr><tr><td>&quot;2. Recurrent N…</td></tr><tr><td>&quot;3. Sequence-to…</td></tr><tr><td>&quot;4. Attention M…</td></tr><tr><td>&quot;5. Reinforceme…</td></tr><tr><td>&quot;6. Transformer…</td></tr><tr><td>&quot;7. Transfer Le…</td></tr><tr><td>&quot;8. Evaluation …</td></tr><tr><td>&quot;9. Active Lear…</td></tr><tr><td>&quot;10. Ethical Co…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (27, 1)\n",
       "┌───────────────────────────────────┐\n",
       "│ column_0                          │\n",
       "│ ---                               │\n",
       "│ str                               │\n",
       "╞═══════════════════════════════════╡\n",
       "│ Python Code Summarizer and machi… │\n",
       "│ 1. Automatic code summarization … │\n",
       "│ 2. Transfer learning for code su… │\n",
       "│ 3. Multi-modal code summarizatio… │\n",
       "│ …                                 │\n",
       "│ 7. Transfer Learning: Transfer l… │\n",
       "│ 8. Evaluation Metrics: To assess… │\n",
       "│ 9. Active Learning: Active learn… │\n",
       "│ 10. Ethical Considerations: Anal… │\n",
       "└───────────────────────────────────┘"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "out_path = \"/Users/danielhug/neuraldragon/frames_arc/BabyDragon/notebooks/batch_generator/babydragon_summary_output.ndjson\"\n",
    "#load output file to list\n",
    "with open(out_path) as f:\n",
    "    output = f.readlines()\n",
    "\n",
    "#split each line into a list item by newline\n",
    "output = [x.strip() for x in output]\n",
    "output = [json.loads(x) for x in output]\n",
    "output\n",
    "prompts = [x['output'].split('\\n') for x in output]\n",
    "prompts\n",
    "#flatten list and remove empty strings\n",
    "prompts = [item for sublist in prompts for item in sublist if item != '']\n",
    "#convert to polars df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>code</th><th>libcst tree</th><th>filename</th><th>tokens|code</th><th>tokens_len|code</th><th>embedding|code</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[i64]</td><td>i64</td><td>list[f64]</td></tr></thead><tbody><tr><td>&quot;\n",
       "class Embedda…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[198, 1058, … 198]</td><td>40</td><td>[-0.012073, -0.004771, … -0.043605]</td></tr><tr><td>&quot;\n",
       "def infer_emb…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 14790]</td><td>194</td><td>[0.030298, 0.011617, … -0.039327]</td></tr><tr><td>&quot;\n",
       "def numeric_e…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[198, 755, … 198]</td><td>59</td><td>[0.012848, 0.010885, … -0.027489]</td></tr><tr><td>&quot;\n",
       "\n",
       "class Embedd…</td><td>&quot;ClassDef(\n",
       "    …</td><td>&quot;/Users/danielh…</td><td>[271, 1058, … 198]</td><td>213</td><td>[-0.025782, -0.008832, … -0.04199]</td></tr><tr><td>&quot;def __init__(\n",
       "…</td><td>&quot;FunctionDef(\n",
       " …</td><td>&quot;/Users/danielh…</td><td>[755, 1328, … 198]</td><td>102</td><td>[-0.018791, -0.018855, … -0.04652]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌────────────────┬────────────────┬────────────────┬───────────────┬───────────────┬───────────────┐\n",
       "│ code           ┆ libcst tree    ┆ filename       ┆ tokens|code   ┆ tokens_len|co ┆ embedding|cod │\n",
       "│ ---            ┆ ---            ┆ ---            ┆ ---           ┆ de            ┆ e             │\n",
       "│ str            ┆ str            ┆ str            ┆ list[i64]     ┆ ---           ┆ ---           │\n",
       "│                ┆                ┆                ┆               ┆ i64           ┆ list[f64]     │\n",
       "╞════════════════╪════════════════╪════════════════╪═══════════════╪═══════════════╪═══════════════╡\n",
       "│                ┆ ClassDef(      ┆ /Users/danielh ┆ [198, 1058, … ┆ 40            ┆ [-0.012073,   │\n",
       "│ class Embeddab ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ -0.004771, …  │\n",
       "│ leType(Enum):  ┆        …       ┆ n/gi…          ┆               ┆               ┆ -0.0436…      │\n",
       "│    …           ┆                ┆                ┆               ┆               ┆               │\n",
       "│                ┆ FunctionDef(   ┆ /Users/danielh ┆ [198, 755, …  ┆ 194           ┆ [0.030298,    │\n",
       "│ def infer_embe ┆     name=Name( ┆ ug/neuraldrago ┆ 14790]        ┆               ┆ 0.011617, …   │\n",
       "│ ddable_type(co ┆     …          ┆ n/gi…          ┆               ┆               ┆ -0.039327…    │\n",
       "│ lum…           ┆                ┆                ┆               ┆               ┆               │\n",
       "│                ┆ FunctionDef(   ┆ /Users/danielh ┆ [198, 755, …  ┆ 59            ┆ [0.012848,    │\n",
       "│ def numeric_em ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ 0.010885, …   │\n",
       "│ bedder(column) ┆     …          ┆ n/gi…          ┆               ┆               ┆ -0.027489…    │\n",
       "│ :              ┆                ┆                ┆               ┆               ┆               │\n",
       "│  …             ┆                ┆                ┆               ┆               ┆               │\n",
       "│                ┆ ClassDef(      ┆ /Users/danielh ┆ [271, 1058, … ┆ 213           ┆ [-0.025782,   │\n",
       "│                ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ -0.008832, …  │\n",
       "│ class Embeddin ┆        …       ┆ n/gi…          ┆               ┆               ┆ -0.0419…      │\n",
       "│ gTask(BaseTask ┆                ┆                ┆               ┆               ┆               │\n",
       "│ ):…            ┆                ┆                ┆               ┆               ┆               │\n",
       "│ def __init__(  ┆ FunctionDef(   ┆ /Users/danielh ┆ [755, 1328, … ┆ 102           ┆ [-0.018791,   │\n",
       "│     self,      ┆     name=Name( ┆ ug/neuraldrago ┆ 198]          ┆               ┆ -0.018855, …  │\n",
       "│     embe…      ┆     …          ┆ n/gi…          ┆               ┆               ┆ -0.0465…      │\n",
       "└────────────────┴────────────────┴────────────────┴───────────────┴───────────────┴───────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfp.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from bertopic.representation import OpenAI\n",
    "import numpy as np\n",
    "#convert code column to list\n",
    "code_list = mfp.df['code'].to_list()\n",
    "embeddings = mfp.df['embedding|code'].to_list()\n",
    "representation_model = OpenAI(model=\"gpt-3.5-turbo-16k\", chat=True, nr_docs=10, diversity=0.5)\n",
    "topic_model = BERTopic(representation_model=representation_model)\n",
    "topics, probs = topic_model.fit_transform(documents=code_list, embeddings=np.array(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>66</td>\n",
       "      <td>-1_CSTVisitor and Operator Collection in Python</td>\n",
       "      <td>[CSTVisitor and Operator Collection in Python]</td>\n",
       "      <td>[\\ndef visit_LeftShift(self, node: cst.LeftShi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>422</td>\n",
       "      <td>0_Python coding and NLP</td>\n",
       "      <td>[Python coding and NLP]</td>\n",
       "      <td>[\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>1_Python code initialization and manipulation</td>\n",
       "      <td>[Python code initialization and manipulation]</td>\n",
       "      <td>[\\n\\nclass PythonMinifier:\\n    def __init__(s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>2_Code Analysis and Metrics</td>\n",
       "      <td>[Code Analysis and Metrics]</td>\n",
       "      <td>[\\ndef collect(self):\\n    self.module.visit(s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>3_Code Processing and Analysis with GithubRepo...</td>\n",
       "      <td>[Code Processing and Analysis with GithubRepos...</td>\n",
       "      <td>[\\n\\nclass PythonParser(OsProcessor):\\n    def...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>4_Code Analysis Tools</td>\n",
       "      <td>[Code Analysis Tools]</td>\n",
       "      <td>[\\n\\n\\n# With Statement Collector\\nclass WithS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>5_Counting Python operators in an Abstract Syn...</td>\n",
       "      <td>[Counting Python operators in an Abstract Synt...</td>\n",
       "      <td>[\\ndef visit_DivideAssign(self, node: cst.Divi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>6_Code Analysis and Operator Counters</td>\n",
       "      <td>[Code Analysis and Operator Counters]</td>\n",
       "      <td>[\\n\\n# Binary Operators\\nclass AddOperatorColl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>7_Python code analysis and modification</td>\n",
       "      <td>[Python code analysis and modification]</td>\n",
       "      <td>[\\ndef visit_Module(self, node: cst.Module) -&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "      <td>8_Syntax Analysis in Python</td>\n",
       "      <td>[Syntax Analysis in Python]</td>\n",
       "      <td>[\\ndef visit_ImportFrom(self, node: cst.Import...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>9_Data Types and Field Validation in Python</td>\n",
       "      <td>[Data Types and Field Validation in Python]</td>\n",
       "      <td>[\\nclass MultiDimensionalDiscreteList(BDType):...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>10_Document Fetching and Parsing</td>\n",
       "      <td>[Document Fetching and Parsing]</td>\n",
       "      <td>[\\n\\nclass ArxivVanityParser:\\n    def __init_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>11_Python operator count</td>\n",
       "      <td>[Python operator count]</td>\n",
       "      <td>[\\ndef visit_BitOrAssign(self, node: cst.BitOr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>12_Python Syntax Operators and Statements</td>\n",
       "      <td>[Python Syntax Operators and Statements]</td>\n",
       "      <td>[\\ndef visit_DictComp(self, node: cst.DictComp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>13_Operator Counting in CST</td>\n",
       "      <td>[Operator Counting in CST]</td>\n",
       "      <td>[\\n# Boolean Operators\\nclass AndOperatorCount...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>14_Python code parsing and bit manipulation op...</td>\n",
       "      <td>[Python code parsing and bit manipulation oper...</td>\n",
       "      <td>[def __init__(self, code: str):\\n    self.modu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15_Bitwise Operator Counts in Python</td>\n",
       "      <td>[Bitwise Operator Counts in Python]</td>\n",
       "      <td>[def collect(self):\\n    self.module.visit(sel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>16_Operators in CSTVisitor Classes</td>\n",
       "      <td>[Operators in CSTVisitor Classes]</td>\n",
       "      <td>[\\n# Comparison Operators\\nclass EqualOperator...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name  \\\n",
       "0      -1     66    -1_CSTVisitor and Operator Collection in Python   \n",
       "1       0    422                            0_Python coding and NLP   \n",
       "2       1    137      1_Python code initialization and manipulation   \n",
       "3       2    136                        2_Code Analysis and Metrics   \n",
       "4       3     54  3_Code Processing and Analysis with GithubRepo...   \n",
       "5       4     53                              4_Code Analysis Tools   \n",
       "6       5     49  5_Counting Python operators in an Abstract Syn...   \n",
       "7       6     39              6_Code Analysis and Operator Counters   \n",
       "8       7     38            7_Python code analysis and modification   \n",
       "9       8     37                        8_Syntax Analysis in Python   \n",
       "10      9     27        9_Data Types and Field Validation in Python   \n",
       "11     10     25                   10_Document Fetching and Parsing   \n",
       "12     11     23                           11_Python operator count   \n",
       "13     12     19          12_Python Syntax Operators and Statements   \n",
       "14     13     18                        13_Operator Counting in CST   \n",
       "15     14     15  14_Python code parsing and bit manipulation op...   \n",
       "16     15     15               15_Bitwise Operator Counts in Python   \n",
       "17     16     11                 16_Operators in CSTVisitor Classes   \n",
       "\n",
       "                                       Representation  \\\n",
       "0      [CSTVisitor and Operator Collection in Python]   \n",
       "1                             [Python coding and NLP]   \n",
       "2       [Python code initialization and manipulation]   \n",
       "3                         [Code Analysis and Metrics]   \n",
       "4   [Code Processing and Analysis with GithubRepos...   \n",
       "5                               [Code Analysis Tools]   \n",
       "6   [Counting Python operators in an Abstract Synt...   \n",
       "7               [Code Analysis and Operator Counters]   \n",
       "8             [Python code analysis and modification]   \n",
       "9                         [Syntax Analysis in Python]   \n",
       "10        [Data Types and Field Validation in Python]   \n",
       "11                    [Document Fetching and Parsing]   \n",
       "12                            [Python operator count]   \n",
       "13           [Python Syntax Operators and Statements]   \n",
       "14                         [Operator Counting in CST]   \n",
       "15  [Python code parsing and bit manipulation oper...   \n",
       "16                [Bitwise Operator Counts in Python]   \n",
       "17                  [Operators in CSTVisitor Classes]   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [\\ndef visit_LeftShift(self, node: cst.LeftShi...  \n",
       "1   [\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...  \n",
       "2   [\\n\\nclass PythonMinifier:\\n    def __init__(s...  \n",
       "3   [\\ndef collect(self):\\n    self.module.visit(s...  \n",
       "4   [\\n\\nclass PythonParser(OsProcessor):\\n    def...  \n",
       "5   [\\n\\n\\n# With Statement Collector\\nclass WithS...  \n",
       "6   [\\ndef visit_DivideAssign(self, node: cst.Divi...  \n",
       "7   [\\n\\n# Binary Operators\\nclass AddOperatorColl...  \n",
       "8   [\\ndef visit_Module(self, node: cst.Module) ->...  \n",
       "9   [\\ndef visit_ImportFrom(self, node: cst.Import...  \n",
       "10  [\\nclass MultiDimensionalDiscreteList(BDType):...  \n",
       "11  [\\n\\nclass ArxivVanityParser:\\n    def __init_...  \n",
       "12  [\\ndef visit_BitOrAssign(self, node: cst.BitOr...  \n",
       "13  [\\ndef visit_DictComp(self, node: cst.DictComp...  \n",
       "14  [\\n# Boolean Operators\\nclass AndOperatorCount...  \n",
       "15  [def __init__(self, code: str):\\n    self.modu...  \n",
       "16  [def collect(self):\\n    self.module.visit(sel...  \n",
       "17  [\\n# Comparison Operators\\nclass EqualOperator...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Representative_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nclass EmbeddableType(Enum):\\n    TEXT = \"tex...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_Python coding and NLP</td>\n",
       "      <td>[Python coding and NLP]</td>\n",
       "      <td>[\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...</td>\n",
       "      <td>Python coding and NLP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\ndef infer_embeddable_type(column) -&gt; Tuple[E...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_Python coding and NLP</td>\n",
       "      <td>[Python coding and NLP]</td>\n",
       "      <td>[\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...</td>\n",
       "      <td>Python coding and NLP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\ndef numeric_embedder(column):\\n    # Impleme...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_Python coding and NLP</td>\n",
       "      <td>[Python coding and NLP]</td>\n",
       "      <td>[\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...</td>\n",
       "      <td>Python coding and NLP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nclass EmbeddingTask(BaseTask):\\n    def __...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_Python coding and NLP</td>\n",
       "      <td>[Python coding and NLP]</td>\n",
       "      <td>[\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...</td>\n",
       "      <td>Python coding and NLP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def __init__(\\n    self,\\n    embedder: OpenAi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_Python coding and NLP</td>\n",
       "      <td>[Python coding and NLP]</td>\n",
       "      <td>[\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...</td>\n",
       "      <td>Python coding and NLP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>\\nclass PerspectivePromptGenerator:\\n    def _...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_Python coding and NLP</td>\n",
       "      <td>[Python coding and NLP]</td>\n",
       "      <td>[\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...</td>\n",
       "      <td>Python coding and NLP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>def __init__(self, subjects, perspectives, max...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_Python coding and NLP</td>\n",
       "      <td>[Python coding and NLP]</td>\n",
       "      <td>[\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...</td>\n",
       "      <td>Python coding and NLP</td>\n",
       "      <td>0.883771</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>\\ndef handle_future(self, future):\\n    try:\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_Python coding and NLP</td>\n",
       "      <td>[Python coding and NLP]</td>\n",
       "      <td>[\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...</td>\n",
       "      <td>Python coding and NLP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>\\ndef generate_prompts(self):\\n    for subject...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_Python coding and NLP</td>\n",
       "      <td>[Python coding and NLP]</td>\n",
       "      <td>[\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...</td>\n",
       "      <td>Python coding and NLP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>\\ndef save_prompts_to_json(self, filename):\\n ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_Python coding and NLP</td>\n",
       "      <td>[Python coding and NLP]</td>\n",
       "      <td>[\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...</td>\n",
       "      <td>Python coding and NLP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1184 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Document  Topic  \\\n",
       "0     \\nclass EmbeddableType(Enum):\\n    TEXT = \"tex...      0   \n",
       "1     \\ndef infer_embeddable_type(column) -> Tuple[E...      0   \n",
       "2     \\ndef numeric_embedder(column):\\n    # Impleme...      0   \n",
       "3     \\n\\nclass EmbeddingTask(BaseTask):\\n    def __...      0   \n",
       "4     def __init__(\\n    self,\\n    embedder: OpenAi...      0   \n",
       "...                                                 ...    ...   \n",
       "1179  \\nclass PerspectivePromptGenerator:\\n    def _...      0   \n",
       "1180  def __init__(self, subjects, perspectives, max...      0   \n",
       "1181  \\ndef handle_future(self, future):\\n    try:\\n...      0   \n",
       "1182  \\ndef generate_prompts(self):\\n    for subject...      0   \n",
       "1183  \\ndef save_prompts_to_json(self, filename):\\n ...      0   \n",
       "\n",
       "                         Name           Representation  \\\n",
       "0     0_Python coding and NLP  [Python coding and NLP]   \n",
       "1     0_Python coding and NLP  [Python coding and NLP]   \n",
       "2     0_Python coding and NLP  [Python coding and NLP]   \n",
       "3     0_Python coding and NLP  [Python coding and NLP]   \n",
       "4     0_Python coding and NLP  [Python coding and NLP]   \n",
       "...                       ...                      ...   \n",
       "1179  0_Python coding and NLP  [Python coding and NLP]   \n",
       "1180  0_Python coding and NLP  [Python coding and NLP]   \n",
       "1181  0_Python coding and NLP  [Python coding and NLP]   \n",
       "1182  0_Python coding and NLP  [Python coding and NLP]   \n",
       "1183  0_Python coding and NLP  [Python coding and NLP]   \n",
       "\n",
       "                                    Representative_Docs  \\\n",
       "0     [\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...   \n",
       "1     [\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...   \n",
       "2     [\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...   \n",
       "3     [\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...   \n",
       "4     [\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...   \n",
       "...                                                 ...   \n",
       "1179  [\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...   \n",
       "1180  [\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...   \n",
       "1181  [\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...   \n",
       "1182  [\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...   \n",
       "1183  [\\n\\nclass VectorChat(VectorThread, Chat):\\n  ...   \n",
       "\n",
       "                Top_n_words  Probability  Representative_document  \n",
       "0     Python coding and NLP     1.000000                    False  \n",
       "1     Python coding and NLP     1.000000                    False  \n",
       "2     Python coding and NLP     1.000000                    False  \n",
       "3     Python coding and NLP     1.000000                    False  \n",
       "4     Python coding and NLP     1.000000                    False  \n",
       "...                     ...          ...                      ...  \n",
       "1179  Python coding and NLP     1.000000                    False  \n",
       "1180  Python coding and NLP     0.883771                    False  \n",
       "1181  Python coding and NLP     1.000000                    False  \n",
       "1182  Python coding and NLP     1.000000                    False  \n",
       "1183  Python coding and NLP     1.000000                    False  \n",
       "\n",
       "[1184 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_document_info(code_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from typing import List, Callable\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.cluster import hierarchy as sch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def debug_hierarchical_topics_with_dict(topic_model,\n",
    "                                        docs,\n",
    "                                        linkage_function: Callable[[csr_matrix], np.ndarray] = None,\n",
    "                                        distance_function: Callable[[csr_matrix], csr_matrix] = None) -> pd.DataFrame:\n",
    "    \n",
    "    hier_topics_dict = {\"Parent_ID\": [], \"Parent_Name\": [], \"Topics\": [],\n",
    "                        \"Child_Left_ID\": [], \"Child_Left_Name\": [],\n",
    "                        \"Child_Right_ID\": [], \"Child_Right_Name\": []}\n",
    "    \n",
    "    if distance_function is None:\n",
    "        distance_function = lambda x: 1 - cosine_similarity(x)\n",
    "        \n",
    "    if linkage_function is None:\n",
    "        linkage_function = lambda x: sch.linkage(x, 'ward', optimal_ordering=True)\n",
    "        \n",
    "    embeddings = topic_model.c_tf_idf_[topic_model._outliers:]\n",
    "    X = distance_function(embeddings)\n",
    "    Z = linkage_function(X)\n",
    "\n",
    "    documents = pd.DataFrame({\"Document\": docs, \"ID\": range(len(docs)), \"Topic\": topic_model.topics_})\n",
    "    documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "    documents_per_topic = documents_per_topic.loc[documents_per_topic.Topic != -1, :]\n",
    "    clean_documents = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "    words = topic_model.vectorizer_model.get_feature_names_out()\n",
    "    bow = topic_model.vectorizer_model.transform(clean_documents)\n",
    "    \n",
    "    for index in range(len(Z)):\n",
    "        try:\n",
    "            clusters = sch.fcluster(Z, t=Z[index][2], criterion='distance') - topic_model._outliers\n",
    "            cluster_df = pd.DataFrame({\"Topic\": range(len(clusters)), \"Cluster\": clusters})\n",
    "            cluster_df = cluster_df.groupby(\"Cluster\").agg({'Topic': lambda x: list(x)}).reset_index()\n",
    "            nr_clusters = len(clusters)\n",
    "            topic = None\n",
    "            val = Z[index][0]\n",
    "            while topic is None:\n",
    "                if val - len(clusters) < 0:\n",
    "                    topic = int(val)\n",
    "                else:\n",
    "                    val = Z[int(val - len(clusters))][0]\n",
    "            clustered_topics = [i for i, x in enumerate(clusters) if x == clusters[topic]]\n",
    "\n",
    "            grouped = csr_matrix(bow[clustered_topics].sum(axis=0))\n",
    "            c_tf_idf = topic_model.ctfidf_model.transform(grouped)\n",
    "            selection = documents.loc[documents.Topic.isin(clustered_topics), :]\n",
    "            selection.Topic = 0\n",
    "            words_per_topic = topic_model._extract_words_per_topic(words, selection, c_tf_idf, calculate_aspects=False)\n",
    "            \n",
    "            parent_id = index + len(clusters)\n",
    "            parent_name = \"_\".join([x[0] for x in words_per_topic[0]][:5])\n",
    "            \n",
    "            Z_id = Z[index][0]\n",
    "            child_left_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters\n",
    "            child_left_name = hier_topics_dict[\"Parent_Name\"][-1] if Z_id - nr_clusters >= 0 else \"_\".join([x[0] for x in topic_model.get_topic(Z_id)][:5])\n",
    "            \n",
    "            Z_id = Z[index][1]\n",
    "            child_right_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters\n",
    "            child_right_name = hier_topics_dict[\"Parent_Name\"][-1] if Z_id - nr_clusters >= 0 else \"_\".join([x[0] for x in topic_model.get_topic(Z_id)][:5])\n",
    "            \n",
    "            hier_topics_dict[\"Parent_ID\"].append(parent_id)\n",
    "            hier_topics_dict[\"Parent_Name\"].append(parent_name)\n",
    "            hier_topics_dict[\"Topics\"].append(clustered_topics)\n",
    "            hier_topics_dict[\"Child_Left_ID\"].append(int(Z[index][0]))\n",
    "            hier_topics_dict[\"Child_Left_Name\"].append(child_left_name)\n",
    "            hier_topics_dict[\"Child_Right_ID\"].append(int(Z[index][1]))\n",
    "            hier_topics_dict[\"Child_Right_Name\"].append(child_right_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {index}: {e}\")\n",
    "            continue\n",
    "\n",
    "    hier_topics = pd.DataFrame.from_dict(hier_topics_dict)\n",
    "    hier_topics[\"Distance\"] = Z[:, 2]\n",
    "    hier_topics = hier_topics.sort_values(\"Parent_ID\", ascending=False)\n",
    "    hier_topics[[\"Parent_ID\", \"Child_Left_ID\", \"Child_Right_ID\"]] = hier_topics[[\"Parent_ID\", \"Child_Left_ID\", \"Child_Right_ID\"]].astype(str)\n",
    "\n",
    "    return hier_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming topic_model is your trained BERTopic model and docs is your list of documents\n",
    "hierarchical_topics = debug_hierarchical_topics_with_dict(topic_model, docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = topic_model.get_topic_tree(hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "├─selfmodulevisitself_collectself_def_cstparsemodulecode_selfmodule\n",
      "│    ├─the_if_none_is_in\n",
      "│    │    ├─cstfunctiondef_file_docstring_for_the\n",
      "│    │    │    ├─node_true_bool_def_return\n",
      "│    │    │    │    ├─■──role_specific_message_pldataframe_messagedict ── Topic: 19\n",
      "│    │    │    │    └─■──memory_the_question_prompt_param ── Topic: 11\n",
      "│    │    │    └─■──if_prompt_response_request_else ── Topic: 1\n",
      "│    │    └─cstfunctiondef_file_docstring_for_the\n",
      "│    │         ├─node_def_true_class_bool\n",
      "│    │         │    ├─■──range_alphabet_maxvalue_minvalue_not ── Topic: 10\n",
      "│    │         │    └─prompt_message_the_memory_if\n",
      "│    │         │         ├─■──embeddings_index_is_none_value ── Topic: 5\n",
      "│    │         │         └─■──column_dataframe_columns_contextcolumns_values ── Topic: 3\n",
      "│    │         └─■──paths_numclusters_matrix_kernel_for ── Topic: 6\n",
      "│    └─the_if_none_is_in\n",
      "│         ├─repo_reponame_repopath_selfdirectoryprocessor_url\n",
      "│         │    ├─values_not_if_none_embeddings\n",
      "│         │    │    ├─■──cstfunctiondef_docstring_node_cstclassdef_updatednode ── Topic: 7\n",
      "│         │    │    └─■──directorypath_directory_files_file_allfiles ── Topic: 27\n",
      "│         │    └─■──perspective_subject_subjectandperspective_file_as ── Topic: 21\n",
      "│         └─repo_reponame_repopath_selfdirectoryprocessor_url\n",
      "│              ├─■──arxivid_pubmedid_maxresults10_handleclose_paperlist ── Topic: 12\n",
      "│              └─■──repo_reponame_repopath_selfdirectoryprocessor_selfreponame ── Topic: 15\n",
      "└─selfmodulevisitself_collectself_def_cstparsemodulecode_selfmodule\n",
      "     ├─selfmodulevisitself_collectself_return_def_node\n",
      "     │    ├─collectself_selfmodulevisitself_def_return_node\n",
      "     │    │    ├─■──node_true_bool_visitbitandassignself_cstbitandassign ── Topic: 23\n",
      "     │    │    └─isinstancenodeoperator_def_node_class_cstparsemodulecode\n",
      "     │    │         ├─■──node_true_bool_return_def ── Topic: 2\n",
      "     │    │         └─■──node_true_bool_cstassignequal_visitassignequalself ── Topic: 25\n",
      "     │    └─collectself_selfmodulevisitself_def_return_node\n",
      "     │         ├─the_if_none_is_in\n",
      "     │         │    ├─cstfunctiondef_docstring_node_file_cstclassdef\n",
      "     │         │    │    ├─■──def_class_cstparsemodulecode_selfmodule_collectself ── Topic: 22\n",
      "     │         │    │    └─memory_the_message_none_question\n",
      "     │         │    │         ├─embeddings_none_values_not_index\n",
      "     │         │    │         │    ├─■──counter_def_class_cstparsemodulecode_selfmodulevisitself ── Topic: 18\n",
      "     │         │    │         │    └─■──collector_statement_def_class_cstparsemodulecode ── Topic: 16\n",
      "     │         │    │         └─■──isinstancenodeoperator_or_node_def_class ── Topic: 4\n",
      "     │         │    └─cstfunctiondef_docstring_node_file_cstclassdef\n",
      "     │         │         ├─■──visitrightshiftassignself_cstrightshiftassign_node_true_bool ── Topic: 17\n",
      "     │         │         └─■──node_true_bool_visitdivideassignself_visitfloordivideassignself ── Topic: 20\n",
      "     │         └─the_if_none_is_in\n",
      "     │              ├─■──collectself_selfmodulevisitself_return_def_importcode ── Topic: 0\n",
      "     │              └─■──selfmodulevisitself_collectself_return_selfbitandassigncount_def ── Topic: 24\n",
      "     └─selfmodulevisitself_collectself_return_def_node\n",
      "          ├─■──cstparsemodulecode_selfmodule_code_initself_str ── Topic: 26\n",
      "          └─collectself_selfmodulevisitself_return_def_importcode\n",
      "               ├─cstparsemodulecode_selfmodule_code_initself_str\n",
      "               │    ├─■──cstparsemodulecode_selfmodule_code_initself_str ── Topic: 13\n",
      "               │    └─cstparsemodulecode_selfmodule_code_initself_str\n",
      "               │         ├─■──cstparsemodulecode_selfmodule_code_initself_str ── Topic: 8\n",
      "               │         └─■──cstparsemodulecode_selfmodule_code_initself_str ── Topic: 14\n",
      "               └─■──code_initself_cstparsemodulecode_selfmodule_str ── Topic: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
