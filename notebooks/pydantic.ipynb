{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import Union, List, Optional\n",
    "import tiktoken\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Base class for all bd_types\n",
    "class BDType(BaseModel):\n",
    "    source: str = Field(..., description=\"The source of the data.\")\n",
    "    timestamp: Optional[datetime.datetime] = Field(None, description=\"When the data was collected or created. If not provided, the current time is used.\")\n",
    "    id: Optional[str] = Field(None, description=\"A unique ID for the data instance. If not provided, it's generated by combining source, timestamp, and a uuid.\")\n",
    "    data_name: Optional[str] = Field(None, description=\"Name of the data.\")\n",
    "    elements_name: Optional[List[str]] = Field(None, description=\"Names of the elements if the data is a list.\")\n",
    "    \n",
    "    @validator(\"timestamp\", pre=True, always=True)\n",
    "    def set_timestamp(cls, v):\n",
    "        return v or datetime.datetime.now()\n",
    "\n",
    "    @validator(\"id\", pre=True, always=True)\n",
    "    def set_id(cls, v, values, **kwargs):\n",
    "        if \"source\" in values and \"timestamp\" in values:\n",
    "            source = values[\"source\"]\n",
    "            timestamp = values[\"timestamp\"]\n",
    "            return v or f\"{source}-{timestamp}-{uuid.uuid4()}\"\n",
    "        return v\n",
    "\n",
    "class NaturalLanguageSingle(BDType):\n",
    "    text: str = Field(..., description=\"The natural language text. It should be less than or equal to `max_tokens` in length when tokenized.\")\n",
    "    max_tokens: int = Field(8000, description=\"The maximum allowed length of the text in tokens. The default value is 8000.\")\n",
    "    \n",
    "    @validator(\"text\")\n",
    "    def validate_text(cls, v, values):\n",
    "        try:\n",
    "            token_count = len(tokenizer.encode(v))\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Failed to tokenize text.\") from e\n",
    "\n",
    "        max_tokens = values.get(\"max_tokens\", 8000)  # Get max_tokens from values, if not available, default to 8000\n",
    "        if token_count > max_tokens:\n",
    "            raise ValueError(f\"Text is longer than {max_tokens} tokens.\")\n",
    "        return v\n",
    "\n",
    "class NaturalLanguageList(BDType):\n",
    "    texts: List[NaturalLanguageSingle] = Field(..., description=\"A list of `NaturalLanguageSingle` objects. Each object should pass the validation requirements of the `NaturalLanguageSingle` class.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation time for a NaturalLanguageSingle list of length 10: 0.05250287055969238 seconds\n",
      "Initialization time for a NaturalLanguageList with a list of length 10: 0.0 seconds\n",
      "---\n",
      "Creation time for a NaturalLanguageSingle list of length 100: 0.5040838718414307 seconds\n",
      "Initialization time for a NaturalLanguageList with a list of length 100: 0.0 seconds\n",
      "---\n",
      "Creation time for a NaturalLanguageSingle list of length 1000: 4.989835739135742 seconds\n",
      "Initialization time for a NaturalLanguageList with a list of length 1000: 0.0014958381652832031 seconds\n",
      "---\n",
      "Creation time for a NaturalLanguageSingle list of length 10000: 49.594555616378784 seconds\n",
      "Initialization time for a NaturalLanguageList with a list of length 10000: 0.01700115203857422 seconds\n",
      "---\n",
      "Creation time for a NaturalLanguageSingle list of length 100000: 502.29613733291626 seconds\n",
      "Initialization time for a NaturalLanguageList with a list of length 100000: 0.230055570602417 seconds\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import string\n",
    "\n",
    "# Function to generate a list of strings of specific length\n",
    "def generate_strings(num_strings, tokens_per_string):\n",
    "    return [string.ascii_lowercase * (tokens_per_string // 26) for _ in range(num_strings)]\n",
    "\n",
    "# Lengths of the lists to generate\n",
    "list_lengths = [10, 100, 1000, 10000, 100000]\n",
    "\n",
    "# Tokens per string\n",
    "tokens_per_string = 4000\n",
    "\n",
    "for length in list_lengths:\n",
    "    # Generate a list of strings\n",
    "    strings = generate_strings(length, tokens_per_string)\n",
    "    \n",
    "    # Start timer for NaturalLanguageSingle list creation\n",
    "    start_time_nls_list = time.time()\n",
    "    \n",
    "    # Create a NaturalLanguageSingle list\n",
    "    nls_list = [NaturalLanguageSingle(source='source', text=text) for text in strings]\n",
    "    \n",
    "    # End timer for NaturalLanguageSingle list creation\n",
    "    end_time_nls_list = time.time()\n",
    "    \n",
    "    # Start timer for NaturalLanguageList initialization\n",
    "    start_time_nll = time.time()\n",
    "    \n",
    "    # Initialize a NaturalLanguageList instance\n",
    "    nll = NaturalLanguageList(source='source', texts=nls_list)\n",
    "    \n",
    "    # End timer for NaturalLanguageList initialization\n",
    "    end_time_nll = time.time()\n",
    "    \n",
    "    # Print the creation and initialization times\n",
    "    print(f\"Creation time for a NaturalLanguageSingle list of length {length}: {end_time_nls_list - start_time_nls_list} seconds\")\n",
    "    print(f\"Initialization time for a NaturalLanguageList with a list of length {length}: {end_time_nll - start_time_nll} seconds\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to generate strings for size 10: 0.03900623321533203 seconds\n",
      "Time to create DataFrame for size 10: 0.019512414932250977 seconds\n",
      "Time to create NaturalLanguageSingle instances for size 10: 0.05900073051452637 seconds\n",
      "Time to create NaturalLanguageList for size 10: 0.09901261329650879 seconds\n",
      "Time to generate strings for size 100: 0.0 seconds\n",
      "Time to create DataFrame for size 100: 0.0004987716674804688 seconds\n",
      "Time to create NaturalLanguageSingle instances for size 100: 0.5225801467895508 seconds\n",
      "Time to create NaturalLanguageList for size 100: 0.0 seconds\n",
      "Time to generate strings for size 1000: 0.0005016326904296875 seconds\n",
      "Time to create DataFrame for size 1000: 0.003997802734375 seconds\n",
      "Time to create NaturalLanguageSingle instances for size 1000: 5.029733180999756 seconds\n",
      "Time to create NaturalLanguageList for size 1000: 0.0019996166229248047 seconds\n",
      "Time to generate strings for size 10000: 0.00900125503540039 seconds\n",
      "Time to create DataFrame for size 10000: 0.04849886894226074 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import polars as pl\n",
    "import string\n",
    "\n",
    "def create_nls(text):\n",
    "    return NaturalLanguageSingle(source='source', text=text)\n",
    "    \n",
    "def generate_strings(num_strings, tokens_per_string):\n",
    "    return [string.ascii_lowercase * (tokens_per_string // 26) for _ in range(num_strings)]\n",
    "\n",
    "# For different sizes of the DataFrame\n",
    "for size in list_lengths:\n",
    "    # Create a DataFrame with 'size' number of strings\n",
    "    start_time = time.time()\n",
    "    text = generate_strings(size, 4000)\n",
    "    print(f\"Time to generate strings for size {size}: {time.time() - start_time} seconds\")\n",
    "    start_time = time.time()\n",
    "    df = pl.DataFrame({\n",
    "        \"text\": text\n",
    "    })\n",
    "    print(f\"Time to create DataFrame for size {size}: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Create NaturalLanguageSingle instances and measure the time\n",
    "    start_time = time.time()\n",
    "    df = df.with_columns(pl.col(\"text\").apply(create_nls).alias(\"nls\"))\n",
    "    print(f\"Time to create NaturalLanguageSingle instances for size {size}: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Create a NaturalLanguageList and measure the time\n",
    "    start_time = time.time()\n",
    "    nll = NaturalLanguageList(source='source', texts=df['nls'].to_list())\n",
    "    print(f\"Time to create NaturalLanguageList for size {size}: {time.time() - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to generate strings for size 10: 0.02899765968322754 seconds\n",
      "Time to create DataFrame for size 10: 0.003998279571533203 seconds\n",
      "Time to create NaturalLanguageSingle instances for size 10: 0.0055103302001953125 seconds\n",
      "Time to create NaturalLanguageList for size 10: 0.062011003494262695 seconds\n",
      "Time to generate strings for size 100: 0.0 seconds\n",
      "Time to create DataFrame for size 100: 0.0005035400390625 seconds\n",
      "Time to create NaturalLanguageSingle instances for size 100: 0.0009975433349609375 seconds\n",
      "Time to create NaturalLanguageList for size 100: 0.0005028247833251953 seconds\n",
      "Time to generate strings for size 1000: 0.0004999637603759766 seconds\n",
      "Time to create DataFrame for size 1000: 0.005998849868774414 seconds\n",
      "Time to create NaturalLanguageSingle instances for size 1000: 0.003997802734375 seconds\n",
      "Time to create NaturalLanguageList for size 1000: 0.0020008087158203125 seconds\n",
      "Time to generate strings for size 10000: 0.014501094818115234 seconds\n",
      "Time to create DataFrame for size 10000: 0.07552051544189453 seconds\n",
      "Time to create NaturalLanguageSingle instances for size 10000: 0.04250454902648926 seconds\n",
      "Time to create NaturalLanguageList for size 10000: 0.016017913818359375 seconds\n",
      "Time to generate strings for size 100000: 0.1975412368774414 seconds\n",
      "Time to create DataFrame for size 100000: 1.481776237487793 seconds\n",
      "Time to create NaturalLanguageSingle instances for size 100000: 0.7251451015472412 seconds\n",
      "Time to create NaturalLanguageList for size 100000: 0.20403409004211426 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def generate_strings(num_strings, tokens_per_string):\n",
    "    return [string.ascii_lowercase * (tokens_per_string // 26) for _ in range(num_strings)]\n",
    "\n",
    "@lru_cache\n",
    "def create_nls(text):\n",
    "    return NaturalLanguageSingle(source='source', text=text)\n",
    "\n",
    "# For different sizes of the DataFrame\n",
    "for size in list_lengths:\n",
    "    # Create a DataFrame with 'size' number of strings\n",
    "    start_time = time.time()\n",
    "    text = generate_strings(size, 4000)\n",
    "    print(f\"Time to generate strings for size {size}: {time.time() - start_time} seconds\")\n",
    "    start_time = time.time()\n",
    "    df = pl.DataFrame({\n",
    "        \"text\": text\n",
    "    })\n",
    "    print(f\"Time to create DataFrame for size {size}: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Create NaturalLanguageSingle instances and measure the time\n",
    "    start_time = time.time()\n",
    "    df = df.with_columns(pl.col(\"text\").apply(create_nls).alias(\"nls\"))\n",
    "    print(f\"Time to create NaturalLanguageSingle instances for size {size}: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Create a NaturalLanguageList and measure the time\n",
    "    start_time = time.time()\n",
    "    nll = NaturalLanguageList(source='source', texts=df['nls'].to_list())\n",
    "    print(f\"Time to create NaturalLanguageList for size {size}: {time.time() - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to generate strings for size 10: 0.039513587951660156 seconds\n",
      "Time to create DataFrame for size 10: 0.001999378204345703 seconds\n",
      "Time to create NaturalLanguageSingle with Threading instances for size 10: 0.015501976013183594 seconds\n",
      "Time to create NaturalLanguageList for size 10: 0.001998424530029297 seconds\n",
      "Time to generate strings for size 100: 0.0 seconds\n",
      "Time to create DataFrame for size 100: 0.0005009174346923828 seconds\n",
      "Time to create NaturalLanguageSingle with Threading instances for size 100: 0.06851530075073242 seconds\n",
      "Time to create NaturalLanguageList for size 100: 0.0 seconds\n",
      "Time to generate strings for size 1000: 0.0004992485046386719 seconds\n",
      "Time to create DataFrame for size 1000: 0.004498958587646484 seconds\n",
      "Time to create NaturalLanguageSingle with Threading instances for size 1000: 0.6605849266052246 seconds\n",
      "Time to create NaturalLanguageList for size 1000: 0.0024993419647216797 seconds\n",
      "Time to generate strings for size 10000: 0.012504816055297852 seconds\n",
      "Time to create DataFrame for size 10000: 0.06151294708251953 seconds\n",
      "Time to create NaturalLanguageSingle with Threading instances for size 10000: 6.629446268081665 seconds\n",
      "Time to create NaturalLanguageList for size 10000: 0.02000284194946289 seconds\n",
      "Time to generate strings for size 100000: 0.13852214813232422 seconds\n",
      "Time to create DataFrame for size 100000: 1.4477860927581787 seconds\n",
      "Time to create NaturalLanguageSingle with Threading instances for size 100000: 65.37253189086914 seconds\n",
      "Time to create NaturalLanguageList for size 100000: 0.27506589889526367 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def generate_strings(num_strings, tokens_per_string):\n",
    "    return [string.ascii_lowercase * (tokens_per_string // 26) for _ in range(num_strings)]\n",
    "def create_nls(text):\n",
    "    return NaturalLanguageSingle(source='source', text=text)\n",
    "# For different sizes of the DataFrame\n",
    "for size in list_lengths:\n",
    "    # Create a DataFrame with 'size' number of strings\n",
    "    start_time = time.time()\n",
    "    text = generate_strings(size, 4000)\n",
    "    print(f\"Time to generate strings for size {size}: {time.time() - start_time} seconds\")\n",
    "    start_time = time.time()\n",
    "    df = pl.DataFrame({\n",
    "        \"text\": text\n",
    "    })\n",
    "    print(f\"Time to create DataFrame for size {size}: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Create NaturalLanguageSingle instances and measure the time\n",
    "    start_time = time.time()\n",
    "    df = df.with_columns(pl.col(\"text\").apply(create_nls, strategy= 'threading').alias(\"nls\"))\n",
    "    print(f\"Time to create NaturalLanguageSingle with Threading instances for size {size}: {time.time() - start_time} seconds\")\n",
    "\n",
    "    # Create a NaturalLanguageList and measure the time\n",
    "    start_time = time.time()\n",
    "    nll = NaturalLanguageList(source='source', texts=df['nls'].to_list())\n",
    "    print(f\"Time to create NaturalLanguageList for size {size}: {time.time() - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
