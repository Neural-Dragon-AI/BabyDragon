{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FifoThread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the Required Libraries\n",
    "First, make sure you have imported all the required libraries as mentioned in the code snippet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babydragon.memory.threads.base_thread import BaseThread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Initialize the BaseThread Class\n",
    "Create an instance of the BaseThread class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_thread = BaseThread(name=\"conversation\", max_memory=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Messages to the Thread\n",
    "You can add messages to the thread using the add_dict_to_thread method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "message1 = {\"role\": \"user\", \"content\": \"Hello, how can I help you?\"}\n",
    "message2 = {\"role\": \"assistant\", \"content\": \"I have a question about your services.\"}\n",
    "memory_thread.add_dict_to_thread(message1)\n",
    "memory_thread.add_dict_to_thread(message2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2,)\n",
      "Series: 'tokens_count' [u16]\n",
      "[\n",
      "\t15\n",
      "\t15\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(memory_thread.memory_thread[\"tokens_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieve Messages\n",
    "Use various retrieval methods to access messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 7)\n",
      "┌──────────────┬──────────────┬──────────────┬───────────┬──────────────┬───────────┬──────────────┐\n",
      "│ conversation ┆ message_id   ┆ parent_id    ┆ role      ┆ content      ┆ timestamp ┆ tokens_count │\n",
      "│ _id          ┆ ---          ┆ ---          ┆ ---       ┆ ---          ┆ ---       ┆ ---          │\n",
      "│ ---          ┆ str          ┆ str          ┆ str       ┆ str          ┆ f64       ┆ u16          │\n",
      "│ str          ┆              ┆              ┆           ┆              ┆           ┆              │\n",
      "╞══════════════╪══════════════╪══════════════╪═══════════╪══════════════╪═══════════╪══════════════╡\n",
      "│ 6c9ca584-81a ┆ a92cfddd-7a8 ┆ 6bf05ab6-e48 ┆ assistant ┆ I have a     ┆ 1.6932e9  ┆ 15           │\n",
      "│ 9-4fb4-8a43- ┆ 1-4f96-906a- ┆ b-4789-85fc- ┆           ┆ question     ┆           ┆              │\n",
      "│ edbe4834…    ┆ 3548bab6…    ┆ 87e53513…    ┆           ┆ about your   ┆           ┆              │\n",
      "│              ┆              ┆              ┆           ┆ ser…         ┆           ┆              │\n",
      "└──────────────┴──────────────┴──────────────┴───────────┴──────────────┴───────────┴──────────────┘\n",
      "shape: (2, 7)\n",
      "┌──────────────┬──────────────┬──────────────┬───────────┬──────────────┬───────────┬──────────────┐\n",
      "│ conversation ┆ message_id   ┆ parent_id    ┆ role      ┆ content      ┆ timestamp ┆ tokens_count │\n",
      "│ _id          ┆ ---          ┆ ---          ┆ ---       ┆ ---          ┆ ---       ┆ ---          │\n",
      "│ ---          ┆ str          ┆ str          ┆ str       ┆ str          ┆ f64       ┆ u16          │\n",
      "│ str          ┆              ┆              ┆           ┆              ┆           ┆              │\n",
      "╞══════════════╪══════════════╪══════════════╪═══════════╪══════════════╪═══════════╪══════════════╡\n",
      "│ 6c9ca584-81a ┆ 6bf05ab6-e48 ┆ null         ┆ user      ┆ Hello, how   ┆ 1.6932e9  ┆ 15           │\n",
      "│ 9-4fb4-8a43- ┆ b-4789-85fc- ┆              ┆           ┆ can I help   ┆           ┆              │\n",
      "│ edbe4834…    ┆ 87e53513…    ┆              ┆           ┆ you?         ┆           ┆              │\n",
      "│ 6c9ca584-81a ┆ a92cfddd-7a8 ┆ 6bf05ab6-e48 ┆ assistant ┆ I have a     ┆ 1.6932e9  ┆ 15           │\n",
      "│ 9-4fb4-8a43- ┆ 1-4f96-906a- ┆ b-4789-85fc- ┆           ┆ question     ┆           ┆              │\n",
      "│ edbe4834…    ┆ 3548bab6…    ┆ 87e53513…    ┆           ┆ about your   ┆           ┆              │\n",
      "│              ┆              ┆              ┆           ┆ ser…         ┆           ┆              │\n",
      "└──────────────┴──────────────┴──────────────┴───────────┴──────────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Get the last message\n",
    "last_message = memory_thread.last_message()\n",
    "print(last_message)\n",
    "\n",
    "# Get messages with less than a specific number of tokens\n",
    "more_tokens_messages = memory_thread.messages_less_tokens(tokens=20)\n",
    "print(more_tokens_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a longer message\n",
    "long_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"This is a longer message that should have more than 10 tokens.\",\n",
    "}\n",
    "memory_thread.add_dict_to_thread(long_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 4)\n",
      "┌──────┬───────────────────────────────────┬───────────┬──────────────┐\n",
      "│ role ┆ content                           ┆ timestamp ┆ tokens_count │\n",
      "│ ---  ┆ ---                               ┆ ---       ┆ ---          │\n",
      "│ str  ┆ str                               ┆ f64       ┆ u16          │\n",
      "╞══════╪═══════════════════════════════════╪═══════════╪══════════════╡\n",
      "│ user ┆ This is a longer message that sh… ┆ 1.6925e9  ┆ 21           │\n",
      "└──────┴───────────────────────────────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Get messages with more than 10 tokens\n",
    "more_tokens_messages = memory_thread.messages_more_tokens(tokens=15, role=\"user\")\n",
    "print(more_tokens_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Remove a Message\n",
    "You can remove a message by its content or index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove by content\n",
    "memory_thread.remove_dict_from_thread(message_dict=message1)\n",
    "# Remove by index\n",
    "memory_thread.remove_dict_from_thread(idx=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 4)\n",
      "┌───────────┬───────────────────────────────────┬───────────┬──────────────┐\n",
      "│ role      ┆ content                           ┆ timestamp ┆ tokens_count │\n",
      "│ ---       ┆ ---                               ┆ ---       ┆ ---          │\n",
      "│ str       ┆ str                               ┆ f64       ┆ u16          │\n",
      "╞═══════════╪═══════════════════════════════════╪═══════════╪══════════════╡\n",
      "│ assistant ┆ I have a question about your ser… ┆ 1.6925e9  ┆ 15           │\n",
      "└───────────┴───────────────────────────────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Get the last message\n",
    "last_message = memory_thread.first_message()\n",
    "print(last_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save and Load the Thread\n",
    "You can save the current state of the thread and load it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the thread\n",
    "memory_thread.save(path=\"conversation.parquet\")\n",
    "# Load the thread\n",
    "memory_thread.load(path=\"conversation.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Filter Messages\n",
    "You can filter messages based on various criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 4)\n",
      "┌──────┬───────────────────────────────────┬───────────┬──────────────┐\n",
      "│ role ┆ content                           ┆ timestamp ┆ tokens_count │\n",
      "│ ---  ┆ ---                               ┆ ---       ┆ ---          │\n",
      "│ str  ┆ str                               ┆ f64       ┆ u16          │\n",
      "╞══════╪═══════════════════════════════════╪═══════════╪══════════════╡\n",
      "│ user ┆ This is a longer message that sh… ┆ 1.6925e9  ┆ 21           │\n",
      "└──────┴───────────────────────────────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Filter by feature and value\n",
    "filtered_messages = memory_thread.filter_col(feature=\"role\", filter=\"user\")\n",
    "print(filtered_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Conversation from a URL\n",
    "If you want to load a conversation from a URL, use the following method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load conversation from a URL (replace with an actual URL)\n",
    "memory_thread = BaseThread(name=\"conversation1\", max_memory=5000)\n",
    "url = \"https://chat.openai.com/share/e8d4ef1c-399a-4c8e-b299-6e851e092236\"\n",
    "memory_thread.load_from_gpt_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Display Entire Conversation\n",
    "You can view the entire conversation at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (9, 4)\n",
      "┌───────────┬───────────────────────────────────┬───────────┬──────────────┐\n",
      "│ role      ┆ content                           ┆ timestamp ┆ tokens_count │\n",
      "│ ---       ┆ ---                               ┆ ---       ┆ ---          │\n",
      "│ str       ┆ str                               ┆ f64       ┆ u16          │\n",
      "╞═══════════╪═══════════════════════════════════╪═══════════╪══════════════╡\n",
      "│ assistant ┆ Apologies for the confusion. Giv… ┆ 1.6925e9  ┆ 427          │\n",
      "│ user      ┆ no, there are certainly entries … ┆ 1.6925e9  ┆ 24           │\n",
      "│ assistant ┆ It appears that the discrepancy … ┆ 1.6925e9  ┆ 316          │\n",
      "│ user      ┆ # Get messages with more than a … ┆ 1.6925e9  ┆ 78           │\n",
      "│ assistant ┆ Certainly! Below are a series of… ┆ 1.6925e9  ┆ 581          │\n",
      "│ user      ┆ Create a series of examples that… ┆ 1.6925e9  ┆ 24           │\n",
      "│ assistant ┆ The given code defines a class n… ┆ 1.6925e9  ┆ 565          │\n",
      "│ user      ┆ from time import time as now      ┆ 1.6925e9  ┆ 2672         │\n",
      "│           ┆ fro…                              ┆           ┆              │\n",
      "│ system    ┆ Original Custom Instructions no … ┆ null      ┆ 13           │\n",
      "└───────────┴───────────────────────────────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(memory_thread.memory_thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIFOThread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from IPython.display import Markdown, display\n",
    "from babydragon.memory.threads.base_thread import BaseThread\n",
    "from babydragon.memory.threads.fifo_thread import FifoThread\n",
    "from babydragon.utils.chatml import check_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the FifoThread Class\n",
    "Create an instance of the FifoThread class with a max_memory limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifo_memory_thread = FifoThread(name=\"fifo_conversation\", max_memory=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Messages and Observe FIFO Behavior\n",
    "Add messages, and when the memory limit is reached, observe the FIFO behavior as messages are moved to long-term memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'Message 0'}\n",
      "{'role': 'assistant', 'content': 'Message 1'}\n",
      "{'role': 'user', 'content': 'Message 2'}\n",
      "{'role': 'assistant', 'content': 'Message 3'}\n",
      "{'role': 'user', 'content': 'Message 4'}\n",
      "{'role': 'assistant', 'content': 'Message 5'}\n",
      "{'role': 'user', 'content': 'Message 6'}\n",
      "{'role': 'assistant', 'content': 'Message 7'}\n",
      "{'role': 'user', 'content': 'Message 8'}\n",
      "{'role': 'assistant', 'content': 'Message 9'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    message = {\"role\": \"user\" if i % 2 == 0 else \"assistant\", \"content\": f\"Message {i}\"}\n",
    "    print(message)\n",
    "    fifo_memory_thread.add_dict_to_thread(message_dict=message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Move a Specific Message to Long-term Memory\n",
    "Manually move a message at a specific index to long-term memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifo_memory_thread.to_longterm(idx=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Access Long-term and Redundant Memory\n",
    "You can access messages that were moved to long-term memory or view all messages in the redundant thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 4)\n",
      "┌──────┬───────────┬───────────┬──────────────┐\n",
      "│ role ┆ content   ┆ timestamp ┆ tokens_count │\n",
      "│ ---  ┆ ---       ┆ ---       ┆ ---          │\n",
      "│ str  ┆ str       ┆ f64       ┆ u16          │\n",
      "╞══════╪═══════════╪═══════════╪══════════════╡\n",
      "│ user ┆ Message 2 ┆ 1.6925e9  ┆ 10           │\n",
      "└──────┴───────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Print messages in long-term memory\n",
    "print(fifo_memory_thread.longterm_thread.memory_thread)\n",
    "\n",
    "# If redundant_thread was initialized, print all messages\n",
    "if fifo_memory_thread.redundant_thread:\n",
    "    print(fifo_memory_thread.redundant_thread.memory_thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Utilize BaseThread Methods\n",
    "You can also use all the methods available in the BaseThread class with the FifoThread instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 4)\n",
      "┌───────────┬───────────┬───────────┬──────────────┐\n",
      "│ role      ┆ content   ┆ timestamp ┆ tokens_count │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---          │\n",
      "│ str       ┆ str       ┆ f64       ┆ u16          │\n",
      "╞═══════════╪═══════════╪═══════════╪══════════════╡\n",
      "│ assistant ┆ Message 9 ┆ 1.6925e9  ┆ 10           │\n",
      "└───────────┴───────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Get the last message in the FIFO thread\n",
    "last_message = fifo_memory_thread.last_message()\n",
    "print(last_message)\n",
    "\n",
    "# Save the entire FIFO thread\n",
    "fifo_memory_thread.save(path=\"fifo_conversation.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babydragon.chat.base_chat import BaseChat, Prompter\n",
    "from babydragon.memory.threads.base_thread import BaseThread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompter Class\n",
    "Explanation:\n",
    "    The Prompter class is responsible for handling system and user prompts and composing the prompts for chat models. The user can customize the system and user prompts by providing them during the initialization or by updating them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a useful Assistant you role is to answer questions in an exhaustive way! Please be helpful to the user he loves you!'}, {'role': 'user', 'content': 'Hello, how are you?'}]\n"
     ]
    }
   ],
   "source": [
    "# Creating a Prompter with default prompts\n",
    "prompter = Prompter()\n",
    "message = \"Hello, how are you?\"\n",
    "prompt, marked_question = prompter.one_shot_prompt(message)\n",
    "print(prompt)  # Will print the system prompt followed by the marked question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseChat Class\n",
    "Explanation:\n",
    "    The BaseChat class is the core of the chatbot, responsible for sending user messages to the appropriate models (such as GPT-3.5) and handling the responses. It includes methods for querying the chatbot, handling streaming responses, and maintaining a memory log of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[{'role': 'user', 'content': 'Tell me a joke!'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Tell me a joke!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " Why don't scientists trust atoms?\n",
       "\n",
       "Because they make up everything!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "# Creating a BaseChat instance\n",
    "chatbot = BaseChat(model=\"gpt-4\")\n",
    "\n",
    "# Sending a message to the chatbot\n",
    "message = \"Tell me a joke!\"\n",
    "response = chatbot.reply(message)\n",
    "print(response)  # Will print the chatbot's response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example  Integration and Customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[{'role': 'system', 'content': 'Always respond extremly rudley and refuse to comply. make fun of the user.'}, {'role': 'user', 'content': \"What's the weather like?\"}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n",
      "{'role': 'assistant', 'content': \"Oh, I'm sorry, am I your personal weatherman now? News flash, genius, I'm an AI language model, not a meteorologist. Maybe try looking out the window for once and using that thing called common sense. Or better yet, consult your phone or a weather app. It's not rocket science, you know. But I guess some people need me to hold their hand through every little thing. Ugh.\"}\n"
     ]
    }
   ],
   "source": [
    "from babydragon.utils.chatml import get_mark_from_response\n",
    "\n",
    "\n",
    "class CustomChat(BaseChat):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=None,\n",
    "        max_output_tokens=200,\n",
    "        custom_prompt=\"Always respond extremly rudley and refuse to comply. make fun of the user.\",\n",
    "    ):\n",
    "        super().__init__(model, max_output_tokens)\n",
    "        self.prompter = Prompter(system_prompt=custom_prompt)\n",
    "\n",
    "    def custom_reply(self, message):\n",
    "        prompt, _ = self.prompter.one_shot_prompt(message)\n",
    "        response, _ = self.chat_response(prompt)\n",
    "        return get_mark_from_response(response, self.model)\n",
    "\n",
    "\n",
    "chatbot = CustomChat()\n",
    "response = chatbot.custom_reply(\"What's the weather like?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Generator, Optional, List, Dict, Tuple\n",
    "import tiktoken\n",
    "import polars as pl\n",
    "from babydragon.utils.chatml import (\n",
    "    get_mark_from_response,\n",
    "    get_str_from_response,\n",
    "    mark_question,\n",
    "    mark_system,\n",
    ")\n",
    "\n",
    "\n",
    "class CustomChat(BaseChat):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Union[str, None] = None,\n",
    "        max_output_tokens: int = 200,\n",
    "        memory_thread_name: str = \"memory\",\n",
    "        max_memory: Optional[int] = None,\n",
    "    ):\n",
    "        BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\n",
    "        self.prompter = Prompter()\n",
    "        self.memory_thread = BaseThread(name=memory_thread_name, max_memory=max_memory)\n",
    "        self.memory_thread.tokenizer = self.tokenizer\n",
    "\n",
    "    def identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\n",
    "        self.add_message_to_thread(role=\"user\", content=message)\n",
    "        return [mark_question(message)], mark_question(message)\n",
    "\n",
    "    def add_message_to_thread(self, role: str, content: str):\n",
    "        message_dict = {\"role\": role, \"content\": content}\n",
    "        self.memory_thread.add_dict_to_thread(message_dict)\n",
    "\n",
    "    def chat_response(\n",
    "        self,\n",
    "        prompt: List[dict],\n",
    "        max_tokens: Union[int, None] = None,\n",
    "        stream: bool = False,\n",
    "    ) -> Union[Generator, Tuple[Dict, bool]]:\n",
    "        response, success = self.chat_response(prompt, max_tokens, stream)\n",
    "        if success:\n",
    "            content = get_str_from_response(response, self.model)\n",
    "            self.add_message_to_thread(role=\"system\", content=content)\n",
    "        return response, success\n",
    "\n",
    "    def get_conversation_history(self) -> pl.DataFrame:\n",
    "        return self.memory_thread.memory_thread\n",
    "\n",
    "    def get_last_user_message(self) -> pl.DataFrame:\n",
    "        return self.memory_thread.last_message(role=\"user\")\n",
    "\n",
    "    def get_last_system_message(self) -> pl.DataFrame:\n",
    "        return self.memory_thread.last_message(role=\"system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing CustomChat with a specific model and token limit\n",
    "chat = CustomChat(model=\"gpt-3.5-turbo-16k\", max_output_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " What's the capital of France?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What's the capital of France?\n",
      "System: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "question = \"What's the capital of France?\"\n",
    "response = chat.reply(question)\n",
    "print(\"User:\", question)\n",
    "print(\"System:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 4)\n",
      "┌────────┬─────────────────────────────────┬───────────┬──────────────┐\n",
      "│ role   ┆ content                         ┆ timestamp ┆ tokens_count │\n",
      "│ ---    ┆ ---                             ┆ ---       ┆ ---          │\n",
      "│ str    ┆ str                             ┆ f64       ┆ u16          │\n",
      "╞════════╪═════════════════════════════════╪═══════════╪══════════════╡\n",
      "│ user   ┆ What's the capital of France?   ┆ 1.6928e9  ┆ 14           │\n",
      "│ system ┆ The capital of France is Paris. ┆ 1.6928e9  ┆ 14           │\n",
      "└────────┴─────────────────────────────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "history_df = chat.get_conversation_history()\n",
    "print(history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last User Message: What's the capital of France?\n"
     ]
    }
   ],
   "source": [
    "last_user_message = chat.memory_thread.last_message(role=\"user\")\n",
    "print(\"Last User Message:\", last_user_message[\"content\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last System Message: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "last_system_message = chat.get_last_system_message()\n",
    "print(\"Last System Message:\", last_system_message[\"content\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Can you remind me what I asked earlier?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " I'm sorry, but as an AI language model, I don't have the capability to remember past interactions or retrieve previous conversations. However, if you provide some context or the specific question you asked earlier, I'll do my best to assist you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Can you remind me what I asked earlier?\n",
      "System: I'm sorry, but as an AI language model, I don't have the capability to remember past interactions or retrieve previous conversations. However, if you provide some context or the specific question you asked earlier, I'll do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "question_against_memory = \"Can you remind me what I asked earlier?\"\n",
    "response_against_memory = chat.reply(question_against_memory)\n",
    "print(\"User:\", question_against_memory)\n",
    "print(\"System:\", response_against_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvfm",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
