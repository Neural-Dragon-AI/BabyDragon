{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import openai\n",
    "from babydragon.memory.indexes.numpy_index import NpIndex\n",
    "from babydragon.models.embedders.ada2 import OpenAiEmbedder\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an OpenAiEmbedder instance\n",
    "openai_embedder = OpenAiEmbedder()\n",
    "openai.api_key = \"\"\n",
    "# Initialize a NpIndex instance using OpenAiEmbedder\n",
    "# values = ['Hello, world!', 'This is a test sentence.', 'OpenAI is amazing!','cake','pie','ice cream', 'Buffer Errors']\n",
    "# embeddings = openai_embedder.embed(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(rows):\n",
    "    # Generate a list of unique strings\n",
    "    # start = time.time()\n",
    "    # unique_strings = [str(uuid.uuid4()) for _ in range(rows)]\n",
    "    # end = time.time()\n",
    "    # print(f\"generated unique strings in {end-start} seconds\")\n",
    "    # Generate a numpy array of size (rows, 1508) with random values\n",
    "    print(\"generating random array\")\n",
    "    start = time.time()\n",
    "    random_array = np.random.rand(rows, 1536).astype(np.float32)\n",
    "    end = time.time()\n",
    "    query_vector = np.random.rand(1536).astype(np.float32)  \n",
    "    print(f\"generated random array in {end-start} seconds\")\n",
    "    start = time.time()\n",
    "    # Normalize each vector in the numpy array\n",
    "    norms = np.linalg.norm(random_array, axis=1, keepdims=True)\n",
    "    normalized_array = random_array / norms\n",
    "    # normalized_array = random_array.astype(np.float32)\n",
    "    normalized_query_vector = query_vector / np.linalg.norm(query_vector)\n",
    "    end = time.time()\n",
    "    print(f\"normalized random array in {end-start} seconds\")\n",
    "    # Convert numpy array to list of lists\n",
    "    start = time.time()\n",
    "    # list_of_lists = normalized_array.astype(np.float32).tolist()\n",
    "    end = time.time()\n",
    "    # query_vector_as_list = normalized_query_vector.astype(np.float32).tolist()\n",
    "    return normalized_array,normalized_query_vector\n",
    "\n",
    "def convert_to_numpy_array(embeddings_list,embedding_query):\n",
    "    embeddings_array = np.array(embeddings_list)\n",
    "    query_vector = np.array(embedding_query)\n",
    "    return embeddings_array, query_vector\n",
    "\n",
    "def convert_to_polars_array(embeddings_array,embedding_query):\n",
    "    df = pl.DataFrame({'embeddings': embeddings_array}, schema={'embeddings':pl.List( inner=pl.Float32) }) #:pl.Array(width=1536, inner=pl.Float64)\n",
    "    query_as_series = pl.Series(embedding_query, dtype=pl.Float32)\n",
    "    return df,query_as_series\n",
    "\n",
    "\n",
    "def numpy_search(query_vector, embeddings_array,  top_k):\n",
    "    # Compute dot product of query vector with each row in the embeddings array\n",
    "    dot_product_array = np.dot(embeddings_array, query_vector)\n",
    "    # Sort dot product array in descending order and get the indices of the top_k rows\n",
    "    top_k_indices = np.argsort(dot_product_array)[::-1][:top_k]\n",
    "    # Get the values corresponding to the top_k_indices\n",
    "    top_k_scores = [dot_product_array[i] for i in top_k_indices]\n",
    "    return top_k_scores\n",
    "\n",
    "def polar_search(query_as_series,df,top_k):\n",
    "    dot_product_frame = df.with_columns(df[\"embeddings\"].list.eval(pl.element().explode().dot(query_as_series),parallel=True).list.first().alias(\"dot_product\"))\n",
    "    # Sort by dot product and select top_k rows\n",
    "    result = dot_product_frame.sort('dot_product', descending=True).slice(0, top_k)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_speed():\n",
    "    row_sizes = [10, 100, 1000, 10000, 100000, 1000000]  # Add more if needed\n",
    "    top_k = 10\n",
    "\n",
    "    for rows in row_sizes:\n",
    "        print(f\"Testing with {rows} rows...\")\n",
    "        embeddings_array, query_vector = generate_data(rows)\n",
    "\n",
    "\n",
    "       \n",
    "        # Execute top-k search with numpy and measure time\n",
    "        start_time = time.time()\n",
    "        top_k_scores = numpy_search(query_vector, embeddings_array, top_k)\n",
    "        numpy_search_time = time.time() - start_time\n",
    "        print(f\"Time to execute top-k search with numpy: {numpy_search_time} seconds\")\n",
    "\n",
    "         # Convert to polars dataframe and measure time\n",
    "        start_time = time.time()\n",
    "        polars_df, polars_query = convert_to_polars_array(embeddings_array, query_vector)\n",
    "        polars_conversion_time = time.time() - start_time\n",
    "        print(f\"Time to convert to polars dataframe: {polars_conversion_time} seconds\")\n",
    "\n",
    "        del embeddings_array\n",
    "        # Execute top-k search with polars and measure time\n",
    "        start_time = time.time()\n",
    "        result = polar_search(polars_query, polars_df, top_k)\n",
    "        polars_search_time = time.time() - start_time\n",
    "        del polars_df\n",
    "        print(f\"Time to execute top-k search with polars: {polars_search_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 10 rows...\n",
      "generating random array\n",
      "generated random array in 0.0015056133270263672 seconds\n",
      "normalized random array in 0.0004944801330566406 seconds\n",
      "Time to execute top-k search with numpy: 0.0010006427764892578 seconds\n",
      "Time to convert to polars dataframe: 0.0009987354278564453 seconds\n",
      "Time to execute top-k search with polars: 0.0040056705474853516 seconds\n",
      "Testing with 100 rows...\n",
      "generating random array\n",
      "generated random array in 0.001997709274291992 seconds\n",
      "normalized random array in 0.0005006790161132812 seconds\n",
      "Time to execute top-k search with numpy: 0.0 seconds\n",
      "Time to convert to polars dataframe: 0.0010018348693847656 seconds\n",
      "Time to execute top-k search with polars: 0.002497434616088867 seconds\n",
      "Testing with 1000 rows...\n",
      "generating random array\n",
      "generated random array in 0.009997129440307617 seconds\n",
      "normalized random array in 0.002502918243408203 seconds\n",
      "Time to execute top-k search with numpy: 0.0005006790161132812 seconds\n",
      "Time to convert to polars dataframe: 0.009999275207519531 seconds\n",
      "Time to execute top-k search with polars: 0.012997865676879883 seconds\n",
      "Testing with 10000 rows...\n",
      "generating random array\n",
      "generated random array in 0.0960092544555664 seconds\n",
      "normalized random array in 0.029500961303710938 seconds\n",
      "Time to execute top-k search with numpy: 0.002498149871826172 seconds\n",
      "Time to convert to polars dataframe: 0.09251785278320312 seconds\n",
      "Time to execute top-k search with polars: 0.13553571701049805 seconds\n",
      "Testing with 100000 rows...\n",
      "generating random array\n",
      "generated random array in 1.0187137126922607 seconds\n",
      "normalized random array in 0.3120434284210205 seconds\n",
      "Time to execute top-k search with numpy: 0.028015613555908203 seconds\n",
      "Time to convert to polars dataframe: 0.8406517505645752 seconds\n",
      "Time to execute top-k search with polars: 2.318877696990967 seconds\n",
      "Testing with 1000000 rows...\n",
      "generating random array\n",
      "generated random array in 10.005863904953003 seconds\n",
      "normalized random array in 3.2981226444244385 seconds\n",
      "Time to execute top-k search with numpy: 0.26856184005737305 seconds\n",
      "Time to convert to polars dataframe: 7.8992273807525635 seconds\n",
      "Time to execute top-k search with polars: 13.31764030456543 seconds\n",
      "Testing with 10000000 rows...\n",
      "generating random array\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 114. GiB for an array with shape (10000000, 1536) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_speed()\n",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m, in \u001b[0;36mtest_speed\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m rows \u001b[39min\u001b[39;00m row_sizes:\n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTesting with \u001b[39m\u001b[39m{\u001b[39;00mrows\u001b[39m}\u001b[39;00m\u001b[39m rows...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     embeddings_array, query_vector \u001b[39m=\u001b[39m generate_data(rows)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# Execute top-k search with numpy and measure time\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m, in \u001b[0;36mgenerate_data\u001b[1;34m(rows)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgenerating random array\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 18\u001b[0m random_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrand(rows, \u001b[39m1536\u001b[39;49m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     19\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     20\u001b[0m query_vector \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(\u001b[39m1536\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)  \n",
      "File \u001b[1;32mmtrand.pyx:1182\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.rand\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmtrand.pyx:425\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.random_sample\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_common.pyx:307\u001b[0m, in \u001b[0;36mnumpy.random._common.double_fill\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 114. GiB for an array with shape (10000000, 1536) and data type float64"
     ]
    }
   ],
   "source": [
    "test_speed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
