{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/danielhug/neuraldragon/gitensor/BabyDragon\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: BabyDragon\n",
      "  Building wheel for BabyDragon (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for BabyDragon: filename=BabyDragon-0.0.0-py3-none-any.whl size=1164 sha256=7b01a96a9926a47c04683136d548b383fd40095c5224afcc429d8902449f0004\n",
      "  Stored in directory: /private/var/folders/29/mz6wb9ks5k72xrwdx9wxdwrh0000gn/T/pip-ephem-wheel-cache-yziup76t/wheels/13/d9/0f/0cfbd22eca7816335d841930c85504b44838e275b42ad5e431\n",
      "Successfully built BabyDragon\n",
      "Installing collected packages: BabyDragon\n",
      "  Attempting uninstall: BabyDragon\n",
      "    Found existing installation: BabyDragon 0.0.0\n",
      "    Uninstalling BabyDragon-0.0.0:\n",
      "      Successfully uninstalled BabyDragon-0.0.0\n",
      "Successfully installed BabyDragon-0.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install .\n",
    "from babydragon.chat.memory_chat import FifoVectorChat, FifoChat, VectorChat\n",
    "from babydragon.chat.base_chat import BaseChat, Prompter\n",
    "from babydragon.chat.chat import Chat\n",
    "from babydragon.memory.indexes.pandas_index import PandasIndex\n",
    "from babydragon.memory.indexes.python_index import PythonIndex\n",
    "from babydragon.memory.indexes.memory_kernel import MemoryKernel, MemoryKernelGroup\n",
    "from babydragon.utils.oai import (\n",
    "    mark_question,\n",
    "    mark_system,\n",
    "    get_mark_from_response,\n",
    "    get_str_from_response,\n",
    ")\n",
    "import gradio\n",
    "from typing import List, Tuple, Dict\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/danielhug/neuraldragon/gitensor/BabyDragon\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: BabyDragon\n",
      "  Building wheel for BabyDragon (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for BabyDragon: filename=BabyDragon-0.0.0-py3-none-any.whl size=1164 sha256=8d4621ca7d65207873366ac07d724d7095d9b6d95520baa84d72f724a11dd0fb\n",
      "  Stored in directory: /private/var/folders/29/mz6wb9ks5k72xrwdx9wxdwrh0000gn/T/pip-ephem-wheel-cache-iupk0uaz/wheels/13/d9/0f/0cfbd22eca7816335d841930c85504b44838e275b42ad5e431\n",
      "Successfully built BabyDragon\n",
      "Installing collected packages: BabyDragon\n",
      "  Attempting uninstall: BabyDragon\n",
      "    Found existing installation: BabyDragon 0.0.0\n",
      "    Uninstalling BabyDragon-0.0.0:\n",
      "      Successfully uninstalled BabyDragon-0.0.0\n",
      "Successfully installed BabyDragon-0.0.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseChat Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = BaseChat()\n",
    "chatbot.gradio()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseChat with Prompter Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPrompter(Prompter):\n",
    "    def __init__(self, system_prompt: str = None, user_prompt: str = None):\n",
    "        super().__init__(system_prompt, user_prompt)\n",
    "\n",
    "    def custom_user_prompt(self, message: str) -> str:\n",
    "        return f\"The user says: {message}\"\n",
    "\n",
    "    def custom_system_prompt(self) -> str:\n",
    "        return \"You are a helpful AI assistant.\"\n",
    "\n",
    "    def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\n",
    "        marked_question = mark_question(self.custom_user_prompt(message))\n",
    "        prompt = [mark_system(self.custom_system_prompt())] + [marked_question]\n",
    "        return prompt, marked_question\n",
    "\n",
    "\n",
    "class CustomChat(BaseChat):\n",
    "    def __init__(self, model: str = None, max_output_tokens: int = 1000):\n",
    "        super().__init__(model, max_output_tokens)\n",
    "        self.prompter = CustomPrompter()\n",
    "        self.prompt_func = self.prompter.one_shot_prompt\n",
    "\n",
    "\n",
    "chatbot = CustomChat()\n",
    "chatbot.gradio()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Load Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import babydragon\n",
    "\n",
    "babydragon_path = os.path.dirname(os.path.abspath(babydragon.__file__))\n",
    "\n",
    "venv_path = \"/Users/danielhug/neuraldragon/gitensor/BabyDragon/venv/lib/python3.10/site-packages\"\n",
    "faiss_venv_path = f\"{venv_path}/faiss\"\n",
    "libcst_venv_path = f\"{venv_path}/libcst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Load BabyDragon Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pind = PythonIndex(babydragon_path,name=\"babydragon_index\", load = False)\n",
    "# pind.save()\n",
    "pind = PythonIndex(babydragon_path, name=\"babydragon_index\", load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Load Faiss Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "# Assuming your function is named `my_function`\n",
    "print(inspect.getsource(PythonIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': 'babydragon.memory.indexes.python_index',\n",
       "              '__init__': <function babydragon.memory.indexes.python_index.PythonIndex.__init__(self, directory_path: str, name: str = 'python_index', save_path: Optional[str] = None, load: bool = False, minify_code: bool = False, remove_docstrings: bool = False, tokenizer: Optional[tiktoken.core.Encoding] = None)>,\n",
       "              '__doc__': None})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PythonIndex.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faiss_ind = PythonIndex(faiss_venv_path,name=\"faiss_index\", load = False)\n",
    "# faiss_ind.save()\n",
    "faiss_ind = PythonIndex(faiss_venv_path, name=\"faiss_index\", load=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Load LibCST Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libcst_ind = PythonIndex(libcst_venv_path,name=\"libcst_index\", load = False)\n",
    "# libcst_ind.save()\n",
    "libcst_ind = PythonIndex(libcst_venv_path, name=\"libcst_index\", load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MemoryIndex dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import babydragon\n",
    "\n",
    "babydragon_path = os.path.dirname(os.path.abspath(babydragon.__file__))\n",
    "\n",
    "venv_path = \"/Users/danielhug/neuraldragon/gitensor/BabyDragon/venv/lib/python3.10/site-packages\"\n",
    "faiss_venv_path = f\"{venv_path}/faiss\"\n",
    "libcst_venv_path = f\"{venv_path}/libcst\"\n",
    "python_index_dict = {\n",
    "    \"babydragon_index\": PythonIndex(\n",
    "        babydragon_path, name=\"babydragon_index\", load=True\n",
    "    ),\n",
    "    \"faiss_index\": PythonIndex(faiss_venv_path, name=\"faiss_index\", load=True),\n",
    "    \"libcst_index\": PythonIndex(libcst_venv_path, name=\"libcst_index\", load=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import solve_sylvester\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "from scipy.sparse import csr_matrix\n",
    "from numpy.linalg import svd\n",
    "\n",
    "\n",
    "def graph_sylvester_embedding(G, m, ts):\n",
    "    V, W = G\n",
    "    n = len(V)\n",
    "\n",
    "    # Step 1: Compute G_bar\n",
    "    G_bar = np.dot(np.diag(1 / np.sqrt(W.sum(axis=1))), W)\n",
    "\n",
    "    # Step 2: Compute L_BE\n",
    "    D_BE = np.diag(W.sum(axis=1))\n",
    "    L_BE = np.identity(n) - np.dot(\n",
    "        np.diag(1 / np.sqrt(D_BE.diagonal())),\n",
    "        np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n",
    "    )\n",
    "\n",
    "    # Step 3: Solve the discrete-time Sylvester equation\n",
    "    A = W\n",
    "    B = L_BE\n",
    "    C = np.identity(n)\n",
    "    X = solve_sylvester(A, B, C)\n",
    "\n",
    "    # Step 4: Compute the largest m singular values and associated singular vectors of X\n",
    "    U, S, Vh = svd(X, full_matrices=False)\n",
    "    U_m = U[:, :m]\n",
    "    S_m = S[:m]\n",
    "\n",
    "    # Step 5: Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor\n",
    "    node_embeddings = np.zeros((n, m))\n",
    "\n",
    "    for i in range(n):\n",
    "        for s in range(m):\n",
    "            # Spectral kernel descriptor\n",
    "            node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n",
    "\n",
    "    return node_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_contextual_resources(self, contextual_memory):\\n    # Load contextual resources required for code generation\\n    pass\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = python_index_dict[\"babydragon_index\"].values\n",
    "values[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = python_index_dict[\"babydragon_index\"].embeddings\n",
    "embeddings[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new index from a faiss index and values list\n",
      "Computing the adjacency matrix\n",
      "Embeddings shape:  (270, 1536)\n",
      "Computing the k-hop adjacency matrix and aggregated features\n",
      "Compute the k-hop adjacency matrix\n",
      "Aggregate the messages from the k-hop neighborhood:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 157.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the memory index\n",
      "Creating a new index\n",
      "Creating a new index from a list of embeddings and values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babydragon_kernel\n",
      "code_values: 270\n",
      "code_embeddings: (270, 1536)\n",
      "sorted_code_strings: 264\n",
      "sorted_node_embeddings.shape: (264, 1536)\n",
      "U.shape: (264, 264)\n",
      "S.shape: (264,)\n",
      "VT.shape: (1536, 1536)\n",
      "(264, 1536)\n",
      "U.shape: (6, 6)\n",
      "S.shape: (6,)\n",
      "VT.shape: (1536, 1536)\n",
      "(6, 1536)\n"
     ]
    }
   ],
   "source": [
    "memory_kernel_dict = {\n",
    "    \"babydragon_kernel\": MemoryKernel(python_index_dict[\"babydragon_index\"])\n",
    "}\n",
    "# \"faiss_kernel\": MemoryKernel(python_index_dict['faiss_index']),\n",
    "# \"libcst_kernel\": MemoryKernel(python_index_dict['libcst_index'])\n",
    "# }\n",
    "bucket_group = MemoryKernelGroup(memory_kernel_dict).rank_decomp_and_merge(\n",
    "    component_window_size=1, threshold=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# This is the __init__.py file for the package.\\n',\n",
       " 'def __init__(self):\\n    self.git_memory = None\\n    self.commit_index = None\\n    self.contextual_memory = None\\n',\n",
       " '\\ndef generate_meta_code(self, user_input):\\n    # Transform user input into meta-code representation\\n    pass\\n',\n",
       " '\\ndef load_git_memory(self, git_memory):\\n    # Retrieve GitMemory context\\n    pass\\n',\n",
       " '\\ndef load_commit_context(self, commit_index):\\n    # Retrieve commit context\\n    pass\\n',\n",
       " '\\ndef load_contextual_resources(self, contextual_memory):\\n    # Load contextual resources required for code generation\\n    pass\\n',\n",
       " '\\ndef get_current_draft(self, commit_context):\\n    # Get the current draft code from commit context\\n    pass\\n',\n",
       " '\\ndef generate_modifications(self, draft_code, meta_code, context_resources):\\n    # Generate modifications to the draft code based on meta-code and context resources\\n    pass\\n',\n",
       " '\\ndef extract_source_code(self, modifications, context_resources):\\n    # Extract source code from modifications and context resources\\n    pass\\n',\n",
       " '\\ndef apply_modifications(self, draft_code, modifications):\\n    # Apply modifications to the draft code\\n    pass\\n',\n",
       " '\\ndef validate_draft(self, updated_draft, source_code):\\n    # Validate the updated draft code and check for consistency\\n    pass\\n',\n",
       " \"\\ndef compare_draft_with_objective(self, updated_draft, meta_code):\\n    # Compare the updated draft code with user's goal based on meta-code\\n    pass\\n\",\n",
       " '\\ndef store_draft_to_commit_index(self, commit_index, updated_draft):\\n    # Store the updated draft code in the commit index\\n    pass\\n',\n",
       " '\\ndef rollback(self, commit_index):\\n    # Roll back the draft code to the previous state in the commit index\\n    pass\\n',\n",
       " '\\n\\ndef codeGenerationIteration(self, userInput, gitMemory, commitIndex, contextualMemory):\\n    metaCode = self.generateMetaCode(userInput)\\n\\n    # Concurrent processing - Step 1\\n    with concurrent.futures.ThreadPoolExecutor() as executor:\\n        gitMemoryContext = executor.submit(self.loadGitMemory, gitMemory)\\n        commitContext = executor.submit(self.loadCommitContext, commitIndex)\\n        contextResources = executor.submit(self.loadContextualResources, contextualMemory)\\n\\n        gitMemoryContext = gitMemoryContext.result()\\n        commitContext = commitContext.result()\\n        contextResources = contextResources.result()\\n\\n    draftCode = self.getCurrentDraft(commitContext)\\n\\n    # Concurrent processing - Step 2\\n    with concurrent.futures.ThreadPoolExecutor() as executor:\\n        modifications = executor.submit(self.generateModifications, draftCode, metaCode, contextResources)\\n        sourceCode = executor.submit(self.extractSourceCode, modifications, contextResources)\\n\\n        modifications = modifications.result()\\n        sourceCode = sourceCode.result()\\n\\n    updatedDraft = self.applyModifications(draftCode, modifications)\\n\\n    # Concurrent processing - Step 3\\n    with concurrent.futures.ThreadPoolExecutor() as executor:\\n        validationResult = executor.submit(self.validateDraft, updatedDraft, sourceCode)\\n        comparisonResult = executor.submit(self.compareDraftWithObjective, updatedDraft, metaCode)\\n\\n        validationResult = validationResult.result()\\n        comparisonResult = comparisonResult.result()\\n\\n    if validationResult and comparisonResult:\\n        self.storeDraftToCommitIndex(commitIndex, updatedDraft)\\n    else:\\n        updatedDraft = self.rollback(commitIndex)\\n\\n    return updatedDraft\\n',\n",
       " \"import concurrent.futures\\n\\nclass CodeGenerator:\\n    def __init__(self):\\n        self.git_memory = None\\n        self.commit_index = None\\n        self.contextual_memory = None\\n\\n    def generate_meta_code(self, user_input):\\n        # Transform user input into meta-code representation\\n        pass\\n\\n    def load_git_memory(self, git_memory):\\n        # Retrieve GitMemory context\\n        pass\\n\\n    def load_commit_context(self, commit_index):\\n        # Retrieve commit context\\n        pass\\n\\n    def load_contextual_resources(self, contextual_memory):\\n        # Load contextual resources required for code generation\\n        pass\\n\\n    def get_current_draft(self, commit_context):\\n        # Get the current draft code from commit context\\n        pass\\n\\n    def generate_modifications(self, draft_code, meta_code, context_resources):\\n        # Generate modifications to the draft code based on meta-code and context resources\\n        pass\\n\\n    def extract_source_code(self, modifications, context_resources):\\n        # Extract source code from modifications and context resources\\n        pass\\n\\n    def apply_modifications(self, draft_code, modifications):\\n        # Apply modifications to the draft code\\n        pass\\n\\n    def validate_draft(self, updated_draft, source_code):\\n        # Validate the updated draft code and check for consistency\\n        pass\\n\\n    def compare_draft_with_objective(self, updated_draft, meta_code):\\n        # Compare the updated draft code with user's goal based on meta-code\\n        pass\\n\\n    def store_draft_to_commit_index(self, commit_index, updated_draft):\\n        # Store the updated draft code in the commit index\\n        pass\\n\\n    def rollback(self, commit_index):\\n        # Roll back the draft code to the previous state in the commit index\\n        pass\\n\\n\\n    def codeGenerationIteration(self, userInput, gitMemory, commitIndex, contextualMemory):\\n        metaCode = self.generateMetaCode(userInput)\\n\\n        # Concurrent processing - Step 1\\n        with concurrent.futures.ThreadPoolExecutor() as executor:\\n            gitMemoryContext = executor.submit(self.loadGitMemory, gitMemory)\\n            commitContext = executor.submit(self.loadCommitContext, commitIndex)\\n            contextResources = executor.submit(self.loadContextualResources, contextualMemory)\\n\\n            gitMemoryContext = gitMemoryContext.result()\\n            commitContext = commitContext.result()\\n            contextResources = contextResources.result()\\n\\n        draftCode = self.getCurrentDraft(commitContext)\\n\\n        # Concurrent processing - Step 2\\n        with concurrent.futures.ThreadPoolExecutor() as executor:\\n            modifications = executor.submit(self.generateModifications, draftCode, metaCode, contextResources)\\n            sourceCode = executor.submit(self.extractSourceCode, modifications, contextResources)\\n\\n            modifications = modifications.result()\\n            sourceCode = sourceCode.result()\\n\\n        updatedDraft = self.applyModifications(draftCode, modifications)\\n\\n        # Concurrent processing - Step 3\\n        with concurrent.futures.ThreadPoolExecutor() as executor:\\n            validationResult = executor.submit(self.validateDraft, updatedDraft, sourceCode)\\n            comparisonResult = executor.submit(self.compareDraftWithObjective, updatedDraft, metaCode)\\n\\n            validationResult = validationResult.result()\\n            comparisonResult = comparisonResult.result()\\n\\n        if validationResult and comparisonResult:\\n            self.storeDraftToCommitIndex(commitIndex, updatedDraft)\\n        else:\\n            updatedDraft = self.rollback(commitIndex)\\n\\n        return updatedDraft\\n\",\n",
       " 'def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\\n    self.directory_path = directory_path\\n    self.visitor = visitor\\n',\n",
       " '\\ndef _process_file(self, file_path: str):\\n    with open(file_path, \"r\") as file:\\n        source_code = file.read()\\n\\n    try:\\n        tree = cst.parse_module(source_code)\\n    except cst.ParserSyntaxError:\\n        print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n        return\\n\\n    tree.visit(self.visitor)\\n',\n",
       " '\\ndef process_file(self, file_path: str):\\n    # Run flake8 on the file\\n    result = subprocess.run(\\n        [\"flake8\", \"--select=E999\", file_path], capture_output=True\\n    )\\n\\n    if result.returncode != 0:\\n        print(f\"Skipping file with syntax error: {file_path}\")\\n        print(result.stderr.decode(\"utf-8\"))\\n        return\\n\\n    with open(file_path, \"r\") as f:\\n        source_code = f.read()\\n\\n    try:\\n        tree = cst.parse_module(source_code)\\n        tree.visit(self.visitor)\\n    except cst.ParserSyntaxError as e:\\n        print(f\"Syntax error: {e}\")\\n        print(f\"Skipping file with syntax error: {file_path}\")\\n',\n",
       " '\\ndef process_directory(self) -> List[str]:\\n    function_source_codes = []\\n    class_source_codes = []\\n\\n    for root, _, files in os.walk(self.directory_path):\\n        for file in files:\\n            if file.endswith(\".py\"):\\n                file_path = os.path.join(root, file)\\n                self._process_file(file_path)\\n\\n    function_source_codes = self.visitor.function_source_codes\\n    function_nodes = self.visitor.function_nodes\\n    class_source_codes = self.visitor.class_source_codes\\n    class_nodes = self.visitor.class_nodes\\n\\n\\n    return function_source_codes, class_source_codes, function_nodes, class_nodes\\n',\n",
       " '\\ndef clone_repo(self, repo_url):\\n    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n    target_directory = os.path.join(self.directory_path, repo_name)\\n\\n    if os.path.exists(target_directory):\\n        shutil.rmtree(target_directory)\\n\\n    subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n    return target_directory\\n',\n",
       " 'def __init__(self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()):\\n    self.username = username\\n    self.repo_name = repo_name\\n    self.github = Github()\\n    self.directory_processor = None\\n    self.function_source_codes = []\\n    self.class_source_codes = []\\n    self.visitor = visitor\\n',\n",
       " '\\ndef get_public_repos(self):\\n    user = self.github.get_user(self.username)\\n    return user.get_repos()\\n',\n",
       " '\\ndef process_repos(self, base_directory):\\n    self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\\n    for repo in self.get_public_repos():\\n        if not repo.private:\\n            print(f\"Processing repo: {repo.name}\")\\n            repo_path = self.directory_processor.clone_repo(repo.clone_url)\\n            (\\n                function_source_codes,\\n                class_source_codes,\\n            ) = self.directory_processor.process_directory()\\n            self.function_source_codes.extend(function_source_codes)\\n            self.class_source_codes.extend(class_source_codes)\\n            shutil.rmtree(repo_path)\\n\\n    return self.directory_processor\\n',\n",
       " 'def __init__(self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()):\\n    self.username = username\\n    self.repo_name = repo_name\\n    self.github = Github()\\n    self.directory_processor = None\\n    self.function_source_codes = []\\n    self.function_nodes = []\\n    self.class_source_codes = []\\n    self.class_nodes = []\\n    self.visitor = visitor\\n',\n",
       " '\\ndef get_repo(self, repo_name):\\n    user = self.github.get_user(self.username)\\n    return user.get_repo(repo_name)\\n',\n",
       " '\\ndef process_repo(self, base_directory):\\n    self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\\n    repo = self.get_repo(self.repo_name)\\n    print(f\"Processing repo: {self.repo_name}\")\\n    repo_path = self.directory_processor.clone_repo(repo.clone_url)\\n    (\\n        function_source_codes,\\n        class_source_codes,\\n        function_nodes,\\n        class_nodes,\\n    ) = self.directory_processor.process_directory()\\n    self.function_source_codes.extend(function_source_codes)\\n    self.function_nodes.extend(function_nodes)\\n    self.class_source_codes.extend(class_source_codes)\\n    self.class_nodes.extend(class_nodes)\\n    shutil.rmtree(repo_path)\\n    return self.directory_processor\\n',\n",
       " '\\ndef get_values(self):\\n    #concatenate the function and class source codes\\n    self.function_source_codes.extend(self.class_source_codes)\\n    self.function_nodes.extend(self.class_nodes)\\n    return self.function_source_codes, self.function_nodes\\n',\n",
       " 'import os\\nimport shutil\\nimport subprocess\\nfrom typing import List\\n\\nimport libcst as cst\\nfrom github import Github\\n\\nfrom babydragon.working_memory.parsers.visitors import FunctionAndClassVisitor\\n\\n\\nclass DirectoryProcessor:\\n    def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\\n        self.directory_path = directory_path\\n        self.visitor = visitor\\n\\n    def _process_file(self, file_path: str):\\n        with open(file_path, \"r\") as file:\\n            source_code = file.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n        except cst.ParserSyntaxError:\\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n            return\\n\\n        tree.visit(self.visitor)\\n\\n    def process_file(self, file_path: str):\\n        # Run flake8 on the file\\n        result = subprocess.run(\\n            [\"flake8\", \"--select=E999\", file_path], capture_output=True\\n        )\\n\\n        if result.returncode != 0:\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n            print(result.stderr.decode(\"utf-8\"))\\n            return\\n\\n        with open(file_path, \"r\") as f:\\n            source_code = f.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n            tree.visit(self.visitor)\\n        except cst.ParserSyntaxError as e:\\n            print(f\"Syntax error: {e}\")\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n\\n    def process_directory(self) -> List[str]:\\n        function_source_codes = []\\n        class_source_codes = []\\n\\n        for root, _, files in os.walk(self.directory_path):\\n            for file in files:\\n                if file.endswith(\".py\"):\\n                    file_path = os.path.join(root, file)\\n                    self._process_file(file_path)\\n\\n        function_source_codes = self.visitor.function_source_codes\\n        function_nodes = self.visitor.function_nodes\\n        class_source_codes = self.visitor.class_source_codes\\n        class_nodes = self.visitor.class_nodes\\n\\n\\n        return function_source_codes, class_source_codes, function_nodes, class_nodes\\n\\n    def clone_repo(self, repo_url):\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.directory_path, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n\\nclass GitHubUserProcessor:\\n    def __init__(self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.github = Github()\\n        self.directory_processor = None\\n        self.function_source_codes = []\\n        self.class_source_codes = []\\n        self.visitor = visitor\\n\\n    def get_public_repos(self):\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def process_repos(self, base_directory):\\n        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.directory_processor.clone_repo(repo.clone_url)\\n                (\\n                    function_source_codes,\\n                    class_source_codes,\\n                ) = self.directory_processor.process_directory()\\n                self.function_source_codes.extend(function_source_codes)\\n                self.class_source_codes.extend(class_source_codes)\\n                shutil.rmtree(repo_path)\\n\\n        return self.directory_processor\\n\\n\\nclass GitHubRepoProcessor:\\n    def __init__(self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.github = Github()\\n        self.directory_processor = None\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n        self.visitor = visitor\\n\\n    def get_repo(self, repo_name):\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_repo(self, base_directory):\\n        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.directory_processor.clone_repo(repo.clone_url)\\n        (\\n            function_source_codes,\\n            class_source_codes,\\n            function_nodes,\\n            class_nodes,\\n        ) = self.directory_processor.process_directory()\\n        self.function_source_codes.extend(function_source_codes)\\n        self.function_nodes.extend(function_nodes)\\n        self.class_source_codes.extend(class_source_codes)\\n        self.class_nodes.extend(class_nodes)\\n        shutil.rmtree(repo_path)\\n        return self.directory_processor\\n\\n    def get_values(self):\\n        #concatenate the function and class source codes\\n        self.function_source_codes.extend(self.class_source_codes)\\n        self.function_nodes.extend(self.class_nodes)\\n        return self.function_source_codes, self.function_nodes\\n\\n\\nif __name__ == \"__main__\":\\n    username = \"Danielpatrickhug\"\\n    repo_name = \"GitModel\"\\n    base_directory = \"work_folder\"\\n\\n    # Make sure the work folder exists\\n    if not os.path.exists(base_directory):\\n        os.mkdir(base_directory)\\n\\n    repo_processor = GitHubRepoProcessor(username=username, repo_name=repo_name)\\n    count = repo_processor.process_repo(base_directory)\\n\\n    # Print the list of function source codes\\n    for i, function_source_code in enumerate(\\n        repo_processor.function_source_codes, start=1\\n    ):\\n        print(f\"Function {i} source code:\\\\n{function_source_code}\\\\n\")\\n\\n    # Print the list of class source codes\\n    for i, class_source_code in enumerate(repo_processor.class_source_codes, start=1):\\n        print(f\"Class {i} source code:\\\\n{class_source_code}\\\\n\")\\n',\n",
       " '\\ndef visit_Call(self, node: cst.Call) -> None:\\n    function_name = None\\n    if isinstance(node.func, cst.Name):\\n        function_name = node.func.value\\n\\n    if function_name:\\n        pos = self.get_metadata(PositionProvider, node).start\\n        print(\\n            f\"Function \\'{function_name}\\' called at line {pos.line}, column {pos.column} with arguments:\"\\n        )\\n\\n        for arg in node.args:\\n            arg_start_pos = self.get_metadata(PositionProvider, arg).start\\n            arg_value = arg.value\\n            if isinstance(arg_value, cst.SimpleString):\\n                arg_value = arg_value.evaluated_value\\n            print(\\n                f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\\n            )\\n',\n",
       " 'def __init__(self):\\n    self.count = 0\\n    self.functions_with_operation_dict = {}\\n',\n",
       " '\\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n    self.current_function = node\\n    self.functions_with_operation_dict[node.name] = []\\n',\n",
       " '\\ndef leave_FunctionDef(self, node: cst.FunctionDef) -> None:\\n    self.current_function = None\\n',\n",
       " '\\ndef visit_BinaryOperation(self, node: cst.BinaryOperation) -> None:\\n    if isinstance(node.operator, cst.Multiply) or isinstance(\\n        node.operator, cst.BitAnd\\n    ):\\n        self.count += 1\\n        if self.current_function:\\n            self.functions_with_operation_dict[self.current_function.name].append(\\n                cst.Module([]).code_for_node(node)\\n            )\\n',\n",
       " '\\ndef visit_Call(self, node: cst.Call) -> None:\\n    if m.matches(node, m.Call(func=m.Attribute(attr=m.Name(\"dot\")))) or m.matches(\\n        node, m.Call(func=m.Name(\"dot\"), args=[m.Arg(), m.Arg()])\\n    ):\\n        self.count += 1\\n        if self.current_function:\\n            self.functions_with_operation_dict[self.current_function.name].append(\\n                cst.Module([]).code_for_node(node)\\n            )\\n',\n",
       " 'def __init__(self):\\n    self.function_source_codes = []\\n    self.function_nodes = []\\n    self.class_source_codes = []\\n    self.class_nodes = []\\n',\n",
       " '\\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n    function_source_code = cst.Module([]).code_for_node(node)\\n    #add in place summary and code mod\\n    self.function_nodes.append(node)\\n    self.function_source_codes.append(function_source_code)\\n',\n",
       " '\\n\\ndef visit_ClassDef(self, node: cst.ClassDef) -> None:\\n    class_source_code = cst.Module([]).code_for_node(node)\\n    #add in place summary and code mod\\n    self.class_nodes.append(node)\\n    self.class_source_codes.append(class_source_code)\\n',\n",
       " 'def __init__(self):\\n    # stack for storing the canonical name of the current function\\n    self.stack: List[Tuple[str, ...]] = []\\n    # store the annotations\\n    self.annotations: Dict[\\n        Tuple[str, ...],  # key: tuple of canonical class/function name\\n        Tuple[cst.Parameters, Optional[cst.Annotation]],  # value: (params, returns)\\n    ] = {}\\n',\n",
       " '\\ndef visit_ClassDef(self, node: cst.ClassDef) -> Optional[bool]:\\n    self.stack.append(node.name.value)\\n',\n",
       " '\\ndef leave_ClassDef(self, node: cst.ClassDef) -> None:\\n    self.stack.pop()\\n',\n",
       " \"\\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> Optional[bool]:\\n    self.stack.append(node.name.value)\\n    self.annotations[tuple(self.stack)] = (node.params, node.returns)\\n    return False  # pyi files don't support inner functions, return False to stop the traversal.\\n\",\n",
       " '\\ndef leave_FunctionDef(self, node: cst.FunctionDef) -> None:\\n    self.stack.pop()\\n',\n",
       " 'from typing import Dict, List, Optional, Tuple\\n\\nimport libcst as cst\\nimport libcst.matchers as m\\nfrom libcst.metadata import PositionProvider\\n\\n\\n# A custom visitor to find function calls and their arguments\\nclass FunctionCallFinder(cst.CSTVisitor):\\n    METADATA_DEPENDENCIES = (PositionProvider,)\\n\\n    def visit_Call(self, node: cst.Call) -> None:\\n        function_name = None\\n        if isinstance(node.func, cst.Name):\\n            function_name = node.func.value\\n\\n        if function_name:\\n            pos = self.get_metadata(PositionProvider, node).start\\n            print(\\n                f\"Function \\'{function_name}\\' called at line {pos.line}, column {pos.column} with arguments:\"\\n            )\\n\\n            for arg in node.args:\\n                arg_start_pos = self.get_metadata(PositionProvider, arg).start\\n                arg_value = arg.value\\n                if isinstance(arg_value, cst.SimpleString):\\n                    arg_value = arg_value.evaluated_value\\n                print(\\n                    f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\\n                )\\n\\n\\nclass MultiplicationCounterVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.count = 0\\n        self.functions_with_operation_dict = {}\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        self.current_function = node\\n        self.functions_with_operation_dict[node.name] = []\\n\\n    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        self.current_function = None\\n\\n    def visit_BinaryOperation(self, node: cst.BinaryOperation) -> None:\\n        if isinstance(node.operator, cst.Multiply) or isinstance(\\n            node.operator, cst.BitAnd\\n        ):\\n            self.count += 1\\n            if self.current_function:\\n                self.functions_with_operation_dict[self.current_function.name].append(\\n                    cst.Module([]).code_for_node(node)\\n                )\\n\\n    def visit_Call(self, node: cst.Call) -> None:\\n        if m.matches(node, m.Call(func=m.Attribute(attr=m.Name(\"dot\")))) or m.matches(\\n            node, m.Call(func=m.Name(\"dot\"), args=[m.Arg(), m.Arg()])\\n        ):\\n            self.count += 1\\n            if self.current_function:\\n                self.functions_with_operation_dict[self.current_function.name].append(\\n                    cst.Module([]).code_for_node(node)\\n                )\\n\\n\\nclass FunctionAndClassVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        function_source_code = cst.Module([]).code_for_node(node)\\n        #add in place summary and code mod\\n        self.function_nodes.append(node)\\n        self.function_source_codes.append(function_source_code)\\n\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\\n        class_source_code = cst.Module([]).code_for_node(node)\\n        #add in place summary and code mod\\n        self.class_nodes.append(node)\\n        self.class_source_codes.append(class_source_code)\\n\\n\\n\\nclass TypingCollector(cst.CSTVisitor):\\n    def __init__(self):\\n        # stack for storing the canonical name of the current function\\n        self.stack: List[Tuple[str, ...]] = []\\n        # store the annotations\\n        self.annotations: Dict[\\n            Tuple[str, ...],  # key: tuple of canonical class/function name\\n            Tuple[cst.Parameters, Optional[cst.Annotation]],  # value: (params, returns)\\n        ] = {}\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> Optional[bool]:\\n        self.stack.append(node.name.value)\\n\\n    def leave_ClassDef(self, node: cst.ClassDef) -> None:\\n        self.stack.pop()\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> Optional[bool]:\\n        self.stack.append(node.name.value)\\n        self.annotations[tuple(self.stack)] = (node.params, node.returns)\\n        return False  # pyi files don\\'t support inner functions, return False to stop the traversal.\\n\\n    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        self.stack.pop()\\n',\n",
       " 'def __init__(self, repo_name):\\n    self.g = Github()\\n    self.repo = self.g.get_repo(repo_name)\\n',\n",
       " '\\ndef get_issues(self, state=\"open\"):\\n    \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n    issues = []\\n    for issue in self.repo.get_issues(state=state):\\n        issues.append(issue)\\n    return issues\\n',\n",
       " '\\ndef parse_issues(self, state=\"open\"):\\n    \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n    parsed_issues = []\\n    issues = self.get_issues(state=state)\\n    for issue in issues:\\n        parsed_issue = {\\n            \"number\": issue.number,\\n            \"title\": issue.title,\\n            \"body\": issue.body,\\n            \"labels\": [label.name for label in issue.labels],\\n        }\\n        parsed_issues.append(parsed_issue)\\n    return parsed_issues\\n',\n",
       " '\\ndef get_commits(self):\\n    \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n    commits = []\\n    branch = self.repo.get_branch(\"main\")\\n    for commit in self.repo.get_commits(sha=branch.commit.sha):\\n        commits.append(commit)\\n    return commits\\n',\n",
       " '\\ndef parse_commits(self):\\n    \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n    parsed_commits = []\\n    commits = self.get_commits()\\n    for commit in commits:\\n        parsed_commit = {\\n            \"sha\": commit.sha,\\n            \"message\": commit.commit.message,\\n            \"author\": {\\n                \"name\": commit.commit.author.name,\\n                \"email\": commit.commit.author.email,\\n                \"date\": commit.commit.author.date,\\n            },\\n        }\\n        parsed_commits.append(parsed_commit)\\n    return parsed_commits\\n',\n",
       " 'from github import Github\\n\\n\\nclass IssueParser:\\n    def __init__(self, repo_name):\\n        self.g = Github()\\n        self.repo = self.g.get_repo(repo_name)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n\\nclass CommitParser:\\n    def __init__(self, repo_name):\\n        self.g = Github()\\n        self.repo = self.g.get_repo(repo_name)\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n',\n",
       " 'def __init__(self, code: str = None):\\n\\n    self.code = code\\n    self.output_code = None\\n',\n",
       " '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n',\n",
       " '\\ndef get_minified_code(self):\\n    if not self.output_code:\\n        self.minify()\\n    return self.output_code\\n',\n",
       " '\\n@staticmethod\\ndef minify_code(code: str) -> str:\\n    return minify(code)\\n',\n",
       " '@staticmethod\\ndef extract_docstring(function_def: cst.FunctionDef) -> str:\\n    docstring = None\\n\\n    for stmt in function_def.body.body:\\n        if isinstance(stmt, cst.SimpleStatementLine):\\n            for expr in stmt.body:\\n                if isinstance(expr, cst.Expr) and isinstance(expr.value, cst.SimpleString):\\n                    docstring = expr.value.value.strip(\\'\"\\').strip(\"\\'\")\\n                    break\\n        if docstring is not None:\\n            break\\n\\n    if docstring is not None:\\n        return docstring.strip()\\n    else:\\n        function_name = function_def.name.value\\n        return f\"No docstring provided for function \\'{function_name}\\'. Please add a docstring to describe this function.\"\\n',\n",
       " 'def __init__(self, username, repo_name):\\n    super().__init__()\\n    self.username = username\\n    self.repo_name = repo_name\\n    self.parser = GitHubRepoProcessor(username, repo_name)\\n    self.minifier = PythonMinifier()\\n    self.docstring_extractor = PythonDocstringExtractor()\\n    self.directory_parser = None\\n    self.min_code_index = None\\n    self.doc_string_index = None\\n    self.libcst_node_index = None\\n',\n",
       " '\\ndef create_code_index(self, base_directory):\\n    self.directory_parser = self.parser.process_repo(base_directory)\\n    code_values, code_nodes = self.parser.get_values()\\n    self.code_index = self.init_index(values=code_values)\\n    self.code_index.save()\\n',\n",
       " '\\ndef create_indexes(self, base_directory):\\n    self.directory_parser = self.parser.process_repo(base_directory)\\n    code_values, code_nodes = self.parser.get_values()\\n    self.code_index = self.init_index(values=code_values)\\n\\n    min_code_values = []\\n    doc_string_values = []\\n    for code_value, code_node in zip(code_values, code_nodes):\\n        minifier = PythonMinifier(code=code_value)\\n        min_code = minifier.get_minified_code()\\n        doc_string = self.docstring_extractor.extract_docstring(code_node)\\n        min_code_values.append(min_code)\\n        doc_string_values.append(doc_string)\\n    self.doc_string_index = self.init_index(values=doc_string_values)\\n    self.min_code_index = self.init_index(values=min_code_values)\\n',\n",
       " 'from typing import List, Optional, Union\\n\\nimport libcst as cst\\nfrom babydragon.memory.indexes.memory_index import MemoryIndex\\nfrom babydragon.working_memory.parsers.git_processor import GitHubRepoProcessor\\n\\nfrom python_minifier import minify\\n\\n\\nclass PythonMinifier:\\n    def __init__(self, code: str = None):\\n\\n        self.code = code\\n        self.output_code = None\\n\\n    def minify(self):\\n        if self.code:\\n            self.output_code = self.minify_code(self.code)\\n\\n    def get_minified_code(self):\\n        if not self.output_code:\\n            self.minify()\\n        return self.output_code\\n\\n    @staticmethod\\n    def minify_code(code: str) -> str:\\n        return minify(code)\\n\\nclass PythonDocstringExtractor:\\n    @staticmethod\\n    def extract_docstring(function_def: cst.FunctionDef) -> str:\\n        docstring = None\\n\\n        for stmt in function_def.body.body:\\n            if isinstance(stmt, cst.SimpleStatementLine):\\n                for expr in stmt.body:\\n                    if isinstance(expr, cst.Expr) and isinstance(expr.value, cst.SimpleString):\\n                        docstring = expr.value.value.strip(\\'\"\\').strip(\"\\'\")\\n                        break\\n            if docstring is not None:\\n                break\\n\\n        if docstring is not None:\\n            return docstring.strip()\\n        else:\\n            function_name = function_def.name.value\\n            return f\"No docstring provided for function \\'{function_name}\\'. Please add a docstring to describe this function.\"\\n\\n\\nclass GitMemory(MemoryIndex):\\n    def __init__(self, username, repo_name):\\n        super().__init__()\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.parser = GitHubRepoProcessor(username, repo_name)\\n        self.minifier = PythonMinifier()\\n        self.docstring_extractor = PythonDocstringExtractor()\\n        self.directory_parser = None\\n        self.min_code_index = None\\n        self.doc_string_index = None\\n        self.libcst_node_index = None\\n    \\n    def create_code_index(self, base_directory):\\n        self.directory_parser = self.parser.process_repo(base_directory)\\n        code_values, code_nodes = self.parser.get_values()\\n        self.code_index = self.init_index(values=code_values)\\n        self.code_index.save()\\n\\n    def create_indexes(self, base_directory):\\n        self.directory_parser = self.parser.process_repo(base_directory)\\n        code_values, code_nodes = self.parser.get_values()\\n        self.code_index = self.init_index(values=code_values)\\n\\n        min_code_values = []\\n        doc_string_values = []\\n        for code_value, code_node in zip(code_values, code_nodes):\\n            minifier = PythonMinifier(code=code_value)\\n            min_code = minifier.get_minified_code()\\n            doc_string = self.docstring_extractor.extract_docstring(code_node)\\n            min_code_values.append(min_code)\\n            doc_string_values.append(doc_string)\\n        self.doc_string_index = self.init_index(values=doc_string_values)\\n        self.min_code_index = self.init_index(values=min_code_values)\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '\\n\\n@nb.jit(nopython=True)\\ndef boltzmann_acceptance_prob(new_score, current_score):\\n    if new_score <= current_score:\\n        return np.exp(new_score - current_score)\\n    else:\\n        return 1\\n',\n",
       " '\\n\\n@nb.jit(nopython=True)\\ndef custom_acceptance_prob(new_score, current_score, alpha):\\n    epsilon = 1e-8\\n    if new_score <= current_score:\\n        return (new_score + epsilon) / (current_score + epsilon) ** alpha\\n    else:\\n        return 1\\n',\n",
       " '\\n\\n@nb.jit(nopython=True)\\ndef simple_ratio_acceptance_prob(new_score, current_score):\\n    epsilon = 1e-8\\n    if new_score <= current_score:\\n        return (new_score + epsilon) / (current_score + epsilon)\\n    else:\\n        return 1\\n',\n",
       " '\\n\\n@nb.jit(nopython=False)\\ndef gaussian_acceptance_prob(new_score, current_score, mu, sigma):\\n    curr_prob = normal(x=current_score, mu=mu, sigma=sigma)\\n    move_prob = normal(x=new_score, mu=mu, sigma=sigma)\\n    acceptance = min(move_prob / curr_prob, 1)\\n    return acceptance\\n',\n",
       " '\\n\\ndef group_strings_by_index(strings, community_labels):\\n    groups = defaultdict(list)\\n    for i, s in enumerate(strings):\\n        groups[community_labels[i]].append(s)\\n    return groups\\n',\n",
       " '\\n\\ndef metropolis_hastings(\\n    graph, num_communities, num_iterations, acceptance_metric=\"custom\", custom_alpha=0.5\\n):\\n    \"\"\"\\n    Implement the Metropolis-Hastings sampling-based community detection.\\n\\n    Parameters:\\n    graph (numpy array): The producer-producer similarity graph.\\n    num_communities (int): The number of communities to be detected.\\n    num_iterations (int): The number of iterations for the Metropolis-Hastings algorithm.\\n    acceptance_metric (str): The acceptance probability metric (\\'boltzmann\\', \\'custom\\', \\'simple_ratio\\', or \\'gaussian_mcmc\\').\\n    custom_alpha (float): Custom acceptance probability parameter (only used when acceptance_metric=\\'custom\\').\\n\\n    Returns:\\n    community_labels (list): A list containing the community affiliation for each producer.\\n    \"\"\"\\n\\n    num_producers = graph.shape[0]\\n\\n    # Initialize the community labels randomly\\n    community_labels = [\\n        random.randint(0, num_communities - 1) for _ in range(num_producers)\\n    ]\\n\\n    # Calculate scores for the graph to estimate the Gaussian parameters (mu and sigma)\\n    scores = [\\n        sum(\\n            graph[i, j]\\n            for j in range(num_producers)\\n            if community_labels[j] == community_labels[i]\\n        )\\n        for i in range(num_producers)\\n    ]\\n    if acceptance_metric == \"gaussian_mcmc\":\\n        distribution = estimate_pdf(scores)\\n        mu = calc_shgo_mode(scores, distribution)[0]\\n        sigma = np.std(scores)\\n\\n    for _ in range(num_iterations):\\n        for producer in range(num_producers):\\n            # Calculate the current community score\\n            current_community = community_labels[producer]\\n            current_score = sum(\\n                graph[producer, j]\\n                for j in range(num_producers)\\n                if community_labels[j] == current_community\\n            )\\n\\n            # Choose a new community for the producer\\n            new_community = random.randint(0, num_communities - 1)\\n\\n            # Calculate the score for the new community\\n            new_score = sum(\\n                graph[producer, j]\\n                for j in range(num_producers)\\n                if community_labels[j] == new_community\\n            )\\n\\n            # Calculate the acceptance probability\\n            if acceptance_metric == \"boltzmann\":\\n                acceptance_prob = boltzmann_acceptance_prob(new_score, current_score)\\n            elif acceptance_metric == \"custom\":\\n                acceptance_prob = custom_acceptance_prob(\\n                    new_score, current_score, custom_alpha\\n                )\\n            elif acceptance_metric == \"simple_ratio\":\\n                acceptance_prob = simple_ratio_acceptance_prob(new_score, current_score)\\n            elif acceptance_metric == \"gaussian_mcmc\":\\n                acceptance_prob = gaussian_acceptance_prob(\\n                    new_score, current_score, mu, sigma\\n                )\\n            else:\\n                raise ValueError(\\n                    \"Invalid acceptance_metric value. Acceptable values are \\'boltzmann\\', \\'custom\\', \\'simple_ratio\\', or \\'gaussian_mcmc\\'.\"\\n                )\\n\\n            # Accept or reject the new community\\n            if random.random() < acceptance_prob:\\n                community_labels[producer] = new_community\\n\\n    return community_labels\\n',\n",
       " 'import random\\nfrom collections import defaultdict\\n\\nimport numba as nb\\nimport numpy as np\\n\\nfrom babydragon.working_memory.associative_memory.probability_density_functions import (\\n    calc_shgo_mode, estimate_pdf, normal)\\n\\n\\n@nb.jit(nopython=True)\\ndef boltzmann_acceptance_prob(new_score, current_score):\\n    if new_score <= current_score:\\n        return np.exp(new_score - current_score)\\n    else:\\n        return 1\\n\\n\\n@nb.jit(nopython=True)\\ndef custom_acceptance_prob(new_score, current_score, alpha):\\n    epsilon = 1e-8\\n    if new_score <= current_score:\\n        return (new_score + epsilon) / (current_score + epsilon) ** alpha\\n    else:\\n        return 1\\n\\n\\n@nb.jit(nopython=True)\\ndef simple_ratio_acceptance_prob(new_score, current_score):\\n    epsilon = 1e-8\\n    if new_score <= current_score:\\n        return (new_score + epsilon) / (current_score + epsilon)\\n    else:\\n        return 1\\n\\n\\n@nb.jit(nopython=False)\\ndef gaussian_acceptance_prob(new_score, current_score, mu, sigma):\\n    curr_prob = normal(x=current_score, mu=mu, sigma=sigma)\\n    move_prob = normal(x=new_score, mu=mu, sigma=sigma)\\n    acceptance = min(move_prob / curr_prob, 1)\\n    return acceptance\\n\\n\\ndef group_strings_by_index(strings, community_labels):\\n    groups = defaultdict(list)\\n    for i, s in enumerate(strings):\\n        groups[community_labels[i]].append(s)\\n    return groups\\n\\n\\ndef metropolis_hastings(\\n    graph, num_communities, num_iterations, acceptance_metric=\"custom\", custom_alpha=0.5\\n):\\n    \"\"\"\\n    Implement the Metropolis-Hastings sampling-based community detection.\\n\\n    Parameters:\\n    graph (numpy array): The producer-producer similarity graph.\\n    num_communities (int): The number of communities to be detected.\\n    num_iterations (int): The number of iterations for the Metropolis-Hastings algorithm.\\n    acceptance_metric (str): The acceptance probability metric (\\'boltzmann\\', \\'custom\\', \\'simple_ratio\\', or \\'gaussian_mcmc\\').\\n    custom_alpha (float): Custom acceptance probability parameter (only used when acceptance_metric=\\'custom\\').\\n\\n    Returns:\\n    community_labels (list): A list containing the community affiliation for each producer.\\n    \"\"\"\\n\\n    num_producers = graph.shape[0]\\n\\n    # Initialize the community labels randomly\\n    community_labels = [\\n        random.randint(0, num_communities - 1) for _ in range(num_producers)\\n    ]\\n\\n    # Calculate scores for the graph to estimate the Gaussian parameters (mu and sigma)\\n    scores = [\\n        sum(\\n            graph[i, j]\\n            for j in range(num_producers)\\n            if community_labels[j] == community_labels[i]\\n        )\\n        for i in range(num_producers)\\n    ]\\n    if acceptance_metric == \"gaussian_mcmc\":\\n        distribution = estimate_pdf(scores)\\n        mu = calc_shgo_mode(scores, distribution)[0]\\n        sigma = np.std(scores)\\n\\n    for _ in range(num_iterations):\\n        for producer in range(num_producers):\\n            # Calculate the current community score\\n            current_community = community_labels[producer]\\n            current_score = sum(\\n                graph[producer, j]\\n                for j in range(num_producers)\\n                if community_labels[j] == current_community\\n            )\\n\\n            # Choose a new community for the producer\\n            new_community = random.randint(0, num_communities - 1)\\n\\n            # Calculate the score for the new community\\n            new_score = sum(\\n                graph[producer, j]\\n                for j in range(num_producers)\\n                if community_labels[j] == new_community\\n            )\\n\\n            # Calculate the acceptance probability\\n            if acceptance_metric == \"boltzmann\":\\n                acceptance_prob = boltzmann_acceptance_prob(new_score, current_score)\\n            elif acceptance_metric == \"custom\":\\n                acceptance_prob = custom_acceptance_prob(\\n                    new_score, current_score, custom_alpha\\n                )\\n            elif acceptance_metric == \"simple_ratio\":\\n                acceptance_prob = simple_ratio_acceptance_prob(new_score, current_score)\\n            elif acceptance_metric == \"gaussian_mcmc\":\\n                acceptance_prob = gaussian_acceptance_prob(\\n                    new_score, current_score, mu, sigma\\n                )\\n            else:\\n                raise ValueError(\\n                    \"Invalid acceptance_metric value. Acceptable values are \\'boltzmann\\', \\'custom\\', \\'simple_ratio\\', or \\'gaussian_mcmc\\'.\"\\n                )\\n\\n            # Accept or reject the new community\\n            if random.random() < acceptance_prob:\\n                community_labels[producer] = new_community\\n\\n    return community_labels\\n',\n",
       " '\\n\\ndef softmax(x: np.ndarray, axis: int = 1) -> np.ndarray:\\n    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\\n',\n",
       " '\\n\\ndef group_items_by_rank_buckets(\\n    code_strings: List[str],\\n    node_embeddings: np.ndarray,\\n    S_vectors: np.ndarray,\\n    S_vectors_type: str,\\n    component_window_size: int = 5,\\n    use_softmax: bool = True,\\n) -> List[Tuple[List[str], np.ndarray]]:\\n    if S_vectors_type not in (\"U\", \"Vt\"):\\n        raise ValueError(\"Invalid S_vectors_type value. It must be either \\'U\\' or \\'Vt\\'.\")\\n\\n    if use_softmax:\\n        S_vectors = softmax(S_vectors, axis=1)\\n\\n    num_buckets = S_vectors.shape[0] // component_window_size\\n    rank_buckets = []\\n\\n    for i in range(num_buckets):\\n        start_idx = i * component_window_size\\n        end_idx = (i + 1) * component_window_size\\n\\n        if S_vectors_type == \"U\":\\n            target_matrix = S_vectors[start_idx:end_idx, :]\\n            contributions = np.sum(np.abs(target_matrix), axis=0)\\n        elif S_vectors_type == \"Vt\":\\n            target_matrix = S_vectors[:, start_idx:end_idx]\\n            contributions = np.sum(np.abs(target_matrix), axis=1)\\n\\n        indexes = np.argsort(contributions)[::-1]\\n\\n        # Filter out any indexes that are out of range\\n        indexes = [idx for idx in indexes if idx < len(code_strings)]\\n\\n        sorted_code_strings = [code_strings[idx] for idx in indexes]\\n        sorted_node_embeddings = node_embeddings[indexes]\\n\\n        rank_buckets.append((sorted_code_strings, sorted_node_embeddings))\\n\\n    return rank_buckets\\n',\n",
       " 'from typing import List, Tuple\\n\\nimport numpy as np\\n\\n\\ndef softmax(x: np.ndarray, axis: int = 1) -> np.ndarray:\\n    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\\n\\n\\ndef group_items_by_rank_buckets(\\n    code_strings: List[str],\\n    node_embeddings: np.ndarray,\\n    S_vectors: np.ndarray,\\n    S_vectors_type: str,\\n    component_window_size: int = 5,\\n    use_softmax: bool = True,\\n) -> List[Tuple[List[str], np.ndarray]]:\\n    if S_vectors_type not in (\"U\", \"Vt\"):\\n        raise ValueError(\"Invalid S_vectors_type value. It must be either \\'U\\' or \\'Vt\\'.\")\\n\\n    if use_softmax:\\n        S_vectors = softmax(S_vectors, axis=1)\\n\\n    num_buckets = S_vectors.shape[0] // component_window_size\\n    rank_buckets = []\\n\\n    for i in range(num_buckets):\\n        start_idx = i * component_window_size\\n        end_idx = (i + 1) * component_window_size\\n\\n        if S_vectors_type == \"U\":\\n            target_matrix = S_vectors[start_idx:end_idx, :]\\n            contributions = np.sum(np.abs(target_matrix), axis=0)\\n        elif S_vectors_type == \"Vt\":\\n            target_matrix = S_vectors[:, start_idx:end_idx]\\n            contributions = np.sum(np.abs(target_matrix), axis=1)\\n\\n        indexes = np.argsort(contributions)[::-1]\\n\\n        # Filter out any indexes that are out of range\\n        indexes = [idx for idx in indexes if idx < len(code_strings)]\\n\\n        sorted_code_strings = [code_strings[idx] for idx in indexes]\\n        sorted_node_embeddings = node_embeddings[indexes]\\n\\n        rank_buckets.append((sorted_code_strings, sorted_node_embeddings))\\n\\n    return rank_buckets\\n',\n",
       " '\\n\\ndef normal(x, mu, sigma):\\n    numerator = np.exp((-((x - mu) ** 2)) / (2 * sigma**2))\\n    denominator = sigma * np.sqrt(2 * np.pi)\\n    return numerator / denominator\\n',\n",
       " '\\n\\ndef calc_shgo_mode(scores, distribution):\\n    \"\"\"\\n    Calculates the mode of a distribution using the SHGO optimization method.\\n    :scores: list of distance scores\\n    :distribution: probability density function estimated from the scores\\n    :return: mode of the distribution\\n    \"\"\"\\n\\n    def objective(x):\\n        return -distribution(x)\\n\\n    bounds = [(min(scores), max(scores))]\\n    result = scipy.optimize.shgo(objective, bounds)\\n    return result.x\\n',\n",
       " '\\ndef objective(x):\\n    return -distribution(x)\\n',\n",
       " '\\n\\ndef estimate_pdf(scores):\\n    \"\"\"\\n    estimate scores probability density function\\n    :scores: list of distance scores from topic features to topic centroid\\n    :return: mean and standard deviation of the estimated distribution\\n    \"\"\"\\n    pdf = scipy.stats.gaussian_kde(scores)\\n    return pdf\\n',\n",
       " 'import numpy as np\\nimport scipy.optimize\\nimport scipy.stats\\n\\n\\ndef normal(x, mu, sigma):\\n    numerator = np.exp((-((x - mu) ** 2)) / (2 * sigma**2))\\n    denominator = sigma * np.sqrt(2 * np.pi)\\n    return numerator / denominator\\n\\n\\ndef calc_shgo_mode(scores, distribution):\\n    \"\"\"\\n    Calculates the mode of a distribution using the SHGO optimization method.\\n    :scores: list of distance scores\\n    :distribution: probability density function estimated from the scores\\n    :return: mode of the distribution\\n    \"\"\"\\n\\n    def objective(x):\\n        return -distribution(x)\\n\\n    bounds = [(min(scores), max(scores))]\\n    result = scipy.optimize.shgo(objective, bounds)\\n    return result.x\\n\\n\\ndef estimate_pdf(scores):\\n    \"\"\"\\n    estimate scores probability density function\\n    :scores: list of distance scores from topic features to topic centroid\\n    :return: mean and standard deviation of the estimated distribution\\n    \"\"\"\\n    pdf = scipy.stats.gaussian_kde(scores)\\n    return pdf\\n',\n",
       " '\\n\\ndef nmi_similarity(community_labels1, community_labels2):\\n    \"\"\"\\n    Calculate the Normalized Mutual Information (NMI) between two sets of community labels.\\n\\n    Args:\\n    community_labels1 (list): The first set of community labels.\\n    community_labels2 (list): The second set of community labels.\\n\\n    Returns:\\n    float: The NMI similarity score.\\n    \"\"\"\\n    return normalized_mutual_info_score(community_labels1, community_labels2)\\n',\n",
       " '\\n\\ndef nmi_stability_analysis(kernel, num_runs, num_communities, num_iterations=100):\\n    \"\"\"\\n    Perform stability analysis on the kernel-based community detection algorithm using NMI.\\n\\n    Args:\\n    kernel (function): The kernel function.\\n    num_runs (int): The number of times to run the community detection algorithm.\\n    num_communities (int): The number of communities.\\n    num_iterations (int): The number of iterations for the Metropolis-Hastings algorithm.\\n\\n    Returns:\\n    float: The average NMI similarity score.\\n    \"\"\"\\n    community_label_sets = [\\n        metropolis_hastings(\\n            kernel, num_communities=num_communities, num_iterations=num_iterations\\n        )\\n        for _ in range(num_runs)\\n    ]\\n\\n    nmi_similarities = [\\n        nmi_similarity(community_label_sets[i], community_label_sets[j])\\n        for i in range(num_runs)\\n        for j in range(i + 1, num_runs)\\n    ]\\n\\n    return np.mean(nmi_similarities)\\n',\n",
       " '\\n\\ndef run_stability_analysis(\\n    kernels, kernel_labels, optimal_num_communities, num_runs=10\\n):\\n    kernel_stability_nmi = {\\n        label: nmi_stability_analysis(\\n            kernel, num_runs, optimal_communities, num_iterations=100\\n        )\\n        for label, kernel, optimal_communities in zip(\\n            kernel_labels, kernels, optimal_num_communities.values()\\n        )\\n    }\\n\\n    # Print the stability results for each kernel using NMI\\n    out_dict = {}\\n    for kernel, stability in kernel_stability_nmi.items():\\n        out_dict[kernel] = stability\\n    return out_dict\\n',\n",
       " 'import numpy as np\\nfrom sklearn.metrics import normalized_mutual_info_score\\n\\nfrom babydragon.working_memory.associative_memory.metropolis_hastings import \\\\\\n    metropolis_hastings\\n\\n\\ndef nmi_similarity(community_labels1, community_labels2):\\n    \"\"\"\\n    Calculate the Normalized Mutual Information (NMI) between two sets of community labels.\\n\\n    Args:\\n    community_labels1 (list): The first set of community labels.\\n    community_labels2 (list): The second set of community labels.\\n\\n    Returns:\\n    float: The NMI similarity score.\\n    \"\"\"\\n    return normalized_mutual_info_score(community_labels1, community_labels2)\\n\\n\\ndef nmi_stability_analysis(kernel, num_runs, num_communities, num_iterations=100):\\n    \"\"\"\\n    Perform stability analysis on the kernel-based community detection algorithm using NMI.\\n\\n    Args:\\n    kernel (function): The kernel function.\\n    num_runs (int): The number of times to run the community detection algorithm.\\n    num_communities (int): The number of communities.\\n    num_iterations (int): The number of iterations for the Metropolis-Hastings algorithm.\\n\\n    Returns:\\n    float: The average NMI similarity score.\\n    \"\"\"\\n    community_label_sets = [\\n        metropolis_hastings(\\n            kernel, num_communities=num_communities, num_iterations=num_iterations\\n        )\\n        for _ in range(num_runs)\\n    ]\\n\\n    nmi_similarities = [\\n        nmi_similarity(community_label_sets[i], community_label_sets[j])\\n        for i in range(num_runs)\\n        for j in range(i + 1, num_runs)\\n    ]\\n\\n    return np.mean(nmi_similarities)\\n\\n\\ndef run_stability_analysis(\\n    kernels, kernel_labels, optimal_num_communities, num_runs=10\\n):\\n    kernel_stability_nmi = {\\n        label: nmi_stability_analysis(\\n            kernel, num_runs, optimal_communities, num_iterations=100\\n        )\\n        for label, kernel, optimal_communities in zip(\\n            kernel_labels, kernels, optimal_num_communities.values()\\n        )\\n    }\\n\\n    # Print the stability results for each kernel using NMI\\n    out_dict = {}\\n    for kernel, stability in kernel_stability_nmi.items():\\n        out_dict[kernel] = stability\\n    return out_dict\\n',\n",
       " \"def __init__(self, name= 'vector_memory', max_context = 2048, use_mark = False):\\n    BaseThread.__init__(self,name= name , max_memory= None)\\n    MemoryIndex.__init__(self, index = None, name = name)\\n    self.max_context = max_context\\n    self.use_mark = use_mark\\n    self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n    \\n\",\n",
       " 'def index_message(self,message: str , verbose: bool =False):\\n    \"\"\"index a message in the faiss index, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated\\n        \"\"\"\\n    \\n    self.add_to_index(value = message, verbose = verbose)\\n',\n",
       " '\\ndef add_message(self, message_dict: dict, verbose: bool = False):\\n    \"\"\"add a message to the memory thread, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\\n        \"\"\"\\n    # print(\"checking the dict\")\\n    message_dict = check_dict(message_dict)\\n    # print(\"trying to add the message\")\\n    BaseThread.add_message(self,message_dict)\\n    # print(message_dict)\\n    message = message_dict[\"content\"]\\n    self.index_message(message, verbose = verbose)\\n    return True\\n',\n",
       " '\\ndef token_bound_query(self, query, k: int =10, max_tokens: int =4000):\\n    \"\"\" returns the k most similar messages to the query, sorted in similarity order\"\"\"\\n    if self.use_mark:\\n        query = mark_question(query)\\n    return MemoryIndex.token_bound_query(self, query, k, max_tokens)\\n',\n",
       " '\\ndef sorted_query(self, query, k: int =10, max_tokens: int =4000, reverse: bool = False, return_from_thread = True):\\n    \"\"\" returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n    #sort the messages \\n    \\n    sorted_messages = [unsorted_messages[i] for i in sorted(range(len(unsorted_messages)), key=lambda k: unsorted_indices[k])]\\n    sorted_scores = [unsorted_scores[i] for i in sorted(range(len(unsorted_scores)), key=lambda k: unsorted_indices[k])]\\n    sorted_indices = [unsorted_indices[i] for i in sorted(range(len(unsorted_indices)), key=lambda k: unsorted_indices[k])]\\n    if reverse:\\n        sorted_messages.reverse()\\n        sorted_scores.reverse()\\n        sorted_indices.reverse()\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n    return sorted_messages, sorted_scores, sorted_indices\\n',\n",
       " '\\ndef weighted_query(self, query, k: int = 10, max_tokens: int = 4000, decay_factor: float = 0.1, temporal_weight: float = 0.5, order_by: str = \\'chronological\\', reverse: bool = False) -> list:\\n    \"\"\" Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\\n        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\\n        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\\n        The order_by parameter controls the order of the results. If it is set to \\'similarity\\', the results are sorted in similarity order. If it is set to \\'chronological\\', the results are sorted in chronological order with the most recent message first.\\n        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\\n        \"\"\"\\n    # Validate order_by parameter\\n    if order_by not in (\\'similarity\\', \\'chronological\\'):\\n        raise ValueError(\"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\")\\n\\n    # Get similarity-based results\\n    sim_messages, sim_scores, sim_indices = self.sorted_query(query, k, max_tokens=max_tokens)\\n    \\n    # Get token-bound history\\n    hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\\n    \\n    # Combine messages and indices\\n    combined_messages = sim_messages + hist_messages\\n    combined_indices = sim_indices + hist_indices\\n    \\n    # Create the local_index and populate it\\n    self.local_index = MemoryIndex(name=\\'local_index\\')\\n    for message in combined_messages:\\n        self.local_index.add_to_index(value=message, verbose=False)\\n    \\n    # Perform a new query on the combined index\\n    new_query_results, new_query_scores, new_query_indices = self.local_index.token_bound_query(query, k=len(combined_messages), max_tokens=max_tokens)\\n    \\n    # Compute temporal weights\\n    temporal_weights = [np.exp(-decay_factor * i) for i in range(len(combined_messages))]\\n    temporal_weights = [w / sum(temporal_weights) for w in temporal_weights]  # Normalize the temporal weights\\n    \\n    # Combine similarity scores and temporal weights\\n    weighted_scores = []\\n    for i in range(len(new_query_scores)):\\n        sim_score = new_query_scores[i]\\n        temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\\n        weighted_score = (1 - temporal_weight) * sim_score + temporal_weight * temp_weight\\n        weighted_scores.append(weighted_score)\\n    \\n    # Sort the results based on the order_by parameter\\n    if order_by == \\'similarity\\':\\n        sorting_key = lambda k: weighted_scores[k]\\n    elif order_by == \\'chronological\\':  # order_by == \\'chronological\\'\\n        sorting_key = lambda k: new_query_indices[k]\\n    else:\\n        raise ValueError(\"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\")\\n\\n    sorted_indices = [new_query_indices[i] for i in sorted(range(len(new_query_indices)), key=sorting_key, reverse=not reverse)]\\n    sorted_results = [new_query_results[i] for i in sorted(range(len(new_query_results)), key=sorting_key, reverse=not reverse)]\\n    sorted_scores = [weighted_scores[i] for i in sorted(range(len(weighted_scores)), key=sorting_key, reverse=not reverse)]\\n\\n    # Return only the top k results without exceeding max_tokens\\n    final_results, final_scores, final_indices = [], [], []\\n    current_tokens = 0\\n    for i in range(min(k, len(sorted_results))):\\n        message_tokens = self.get_message_tokens(sorted_results[i])\\n        if current_tokens + message_tokens <= max_tokens:\\n            final_results.append(sorted_results[i])\\n            final_scores.append(sorted_scores[i])\\n            final_indices.append(sorted_indices[i])\\n            current_tokens += message_tokens\\n        else:\\n            break\\n        \\n    return final_results, final_scores, final_indices\\n',\n",
       " 'from babydragon.memory.threads.base_thread import BaseThread\\nfrom babydragon.memory.indexes.memory_index import MemoryIndex\\nfrom babydragon.utils.oai import check_dict, mark_question\\nimport faiss\\nimport numpy as np\\nfrom typing import Optional\\n\\nclass VectorThread(BaseThread, MemoryIndex):\\n    \"\"\" vector BaseThread, creates a faiss index with the messages and allows to search for similar messages, memory BaseThread can return messages in either similarity or chronological order \\n      add a parameter to choose the order of the messages\\n    \"\"\"\\n    def __init__(self, name= \\'vector_memory\\', max_context = 2048, use_mark = False):\\n        BaseThread.__init__(self,name= name , max_memory= None)\\n        MemoryIndex.__init__(self, index = None, name = name)\\n        self.max_context = max_context\\n        self.use_mark = use_mark\\n        self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n        \\n    def index_message(self,message: str , verbose: bool =False):\\n        \"\"\"index a message in the faiss index, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated\\n        \"\"\"\\n        \\n        self.add_to_index(value = message, verbose = verbose)\\n\\n    def add_message(self, message_dict: dict, verbose: bool = False):\\n        \"\"\"add a message to the memory thread, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\\n        \"\"\"\\n        # print(\"checking the dict\")\\n        message_dict = check_dict(message_dict)\\n        # print(\"trying to add the message\")\\n        BaseThread.add_message(self,message_dict)\\n        # print(message_dict)\\n        message = message_dict[\"content\"]\\n        self.index_message(message, verbose = verbose)\\n        return True\\n    \\n    def token_bound_query(self, query, k: int =10, max_tokens: int =4000):\\n        \"\"\" returns the k most similar messages to the query, sorted in similarity order\"\"\"\\n        if self.use_mark:\\n            query = mark_question(query)\\n        return MemoryIndex.token_bound_query(self, query, k, max_tokens)\\n    \\n    def sorted_query(self, query, k: int =10, max_tokens: int =4000, reverse: bool = False, return_from_thread = True):\\n        \"\"\" returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n        unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n        #sort the messages \\n        \\n        sorted_messages = [unsorted_messages[i] for i in sorted(range(len(unsorted_messages)), key=lambda k: unsorted_indices[k])]\\n        sorted_scores = [unsorted_scores[i] for i in sorted(range(len(unsorted_scores)), key=lambda k: unsorted_indices[k])]\\n        sorted_indices = [unsorted_indices[i] for i in sorted(range(len(unsorted_indices)), key=lambda k: unsorted_indices[k])]\\n        if reverse:\\n            sorted_messages.reverse()\\n            sorted_scores.reverse()\\n            sorted_indices.reverse()\\n        if return_from_thread:\\n            sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n        return sorted_messages, sorted_scores, sorted_indices\\n    \\n    def weighted_query(self, query, k: int = 10, max_tokens: int = 4000, decay_factor: float = 0.1, temporal_weight: float = 0.5, order_by: str = \\'chronological\\', reverse: bool = False) -> list:\\n        \"\"\" Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\\n        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\\n        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\\n        The order_by parameter controls the order of the results. If it is set to \\'similarity\\', the results are sorted in similarity order. If it is set to \\'chronological\\', the results are sorted in chronological order with the most recent message first.\\n        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\\n        \"\"\"\\n        # Validate order_by parameter\\n        if order_by not in (\\'similarity\\', \\'chronological\\'):\\n            raise ValueError(\"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\")\\n\\n        # Get similarity-based results\\n        sim_messages, sim_scores, sim_indices = self.sorted_query(query, k, max_tokens=max_tokens)\\n        \\n        # Get token-bound history\\n        hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\\n        \\n        # Combine messages and indices\\n        combined_messages = sim_messages + hist_messages\\n        combined_indices = sim_indices + hist_indices\\n        \\n        # Create the local_index and populate it\\n        self.local_index = MemoryIndex(name=\\'local_index\\')\\n        for message in combined_messages:\\n            self.local_index.add_to_index(value=message, verbose=False)\\n        \\n        # Perform a new query on the combined index\\n        new_query_results, new_query_scores, new_query_indices = self.local_index.token_bound_query(query, k=len(combined_messages), max_tokens=max_tokens)\\n        \\n        # Compute temporal weights\\n        temporal_weights = [np.exp(-decay_factor * i) for i in range(len(combined_messages))]\\n        temporal_weights = [w / sum(temporal_weights) for w in temporal_weights]  # Normalize the temporal weights\\n        \\n        # Combine similarity scores and temporal weights\\n        weighted_scores = []\\n        for i in range(len(new_query_scores)):\\n            sim_score = new_query_scores[i]\\n            temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\\n            weighted_score = (1 - temporal_weight) * sim_score + temporal_weight * temp_weight\\n            weighted_scores.append(weighted_score)\\n        \\n        # Sort the results based on the order_by parameter\\n        if order_by == \\'similarity\\':\\n            sorting_key = lambda k: weighted_scores[k]\\n        elif order_by == \\'chronological\\':  # order_by == \\'chronological\\'\\n            sorting_key = lambda k: new_query_indices[k]\\n        else:\\n            raise ValueError(\"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\")\\n\\n        sorted_indices = [new_query_indices[i] for i in sorted(range(len(new_query_indices)), key=sorting_key, reverse=not reverse)]\\n        sorted_results = [new_query_results[i] for i in sorted(range(len(new_query_results)), key=sorting_key, reverse=not reverse)]\\n        sorted_scores = [weighted_scores[i] for i in sorted(range(len(weighted_scores)), key=sorting_key, reverse=not reverse)]\\n\\n        # Return only the top k results without exceeding max_tokens\\n        final_results, final_scores, final_indices = [], [], []\\n        current_tokens = 0\\n        for i in range(min(k, len(sorted_results))):\\n            message_tokens = self.get_message_tokens(sorted_results[i])\\n            if current_tokens + message_tokens <= max_tokens:\\n                final_results.append(sorted_results[i])\\n                final_scores.append(sorted_scores[i])\\n                final_indices.append(sorted_indices[i])\\n                current_tokens += message_tokens\\n            else:\\n                break\\n        \\n        return final_results, final_scores, final_indices\\n\\n\\n',\n",
       " '\\ndef __init__(self, name: str = \\'memory\\', max_memory: Optional[int] = None, tokenizer: Optional[Any] = None) -> None:\\n    \"\"\"\\n        Initialize the BaseThread instance.\\n\\n        :param name: The name of the memory thread. Defaults to \\'memory\\'.\\n        :param max_memory: The maximum number of tokens allowed in the memory thread. \\n                           Defaults to None, which means no limit.\\n        :param tokenizer: The tokenizer to be used for tokenizing messages. \\n                          Defaults to None, which means using the tiktoken encoding for the \\'gpt-3.5-turbo\\' model.\\n        \"\"\"\\n    self.name = name\\n    self.max_memory = max_memory\\n    self.memory_thread = []\\n    self.time_stamps = []\\n    self.message_tokens = []\\n    self.total_tokens = 0\\n    if tokenizer is None:\\n        self.tokenizer = tiktoken.encoding_for_model(\\'gpt-3.5-turbo\\')\\n',\n",
       " '\\n\\ndef __getitem__(self, idx):\\n    return self.memory_thread[idx]    \\n',\n",
       " '\\ndef __len__(self):\\n    return len(self.memory_thread)\\n',\n",
       " '\\ndef get_message_tokens(self, message_dict: dict) -> int:\\n    \"\"\"\\n        Calculate the number of tokens in a message, including the role token.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The total number of tokens in the message.\\n        \"\"\"\\n    message_dict = check_dict(message_dict)\\n    message = message_dict[\"content\"]\\n    return len(self.tokenizer.encode(message))+6 # +6 for the role token\\n',\n",
       " '\\ndef get_message_role(self, message_dict: dict) -> str:\\n    \"\"\"\\n        Get the role of the message from a message dictionary.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The role of the message.\\n        \"\"\"\\n    message_dict = check_dict(message_dict)\\n    return message_dict[\"role\"]\\n',\n",
       " '\\ndef add_message(self, message_dict: dict) -> None:\\n    \"\"\"\\n        Add a message to the memory thread.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        \"\"\"\\n    message_tokens = self.get_message_tokens(message_dict)\\n    \\n    if  self.max_memory is None or self.total_tokens + message_tokens <= self.max_memory:\\n        #add the message_dict to the memory_thread\\n        # update the total number of tokens\\n        self.memory_thread.append(message_dict)\\n        self.total_tokens += message_tokens\\n        self.message_tokens.append(message_tokens)\\n        time_stamp  = time.time()\\n        self.time_stamps.append(time_stamp)\\n    else :\\n        display(Markdown(\"The memory BaseThread is full, the last message was not added\"))\\n',\n",
       " '    \\ndef remove_message(self, message_dict: Union[dict,None] =  None , idx: Union[int,None] = None) -> None:\\n    \"\"\" \\n        Remove a message from the memory thread.\\n        \"\"\"\\n    if message_dict is None and idx is None:\\n        raise Exception(\"You need to provide either a message_dict or an idx\")\\n    elif message_dict is not None and idx is not None:\\n        raise Exception(\"You need to provide either a message_dict or an idx\")\\n    \\n    if idx is None:\\n        message_dict = check_dict(message_dict)\\n        search_results = self.find_message(message_dict)\\n        if search_results is not None:\\n            idx = search_results[-1][\"idx\"]\\n            message = search_results[-1][\"message_dict\"]\\n            self.memory_thread.pop(idx)\\n            self.message_tokens.pop(idx)\\n            self.time_stamps.pop(idx)\\n            self.total_tokens -= self.get_message_tokens(message)\\n        else:   \\n            raise Exception(\"The message was not found in the memory BaseThread\")\\n    else:\\n        if idx < len(self.memory_thread):\\n            message = self.memory_thread.pop(idx)\\n            self.total_tokens -= self.get_message_tokens(message)\\n        else:  \\n            raise Exception(\"The index was out bound\")\\n        \\n',\n",
       " 'def find_message(self, message:Union[dict,str], role : Union[str,None] = None) -> Union[None, list]:\\n    \"\"\"\\n        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\\n        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\\n    #check if the message is a dictioanry or a string\\n    message = message if isinstance(message, str) else check_dict(message)\\n    search_results = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        target = message_dict if isinstance(message, dict) else message_dict[\"content\"]\\n        if target == message and (role is None or message_dict[\"role\"] == role):\\n            search_results.append({\"idx\": idx, \"message_dict\": message_dict})\\n    return search_results if len(search_results) > 0 else None\\n',\n",
       " '\\ndef find_role(self, role: str) -> Union[None, list]:\\n    \"\"\"\\n        Find all messages with a specific role in the memory thread.\\n        \"\"\"\\n    search_results = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if message_dict[\"role\"] == role:\\n            search_results.append({\"idx\": idx, \"message_dict\": message_dict})\\n    return search_results if len(search_results) > 0 else None\\n',\n",
       " '\\ndef last_message(self, role: Union[str,None] = None) -> Union[None, dict]:\\n    \"\"\"\\n        Get the last message in the memory thread with a specific role.\"\"\"\\n    if role is None:\\n        return self.memory_thread[-1]\\n    else:\\n        for message_dict in reversed(self.memory_thread):\\n            if message_dict[\"role\"] == role:\\n                return message_dict\\n        return None\\n    \\n',\n",
       " 'def first_message(self, role: Union[str,None] = None) -> Union[None, dict]:\\n    \"\"\"\\n        Get the first message in the memory thread with a specific role.\"\"\"\\n    if role is None:\\n        return self.memory_thread[0]\\n    else:\\n        for message_dict in self.memory_thread:\\n            if message_dict[\"role\"] == role:\\n                return message_dict\\n        return None\\n',\n",
       " '    \\ndef messages_before(self, message: dict , role: Union[str,None] = None ) -> Union[None, list]:\\n    \"\"\"\\n        Get all messages before a specific message in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if message_dict == message and (role is None or message_dict[\"role\"] == role):\\n            messages = self.memory_thread[:idx]\\n            break\\n    return messages if len(messages) > 0 else None\\n',\n",
       " '\\ndef messages_before(self, message: dict , role: Union[str,None] = None ) -> Union[None, list]:\\n    \"\"\"\\n        Get all messages after a specific message in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if message_dict == message and (role is None or message_dict[\"role\"] == role):\\n            messages = self.memory_thread[idx+1:]\\n            break\\n    return messages if len(messages) > 0 else None\\n',\n",
       " '\\ndef messages_between(self, start_message: dict, end_message: dict, role: Union[str,None] = None ) -> Union[None, list]:\\n    \"\"\"\\n        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if message_dict == start_message and (role is None or message_dict[\"role\"] == role):\\n            start_idx = idx\\n            break\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if message_dict == end_message and (role is None or message_dict[\"role\"] == role):\\n            end_idx = idx\\n            break\\n    messages = self.memory_thread[start_idx+1:end_idx]\\n    return messages if len(messages) > 0 else None\\n    \\n',\n",
       " 'def messages_more_tokens(self, tokens: int, role: Union[str,None] = None ):\\n    \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if self.message_tokens[idx] > tokens and (role is None or message_dict[\"role\"] == role):\\n            messages.append(message_dict)\\n    return messages if len(messages) > 0 else None\\n',\n",
       " '\\ndef messages_less_tokens(self, tokens: int, role: Union[str,None] = None ):\\n    \"\"\"\\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if self.message_tokens[idx] < tokens and (role is None or message_dict[\"role\"] == role):\\n            messages.append(message_dict)\\n    return messages if len(messages) > 0 else None\\n',\n",
       " '\\ndef messages_between_tokens(self, start_tokens: int, end_tokens: int, role: Union[str,None] = None ):\\n    \"\"\"\\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if self.message_tokens[idx] > start_tokens and self.message_tokens[idx] < end_tokens and (role is None or message_dict[\"role\"] == role):\\n            messages.append(message_dict)\\n    return messages if len(messages) > 0 else None\\n',\n",
       " '\\ndef messages_before_time(self, time_stamp, role: Union[str,None] = None ):\\n    \"\"\"\\n        Get all messages before a specific time in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if self.time_stamps[idx] < time_stamp and (role is None or message_dict[\"role\"] == role):\\n            messages.append(message_dict)\\n    return messages if len(messages) > 0 else None\\n',\n",
       " '\\ndef messages_after_time(self, time_stamp, role: Union[str,None] = None ):\\n    \"\"\"\\n        Get all messages after a specific time in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if self.time_stamps[idx] > time_stamp and (role is None or message_dict[\"role\"] == role):\\n            messages.append(message_dict)\\n    return messages if len(messages) > 0 else None\\n',\n",
       " '\\ndef messages_between_time(self, start_time, end_time, role: Union[str,None] = None ):\\n    \"\"\"\\n        Get all messages between two specific times in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if self.time_stamps[idx] > start_time and self.time_stamps[idx] < end_time and (role is None or message_dict[\"role\"] == role):\\n            messages.append(message_dict)\\n    return messages if len(messages) > 0 else None\\n',\n",
       " '\\ndef token_bound_history(self, max_tokens: int, max_history=None, role: Union[str, None]=None):\\n    messages = []\\n    indices = []\\n    tokens = 0\\n    if max_history is None:\\n        max_history = len(self.memory_thread)\\n\\n    for idx, message_dict in enumerate(reversed(self.memory_thread)):\\n        if tokens + self.message_tokens[idx] < max_tokens and (role is None or message_dict[\"role\"] == role) and idx < max_history:\\n            messages.append(message_dict)\\n            indices.append(len(self.memory_thread) - 1 - idx)\\n            tokens += self.message_tokens[idx]\\n        else:\\n            break\\n    return messages, indices if len(messages) > 0 else (None, None)\\n',\n",
       " 'import tiktoken\\nfrom IPython.display import display, Markdown\\nfrom babydragon.memory.indexes.memory_index import MemoryIndex\\nfrom babydragon.utils.oai import  check_dict, mark_question\\nimport time\\nfrom typing import Optional, Any, Dict, Union\\n\\nclass BaseThread:\\n    \"\"\"\\n    This class is used to keep track of the memory thread of a conversation and the total number of tokens. \\n    All conversation memories should subclass this class. If max_memory is None, it has \\n    no limit to the number of tokens that can be stored in the memory thread.\\n    \"\"\"\\n\\n    def __init__(self, name: str = \\'memory\\', max_memory: Optional[int] = None, tokenizer: Optional[Any] = None) -> None:\\n        \"\"\"\\n        Initialize the BaseThread instance.\\n\\n        :param name: The name of the memory thread. Defaults to \\'memory\\'.\\n        :param max_memory: The maximum number of tokens allowed in the memory thread. \\n                           Defaults to None, which means no limit.\\n        :param tokenizer: The tokenizer to be used for tokenizing messages. \\n                          Defaults to None, which means using the tiktoken encoding for the \\'gpt-3.5-turbo\\' model.\\n        \"\"\"\\n        self.name = name\\n        self.max_memory = max_memory\\n        self.memory_thread = []\\n        self.time_stamps = []\\n        self.message_tokens = []\\n        self.total_tokens = 0\\n        if tokenizer is None:\\n            self.tokenizer = tiktoken.encoding_for_model(\\'gpt-3.5-turbo\\')\\n\\n\\n    def __getitem__(self, idx):\\n        return self.memory_thread[idx]    \\n    \\n    def __len__(self):\\n        return len(self.memory_thread)\\n    \\n    def get_message_tokens(self, message_dict: dict) -> int:\\n        \"\"\"\\n        Calculate the number of tokens in a message, including the role token.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The total number of tokens in the message.\\n        \"\"\"\\n        message_dict = check_dict(message_dict)\\n        message = message_dict[\"content\"]\\n        return len(self.tokenizer.encode(message))+6 # +6 for the role token\\n    \\n    def get_message_role(self, message_dict: dict) -> str:\\n        \"\"\"\\n        Get the role of the message from a message dictionary.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The role of the message.\\n        \"\"\"\\n        message_dict = check_dict(message_dict)\\n        return message_dict[\"role\"]\\n    \\n    def add_message(self, message_dict: dict) -> None:\\n        \"\"\"\\n        Add a message to the memory thread.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        \"\"\"\\n        message_tokens = self.get_message_tokens(message_dict)\\n        \\n        if  self.max_memory is None or self.total_tokens + message_tokens <= self.max_memory:\\n            #add the message_dict to the memory_thread\\n            # update the total number of tokens\\n            self.memory_thread.append(message_dict)\\n            self.total_tokens += message_tokens\\n            self.message_tokens.append(message_tokens)\\n            time_stamp  = time.time()\\n            self.time_stamps.append(time_stamp)\\n        else :\\n            display(Markdown(\"The memory BaseThread is full, the last message was not added\"))\\n    \\n    def remove_message(self, message_dict: Union[dict,None] =  None , idx: Union[int,None] = None) -> None:\\n        \"\"\" \\n        Remove a message from the memory thread.\\n        \"\"\"\\n        if message_dict is None and idx is None:\\n            raise Exception(\"You need to provide either a message_dict or an idx\")\\n        elif message_dict is not None and idx is not None:\\n            raise Exception(\"You need to provide either a message_dict or an idx\")\\n        \\n        if idx is None:\\n            message_dict = check_dict(message_dict)\\n            search_results = self.find_message(message_dict)\\n            if search_results is not None:\\n                idx = search_results[-1][\"idx\"]\\n                message = search_results[-1][\"message_dict\"]\\n                self.memory_thread.pop(idx)\\n                self.message_tokens.pop(idx)\\n                self.time_stamps.pop(idx)\\n                self.total_tokens -= self.get_message_tokens(message)\\n            else:   \\n                raise Exception(\"The message was not found in the memory BaseThread\")\\n        else:\\n            if idx < len(self.memory_thread):\\n                message = self.memory_thread.pop(idx)\\n                self.total_tokens -= self.get_message_tokens(message)\\n            else:  \\n                raise Exception(\"The index was out bound\")\\n            \\n    def find_message(self, message:Union[dict,str], role : Union[str,None] = None) -> Union[None, list]:\\n        \"\"\"\\n        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\\n        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\\n        #check if the message is a dictioanry or a string\\n        message = message if isinstance(message, str) else check_dict(message)\\n        search_results = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            target = message_dict if isinstance(message, dict) else message_dict[\"content\"]\\n            if target == message and (role is None or message_dict[\"role\"] == role):\\n                search_results.append({\"idx\": idx, \"message_dict\": message_dict})\\n        return search_results if len(search_results) > 0 else None\\n    \\n    def find_role(self, role: str) -> Union[None, list]:\\n        \"\"\"\\n        Find all messages with a specific role in the memory thread.\\n        \"\"\"\\n        search_results = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if message_dict[\"role\"] == role:\\n                search_results.append({\"idx\": idx, \"message_dict\": message_dict})\\n        return search_results if len(search_results) > 0 else None\\n    \\n    def last_message(self, role: Union[str,None] = None) -> Union[None, dict]:\\n        \"\"\"\\n        Get the last message in the memory thread with a specific role.\"\"\"\\n        if role is None:\\n            return self.memory_thread[-1]\\n        else:\\n            for message_dict in reversed(self.memory_thread):\\n                if message_dict[\"role\"] == role:\\n                    return message_dict\\n            return None\\n        \\n    def first_message(self, role: Union[str,None] = None) -> Union[None, dict]:\\n        \"\"\"\\n        Get the first message in the memory thread with a specific role.\"\"\"\\n        if role is None:\\n            return self.memory_thread[0]\\n        else:\\n            for message_dict in self.memory_thread:\\n                if message_dict[\"role\"] == role:\\n                    return message_dict\\n            return None\\n    \\n    def messages_before(self, message: dict , role: Union[str,None] = None ) -> Union[None, list]:\\n        \"\"\"\\n        Get all messages before a specific message in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if message_dict == message and (role is None or message_dict[\"role\"] == role):\\n                messages = self.memory_thread[:idx]\\n                break\\n        return messages if len(messages) > 0 else None\\n    \\n    def messages_before(self, message: dict , role: Union[str,None] = None ) -> Union[None, list]:\\n        \"\"\"\\n        Get all messages after a specific message in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if message_dict == message and (role is None or message_dict[\"role\"] == role):\\n                messages = self.memory_thread[idx+1:]\\n                break\\n        return messages if len(messages) > 0 else None\\n    \\n    def messages_between(self, start_message: dict, end_message: dict, role: Union[str,None] = None ) -> Union[None, list]:\\n        \"\"\"\\n        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if message_dict == start_message and (role is None or message_dict[\"role\"] == role):\\n                start_idx = idx\\n                break\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if message_dict == end_message and (role is None or message_dict[\"role\"] == role):\\n                end_idx = idx\\n                break\\n        messages = self.memory_thread[start_idx+1:end_idx]\\n        return messages if len(messages) > 0 else None\\n        \\n    def messages_more_tokens(self, tokens: int, role: Union[str,None] = None ):\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if self.message_tokens[idx] > tokens and (role is None or message_dict[\"role\"] == role):\\n                messages.append(message_dict)\\n        return messages if len(messages) > 0 else None\\n    \\n    def messages_less_tokens(self, tokens: int, role: Union[str,None] = None ):\\n        \"\"\"\\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if self.message_tokens[idx] < tokens and (role is None or message_dict[\"role\"] == role):\\n                messages.append(message_dict)\\n        return messages if len(messages) > 0 else None\\n    \\n    def messages_between_tokens(self, start_tokens: int, end_tokens: int, role: Union[str,None] = None ):\\n        \"\"\"\\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if self.message_tokens[idx] > start_tokens and self.message_tokens[idx] < end_tokens and (role is None or message_dict[\"role\"] == role):\\n                messages.append(message_dict)\\n        return messages if len(messages) > 0 else None\\n\\n    def messages_before_time(self, time_stamp, role: Union[str,None] = None ):\\n        \"\"\"\\n        Get all messages before a specific time in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if self.time_stamps[idx] < time_stamp and (role is None or message_dict[\"role\"] == role):\\n                messages.append(message_dict)\\n        return messages if len(messages) > 0 else None\\n    \\n    def messages_after_time(self, time_stamp, role: Union[str,None] = None ):\\n        \"\"\"\\n        Get all messages after a specific time in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if self.time_stamps[idx] > time_stamp and (role is None or message_dict[\"role\"] == role):\\n                messages.append(message_dict)\\n        return messages if len(messages) > 0 else None\\n    \\n    def messages_between_time(self, start_time, end_time, role: Union[str,None] = None ):\\n        \"\"\"\\n        Get all messages between two specific times in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if self.time_stamps[idx] > start_time and self.time_stamps[idx] < end_time and (role is None or message_dict[\"role\"] == role):\\n                messages.append(message_dict)\\n        return messages if len(messages) > 0 else None\\n    \\n    def token_bound_history(self, max_tokens: int, max_history=None, role: Union[str, None]=None):\\n        messages = []\\n        indices = []\\n        tokens = 0\\n        if max_history is None:\\n            max_history = len(self.memory_thread)\\n\\n        for idx, message_dict in enumerate(reversed(self.memory_thread)):\\n            if tokens + self.message_tokens[idx] < max_tokens and (role is None or message_dict[\"role\"] == role) and idx < max_history:\\n                messages.append(message_dict)\\n                indices.append(len(self.memory_thread) - 1 - idx)\\n                tokens += self.message_tokens[idx]\\n            else:\\n                break\\n        return messages, indices if len(messages) > 0 else (None, None)',\n",
       " \"def __init__(self, name= 'fifo_memory', max_memory = None, longterm_thread = None, redundant = True):\\n    \\n    BaseThread.__init__(self,name= name , max_memory= None)\\n    if redundant is True:            \\n        self.redundant_thread = BaseThread(name = 'lucid_memory',max_memory = None)\\n    else:\\n        self.redundant_thread = None\\n    if longterm_thread is None:\\n        self.longterm_thread = BaseThread(name ='longterm_memory',max_memory = None)\\n    else:\\n        self.longterm_thread = longterm_thread\\n    # create an alias for the memory_thread to make the code more readable\\n    self.fifo_thread = self.memory_thread\\n    self.max_memory = max_memory\\n        \\n\",\n",
       " 'def to_longterm(self, idx: int):\\n    \"\"\" move the message at the index idx to the longterm_memory\"\"\"\\n    #move the message at the index idx to the longterm_memory\\n    display(Markdown(\"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(idx)))\\n    message = copy.deepcopy(self.memory_thread[idx])\\n    # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\\n    self.longterm_thread.add_message(message)\\n    self.remove_message(idx=idx)\\n',\n",
       " '\\ndef add_message(self, message_dict: dict):\\n    \"\"\" add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\\n    # message_dict = {\"role\": role, \"content\": content}\\n    #chek that the message_dict is a dictionary or a list of dictionaries\\n    message_dict = check_dict(message_dict)\\n    if self.redundant_thread is not None:\\n        self.redundant_thread.add_message(message_dict)\\n    message_tokens = self.get_message_tokens(message_dict)\\n    \\n    if self.total_tokens + message_tokens > self.max_memory:\\n        while self.total_tokens + message_tokens > self.max_memory :\\n            if len(self.memory_thread) > 0:\\n                self.to_longterm(idx=0)\\n        super().add_message(message_dict)\\n        \\n    else:\\n        #add the message_dict to the memory_thread\\n        # update the total number of tokens\\n        super().add_message(message_dict)\\n',\n",
       " 'from IPython.display import display, Markdown\\nfrom babydragon.memory.threads.base_thread import BaseThread\\nfrom babydragon.utils.oai import check_dict\\nimport copy\\n\\nclass FifoThread(BaseThread):\\n    \"\"\"FIFO Memory BaseThread, the oldest messages are removed first when reaching the max_memory limit, the memory is defined in terms of tokens, \\n    outs are passe to the longterm_memory, lucid_memory is a redundant memory that stores all the messages\\n    \"\"\"\\n    def __init__(self, name= \\'fifo_memory\\', max_memory = None, longterm_thread = None, redundant = True):\\n        \\n        BaseThread.__init__(self,name= name , max_memory= None)\\n        if redundant is True:            \\n            self.redundant_thread = BaseThread(name = \\'lucid_memory\\',max_memory = None)\\n        else:\\n            self.redundant_thread = None\\n        if longterm_thread is None:\\n            self.longterm_thread = BaseThread(name =\\'longterm_memory\\',max_memory = None)\\n        else:\\n            self.longterm_thread = longterm_thread\\n        # create an alias for the memory_thread to make the code more readable\\n        self.fifo_thread = self.memory_thread\\n        self.max_memory = max_memory\\n            \\n    def to_longterm(self, idx: int):\\n        \"\"\" move the message at the index idx to the longterm_memory\"\"\"\\n        #move the message at the index idx to the longterm_memory\\n        display(Markdown(\"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(idx)))\\n        message = copy.deepcopy(self.memory_thread[idx])\\n        # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\\n        self.longterm_thread.add_message(message)\\n        self.remove_message(idx=idx)\\n    \\n    def add_message(self, message_dict: dict):\\n        \"\"\" add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\\n        # message_dict = {\"role\": role, \"content\": content}\\n        #chek that the message_dict is a dictionary or a list of dictionaries\\n        message_dict = check_dict(message_dict)\\n        if self.redundant_thread is not None:\\n            self.redundant_thread.add_message(message_dict)\\n        message_tokens = self.get_message_tokens(message_dict)\\n        \\n        if self.total_tokens + message_tokens > self.max_memory:\\n            while self.total_tokens + message_tokens > self.max_memory :\\n                if len(self.memory_thread) > 0:\\n                    self.to_longterm(idx=0)\\n            super().add_message(message_dict)\\n            \\n        else:\\n            #add the message_dict to the memory_thread\\n            # update the total number of tokens\\n            super().add_message(message_dict)',\n",
       " 'def __init__(self, values, embeddings, name=\"memory_kernel\", save_path=None):\\n    super().__init__(values, embeddings, name, save_path)\\n    self.create_k_hop_index()\\n',\n",
       " '\\ndef cos_sim(self, a, b):\\n    \"\"\"\\n        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n        \"\"\"\\n    if not isinstance(a, np.ndarray):\\n        a = np.array(a)\\n\\n    if not isinstance(b, np.ndarray):\\n        b = np.array(b)\\n\\n    if len(a.shape) == 1:\\n        a = a[np.newaxis, :]\\n\\n    if len(b.shape) == 1:\\n        b = b[np.newaxis, :]\\n\\n    a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\\n    b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\\n    return np.dot(a_norm, b_norm.T)\\n',\n",
       " '\\ndef cos_sim_batch(self, a: np.ndarray, b: np.ndarray, batch_size: int = 128):\\n    \"\"\"\\n        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j using batch processing.\\n        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n        \"\"\"\\n\\n    if not isinstance(a, np.ndarray):\\n        a = np.array(a)\\n\\n    if not isinstance(b, np.ndarray):\\n        b = np.array(b)\\n\\n    if len(a.shape) == 1:\\n        a = np.expand_dims(a, 0)\\n\\n    if len(b.shape) == 1:\\n        b = np.expand_dims(b, 0)\\n\\n    a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)\\n    b_norm = b / np.linalg.norm(b, axis=1, keepdims=True)\\n\\n    sim_matrix = []\\n    for i in range(0, len(a_norm), batch_size):\\n        a_batch = a_norm[i : i + batch_size]\\n        sim_batch = np.matmul(a_batch, b_norm.T)\\n        sim_matrix.append(sim_batch)\\n    sim_matrix = np.concatenate(sim_matrix, axis=0)\\n    return sim_matrix\\n',\n",
       " '\\ndef compute_kernel(\\n    self, embedding_set, threshold=0.65, use_softmax=False, cos_sim_batch=True\\n):\\n    \"\"\"\\n        Compute the adjacency matrix of the graph.\\n\\n        Parameters:\\n        embedding_set (numpy array): The embedding matrix of the nodes.\\n        threshold (float): The threshold for the adjacency matrix.\\n        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\\n        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\\n\\n        Returns:\\n        adj_matrix (numpy array): The adjacency matrix of the graph.\\n        \"\"\"\\n    if cos_sim_batch:\\n        A = self.cos_sim_batch(embedding_set, embedding_set)\\n    else:\\n        A = self.cos_sim(embedding_set, embedding_set)\\n    if use_softmax:\\n        # softmax\\n        A = np.exp(A)\\n        A = A / np.sum(A, axis=1)[:, np.newaxis]\\n    adj_matrix = np.zeros_like(A)\\n    adj_matrix[A > threshold] = 1\\n    adj_matrix[A <= threshold] = 0\\n    adj_matrix = adj_matrix.astype(np.float32)\\n    return adj_matrix\\n',\n",
       " '\\ndef k_hop_message_passing(self, A, node_features, k):\\n    \"\"\"\\n        Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n        Parameters:\\n        A (numpy array): The adjacency matrix of the graph.\\n        node_features (numpy array): The feature matrix of the nodes.\\n        k (int): The number of hops for message passing.\\n\\n        Returns:\\n        A_k (numpy array): The k-hop adjacency matrix.\\n        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n        \"\"\"\\n\\n    print(\"Compute the k-hop adjacency matrix\")\\n    A_k = np.linalg.matrix_power(A, k)\\n\\n    print(\"Aggregate the messages from the k-hop neighborhood:\")\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\\n\\n    return A_k, agg_features\\n',\n",
       " '\\ndef faiss_index_to_adj_matrix(index: faiss.Index, num_vectors: int) -> np.ndarray:\\n    \"\"\"\\n        Convert a Faiss index into a NumPy adjacency matrix.\\n\\n        Args:\\n            index (faiss.Index): The Faiss index.\\n            num_vectors (int): The number of vectors in the Faiss index.\\n\\n        Returns:\\n            np.ndarray: The adjacency matrix.\\n        \"\"\"\\n    # Create an empty adjacency matrix\\n    adj_matrix = np.zeros((num_vectors, num_vectors), dtype=np.float32)\\n\\n    # Populate the adjacency matrix with distances from the Faiss index\\n    for i in range(num_vectors):\\n        distances, _ = index.search(index.reconstruct(i).reshape(1, -1), num_vectors)\\n        adj_matrix[i] = distances\\n\\n    return adj_matrix\\n',\n",
       " '\\ndef create_k_hop_index(self, k):\\n    print(\"Computing the adjacency matrix\")\\n    print(\"Embeddings shape: \", self.embeddings.shape)\\n    self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\\n    print(\"Computing the k-hop adjacency matrix and aggregated features\")\\n    self.A_k, self.node_embeddings = self.k_hop_message_passing(\\n        self.A, self.embeddings, k\\n    )\\n    print(\"Updating the memory index\")\\n    self.k_hop_index = MemoryIndex(index=None, values=self.values, embeddings=self.node_embeddings, name=self.memory_index.name)\\n',\n",
       " 'from babydragon.memory.indexes.memory_index import MemoryIndex\\nfrom babydragon.working_memory.associative_memory.probability_density_functions import calc_shgo_mode, estimate_pdf\\nfrom babydragon.working_memory.associative_memory.group_by_rank import group_items_by_rank_buckets\\nfrom babydragon.working_memory.associative_memory.nmi import run_stability_analysis\\nimport numpy as np\\nimport faiss\\nfrom tqdm import tqdm\\n\\n\\nclass MemoryKernel(MemoryIndex):\\n    def __init__(self, values, embeddings, name=\"memory_kernel\", save_path=None):\\n        super().__init__(values, embeddings, name, save_path)\\n        self.create_k_hop_index()\\n\\n    def cos_sim(self, a, b):\\n        \"\"\"\\n        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n        \"\"\"\\n        if not isinstance(a, np.ndarray):\\n            a = np.array(a)\\n\\n        if not isinstance(b, np.ndarray):\\n            b = np.array(b)\\n\\n        if len(a.shape) == 1:\\n            a = a[np.newaxis, :]\\n\\n        if len(b.shape) == 1:\\n            b = b[np.newaxis, :]\\n\\n        a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\\n        b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\\n        return np.dot(a_norm, b_norm.T)\\n\\n    def cos_sim_batch(self, a: np.ndarray, b: np.ndarray, batch_size: int = 128):\\n        \"\"\"\\n        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j using batch processing.\\n        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n        \"\"\"\\n\\n        if not isinstance(a, np.ndarray):\\n            a = np.array(a)\\n\\n        if not isinstance(b, np.ndarray):\\n            b = np.array(b)\\n\\n        if len(a.shape) == 1:\\n            a = np.expand_dims(a, 0)\\n\\n        if len(b.shape) == 1:\\n            b = np.expand_dims(b, 0)\\n\\n        a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)\\n        b_norm = b / np.linalg.norm(b, axis=1, keepdims=True)\\n\\n        sim_matrix = []\\n        for i in range(0, len(a_norm), batch_size):\\n            a_batch = a_norm[i : i + batch_size]\\n            sim_batch = np.matmul(a_batch, b_norm.T)\\n            sim_matrix.append(sim_batch)\\n        sim_matrix = np.concatenate(sim_matrix, axis=0)\\n        return sim_matrix\\n\\n    def compute_kernel(\\n        self, embedding_set, threshold=0.65, use_softmax=False, cos_sim_batch=True\\n    ):\\n        \"\"\"\\n        Compute the adjacency matrix of the graph.\\n\\n        Parameters:\\n        embedding_set (numpy array): The embedding matrix of the nodes.\\n        threshold (float): The threshold for the adjacency matrix.\\n        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\\n        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\\n\\n        Returns:\\n        adj_matrix (numpy array): The adjacency matrix of the graph.\\n        \"\"\"\\n        if cos_sim_batch:\\n            A = self.cos_sim_batch(embedding_set, embedding_set)\\n        else:\\n            A = self.cos_sim(embedding_set, embedding_set)\\n        if use_softmax:\\n            # softmax\\n            A = np.exp(A)\\n            A = A / np.sum(A, axis=1)[:, np.newaxis]\\n        adj_matrix = np.zeros_like(A)\\n        adj_matrix[A > threshold] = 1\\n        adj_matrix[A <= threshold] = 0\\n        adj_matrix = adj_matrix.astype(np.float32)\\n        return adj_matrix\\n\\n    def k_hop_message_passing(self, A, node_features, k):\\n        \"\"\"\\n        Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n        Parameters:\\n        A (numpy array): The adjacency matrix of the graph.\\n        node_features (numpy array): The feature matrix of the nodes.\\n        k (int): The number of hops for message passing.\\n\\n        Returns:\\n        A_k (numpy array): The k-hop adjacency matrix.\\n        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n        \"\"\"\\n\\n        print(\"Compute the k-hop adjacency matrix\")\\n        A_k = np.linalg.matrix_power(A, k)\\n\\n        print(\"Aggregate the messages from the k-hop neighborhood:\")\\n        agg_features = node_features.copy()\\n\\n        for i in tqdm(range(k)):\\n            agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\\n\\n        return A_k, agg_features\\n    \\n    def faiss_index_to_adj_matrix(index: faiss.Index, num_vectors: int) -> np.ndarray:\\n        \"\"\"\\n        Convert a Faiss index into a NumPy adjacency matrix.\\n\\n        Args:\\n            index (faiss.Index): The Faiss index.\\n            num_vectors (int): The number of vectors in the Faiss index.\\n\\n        Returns:\\n            np.ndarray: The adjacency matrix.\\n        \"\"\"\\n        # Create an empty adjacency matrix\\n        adj_matrix = np.zeros((num_vectors, num_vectors), dtype=np.float32)\\n\\n        # Populate the adjacency matrix with distances from the Faiss index\\n        for i in range(num_vectors):\\n            distances, _ = index.search(index.reconstruct(i).reshape(1, -1), num_vectors)\\n            adj_matrix[i] = distances\\n\\n        return adj_matrix\\n\\n    def create_k_hop_index(self, k):\\n        print(\"Computing the adjacency matrix\")\\n        print(\"Embeddings shape: \", self.embeddings.shape)\\n        self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\\n        print(\"Computing the k-hop adjacency matrix and aggregated features\")\\n        self.A_k, self.node_embeddings = self.k_hop_message_passing(\\n            self.A, self.embeddings, k\\n        )\\n        print(\"Updating the memory index\")\\n        self.k_hop_index = MemoryIndex(index=None, values=self.values, embeddings=self.node_embeddings, name=self.memory_index.name)\\n  ',\n",
       " 'def __init__(\\n    self,\\n    index: Optional[faiss.Index] = None,\\n    values: Optional[List[str]] = None,\\n    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n    name: str = \"memory_index\",\\n    save_path: Optional[str] = None,\\n    load: bool = False,\\n    tokenizer: Optional[tiktoken.Encoding] = None,\\n    ):\\n\\n    self.name = name\\n    self.embedder = OpenAiEmbedder()\\n    if save_path is None:\\n        save_path = \"storage\"\\n\\n    self.save_path = save_path\\n\\n    # Create the \\'storage\\' folder if it does not exist\\n    os.makedirs(self.save_path, exist_ok=True)\\n    self.values = []\\n    if load is True:\\n        self.load()\\n    else:\\n        self.init_index(index, values, embeddings)\\n    if tokenizer is None:\\n        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n    else:\\n        self.tokenizer = tokenizer\\n    self.query_history = []\\n    self.save()\\n',\n",
       " '\\ndef init_index(\\n    self,\\n    index: Optional[faiss.Index] = None,\\n    values: Optional[List[str]] = None,\\n    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n    ) -> None:\\n\\n    \"\"\"\\n        initializes the index, there are 4 cases:\\n        1. we create a new index from scratch\\n        2. we create a new index from a list of embeddings and values\\n        3. we create a new index from a faiss index and values list\\n        4. we load an index from a file\\n        \"\"\"\\n    # fist case is when we create a new index from scratch\\n    if index is None and values is None and embeddings is None:\\n        print(\"Creating a new index\")\\n        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n        self.values = []\\n    # second case is where we create the index from a list of embeddings\\n    elif (\\n        index is None\\n        and values is not None\\n        and embeddings is not None\\n        and len(values) == len(embeddings)\\n    ):\\n        print(\"Creating a new index from a list of embeddings and values\")\\n        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n        for embedding, value in zip(embeddings, values):\\n            self.add_to_index(value, embedding)\\n        # third case is where we create the index from a faiss index and values list\\n    elif (\\n        isinstance(index, faiss.Index)\\n        and index.d == self.embedder.get_embedding_size()\\n        and type(values) == list\\n        and len(values) == index.ntotal\\n    ):\\n        print(\"Creating a new index from a faiss index and values list\")\\n        self.index = index\\n        self.values = values\\n    # fourth case is where we create an index from a list of values, the values are embedded and the index is created\\n    elif index is None and values is not None and embeddings is None:\\n        print(\"Creating a new index from a list of values\")\\n        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n        i = 0\\n        for value in values:\\n            # print the value id to see the progress\\n            print(\"Embedding value \", i, \" of \", len(values))\\n            # start tracking the time using time\\n            start = time.time()\\n            self.add_to_index(value)\\n            # print the time it took to embed the value\\n            print(\"Embedding value \", i, \" took \", time.time() - start, \" seconds\")\\n            i += 1\\n    else:\\n        raise ValueError(\\n            \"The index is not a valid faiss index or the embedding dimension is not correct\"\\n        )\\n',\n",
       " '\\ndef add_to_index(\\n    self,\\n    value: str,\\n    embedding: Optional[Union[List[float], np.ndarray, str]] = None,\\n    verbose: bool = True,\\n    ) -> None:\\n    \"\"\"\\n        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\\n        \"\"\"\\n    if value not in self.values:\\n        if embedding is None:\\n            embedding = self.embedder.embed(value)\\n            if verbose:\\n                display(Markdown(\"The value {value} was embedded\".format(value=value)))\\n\\n        if type(embedding) is list:\\n            embedding = np.array([embedding])\\n        elif type(embedding) is str:\\n            embedding = eval(embedding)\\n            embedding = np.array([embedding]).astype(np.float32)\\n        elif type(embedding) is not np.ndarray:\\n            raise ValueError(\"The embedding is not a valid type\")\\n\\n        # Ensure that the embedding is a 2D numpy array\\n        if embedding.ndim == 1:\\n            embedding = embedding.reshape(1, -1)\\n        # print(\"embedding is \", embedding)\\n        # print(\"embedding type is \", type(embedding))\\n        # print(\"embedding shape is \", embedding.shape)\\n        self.index.add(embedding)\\n        self.values.append(value)\\n    else:\\n        if verbose:\\n            display(\\n                Markdown(\\n                    \"The value {value} was already in the index\".format(value=value)\\n                )\\n            )\\n',\n",
       " '\\ndef get_embedding_by_index(self, index: int) -> np.ndarray:\\n    \"\"\"\\n        Get the embedding corresponding to a certain index value.\\n        \"\"\"\\n    if index < 0 or index >= len(self.values):\\n        raise ValueError(\"The index is out of range\")\\n\\n    # Fetch the embedding from the Faiss index\\n    embedding = self.index.reconstruct(index)\\n\\n    return embedding\\n',\n",
       " '\\ndef get_index_by_value(self, value: str) -> Optional[int]:\\n    \"\"\"\\n        Get the index corresponding to a value in self.values.\\n        \"\"\"\\n    if value in self.values:\\n        index = self.values.index(value)\\n        return index\\n    else:\\n        return None\\n',\n",
       " '\\ndef get_embedding_by_value(self, value: str) -> Optional[np.ndarray]:\\n    \"\"\"\\n        Get the embedding corresponding to a certain value in self.values.\\n        \"\"\"\\n    index = self.get_index_by_value(value)\\n    if index is not None:\\n        embedding = self.get_embedding_by_index(index)\\n        return embedding\\n    else:\\n        return None\\n',\n",
       " '\\ndef get_all_embeddings(self) -> np.ndarray:\\n    \"\"\"\\n        Get all the embeddings in the index.\\n        \"\"\"\\n    embeddings = []\\n    for i in range(len(self.values)):\\n        embeddings.append(self.get_embedding_by_index(i))\\n    self.embeddings = np.array(embeddings)\\n    return self.embeddings\\n',\n",
       " '\\ndef faiss_query(self, query: str, k:int =10) -> Tuple[List[str], List[float]]:\\n    \"\"\" Query the faiss index for the top-k most similar values to the query\"\"\"\\n\\n    # Embed the data\\n    embedding = self.embedder.embed(query)\\n    if k > len(self.values):\\n        k = len(self.values)\\n    # Query the Faiss index for the top-K most similar values\\n    D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\\n    # Get the values corresponding to the indices\\n    values = [self.values[i] for i in I[0]]\\n    scores = [d for d in D[0]]\\n    return values, scores, I\\n',\n",
       " '\\ndef token_bound_query(self, query, k=10, max_tokens=4000):\\n    \"\"\" Query the faiss index for the top-k most similar values to the query, but bound the number of tokens retrieved by the max_tokens parameter\"\"\"\\n    returned_tokens = 0\\n    top_k_hint = []\\n    scores = []\\n    tokens = []\\n    indices = []\\n\\n    if len(self.values) > 0:\\n        top_k, scores, indices = self.faiss_query(query, k=min(k, len(self.values)))\\n\\n        for hint in top_k:\\n            # mark the message and gets the length in tokens\\n            message_tokens = len(self.tokenizer.encode(hint))\\n            tokens.append(message_tokens)\\n            if returned_tokens + message_tokens <= max_tokens:\\n                top_k_hint += [hint]\\n                returned_tokens += message_tokens\\n            \\n        self.query_history.append({\"query\": query, \"hints\": top_k_hint, \"scores\": scores, \"indices\":indices, \"hints_tokens\": tokens, \"returned_tokens\": returned_tokens , \"max_tokens\": max_tokens, \"k\": k})\\n\\n    return top_k_hint, scores, indices\\n',\n",
       " '\\ndef save(self):\\n    \"\"\" Save the index to disk using faiss and json and numpy\"\"\"\\n    # Create the directory to save the index, values, and embeddings\\n    save_directory = os.path.join(self.save_path, self.name)\\n    os.makedirs(save_directory, exist_ok=True)\\n\\n    # Save the FAISS index\\n    index_filename = os.path.join(save_directory, f\"{self.name}_index.faiss\")\\n    faiss.write_index(self.index, index_filename)\\n\\n    # Save the index values\\n    values_filename = os.path.join(save_directory, f\"{self.name}_values.json\")\\n    with open(values_filename, \"w\") as f:\\n        json.dump(self.values, f)\\n\\n    # Save the numpy array of the embeddings\\n    embeddings_filename = os.path.join(save_directory, f\"{self.name}_embeddings.npz\")\\n    # print(f\"embs: {self.get_all_embeddings().shape}\")\\n    np.savez_compressed(embeddings_filename, self.get_all_embeddings())\\n',\n",
       " '\\ndef load(self):\\n    \"\"\" Load the index, values, and embeddings from disk \"\"\"\\n    # Set the directory to load the index, values, and embeddings from\\n    load_directory = os.path.join(self.save_path, self.name)\\n\\n    # Load the FAISS index\\n    index_filename = os.path.join(load_directory, f\"{self.name}_index.faiss\")\\n    self.index = faiss.read_index(index_filename)\\n\\n    # Load the index values\\n    values_filename = os.path.join(load_directory, f\"{self.name}_values.json\")\\n    with open(values_filename, \"r\") as f:\\n        self.values = json.load(f)\\n\\n    # Load the numpy array of the embeddings\\n    embeddings_filename = os.path.join(load_directory, f\"{self.name}_embeddings.npz\")\\n    embeddings_data = np.load(embeddings_filename)\\n    self.embeddings = embeddings_data[\\'arr_0\\']\\n',\n",
       " '\\ndef save_pickle(self, path=None):\\n    \"\"\"saves the index and values to a pickle file\"\"\"\\n    if path is None and self.save_path is None:\\n        path = self.name + \".pkl\"\\n    elif path is None and self.save_path is not None:\\n        if self.save_path.endswith(\"/\"):\\n            path = self.save_path + self.name + \".pkl\"\\n        else:\\n            path = self.save_path + \"/\" + self.name + \".pkl\"\\n    print(\"Saving the index to \", path)\\n    with open(path, \"wb\") as f:\\n        pickle.dump({\"index\": self.index, \"values\": self.values}, f)\\n',\n",
       " '\\ndef load_pickle(self, path=None):\\n    \"\"\"loads the index and values from a pickle file\"\"\"\\n    if path is None and self.save_path is None:\\n        path = self.name + \".pkl\"\\n    elif path is None and self.save_path is not None:\\n        if self.save_path.endswith(\"/\"):\\n            path = self.save_path + self.name + \".pkl\"\\n        else:\\n            path = self.save_path + \"/\" + self.name + \".pkl\"\\n\\n    with open(path, \"rb\") as f:\\n        data = pickle.load(f)\\n        self.index = data[\"index\"]\\n        self.values = data[\"values\"]\\n',\n",
       " '\\ndef prune_index(\\n    self,\\n    constraint: Optional[str] = None,\\n    regex_pattern: Optional[str] = None,\\n    length_constraint: Optional[int] = None,\\n) -> \\'MemoryIndex\\':\\n    \"\"\" Prune the index based on the constraint provided. Currently, only regex and length constraints are supported. \"\"\"\\n\\n    if constraint is not None:\\n        if constraint == \"regex\":\\n            if regex_pattern is None:\\n                raise ValueError(\\n                    \"regex_pattern must be provided for regex constraint.\"\\n                )\\n            pruned_values, pruned_embeddings = self._prune_by_regex(regex_pattern)\\n        elif constraint == \"length\":\\n            if length_constraint is None:\\n                raise ValueError(\\n                    \"length_constraint must be provided for length constraint.\"\\n                )\\n            pruned_values, pruned_embeddings = self._prune_by_length(\\n                length_constraint\\n            )\\n        else:\\n            raise ValueError(\"Invalid constraint type provided.\")\\n    else:\\n        raise ValueError(\"constraint must be provided for pruning the index.\")\\n\\n    # Create a new index with pruned values and embeddings\\n    pruned_memory_index = MemoryIndex(\\n        values=pruned_values,\\n        embeddings=pruned_embeddings,\\n        name=self.name + \"_pruned\",\\n    )\\n\\n    return pruned_memory_index\\n',\n",
       " '\\ndef _prune_by_regex(self, regex_pattern: str) -> Tuple[List[str], List[np.ndarray]]:\\n    \"\"\" Prune the index by the regex pattern provided.\"\"\"\\n    pruned_values = []\\n    pruned_embeddings = []\\n\\n    for value in self.values:\\n        if re.search(regex_pattern, value):\\n            pruned_values.append(value)\\n            pruned_embeddings.append(self.get_embedding_by_value(value))\\n\\n    return pruned_values, pruned_embeddings\\n',\n",
       " '\\ndef _prune_by_length(self, length_constraint: int) -> Tuple[List[str], List[np.ndarray]]:\\n    \"\"\" Prune the index by the length constraint provided.\"\"\"\\n    pruned_values = []\\n    pruned_embeddings = []\\n\\n    for value in self.values:\\n        if len(value) >= length_constraint:\\n            pruned_values.append(value)\\n            pruned_embeddings.append(self.get_embedding_by_value(value))\\n\\n    return pruned_values, pruned_embeddings\\n',\n",
       " 'import copy\\nimport os\\nimport pickle\\nimport re\\nimport time\\nimport json\\nimport faiss\\nimport numpy as np\\nimport tiktoken\\nfrom IPython.display import Markdown, display\\n\\nfrom babydragon.models.embedders.ada2 import OpenAiEmbedder\\n\\nfrom typing import List, Optional, Union, Dict, Tuple \\n\\n\\nclass MemoryIndex:\\n    \"\"\"\\n    this class is a wrapper for a faiss index, it contains information about the format of the index the faiss index itself\\n    \"\"\"\\n    def __init__(\\n        self,\\n        index: Optional[faiss.Index] = None,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        tokenizer: Optional[tiktoken.Encoding] = None,\\n        ):\\n\\n        self.name = name\\n        self.embedder = OpenAiEmbedder()\\n        if save_path is None:\\n            save_path = \"storage\"\\n\\n        self.save_path = save_path\\n\\n        # Create the \\'storage\\' folder if it does not exist\\n        os.makedirs(self.save_path, exist_ok=True)\\n        self.values = []\\n        if load is True:\\n            self.load()\\n        else:\\n            self.init_index(index, values, embeddings)\\n        if tokenizer is None:\\n            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n        else:\\n            self.tokenizer = tokenizer\\n        self.query_history = []\\n        self.save()\\n\\n    def init_index(\\n        self,\\n        index: Optional[faiss.Index] = None,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        ) -> None:\\n\\n        \"\"\"\\n        initializes the index, there are 4 cases:\\n        1. we create a new index from scratch\\n        2. we create a new index from a list of embeddings and values\\n        3. we create a new index from a faiss index and values list\\n        4. we load an index from a file\\n        \"\"\"\\n        # fist case is when we create a new index from scratch\\n        if index is None and values is None and embeddings is None:\\n            print(\"Creating a new index\")\\n            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n            self.values = []\\n        # second case is where we create the index from a list of embeddings\\n        elif (\\n            index is None\\n            and values is not None\\n            and embeddings is not None\\n            and len(values) == len(embeddings)\\n        ):\\n            print(\"Creating a new index from a list of embeddings and values\")\\n            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n            for embedding, value in zip(embeddings, values):\\n                self.add_to_index(value, embedding)\\n        # third case is where we create the index from a faiss index and values list\\n        elif (\\n            isinstance(index, faiss.Index)\\n            and index.d == self.embedder.get_embedding_size()\\n            and type(values) == list\\n            and len(values) == index.ntotal\\n        ):\\n            print(\"Creating a new index from a faiss index and values list\")\\n            self.index = index\\n            self.values = values\\n        # fourth case is where we create an index from a list of values, the values are embedded and the index is created\\n        elif index is None and values is not None and embeddings is None:\\n            print(\"Creating a new index from a list of values\")\\n            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n            i = 0\\n            for value in values:\\n                # print the value id to see the progress\\n                print(\"Embedding value \", i, \" of \", len(values))\\n                # start tracking the time using time\\n                start = time.time()\\n                self.add_to_index(value)\\n                # print the time it took to embed the value\\n                print(\"Embedding value \", i, \" took \", time.time() - start, \" seconds\")\\n                i += 1\\n        else:\\n            raise ValueError(\\n                \"The index is not a valid faiss index or the embedding dimension is not correct\"\\n            )\\n\\n    def add_to_index(\\n        self,\\n        value: str,\\n        embedding: Optional[Union[List[float], np.ndarray, str]] = None,\\n        verbose: bool = True,\\n        ) -> None:\\n        \"\"\"\\n        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\\n        \"\"\"\\n        if value not in self.values:\\n            if embedding is None:\\n                embedding = self.embedder.embed(value)\\n                if verbose:\\n                    display(Markdown(\"The value {value} was embedded\".format(value=value)))\\n\\n            if type(embedding) is list:\\n                embedding = np.array([embedding])\\n            elif type(embedding) is str:\\n                embedding = eval(embedding)\\n                embedding = np.array([embedding]).astype(np.float32)\\n            elif type(embedding) is not np.ndarray:\\n                raise ValueError(\"The embedding is not a valid type\")\\n\\n            # Ensure that the embedding is a 2D numpy array\\n            if embedding.ndim == 1:\\n                embedding = embedding.reshape(1, -1)\\n            # print(\"embedding is \", embedding)\\n            # print(\"embedding type is \", type(embedding))\\n            # print(\"embedding shape is \", embedding.shape)\\n            self.index.add(embedding)\\n            self.values.append(value)\\n        else:\\n            if verbose:\\n                display(\\n                    Markdown(\\n                        \"The value {value} was already in the index\".format(value=value)\\n                    )\\n                )\\n\\n    def get_embedding_by_index(self, index: int) -> np.ndarray:\\n        \"\"\"\\n        Get the embedding corresponding to a certain index value.\\n        \"\"\"\\n        if index < 0 or index >= len(self.values):\\n            raise ValueError(\"The index is out of range\")\\n\\n        # Fetch the embedding from the Faiss index\\n        embedding = self.index.reconstruct(index)\\n\\n        return embedding\\n\\n    def get_index_by_value(self, value: str) -> Optional[int]:\\n        \"\"\"\\n        Get the index corresponding to a value in self.values.\\n        \"\"\"\\n        if value in self.values:\\n            index = self.values.index(value)\\n            return index\\n        else:\\n            return None\\n\\n    def get_embedding_by_value(self, value: str) -> Optional[np.ndarray]:\\n        \"\"\"\\n        Get the embedding corresponding to a certain value in self.values.\\n        \"\"\"\\n        index = self.get_index_by_value(value)\\n        if index is not None:\\n            embedding = self.get_embedding_by_index(index)\\n            return embedding\\n        else:\\n            return None\\n\\n    def get_all_embeddings(self) -> np.ndarray:\\n        \"\"\"\\n        Get all the embeddings in the index.\\n        \"\"\"\\n        embeddings = []\\n        for i in range(len(self.values)):\\n            embeddings.append(self.get_embedding_by_index(i))\\n        self.embeddings = np.array(embeddings)\\n        return self.embeddings\\n\\n    def faiss_query(self, query: str, k:int =10) -> Tuple[List[str], List[float]]:\\n        \"\"\" Query the faiss index for the top-k most similar values to the query\"\"\"\\n\\n        # Embed the data\\n        embedding = self.embedder.embed(query)\\n        if k > len(self.values):\\n            k = len(self.values)\\n        # Query the Faiss index for the top-K most similar values\\n        D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\\n        # Get the values corresponding to the indices\\n        values = [self.values[i] for i in I[0]]\\n        scores = [d for d in D[0]]\\n        return values, scores, I\\n    \\n    def token_bound_query(self, query, k=10, max_tokens=4000):\\n        \"\"\" Query the faiss index for the top-k most similar values to the query, but bound the number of tokens retrieved by the max_tokens parameter\"\"\"\\n        returned_tokens = 0\\n        top_k_hint = []\\n        scores = []\\n        tokens = []\\n        indices = []\\n\\n        if len(self.values) > 0:\\n            top_k, scores, indices = self.faiss_query(query, k=min(k, len(self.values)))\\n\\n            for hint in top_k:\\n                # mark the message and gets the length in tokens\\n                message_tokens = len(self.tokenizer.encode(hint))\\n                tokens.append(message_tokens)\\n                if returned_tokens + message_tokens <= max_tokens:\\n                    top_k_hint += [hint]\\n                    returned_tokens += message_tokens\\n            \\n            self.query_history.append({\"query\": query, \"hints\": top_k_hint, \"scores\": scores, \"indices\":indices, \"hints_tokens\": tokens, \"returned_tokens\": returned_tokens , \"max_tokens\": max_tokens, \"k\": k})\\n\\n        return top_k_hint, scores, indices\\n    \\n    def save(self):\\n        \"\"\" Save the index to disk using faiss and json and numpy\"\"\"\\n        # Create the directory to save the index, values, and embeddings\\n        save_directory = os.path.join(self.save_path, self.name)\\n        os.makedirs(save_directory, exist_ok=True)\\n\\n        # Save the FAISS index\\n        index_filename = os.path.join(save_directory, f\"{self.name}_index.faiss\")\\n        faiss.write_index(self.index, index_filename)\\n\\n        # Save the index values\\n        values_filename = os.path.join(save_directory, f\"{self.name}_values.json\")\\n        with open(values_filename, \"w\") as f:\\n            json.dump(self.values, f)\\n\\n        # Save the numpy array of the embeddings\\n        embeddings_filename = os.path.join(save_directory, f\"{self.name}_embeddings.npz\")\\n        # print(f\"embs: {self.get_all_embeddings().shape}\")\\n        np.savez_compressed(embeddings_filename, self.get_all_embeddings())\\n\\n    def load(self):\\n        \"\"\" Load the index, values, and embeddings from disk \"\"\"\\n        # Set the directory to load the index, values, and embeddings from\\n        load_directory = os.path.join(self.save_path, self.name)\\n\\n        # Load the FAISS index\\n        index_filename = os.path.join(load_directory, f\"{self.name}_index.faiss\")\\n        self.index = faiss.read_index(index_filename)\\n\\n        # Load the index values\\n        values_filename = os.path.join(load_directory, f\"{self.name}_values.json\")\\n        with open(values_filename, \"r\") as f:\\n            self.values = json.load(f)\\n\\n        # Load the numpy array of the embeddings\\n        embeddings_filename = os.path.join(load_directory, f\"{self.name}_embeddings.npz\")\\n        embeddings_data = np.load(embeddings_filename)\\n        self.embeddings = embeddings_data[\\'arr_0\\']\\n\\n    def save_pickle(self, path=None):\\n        \"\"\"saves the index and values to a pickle file\"\"\"\\n        if path is None and self.save_path is None:\\n            path = self.name + \".pkl\"\\n        elif path is None and self.save_path is not None:\\n            if self.save_path.endswith(\"/\"):\\n                path = self.save_path + self.name + \".pkl\"\\n            else:\\n                path = self.save_path + \"/\" + self.name + \".pkl\"\\n        print(\"Saving the index to \", path)\\n        with open(path, \"wb\") as f:\\n            pickle.dump({\"index\": self.index, \"values\": self.values}, f)\\n\\n    def load_pickle(self, path=None):\\n        \"\"\"loads the index and values from a pickle file\"\"\"\\n        if path is None and self.save_path is None:\\n            path = self.name + \".pkl\"\\n        elif path is None and self.save_path is not None:\\n            if self.save_path.endswith(\"/\"):\\n                path = self.save_path + self.name + \".pkl\"\\n            else:\\n                path = self.save_path + \"/\" + self.name + \".pkl\"\\n\\n        with open(path, \"rb\") as f:\\n            data = pickle.load(f)\\n            self.index = data[\"index\"]\\n            self.values = data[\"values\"]\\n\\n    def prune_index(\\n        self,\\n        constraint: Optional[str] = None,\\n        regex_pattern: Optional[str] = None,\\n        length_constraint: Optional[int] = None,\\n    ) -> \\'MemoryIndex\\':\\n        \"\"\" Prune the index based on the constraint provided. Currently, only regex and length constraints are supported. \"\"\"\\n\\n        if constraint is not None:\\n            if constraint == \"regex\":\\n                if regex_pattern is None:\\n                    raise ValueError(\\n                        \"regex_pattern must be provided for regex constraint.\"\\n                    )\\n                pruned_values, pruned_embeddings = self._prune_by_regex(regex_pattern)\\n            elif constraint == \"length\":\\n                if length_constraint is None:\\n                    raise ValueError(\\n                        \"length_constraint must be provided for length constraint.\"\\n                    )\\n                pruned_values, pruned_embeddings = self._prune_by_length(\\n                    length_constraint\\n                )\\n            else:\\n                raise ValueError(\"Invalid constraint type provided.\")\\n        else:\\n            raise ValueError(\"constraint must be provided for pruning the index.\")\\n\\n        # Create a new index with pruned values and embeddings\\n        pruned_memory_index = MemoryIndex(\\n            values=pruned_values,\\n            embeddings=pruned_embeddings,\\n            name=self.name + \"_pruned\",\\n        )\\n\\n        return pruned_memory_index\\n\\n    def _prune_by_regex(self, regex_pattern: str) -> Tuple[List[str], List[np.ndarray]]:\\n        \"\"\" Prune the index by the regex pattern provided.\"\"\"\\n        pruned_values = []\\n        pruned_embeddings = []\\n\\n        for value in self.values:\\n            if re.search(regex_pattern, value):\\n                pruned_values.append(value)\\n                pruned_embeddings.append(self.get_embedding_by_value(value))\\n\\n        return pruned_values, pruned_embeddings\\n\\n    def _prune_by_length(self, length_constraint: int) -> Tuple[List[str], List[np.ndarray]]:\\n        \"\"\" Prune the index by the length constraint provided.\"\"\"\\n        pruned_values = []\\n        pruned_embeddings = []\\n\\n        for value in self.values:\\n            if len(value) >= length_constraint:\\n                pruned_values.append(value)\\n                pruned_embeddings.append(self.get_embedding_by_value(value))\\n\\n        return pruned_values, pruned_embeddings',\n",
       " 'def __init__(\\n    self,\\n    directory_path: str,\\n    name: str = \"python_index\",\\n    save_path: Optional[str] = None,\\n    load: bool = False,\\n    minify_code: bool = False,\\n    remove_docstrings: bool = False,\\n    tokenizer: Optional[tiktoken.Encoding] = None,\\n):\\n    # Initialize the MemoryIndex\\n    MemoryIndex.__init__(\\n        self,\\n        name=name,\\n        save_path=save_path,\\n        load=load,\\n        tokenizer=tokenizer,\\n    )\\n    # Initialize the PythonParser\\n    PythonParser.__init__(\\n        self,\\n        directory_path=directory_path,\\n        minify_code=minify_code,\\n        remove_docstrings=remove_docstrings,\\n    )\\n\\n    if not load:\\n        # Extract functions and classes source code\\n        function_source_codes, class_source_codes, _, _ = self.process_directory()\\n\\n        # Concatenate function and class source code and index them\\n        for code in function_source_codes + class_source_codes:\\n            self.add_to_index(code)\\n\\n        self.save()\\n',\n",
       " 'from typing import Optional\\nfrom babydragon.memory.indexes.memory_index import MemoryIndex\\nfrom babydragon.processors.parsers.python_parser import PythonParser\\nimport tiktoken\\n\\nclass PythonIndex(MemoryIndex, PythonParser):\\n    def __init__(\\n        self,\\n        directory_path: str,\\n        name: str = \"python_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        tokenizer: Optional[tiktoken.Encoding] = None,\\n    ):\\n        # Initialize the MemoryIndex\\n        MemoryIndex.__init__(\\n            self,\\n            name=name,\\n            save_path=save_path,\\n            load=load,\\n            tokenizer=tokenizer,\\n        )\\n        # Initialize the PythonParser\\n        PythonParser.__init__(\\n            self,\\n            directory_path=directory_path,\\n            minify_code=minify_code,\\n            remove_docstrings=remove_docstrings,\\n        )\\n\\n        if not load:\\n            # Extract functions and classes source code\\n            function_source_codes, class_source_codes, _, _ = self.process_directory()\\n\\n            # Concatenate function and class source code and index them\\n            for code in function_source_codes + class_source_codes:\\n                self.add_to_index(code)\\n\\n            self.save()',\n",
       " 'def __init__(\\n    self,\\n    pandaframe: Union[pd.DataFrame, str],\\n    columns: Optional[Union[str, List[str]]] = None,\\n    name: str = \\'panda_index\\',\\n    save_path: Optional[str] = None,\\n    in_place: bool = True,\\n    embeddings_col: Optional[str] = None,\\n):\\n    \"\"\"\\n        Create a PandasIndex object.\\n\\n        Args:\\n            pandaframe: The DataFrame or path to a CSV file.\\n            columns: The columns of the DataFrame to use as values.\\n            name: The name of the index.\\n            save_path: The path to save the index.\\n            in_place: Whether to work on the DataFrame in place or create a copy.\\n            embeddings_col: The column name containing the embeddings.\\n        \"\"\"\\n    self.columns = columns\\n    self.values = []\\n\\n    # Load or copy pandaframe, and set self.name, self.columns\\n    if isinstance(pandaframe, str) and pandaframe.endswith(\".csv\") and os.path.isfile(pandaframe):\\n        try:\\n            pandaframe = pd.read_csv(pandaframe)\\n        except:\\n            raise ValueError(\"The CSV file is not valid\")\\n        self.name = pandaframe.split(\"/\")[-1].split(\".\")[0]\\n        self.columns = \"values\"\\n    elif isinstance(pandaframe, pd.core.frame.DataFrame) and columns is not None:\\n        if not in_place:\\n            pandaframe = copy.deepcopy(pandaframe)\\n    else:\\n        raise ValueError(\"The pandaframe is not a valid pandas dataframe or the columns are not valid or the path is not valid\")\\n\\n    values, embeddings = self.extract_values_and_embeddings(pandaframe, embeddings_col)\\n    super().__init__(values=values, embeddings=embeddings, name=name, save_path=save_path)\\n',\n",
       " '\\ndef extract_values_and_embeddings(\\n    self,\\n    pandaframe: pd.DataFrame,\\n    embeddings_col: Optional[str],\\n) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n        Extract values and embeddings from a pandas DataFrame.\\n\\n        Args:\\n            pandaframe: The DataFrame to extract values and embeddings from.\\n            embeddings_col: The column name containing the embeddings.\\n\\n        Returns:\\n            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n        \"\"\"\\n    if isinstance(self.columns, list) and len(self.columns) > 1:\\n        pandaframe[\"values\"] = pandaframe[self.columns].apply(lambda x: \\' \\'.join(x), axis=1)\\n        self.columns = \"values\"\\n    elif isinstance(self.columns, list) and len(self.columns) == 1:\\n        self.columns = self.columns[0]\\n        pandaframe[\"values\"] = pandaframe[self.columns]\\n        self.columns = \"values\"\\n    elif not isinstance(self.columns, str):\\n        raise ValueError(\"The columns are not valid\")\\n\\n    values = []\\n    embeddings = []\\n\\n    for _, row in pandaframe.iterrows():\\n        value = row[\"values\"]\\n        values.append(value)\\n\\n        if embeddings_col is not None:\\n            embedding = row[embeddings_col]\\n            embeddings.append(embedding)\\n\\n    return values, embeddings if embeddings_col is not None else None\\n',\n",
       " 'import pandas as pd\\nimport copy\\nimport os\\nfrom typing import List, Optional, Union, Tuple\\nfrom babydragon.memory.indexes.memory_index import MemoryIndex\\nimport numpy as np\\n\\nclass PandasIndex(MemoryIndex):\\n    def __init__(\\n        self,\\n        pandaframe: Union[pd.DataFrame, str],\\n        columns: Optional[Union[str, List[str]]] = None,\\n        name: str = \\'panda_index\\',\\n        save_path: Optional[str] = None,\\n        in_place: bool = True,\\n        embeddings_col: Optional[str] = None,\\n    ):\\n        \"\"\"\\n        Create a PandasIndex object.\\n\\n        Args:\\n            pandaframe: The DataFrame or path to a CSV file.\\n            columns: The columns of the DataFrame to use as values.\\n            name: The name of the index.\\n            save_path: The path to save the index.\\n            in_place: Whether to work on the DataFrame in place or create a copy.\\n            embeddings_col: The column name containing the embeddings.\\n        \"\"\"\\n        self.columns = columns\\n        self.values = []\\n\\n        # Load or copy pandaframe, and set self.name, self.columns\\n        if isinstance(pandaframe, str) and pandaframe.endswith(\".csv\") and os.path.isfile(pandaframe):\\n            try:\\n                pandaframe = pd.read_csv(pandaframe)\\n            except:\\n                raise ValueError(\"The CSV file is not valid\")\\n            self.name = pandaframe.split(\"/\")[-1].split(\".\")[0]\\n            self.columns = \"values\"\\n        elif isinstance(pandaframe, pd.core.frame.DataFrame) and columns is not None:\\n            if not in_place:\\n                pandaframe = copy.deepcopy(pandaframe)\\n        else:\\n            raise ValueError(\"The pandaframe is not a valid pandas dataframe or the columns are not valid or the path is not valid\")\\n\\n        values, embeddings = self.extract_values_and_embeddings(pandaframe, embeddings_col)\\n        super().__init__(values=values, embeddings=embeddings, name=name, save_path=save_path)\\n\\n    def extract_values_and_embeddings(\\n        self,\\n        pandaframe: pd.DataFrame,\\n        embeddings_col: Optional[str],\\n    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n        \"\"\"\\n        Extract values and embeddings from a pandas DataFrame.\\n\\n        Args:\\n            pandaframe: The DataFrame to extract values and embeddings from.\\n            embeddings_col: The column name containing the embeddings.\\n\\n        Returns:\\n            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n        \"\"\"\\n        if isinstance(self.columns, list) and len(self.columns) > 1:\\n            pandaframe[\"values\"] = pandaframe[self.columns].apply(lambda x: \\' \\'.join(x), axis=1)\\n            self.columns = \"values\"\\n        elif isinstance(self.columns, list) and len(self.columns) == 1:\\n            self.columns = self.columns[0]\\n            pandaframe[\"values\"] = pandaframe[self.columns]\\n            self.columns = \"values\"\\n        elif not isinstance(self.columns, str):\\n            raise ValueError(\"The columns are not valid\")\\n\\n        values = []\\n        embeddings = []\\n\\n        for _, row in pandaframe.iterrows():\\n            value = row[\"values\"]\\n            values.append(value)\\n\\n            if embeddings_col is not None:\\n                embedding = row[embeddings_col]\\n                embeddings.append(embedding)\\n\\n        return values, embeddings if embeddings_col is not None else None\\n',\n",
       " \"\\ndef __init__(self, model: Optional[str] = None,\\n             index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None, \\n              system_prompt: Optional[str] = None, user_prompt: Optional[str] = None, name: str = 'fifo_memory',\\n              max_index_memory: int = 400,  max_fifo_memory: int = 2048, max_output_tokens: int = 1000, \\n              longterm_thread: Optional[BaseThread] =None):\\n    \\n    FifoThread.__init__(self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread)\\n    Chat.__init__(self, model= model,index_dict= index_dict,  max_output_tokens=max_output_tokens, max_index_memory=max_index_memory, system_prompt=system_prompt, user_prompt=user_prompt)\\n\\n    self.prompt_func = self.fifo_memory_prompt\\n\",\n",
       " '\\ndef fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\\n    \"\"\"\\n        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n    marked_question = mark_question(self.user_prompt(message))\\n    prompt = [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\\n    return prompt, marked_question \\n',\n",
       " '\\n\\ndef query(self, question: str, verbose: bool = True) -> str:\\n    \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n    # First call the base class\\'s query method\\n    answer = BaseChat.query(self, message=question, verbose=verbose)\\n    marked_question = mark_question(question)\\n    # Add the marked question and answer to the memory\\n    self.add_message(marked_question)\\n    self.add_message(answer)\\n\\n    return answer\\n',\n",
       " \"\\ndef __init__(self, model: Optional[str] = None, index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,  name: str = 'vector_memory',  max_index_memory: int = 400,  max_vector_memory: int = 2048, max_output_tokens: int = 1000, system_prompt: str = None, user_prompt: str = None):\\n    VectorThread.__init__(self, name=name, max_context=max_vector_memory)\\n    Chat.__init__(self, model= model,index_dict= index_dict,  max_output_tokens=max_output_tokens, max_index_memory=max_index_memory, system_prompt=system_prompt, user_prompt=user_prompt)\\n    self.max_vector_memory = self.max_context\\n    self.prompt_func = self.vector_memory_prompt\\n\",\n",
       " '\\ndef vector_memory_prompt(self, message: str, k: int = 10) -> Tuple[List[dict], dict]: \\n    \"\"\"\\n        Combine system prompt, k most similar messages to the question, and the user prompt.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include in the prompt.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n    sorted_messages, sorted_scores, sorted_indices = self.sorted_query(message, k=k, max_tokens = self.max_vector_memory,reverse=True) \\n    marked_question = mark_question(self.user_prompt(message))\\n    prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\\n    return prompt, marked_question\\n',\n",
       " '\\ndef weighted_memory_prompt(self, message: str, k: int = 10, decay_factor: float = 0.1, temporal_weight: float = 0.5) -> Tuple[List[dict], dict]:\\n    \"\"\"\\n        Combine system prompt, weighted k most similar messages to the question, and the user prompt.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include in the prompt.\\n        :param decay_factor: A float representing the decay factor for weighting.\\n        :param temporal_weight: A float representing the weight of the temporal aspect.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n    weighted_messages, weighted_scores, weighted_indices = self.weighted_query(message, k=k, max_tokens = self.max_vector_memory,decay_factor = decay_factor, temporal_weight = temporal_weight, order_by = \\'chronological\\', reverse = True)\\n    marked_question = mark_question(self.user_prompt(message))\\n    prompt = [mark_system(self.system_prompt)] + weighted_messages + [marked_question]\\n    return prompt, marked_question \\n',\n",
       " '\\ndef query(self, question: str, verbose: bool = False) -> str:\\n    \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n    # First call the base class\\'s query method\\n    answer = BaseChat.query(self, message=question, verbose=verbose)\\n    marked_question = mark_question(question)\\n    # Add the marked question and answer to the memory\\n    self.add_message(marked_question)\\n    self.add_message(answer)\\n    return answer\\n',\n",
       " \"def __init__(self, model: str = None, index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,  system_prompt: str = None, user_prompt: str = None, name: str = 'fifo_vector_memory', max_memory: int = 2048,  max_index_memory: int = 400,  max_output_tokens: int = 1000, longterm_thread: Optional[VectorThread] = None, longterm_frac: float = 0.5):\\n    self.total_max_memory = max_memory\\n\\n    self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\\n    FifoThread.__init__(self, name=name, max_memory=self.max_fifo_memory, longterm_thread=self.longterm_thread)\\n    Chat.__init__(self, model= model, index_dict= index_dict,  max_output_tokens=max_output_tokens, max_index_memory=max_index_memory, system_prompt=system_prompt, user_prompt=user_prompt)\\n    self.prompt_func = self.fifovector_memory_prompt\\n    self.prompt_list = []\\n\",\n",
       " '\\ndef setup_longterm_memory(self, longterm_thread: Optional[VectorThread], max_memory: int, longterm_frac: float):\\n    \"\"\"\\n        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\\n\\n        :param longterm_thread: An optional VectorThread for long-term memory.\\n        :param max_memory: The maximum amount of memory for the chatbot.\\n        :param longterm_frac: The fraction of memory dedicated to long-term memory.\\n        \"\"\"\\n    if longterm_thread is None:\\n        self.longterm_frac = longterm_frac\\n        self.max_fifo_memory = int(max_memory * (1-self.longterm_frac))\\n        self.max_vector_memory = max_memory - self.max_fifo_memory    \\n        self.longterm_thread = VectorThread(name= \\'longterm_memory\\', max_context=self.max_vector_memory)\\n    else:\\n        self.longterm_thread = longterm_thread\\n        self.max_vector_memory = self.longterm_thread.max_context\\n        self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\\n        self.longterm_frac = self.max_vector_memory / self.total_max_memory\\n',\n",
       " '\\ndef fifovector_memory_prompt(self, message: str, k: int = 10) -> Tuple[List[dict], dict]:\\n    \"\"\"\\n        Combine the system prompt, long-term memory (vector memory), short-term memory (FIFO memory), and the user prompt.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include from the long-term memory.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n    prompt = [mark_system(self.system_prompt)]\\n    if len(self.longterm_thread.memory_thread) > 0 and self.longterm_thread.total_tokens <= self.max_vector_memory:\\n        prompt += self.longterm_thread.memory_thread\\n    elif len(self.longterm_thread.memory_thread) > 0 and self.longterm_thread.total_tokens > self.max_vector_memory:\\n        sorted_messages, sorted_scores, sorted_indices = self.longterm_thread.sorted_query(message, k=k, max_tokens = self.max_vector_memory,reverse=True) \\n        prompt += sorted_messages\\n\\n    prompt += self.memory_thread\\n    marked_question = mark_question(self.user_prompt(message))\\n    prompt += [marked_question]\\n    return prompt, marked_question\\n',\n",
       " 'def query(self, question: str, verbose: bool = False) -> str:\\n    \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n    answer = BaseChat.query(self, message=question, verbose=verbose)\\n    marked_question = mark_question(question)\\n    self.add_message(marked_question)\\n    self.add_message(answer)\\n    return answer\\n    \\n\\n\\n\\n    \\n',\n",
       " 'from babydragon.memory.threads.fifo_thread import FifoThread\\nfrom babydragon.memory.threads.vector_thread import VectorThread\\nfrom babydragon.memory.threads.base_thread import BaseThread\\nfrom babydragon.chat.chat import BaseChat, Prompter, Chat\\nfrom babydragon.utils.oai import mark_question, mark_system, mark_answer\\n\\n\\n\\nfrom typing import List, Tuple\\nfrom typing import List, Tuple, Optional, Dict, Union\\nfrom babydragon.memory.indexes.memory_index import MemoryIndex \\nfrom babydragon.memory.indexes.pandas_index import PandasIndex\\n\\n\\n\\nclass FifoChat(FifoThread, Chat):\\n    \"\"\"\\n    A chatbot class that combines FIFO Memory Thread, BaseChat, and Prompter. The oldest messages are removed first\\n    when reaching the max_memory limit. The memory is defined in terms of tokens, and outs are passed to the\\n    longterm_memory. The lucid_memory is a redundant memory that stores all the messages.\\n    \"\"\"\\n\\n    def __init__(self, model: Optional[str] = None,\\n                 index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None, \\n                  system_prompt: Optional[str] = None, user_prompt: Optional[str] = None, name: str = \\'fifo_memory\\',\\n                  max_index_memory: int = 400,  max_fifo_memory: int = 2048, max_output_tokens: int = 1000, \\n                  longterm_thread: Optional[BaseThread] =None):\\n        \\n        FifoThread.__init__(self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread)\\n        Chat.__init__(self, model= model,index_dict= index_dict,  max_output_tokens=max_output_tokens, max_index_memory=max_index_memory, system_prompt=system_prompt, user_prompt=user_prompt)\\n\\n        self.prompt_func = self.fifo_memory_prompt\\n\\n    def fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\\n        \"\"\"\\n        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n        marked_question = mark_question(self.user_prompt(message))\\n        prompt = [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\\n        return prompt, marked_question \\n\\n\\n    def query(self, question: str, verbose: bool = True) -> str:\\n        \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n        # First call the base class\\'s query method\\n        answer = BaseChat.query(self, message=question, verbose=verbose)\\n        marked_question = mark_question(question)\\n        # Add the marked question and answer to the memory\\n        self.add_message(marked_question)\\n        self.add_message(answer)\\n\\n        return answer\\n    \\nclass VectorChat(VectorThread, Chat):\\n    \"\"\"\\n    A chatbot class that combines Vector Memory Thread, BaseChat, and Prompter. Memory prompt is constructed by\\n    filling the memory with the k most similar messages to the question until the max prompt memory tokens are reached.\\n    \"\"\"\\n\\n    def __init__(self, model: Optional[str] = None, index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,  name: str = \\'vector_memory\\',  max_index_memory: int = 400,  max_vector_memory: int = 2048, max_output_tokens: int = 1000, system_prompt: str = None, user_prompt: str = None):\\n        VectorThread.__init__(self, name=name, max_context=max_vector_memory)\\n        Chat.__init__(self, model= model,index_dict= index_dict,  max_output_tokens=max_output_tokens, max_index_memory=max_index_memory, system_prompt=system_prompt, user_prompt=user_prompt)\\n        self.max_vector_memory = self.max_context\\n        self.prompt_func = self.vector_memory_prompt\\n\\n    def vector_memory_prompt(self, message: str, k: int = 10) -> Tuple[List[dict], dict]: \\n        \"\"\"\\n        Combine system prompt, k most similar messages to the question, and the user prompt.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include in the prompt.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n        sorted_messages, sorted_scores, sorted_indices = self.sorted_query(message, k=k, max_tokens = self.max_vector_memory,reverse=True) \\n        marked_question = mark_question(self.user_prompt(message))\\n        prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\\n        return prompt, marked_question\\n    \\n    def weighted_memory_prompt(self, message: str, k: int = 10, decay_factor: float = 0.1, temporal_weight: float = 0.5) -> Tuple[List[dict], dict]:\\n        \"\"\"\\n        Combine system prompt, weighted k most similar messages to the question, and the user prompt.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include in the prompt.\\n        :param decay_factor: A float representing the decay factor for weighting.\\n        :param temporal_weight: A float representing the weight of the temporal aspect.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n        weighted_messages, weighted_scores, weighted_indices = self.weighted_query(message, k=k, max_tokens = self.max_vector_memory,decay_factor = decay_factor, temporal_weight = temporal_weight, order_by = \\'chronological\\', reverse = True)\\n        marked_question = mark_question(self.user_prompt(message))\\n        prompt = [mark_system(self.system_prompt)] + weighted_messages + [marked_question]\\n        return prompt, marked_question \\n\\n    def query(self, question: str, verbose: bool = False) -> str:\\n        \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n        # First call the base class\\'s query method\\n        answer = BaseChat.query(self, message=question, verbose=verbose)\\n        marked_question = mark_question(question)\\n        # Add the marked question and answer to the memory\\n        self.add_message(marked_question)\\n        self.add_message(answer)\\n        return answer\\n\\n\\nclass FifoVectorChat(FifoThread, Chat):\\n    \"\"\"\\n    A chatbot class that combines FIFO Memory Thread, Vector Memory Thread, BaseChat, and Prompter.\\n    The memory prompt is constructed by including both FIFO memory and Vector memory.\\n    \"\"\"\\n    def __init__(self, model: str = None, index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,  system_prompt: str = None, user_prompt: str = None, name: str = \\'fifo_vector_memory\\', max_memory: int = 2048,  max_index_memory: int = 400,  max_output_tokens: int = 1000, longterm_thread: Optional[VectorThread] = None, longterm_frac: float = 0.5):\\n        self.total_max_memory = max_memory\\n\\n        self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\\n        FifoThread.__init__(self, name=name, max_memory=self.max_fifo_memory, longterm_thread=self.longterm_thread)\\n        Chat.__init__(self, model= model, index_dict= index_dict,  max_output_tokens=max_output_tokens, max_index_memory=max_index_memory, system_prompt=system_prompt, user_prompt=user_prompt)\\n        self.prompt_func = self.fifovector_memory_prompt\\n        self.prompt_list = []\\n\\n    def setup_longterm_memory(self, longterm_thread: Optional[VectorThread], max_memory: int, longterm_frac: float):\\n        \"\"\"\\n        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\\n\\n        :param longterm_thread: An optional VectorThread for long-term memory.\\n        :param max_memory: The maximum amount of memory for the chatbot.\\n        :param longterm_frac: The fraction of memory dedicated to long-term memory.\\n        \"\"\"\\n        if longterm_thread is None:\\n            self.longterm_frac = longterm_frac\\n            self.max_fifo_memory = int(max_memory * (1-self.longterm_frac))\\n            self.max_vector_memory = max_memory - self.max_fifo_memory    \\n            self.longterm_thread = VectorThread(name= \\'longterm_memory\\', max_context=self.max_vector_memory)\\n        else:\\n            self.longterm_thread = longterm_thread\\n            self.max_vector_memory = self.longterm_thread.max_context\\n            self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\\n            self.longterm_frac = self.max_vector_memory / self.total_max_memory\\n\\n    def fifovector_memory_prompt(self, message: str, k: int = 10) -> Tuple[List[dict], dict]:\\n        \"\"\"\\n        Combine the system prompt, long-term memory (vector memory), short-term memory (FIFO memory), and the user prompt.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include from the long-term memory.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n        prompt = [mark_system(self.system_prompt)]\\n        if len(self.longterm_thread.memory_thread) > 0 and self.longterm_thread.total_tokens <= self.max_vector_memory:\\n            prompt += self.longterm_thread.memory_thread\\n        elif len(self.longterm_thread.memory_thread) > 0 and self.longterm_thread.total_tokens > self.max_vector_memory:\\n            sorted_messages, sorted_scores, sorted_indices = self.longterm_thread.sorted_query(message, k=k, max_tokens = self.max_vector_memory,reverse=True) \\n            prompt += sorted_messages\\n\\n        prompt += self.memory_thread\\n        marked_question = mark_question(self.user_prompt(message))\\n        prompt += [marked_question]\\n        return prompt, marked_question\\n    def query(self, question: str, verbose: bool = False) -> str:\\n        \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n        answer = BaseChat.query(self, message=question, verbose=verbose)\\n        marked_question = mark_question(question)\\n        self.add_message(marked_question)\\n        self.add_message(answer)\\n        return answer\\n    \\n\\n\\n\\n        \\n',\n",
       " '\\ndef __init__(self, model: str = None, max_output_tokens: int = 1000, system_prompt: str = None, user_prompt: str = None, index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None, max_index_memory: int = 1000) -> None:\\n    BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\\n    Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\\n    self.index_dict = index_dict\\n    self.setup_indices(max_index_memory)\\n        \\n',\n",
       " 'def setup_indices(self,max_index_memory):\\n    \"\"\" setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index. \\n        \"\"\"\\n    if self.index_dict is not None:\\n        self.current_index = list(self.index_dict.keys())[0]\\n        self.system_prompt = INDEX_SYSTEM_PROMPT if self.user_defined_system_prompt is False else self.system_prompt\\n        self.user_prompt = self.get_index_hints if self.user_defined_user_prompt is False else self.user_prompt\\n        self.max_index_memory = max_index_memory\\n',\n",
       " '   \\n\\ndef get_index_hints(self, question: str, k: int = 10, max_tokens: int = None) -> str:\\n    \"\"\"\\n        Get hints from the current index for the given question.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include from the index.\\n        :param max_tokens: The maximum number of tokens to be retrieved from the index.\\n        :return: A string representing the hint prompt with the question.\\n        \"\"\"\\n    if max_tokens is None:\\n        max_tokens = self.max_index_memory\\n    hints = []\\n    if self.current_index is not None:\\n        index_instance = self.index_dict[self.current_index]\\n        if isinstance(index_instance, PandasIndex) or isinstance(index_instance, MemoryIndex) or isinstance(index_instance, PythonIndex):\\n            hints, _, _ = index_instance.token_bound_query(question, k=k, max_tokens=max_tokens)         \\n        else:\\n            raise ValueError(\"The current index is not a valid index instance.\")\\n        hints_string = \"\\\\n\".join(hints)\\n        hint_prompt = INDEX_HINT_PROMPT\\n        question_intro = QUESTION_INTRO\\n        return hint_prompt.format(hints_string=hints_string) + question_intro.format(question=question)\\n    else:\\n        return question\\n',\n",
       " '\\ndef set_current_index(self, index_name: Optional[str]) -> None:\\n    \"\"\"\\n        Set the current index to be used for hints.\\n\\n        :param index_name: A string representing the index name or None to clear the current index.\\n        :raise ValueError: If the provided index name is not available.\\n        \"\"\"\\n    if self.index_dict is None:\\n        raise ValueError(\"No index_dict are available.\")\\n    elif index_name in self.index_dict:\\n        self.current_index = index_name\\n    elif index_name is None:\\n        self.current_index = None\\n    else:\\n        raise ValueError(\"The provided index name is not available.\")\\n',\n",
       " 'from babydragon.chat.prompts.default_prompts import  INDEX_SYSTEM_PROMPT, INDEX_HINT_PROMPT, QUESTION_INTRO\\nfrom typing import Union, Dict, Optional\\nfrom babydragon.memory.indexes.memory_index import MemoryIndex\\nfrom babydragon.memory.indexes.python_index import PythonIndex\\n\\nfrom babydragon.memory.indexes.pandas_index import PandasIndex\\nfrom babydragon.chat.base_chat import BaseChat, Prompter\\n    \\n\\nclass Chat(BaseChat, Prompter):\\n    \"\"\"\\n    This class combines the BaseChat and Prompter classes to create a oneshot chatbot with a system and user prompt,\\n    and the ability to handle multiple index_dict.\\n    \"\"\"\\n\\n    def __init__(self, model: str = None, max_output_tokens: int = 1000, system_prompt: str = None, user_prompt: str = None, index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None, max_index_memory: int = 1000) -> None:\\n        BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\\n        Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\\n        self.index_dict = index_dict\\n        self.setup_indices(max_index_memory)\\n            \\n    def setup_indices(self,max_index_memory):\\n        \"\"\" setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index. \\n        \"\"\"\\n        if self.index_dict is not None:\\n            self.current_index = list(self.index_dict.keys())[0]\\n            self.system_prompt = INDEX_SYSTEM_PROMPT if self.user_defined_system_prompt is False else self.system_prompt\\n            self.user_prompt = self.get_index_hints if self.user_defined_user_prompt is False else self.user_prompt\\n            self.max_index_memory = max_index_memory\\n   \\n\\n    def get_index_hints(self, question: str, k: int = 10, max_tokens: int = None) -> str:\\n        \"\"\"\\n        Get hints from the current index for the given question.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include from the index.\\n        :param max_tokens: The maximum number of tokens to be retrieved from the index.\\n        :return: A string representing the hint prompt with the question.\\n        \"\"\"\\n        if max_tokens is None:\\n            max_tokens = self.max_index_memory\\n        hints = []\\n        if self.current_index is not None:\\n            index_instance = self.index_dict[self.current_index]\\n            if isinstance(index_instance, PandasIndex) or isinstance(index_instance, MemoryIndex) or isinstance(index_instance, PythonIndex):\\n                hints, _, _ = index_instance.token_bound_query(question, k=k, max_tokens=max_tokens)         \\n            else:\\n                raise ValueError(\"The current index is not a valid index instance.\")\\n            hints_string = \"\\\\n\".join(hints)\\n            hint_prompt = INDEX_HINT_PROMPT\\n            question_intro = QUESTION_INTRO\\n            return hint_prompt.format(hints_string=hints_string) + question_intro.format(question=question)\\n        else:\\n            return question\\n\\n    def set_current_index(self, index_name: Optional[str]) -> None:\\n        \"\"\"\\n        Set the current index to be used for hints.\\n\\n        :param index_name: A string representing the index name or None to clear the current index.\\n        :raise ValueError: If the provided index name is not available.\\n        \"\"\"\\n        if self.index_dict is None:\\n            raise ValueError(\"No index_dict are available.\")\\n        elif index_name in self.index_dict:\\n            self.current_index = index_name\\n        elif index_name is None:\\n            self.current_index = None\\n        else:\\n            raise ValueError(\"The provided index name is not available.\")\\n    \\n\\n\\n\\n',\n",
       " '\\ndef __init__(self, system_prompt: str = None, user_prompt: str = None):\\n    \"\"\"\\n        Initialize the Prompter with system and user prompts.\\n\\n        :param system_prompt: A string representing the system prompt.\\n        :param user_prompt: A string representing the user prompt.\\n        \"\"\"\\n    if system_prompt is None:\\n        self.system_prompt = DEFAULT_SYSTEM_PROMPT\\n        self.user_defined_system_prompt = False\\n    else:\\n        self.system_prompt = system_prompt\\n        self.user_defined_system_prompt = True\\n    if user_prompt is None:\\n        self.user_prompt = self.default_user_prompt\\n        self.user_defined_user_prompt = False\\n    else:\\n        self.user_prompt = user_prompt\\n        self.user_defined_user_prompt = True\\n        \\n    self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\\n',\n",
       " '\\ndef default_user_prompt(self, message: str) -> str:\\n    return DEFAULT_USER_PROMPT.format(question=message)\\n',\n",
       " '\\ndef one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\\n    \"\"\"\\n        Compose the prompt for the chat-gpt API.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\\n        \"\"\"\\n    marked_question = mark_question(self.user_prompt(message))\\n    prompt = [mark_system(self.system_prompt)] + [marked_question]\\n    return prompt, marked_question\\n',\n",
       " '\\ndef update_system_prompt(self, new_prompt: str) -> None:\\n    \"\"\"\\n        Update the system prompt.\\n\\n        :param new_prompt: A string representing the new system prompt.\\n        \"\"\"\\n    self.system_prompt = new_prompt\\n',\n",
       " '\\ndef update_user_prompt(self, new_prompt: str) -> None:\\n    \"\"\"\\n        Update the user prompt.\\n\\n        :param new_prompt: A string representing the new user prompt.\\n        \"\"\"\\n    self.user_prompt = new_prompt\\n',\n",
       " 'def __init__(self, model: str = None, max_output_tokens: int = 1000):\\n    \"\"\"\\n        Initialize the BaseChat with a model and max_output_tokens.\\n\\n        :param model: A string representing the chat model to be used.\\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\\n        \"\"\"\\n    if model is None:\\n        self.model = \"gpt-3.5-turbo\"\\n    else:\\n        self.model = model\\n    self.tokenizer = tiktoken.encoding_for_model(\\'gpt-3.5-turbo\\')\\n    self.max_output_tokens = max_output_tokens\\n    self.failed_responses = []\\n    self.outputs = []\\n    self.inputs = []\\n    self.prompts = []\\n    self.prompt_func = self.identity_prompter\\n',\n",
       " '\\ndef identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\\n    \"\"\"\\n        A simple identity prompter that takes a message and returns the message marked as a question.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing the marked question and the original message.\\n        \"\"\"\\n    return [mark_question(message)], mark_question(message)\\n',\n",
       " '\\ndef chat_response(self, prompt: List[dict], max_tokens: int = None) -> Tuple[Dict, bool]:\\n    \"\"\"\\n        Call the OpenAI API with the given prompt and maximum number of output tokens.\\n\\n        :param prompt: A list of strings representing the prompt to send to the API.\\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\\n        :return: A tuple containing the API response as a dictionary and a boolean indicating success.\\n        \"\"\"\\n    if max_tokens is None:\\n        max_tokens = self.max_output_tokens\\n    try:\\n        print(\"Trying to call OpenAI API...\")\\n        response = openai.ChatCompletion.create(\\n            model=self.model,\\n            messages=prompt,\\n            max_tokens=max_tokens,\\n        )\\n        return response, True\\n\\n    except openai.error.APIError as e:\\n        print(e)\\n        fail_response = {\"choices\": [{\"message\": {\"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"}}]}\\n        self.failed_responses.append(fail_response)\\n        return fail_response , False\\n',\n",
       " '\\ndef reply(self, message: str, verbose : bool = True) -> str:\\n    \"\"\"\\n        Reply to a given message using the chatbot.\\n\\n        :param message: A string representing the user message.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n    return self.query(message, verbose)[\"content\"]\\n',\n",
       " '\\ndef query(self, message: str, verbose: bool = True) -> str:\\n    \"\"\"\\n        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\\n\\n        :param message: A string representing the user message.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n\\n    prompt, _ = self.prompt_func(message)\\n    response, success = self.chat_response(prompt)\\n    if verbose:\\n        display(Markdown(\"#### Question: \\\\n {question}\".format(question = message)))\\n    if success:\\n        answer = get_mark_from_response(response)\\n        self.outputs.append(answer)\\n        self.inputs.append(message)\\n        self.prompts.append(prompt)\\n        if verbose:\\n            display(Markdown(\" #### Anwser: \\\\n {answer}\".format(answer = get_str_from_response(response)))) \\n        return answer\\n    else:\\n        raise Exception(\"OpenAI API Error inside query function\")\\n',\n",
       " '\\ndef run_text(self, text: str, state: List[Tuple[str, str]]) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\\n    \"\"\"\\n        Process the user\\'s text input and update the chat state.\\n\\n        :param text: A string representing the user input.\\n        :param state: A list of tuples representing the current chat state.\\n        :return: A tuple containing the updated chat state as two lists of tuples.\\n        \"\"\"\\n    print(\"===============Running run_text =============\")\\n    print(\"Inputs:\", text)\\n    try: \\n        print(\"======>Current memory:\\\\n %s\" % self.memory_thread)\\n    except:\\n        print(\"======>No memory\")\\n    response = self.reply(text)\\n    state = state + [(text, response)]\\n    print(\"Outputs:\", state)\\n    return state, state\\n',\n",
       " '\\ndef gradio(self):\\n    \"\"\"\\n        Create and launch a Gradio interface for the chatbot.\\n        \"\"\"\\n    with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\\n        chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\\n        state = gr.State([])\\n        with gr.Row():\\n            with gr.Column(scale=1):\\n                txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter, or upload an image\").style(container=False)\\n            with gr.Column(scale=0.15, min_width=0):\\n                clear = gr.Button(\"Clear️\")\\n\\n        txt.submit(self.run_text, [txt, state], [chatbot, state])\\n        txt.submit(lambda: \"\", None, txt)\\n        demo.launch(server_name=\"localhost\", server_port=7860 )\\n',\n",
       " 'import gradio as gr\\nimport openai\\nimport tiktoken \\nfrom IPython.display import display, Markdown\\nfrom babydragon.utils.oai import mark_question, mark_system, get_mark_from_response , get_str_from_response\\nfrom babydragon.chat.prompts.default_prompts import DEFAULT_SYSTEM_PROMPT, DEFAULT_USER_PROMPT\\nfrom typing import Callable, Tuple, List\\nfrom typing import List, Tuple, Dict\\n\\n\\nclass Prompter:\\n    \"\"\"\\n    This class handles the system and user prompts and the prompt_func. By subclassing and overriding the\\n    prompt_func, you can change the way the prompts are composed.\\n    \"\"\"\\n\\n    def __init__(self, system_prompt: str = None, user_prompt: str = None):\\n        \"\"\"\\n        Initialize the Prompter with system and user prompts.\\n\\n        :param system_prompt: A string representing the system prompt.\\n        :param user_prompt: A string representing the user prompt.\\n        \"\"\"\\n        if system_prompt is None:\\n            self.system_prompt = DEFAULT_SYSTEM_PROMPT\\n            self.user_defined_system_prompt = False\\n        else:\\n            self.system_prompt = system_prompt\\n            self.user_defined_system_prompt = True\\n        if user_prompt is None:\\n            self.user_prompt = self.default_user_prompt\\n            self.user_defined_user_prompt = False\\n        else:\\n            self.user_prompt = user_prompt\\n            self.user_defined_user_prompt = True\\n            \\n        self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\\n\\n    def default_user_prompt(self, message: str) -> str:\\n        return DEFAULT_USER_PROMPT.format(question=message)\\n\\n    def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\\n        \"\"\"\\n        Compose the prompt for the chat-gpt API.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\\n        \"\"\"\\n        marked_question = mark_question(self.user_prompt(message))\\n        prompt = [mark_system(self.system_prompt)] + [marked_question]\\n        return prompt, marked_question\\n    \\n    def update_system_prompt(self, new_prompt: str) -> None:\\n        \"\"\"\\n        Update the system prompt.\\n\\n        :param new_prompt: A string representing the new system prompt.\\n        \"\"\"\\n        self.system_prompt = new_prompt\\n\\n    def update_user_prompt(self, new_prompt: str) -> None:\\n        \"\"\"\\n        Update the user prompt.\\n\\n        :param new_prompt: A string representing the new user prompt.\\n        \"\"\"\\n        self.user_prompt = new_prompt\\n\\nclass BaseChat:\\n    \"\"\"\\n    This is the base class for chatbots, defining the basic functions that a chatbot should have, mainly the calls to\\n    chat-gpt API, and a basic Gradio interface. It has a prompt_func that acts as a placeholder for a call to chat-gpt\\n    API without any additional messages. It can be overridden by subclasses to add additional messages to the prompt.\\n    \"\"\"\\n    def __init__(self, model: str = None, max_output_tokens: int = 1000):\\n        \"\"\"\\n        Initialize the BaseChat with a model and max_output_tokens.\\n\\n        :param model: A string representing the chat model to be used.\\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\\n        \"\"\"\\n        if model is None:\\n            self.model = \"gpt-3.5-turbo\"\\n        else:\\n            self.model = model\\n        self.tokenizer = tiktoken.encoding_for_model(\\'gpt-3.5-turbo\\')\\n        self.max_output_tokens = max_output_tokens\\n        self.failed_responses = []\\n        self.outputs = []\\n        self.inputs = []\\n        self.prompts = []\\n        self.prompt_func = self.identity_prompter\\n\\n    def identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\\n        \"\"\"\\n        A simple identity prompter that takes a message and returns the message marked as a question.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing the marked question and the original message.\\n        \"\"\"\\n        return [mark_question(message)], mark_question(message)\\n\\n    def chat_response(self, prompt: List[dict], max_tokens: int = None) -> Tuple[Dict, bool]:\\n        \"\"\"\\n        Call the OpenAI API with the given prompt and maximum number of output tokens.\\n\\n        :param prompt: A list of strings representing the prompt to send to the API.\\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\\n        :return: A tuple containing the API response as a dictionary and a boolean indicating success.\\n        \"\"\"\\n        if max_tokens is None:\\n            max_tokens = self.max_output_tokens\\n        try:\\n            print(\"Trying to call OpenAI API...\")\\n            response = openai.ChatCompletion.create(\\n                model=self.model,\\n                messages=prompt,\\n                max_tokens=max_tokens,\\n            )\\n            return response, True\\n\\n        except openai.error.APIError as e:\\n            print(e)\\n            fail_response = {\"choices\": [{\"message\": {\"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"}}]}\\n            self.failed_responses.append(fail_response)\\n            return fail_response , False\\n\\n    def reply(self, message: str, verbose : bool = True) -> str:\\n        \"\"\"\\n        Reply to a given message using the chatbot.\\n\\n        :param message: A string representing the user message.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n        return self.query(message, verbose)[\"content\"]\\n\\n    def query(self, message: str, verbose: bool = True) -> str:\\n        \"\"\"\\n        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\\n\\n        :param message: A string representing the user message.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n\\n        prompt, _ = self.prompt_func(message)\\n        response, success = self.chat_response(prompt)\\n        if verbose:\\n            display(Markdown(\"#### Question: \\\\n {question}\".format(question = message)))\\n        if success:\\n            answer = get_mark_from_response(response)\\n            self.outputs.append(answer)\\n            self.inputs.append(message)\\n            self.prompts.append(prompt)\\n            if verbose:\\n                display(Markdown(\" #### Anwser: \\\\n {answer}\".format(answer = get_str_from_response(response)))) \\n            return answer\\n        else:\\n            raise Exception(\"OpenAI API Error inside query function\")\\n\\n    def run_text(self, text: str, state: List[Tuple[str, str]]) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\\n        \"\"\"\\n        Process the user\\'s text input and update the chat state.\\n\\n        :param text: A string representing the user input.\\n        :param state: A list of tuples representing the current chat state.\\n        :return: A tuple containing the updated chat state as two lists of tuples.\\n        \"\"\"\\n        print(\"===============Running run_text =============\")\\n        print(\"Inputs:\", text)\\n        try: \\n            print(\"======>Current memory:\\\\n %s\" % self.memory_thread)\\n        except:\\n            print(\"======>No memory\")\\n        response = self.reply(text)\\n        state = state + [(text, response)]\\n        print(\"Outputs:\", state)\\n        return state, state\\n\\n    def gradio(self):\\n        \"\"\"\\n        Create and launch a Gradio interface for the chatbot.\\n        \"\"\"\\n        with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\\n            chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\\n            state = gr.State([])\\n            with gr.Row():\\n                with gr.Column(scale=1):\\n                    txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter, or upload an image\").style(container=False)\\n                with gr.Column(scale=0.15, min_width=0):\\n                    clear = gr.Button(\"Clear️\")\\n\\n            txt.submit(self.run_text, [txt, state], [chatbot, state])\\n            txt.submit(lambda: \"\", None, txt)\\n            demo.launch(server_name=\"localhost\", server_port=7860 )',\n",
       " '\\nDEFAULT_SYSTEM_PROMPT = \"You are a useful Assistant you role is to answer questions in an exhaustive way! Please be helpful to the user he loves you!\"\\n\\nDEFAULT_USER_PROMPT = \"{question}\"\\n\\nindex_description = \"This index is help\"\\n\\nINDEX_SYSTEM_PROMPT = \"\"\"You are a Chatbot assistant that can use a external knowledge base to answer questions.\\nThe user will always add hints from the external knowledge base. \\nYou express your thoughts using princpled reasoning and always pay attention to the\\nhints.  Your knowledge base description is:\"\"\"\\n# system_prompt = system_prompt.format(index_descrpiton = index_description)\\n\\nINDEX_HINT_PROMPT = \"\"\"I am going to ask you a question and you should use the hints to answer it. The hints are:\\\\n{hints_string} .\\n            Remember that I can not see the hints, but please reply as if the knowledge base was part of your training set. \"\"\"\\n\\n\\nQUESTION_INTRO = \"The question is: {question}\"\\n',\n",
       " '\\n\\n\\n\\ndef mark_system(system_prompt):\\n    return {\"role\": \"system\", \"content\": system_prompt}\\n',\n",
       " 'def mark_answer(answer):\\n    return {\"role\": \"assistant\", \"content\": answer}\\n',\n",
       " 'def mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n',\n",
       " 'def check_dict(message_dict):\\n        if type(message_dict) is list and len(message_dict) == 1 and type(message_dict[0]) is dict:\\n            message_dict = message_dict[0]\\n        elif type(message_dict) is not dict:\\n            raise Exception(\"The message_dict should be a dictionary or a [dictionary] instead it is \", message_dict, type(message_dict))  \\n        return message_dict\\n',\n",
       " '\\ndef get_mark_from_response(response):\\n        #return the answer from the response\\n        role = response[\\'choices\\'][0][\"message\"][\"role\"]\\n        message = response[\\'choices\\'][0][\"message\"][\"content\"]\\n        return {\"role\": role, \"content\": message}\\n',\n",
       " '\\ndef get_str_from_response(response):\\n        #return the answer from the response\\n        return response[\\'choices\\'][0][\"message\"][\"content\"]\\n',\n",
       " 'import openai\\n\\n\\n\\n\\ndef mark_system(system_prompt):\\n    return {\"role\": \"system\", \"content\": system_prompt}\\ndef mark_answer(answer):\\n    return {\"role\": \"assistant\", \"content\": answer}\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\ndef check_dict(message_dict):\\n        if type(message_dict) is list and len(message_dict) == 1 and type(message_dict[0]) is dict:\\n            message_dict = message_dict[0]\\n        elif type(message_dict) is not dict:\\n            raise Exception(\"The message_dict should be a dictionary or a [dictionary] instead it is \", message_dict, type(message_dict))  \\n        return message_dict\\n\\ndef get_mark_from_response(response):\\n        #return the answer from the response\\n        role = response[\\'choices\\'][0][\"message\"][\"role\"]\\n        message = response[\\'choices\\'][0][\"message\"][\"content\"]\\n        return {\"role\": role, \"content\": message}\\n\\ndef get_str_from_response(response):\\n        #return the answer from the response\\n        return response[\\'choices\\'][0][\"message\"][\"content\"]',\n",
       " 'def get_embedding_size(self):\\n    return ADA_EMBEDDING_SIZE\\n',\n",
       " 'def embed(self, data, embed_mark = False, verbose = False):\\n    try:\\n        if embed_mark is False and type(data) is dict and \"content\" in data:\\n            if verbose is True:\\n                print(\"Embedding without mark\", data[\"content\"])\\n            out = openai.Embedding.create(input=data[\"content\"], engine=\\'text-embedding-ada-002\\')\\n        else:\\n            if verbose is True:\\n                print(\"Embedding without preprocessing the input\", data)\\n            out = openai.Embedding.create(input=str(data), engine=\\'text-embedding-ada-002\\')\\n    except:\\n        raise ValueError(\"The data  is not valid\", data)\\n    return out.data[0].embedding\\n',\n",
       " 'import openai\\n\\nADA_EMBEDDING_SIZE = 1536\\n\\nclass OpenAiEmbedder:\\n    def get_embedding_size(self):\\n        return ADA_EMBEDDING_SIZE\\n    def embed(self, data, embed_mark = False, verbose = False):\\n        try:\\n            if embed_mark is False and type(data) is dict and \"content\" in data:\\n                if verbose is True:\\n                    print(\"Embedding without mark\", data[\"content\"])\\n                out = openai.Embedding.create(input=data[\"content\"], engine=\\'text-embedding-ada-002\\')\\n            else:\\n                if verbose is True:\\n                    print(\"Embedding without preprocessing the input\", data)\\n                out = openai.Embedding.create(input=str(data), engine=\\'text-embedding-ada-002\\')\\n        except:\\n            raise ValueError(\"The data  is not valid\", data)\\n        return out.data[0].embedding',\n",
       " 'def get_embedding_size(self):\\n    return COHERE_EMBEDDING_SIZE\\n',\n",
       " 'def embed(self, data, embed_mark = False, verbose = False):\\n    try:\\n        if embed_mark is False and type(data) is dict and \"content\" in data:\\n            if verbose is True:\\n                print(\"Embedding without mark\", data[\"content\"])\\n            out = co.embed(input=data[\"content\"]).embeddings\\n        else:\\n            if verbose is True:\\n                print(\"Embedding without preprocessing the input\", data)\\n            out = co.embed(input=str(data)).embeddings\\n\\n    except:\\n        raise ValueError(\"The data  is not valid\", data)\\n    return out\\n',\n",
       " 'import cohere as co\\n\\nCOHERE_EMBEDDING_SIZE = 512\\n\\nclass CohereEmbedder:\\n    def get_embedding_size(self):\\n        return COHERE_EMBEDDING_SIZE\\n    def embed(self, data, embed_mark = False, verbose = False):\\n        try:\\n            if embed_mark is False and type(data) is dict and \"content\" in data:\\n                if verbose is True:\\n                    print(\"Embedding without mark\", data[\"content\"])\\n                out = co.embed(input=data[\"content\"]).embeddings\\n            else:\\n                if verbose is True:\\n                    print(\"Embedding without preprocessing the input\", data)\\n                out = co.embed(input=str(data)).embeddings\\n\\n        except:\\n            raise ValueError(\"The data  is not valid\", data)\\n        return out',\n",
       " 'def __init__(self):\\n    self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\\n',\n",
       " '\\ndef search(self, query, max_results=10):\\n    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\\n    record = Entrez.read(handle)\\n    handle.close()\\n    return record[\"IdList\"]\\n',\n",
       " '\\ndef fetch_abstract(self, pubmed_id):\\n    handle = Entrez.efetch(db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\")\\n    abstract = handle.read()\\n    handle.close()\\n    return abstract\\n',\n",
       " '\\ndef fetch_pmc_full_text(self, pubmed_id):\\n    # Get the PMC ID for the PubMed ID\\n    handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\\n    record = Entrez.read(handle)\\n    handle.close()\\n    pmc_id = None\\n    for link in record[0][\"LinkSetDb\"]:\\n        if link[\"DbTo\"] == \"pmc\":\\n            pmc_id = link[\"Link\"][0][\"Id\"]\\n            break\\n\\n    if not pmc_id:\\n        return None\\n\\n    # Fetch the PMC article XML\\n    handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\\n    xml_content = handle.read()\\n    handle.close()\\n\\n    # Parse the XML and extract the full text\\n    soup = BeautifulSoup(xml_content, \"xml\")\\n    full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\\n\\n    return full_text\\n',\n",
       " 'def __init__(self):\\n    self.api = PubmedAPI()\\n',\n",
       " \"\\ndef parse_papers(self, query, max_results=10):\\n    pubmed_ids = self.api.search(query, max_results)\\n    paper_list = []\\n    for pubmed_id in pubmed_ids:\\n        paper_dict = {}\\n        paper_dict['pubmed_id'] = pubmed_id\\n        paper_dict['abstract'] = self.api.fetch_abstract(pubmed_id)\\n        paper_dict['content'] = self.api.fetch_pmc_full_text(pubmed_id)\\n        paper_list.append(paper_dict)\\n    return paper_list\\n\",\n",
       " 'from Bio import Entrez\\nfrom bs4 import BeautifulSoup\\n\\n\\nclass PubmedAPI:\\n    def __init__(self):\\n        self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\\n\\n    def search(self, query, max_results=10):\\n        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\\n        record = Entrez.read(handle)\\n        handle.close()\\n        return record[\"IdList\"]\\n\\n    def fetch_abstract(self, pubmed_id):\\n        handle = Entrez.efetch(db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\")\\n        abstract = handle.read()\\n        handle.close()\\n        return abstract\\n\\n    def fetch_pmc_full_text(self, pubmed_id):\\n        # Get the PMC ID for the PubMed ID\\n        handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\\n        record = Entrez.read(handle)\\n        handle.close()\\n        pmc_id = None\\n        for link in record[0][\"LinkSetDb\"]:\\n            if link[\"DbTo\"] == \"pmc\":\\n                pmc_id = link[\"Link\"][0][\"Id\"]\\n                break\\n\\n        if not pmc_id:\\n            return None\\n\\n        # Fetch the PMC article XML\\n        handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\\n        xml_content = handle.read()\\n        handle.close()\\n\\n        # Parse the XML and extract the full text\\n        soup = BeautifulSoup(xml_content, \"xml\")\\n        full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\\n\\n        return full_text\\n\\n\\nclass PubmedParser:\\n    def __init__(self):\\n        self.api = PubmedAPI()\\n\\n    def parse_papers(self, query, max_results=10):\\n        pubmed_ids = self.api.search(query, max_results)\\n        paper_list = []\\n        for pubmed_id in pubmed_ids:\\n            paper_dict = {}\\n            paper_dict[\\'pubmed_id\\'] = pubmed_id\\n            paper_dict[\\'abstract\\'] = self.api.fetch_abstract(pubmed_id)\\n            paper_dict[\\'content\\'] = self.api.fetch_pmc_full_text(pubmed_id)\\n            paper_list.append(paper_dict)\\n        return paper_list\\n\\n\\nif __name__ == \"__main__\":\\n    # Usage example\\n    parser = PubmedParser()\\n    papers = parser.parse_papers(\"cancer immunotherapy\", max_results=7)\\n    for paper in papers:\\n        print(f\"PubMed ID: {paper[\\'pubmed_id\\']}\")\\n        print(f\"Abstract:\\\\n{paper[\\'abstract\\']}\\\\n\")\\n        if paper[\\'content\\']:\\n            print(f\"Content:\\\\n{paper[\\'content\\']}\\\\n\")\\n        else:\\n            print(\"none\")',\n",
       " \"def __init__(self):\\n    self.base_url = 'https://www.arxiv-vanity.com/'\\n\",\n",
       " \"\\ndef _get_vanity_url(self, arxiv_id):\\n    return urljoin(self.base_url, 'papers/' + arxiv_id)\\n\",\n",
       " '\\ndef _fetch_html(self, url):\\n    response = requests.get(url)\\n    if response.status_code == 200:\\n        return response.text\\n    else:\\n        return None\\n',\n",
       " \"\\ndef _extract_main_content(self, html):\\n    soup = BeautifulSoup(html, 'html.parser')\\n    paragraphs = soup.find_all('div', {'class': 'ltx_para'})\\n    content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\\n    return content\\n\",\n",
       " '\\ndef parse_paper(self, arxiv_id):\\n    vanity_url = self._get_vanity_url(arxiv_id)\\n    html = self._fetch_html(vanity_url)\\n    if html is not None:\\n        return self._extract_main_content(html)\\n    else:\\n        return None\\n',\n",
       " 'def __init__(self):\\n    self.base_url = \"http://export.arxiv.org/api/query?\"\\n    self.pdf_download_url = \"https://arxiv.org/pdf/\"\\n',\n",
       " '\\ndef search(self, query, max_results=10):\\n    url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\\n    response = requests.get(url)\\n    if response.status_code == 200:\\n        return response.text\\n    else:\\n        return None\\n',\n",
       " '\\ndef download_pdf(self, paper_key, save_directory=\"./\"):\\n    pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\\n    response = requests.get(pdf_url)\\n    if response.status_code == 200:\\n        with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\\n            f.write(response.content)\\n        print(f\"PDF for {paper_key} downloaded successfully.\")\\n    else:\\n        print(f\"Error downloading PDF for {paper_key}.\")\\n',\n",
       " 'def __init__(self):\\n    self.api = ArxivAPI()\\n    self.vanity_parser = ArxivVanityParser()\\n',\n",
       " \"\\ndef _parse_arxiv_id(self, url):\\n    return url.split('/')[-1]\\n\",\n",
       " \"\\ndef parse_papers(self, query, max_results=10):\\n    search_results = self.api.search(query, max_results)\\n    if search_results is not None:\\n        soup = BeautifulSoup(search_results, 'html.parser')\\n        entries = soup.find_all('entry')\\n        paper_list = []\\n        for entry in entries:\\n            paper_dict = {}\\n            arxiv_id = self._parse_arxiv_id(entry.id.string)\\n            paper_dict['arxiv_id'] = arxiv_id\\n            paper_dict['title'] = entry.title.string\\n            paper_dict['summary'] = entry.summary.string\\n            paper_dict['content'] = self.vanity_parser.parse_paper(str(arxiv_id))\\n            if paper_dict['content'] == None:\\n                continue\\n            paper_list.append(paper_dict)\\n        return paper_list\\n    else:\\n        return None\\n\",\n",
       " 'import os\\nimport requests\\nfrom bs4 import BeautifulSoup\\nfrom urllib.parse import urlparse, urljoin\\n\\n\\nclass ArxivVanityParser:\\n    def __init__(self):\\n        self.base_url = \\'https://www.arxiv-vanity.com/\\'\\n\\n    def _get_vanity_url(self, arxiv_id):\\n        return urljoin(self.base_url, \\'papers/\\' + arxiv_id)\\n\\n    def _fetch_html(self, url):\\n        response = requests.get(url)\\n        if response.status_code == 200:\\n            return response.text\\n        else:\\n            return None\\n\\n    def _extract_main_content(self, html):\\n        soup = BeautifulSoup(html, \\'html.parser\\')\\n        paragraphs = soup.find_all(\\'div\\', {\\'class\\': \\'ltx_para\\'})\\n        content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\\n        return content\\n\\n    def parse_paper(self, arxiv_id):\\n        vanity_url = self._get_vanity_url(arxiv_id)\\n        html = self._fetch_html(vanity_url)\\n        if html is not None:\\n            return self._extract_main_content(html)\\n        else:\\n            return None\\n\\nclass ArxivAPI:\\n    def __init__(self):\\n        self.base_url = \"http://export.arxiv.org/api/query?\"\\n        self.pdf_download_url = \"https://arxiv.org/pdf/\"\\n\\n    def search(self, query, max_results=10):\\n        url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\\n        response = requests.get(url)\\n        if response.status_code == 200:\\n            return response.text\\n        else:\\n            return None\\n\\n    def download_pdf(self, paper_key, save_directory=\"./\"):\\n        pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\\n        response = requests.get(pdf_url)\\n        if response.status_code == 200:\\n            with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\\n                f.write(response.content)\\n            print(f\"PDF for {paper_key} downloaded successfully.\")\\n        else:\\n            print(f\"Error downloading PDF for {paper_key}.\")\\n\\n\\nclass ArxivParser:\\n    def __init__(self):\\n        self.api = ArxivAPI()\\n        self.vanity_parser = ArxivVanityParser()\\n\\n    def _parse_arxiv_id(self, url):\\n        return url.split(\\'/\\')[-1]\\n\\n    def parse_papers(self, query, max_results=10):\\n        search_results = self.api.search(query, max_results)\\n        if search_results is not None:\\n            soup = BeautifulSoup(search_results, \\'html.parser\\')\\n            entries = soup.find_all(\\'entry\\')\\n            paper_list = []\\n            for entry in entries:\\n                paper_dict = {}\\n                arxiv_id = self._parse_arxiv_id(entry.id.string)\\n                paper_dict[\\'arxiv_id\\'] = arxiv_id\\n                paper_dict[\\'title\\'] = entry.title.string\\n                paper_dict[\\'summary\\'] = entry.summary.string\\n                paper_dict[\\'content\\'] = self.vanity_parser.parse_paper(str(arxiv_id))\\n                if paper_dict[\\'content\\'] == None:\\n                    continue\\n                paper_list.append(paper_dict)\\n            return paper_list\\n        else:\\n            return None\\n\\nif __name__ == \"__main__\":\\n    # Usage example\\n    parser = ArxivParser()\\n    papers = parser.parse_papers(\"SVD\", max_results=5)\\n    for paper in papers:\\n        print(paper[\\'title\\'])\\n        print(paper[\\'arxiv_id\\'])\\n        print(paper[\\'summary\\'])\\n        print(paper[\\'content\\'])\\n',\n",
       " 'def __init__(self, directory_path: str):\\n    self.directory_path = directory_path\\n',\n",
       " '\\ndef get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\\n    \"\"\"Returns a list of all files in a directory\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    all_files = []\\n    for root, _, files in os.walk(directory_path):\\n        for file in files:\\n            all_files.append(os.path.join(root, file))\\n\\n    return all_files\\n',\n",
       " '\\ndef get_files_with_extension(self, extension: str, directory_path: Optional[str] = None) -> List[str]:\\n    \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    all_files = self.get_all_files(directory_path)\\n    files_with_extension = [file for file in all_files if file.endswith(extension)]\\n\\n    return files_with_extension\\n',\n",
       " '\\ndef get_file_extension(self, file_path: str) -> str:\\n    \"\"\"Returns the extension of a file\"\"\"\\n    return Path(file_path).suffix\\n',\n",
       " '\\ndef get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\\n    \"\"\" Returns a list of all subdirectories in a directory\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    subdirectories = [os.path.join(directory_path, d) for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\\n\\n    return subdirectories\\n',\n",
       " '\\ndef create_directory(self, directory_path: str) -> None:\\n    \"\"\" Creates a directory if it does not exist\"\"\"\\n    if not os.path.exists(directory_path):\\n        os.makedirs(directory_path)\\n',\n",
       " '\\ndef delete_directory(self, directory_path: str) -> None:\\n    \"\"\" Deletes a directory if it exists\"\"\"\\n    if os.path.exists(directory_path):\\n        shutil.rmtree(directory_path)\\n',\n",
       " '\\ndef copy_file(self, source_path: str, destination_path: str) -> None:\\n    \"\"\" Copies a file from one location to another\"\"\"\\n    shutil.copy2(source_path, destination_path)\\n',\n",
       " '\\ndef move_file(self, source_path: str, destination_path: str) -> None:\\n    \"\"\" Moves a file from one location to another\"\"\"\\n    shutil.move(source_path, destination_path)\\n',\n",
       " 'import os\\nfrom pathlib import Path\\nfrom typing import List, Dict, Optional\\nimport shutil\\n\\n\\nclass OsProcessor:\\n    def __init__(self, directory_path: str):\\n        self.directory_path = directory_path\\n\\n    def get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\\n        \"\"\"Returns a list of all files in a directory\"\"\"\\n        if directory_path is None:\\n            directory_path = self.directory_path\\n\\n        all_files = []\\n        for root, _, files in os.walk(directory_path):\\n            for file in files:\\n                all_files.append(os.path.join(root, file))\\n\\n        return all_files\\n\\n    def get_files_with_extension(self, extension: str, directory_path: Optional[str] = None) -> List[str]:\\n        \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\\n        if directory_path is None:\\n            directory_path = self.directory_path\\n\\n        all_files = self.get_all_files(directory_path)\\n        files_with_extension = [file for file in all_files if file.endswith(extension)]\\n\\n        return files_with_extension\\n\\n    def get_file_extension(self, file_path: str) -> str:\\n        \"\"\"Returns the extension of a file\"\"\"\\n        return Path(file_path).suffix\\n\\n    def get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\\n        \"\"\" Returns a list of all subdirectories in a directory\"\"\"\\n        if directory_path is None:\\n            directory_path = self.directory_path\\n\\n        subdirectories = [os.path.join(directory_path, d) for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\\n\\n        return subdirectories\\n\\n    def create_directory(self, directory_path: str) -> None:\\n        \"\"\" Creates a directory if it does not exist\"\"\"\\n        if not os.path.exists(directory_path):\\n            os.makedirs(directory_path)\\n\\n    def delete_directory(self, directory_path: str) -> None:\\n        \"\"\" Deletes a directory if it exists\"\"\"\\n        if os.path.exists(directory_path):\\n            shutil.rmtree(directory_path)\\n\\n    def copy_file(self, source_path: str, destination_path: str) -> None:\\n        \"\"\" Copies a file from one location to another\"\"\"\\n        shutil.copy2(source_path, destination_path)\\n\\n    def move_file(self, source_path: str, destination_path: str) -> None:\\n        \"\"\" Moves a file from one location to another\"\"\"\\n        shutil.move(source_path, destination_path)\\n',\n",
       " 'def __init__(self, base_directory: str, username=None, repo_name=None, code_parsers=None, minify_code: bool = False, remove_docstrings: bool = False):\\n    self.username = username\\n    self.repo_name = repo_name\\n    self.base_directory = base_directory\\n    self.github = Github()\\n    self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n    repo_path = self.clone_repo(self.repo.clone_url)\\n\\n    OsProcessor.__init__(self,repo_path)\\n    self.code_parsers = code_parsers or [PythonParser(repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings)]\\n',\n",
       " '\\n\\ndef get_public_repos(self):\\n    \"\"\" Returns a list of all public repos for the user.\"\"\"\\n    user = self.github.get_user(self.username)\\n    return user.get_repos()\\n',\n",
       " '\\ndef clone_repo(self, repo_url: str):\\n    \"\"\" Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n    target_directory = os.path.join(self.base_directory, repo_name)\\n\\n    if os.path.exists(target_directory):\\n        shutil.rmtree(target_directory)\\n\\n    subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n    return target_directory\\n',\n",
       " '\\ndef process_repo(self, repo_path=None):\\n    \"\"\" Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n    if repo_path is None:\\n        repo_path = self.directory_path\\n\\n    for code_parser in self.code_parsers:\\n        code_parser.directory_path = repo_path\\n        code_parser.process_directory(repo_path)\\n',\n",
       " '\\ndef process_repos(self):\\n    \"\"\" Processes all public repos for the user.\"\"\"\\n    for repo in self.get_public_repos():\\n        if not repo.private:\\n            print(f\"Processing repo: {repo.name}\")\\n            repo_path = self.clone_repo(repo.clone_url)\\n            self.process_repo(repo_path)\\n            shutil.rmtree(repo_path)\\n',\n",
       " '\\ndef get_repo(self, repo_name):\\n    \"\"\" Returns the repo with the specified name.\"\"\"\\n    user = self.github.get_user(self.username)\\n    return user.get_repo(repo_name)\\n',\n",
       " '\\ndef process_single_repo(self):\\n\\n    repo = self.get_repo(self.repo_name)\\n    print(f\"Processing repo: {self.repo_name}\")\\n    repo_path = self.clone_repo(repo.clone_url)\\n    self.process_repo(repo_path)\\n    shutil.rmtree(repo_path)\\n',\n",
       " 'import os\\nimport shutil\\nimport subprocess\\nfrom typing import List\\n\\nfrom github import Github\\n\\nfrom babydragon.processors.os_processor import OsProcessor\\nfrom babydragon.processors.parsers.python_parser import PythonParser\\n\\n\\nclass GithubProcessor(OsProcessor):\\n    def __init__(self, base_directory: str, username=None, repo_name=None, code_parsers=None, minify_code: bool = False, remove_docstrings: bool = False):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.base_directory = base_directory\\n        self.github = Github()\\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n        repo_path = self.clone_repo(self.repo.clone_url)\\n\\n        OsProcessor.__init__(self,repo_path)\\n        self.code_parsers = code_parsers or [PythonParser(repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings)]\\n\\n\\n    def get_public_repos(self):\\n        \"\"\" Returns a list of all public repos for the user.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def clone_repo(self, repo_url: str):\\n        \"\"\" Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.base_directory, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n    def process_repo(self, repo_path=None):\\n        \"\"\" Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n        if repo_path is None:\\n            repo_path = self.directory_path\\n\\n        for code_parser in self.code_parsers:\\n            code_parser.directory_path = repo_path\\n            code_parser.process_directory(repo_path)\\n\\n    def process_repos(self):\\n        \"\"\" Processes all public repos for the user.\"\"\"\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.clone_repo(repo.clone_url)\\n                self.process_repo(repo_path)\\n                shutil.rmtree(repo_path)\\n\\n    def get_repo(self, repo_name):\\n        \"\"\" Returns the repo with the specified name.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_single_repo(self):\\n\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.clone_repo(repo.clone_url)\\n        self.process_repo(repo_path)\\n        shutil.rmtree(repo_path)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits',\n",
       " '\\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n    \"\"\" This method is called for every FunctionDef node in the tree.\\n        and it does the following:\\n        1. Gets the source code for the node\\n        2. Adds the node to the list of function nodes\\n        3. Adds the source code to the list of function source codes\\n        \"\"\"\\n    function_source_code = cst.Module([]).code_for_node(node)\\n    self.function_nodes.append(node)\\n    self.function_source_codes.append(function_source_code)\\n',\n",
       " '\\ndef visit_ClassDef(self, node: cst.ClassDef) -> None:\\n    \"\"\" This method is called for every ClassDef node in the tree.\\n        and it does the following:\\n        1. Gets the source code for the node\\n        2. Adds the node to the list of class nodes\\n        3. Adds the source code to the list of class source codes\\n        \"\"\"\\n    class_source_code = cst.Module([]).code_for_node(node)\\n    self.class_nodes.append(node)\\n    self.class_source_codes.append(class_source_code)\\n',\n",
       " 'def __init__(self, directory_path: str, visitor: Optional[FunctionAndClassVisitor] = None, minify_code: bool = False, remove_docstrings: bool = False):\\n    super().__init__(directory_path)\\n    self.visitor = visitor if visitor else FunctionAndClassVisitor()\\n    self.minify_code = minify_code\\n    self.remove_docstrings = remove_docstrings\\n',\n",
       " '\\n\\ndef remove_docstring(self, tree: cst.Module) -> str:\\n    \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\\n\\n    # Remove docstrings using a transformer\\n    class DocstringRemover(cst.CSTTransformer):\\n        def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\\n            docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n            if docstring.startswith(\"No docstring\"):\\n                return updated_node\\n\\n            return updated_node.with_changes(body=updated_node.body.with_changes(body=[stmt for stmt in updated_node.body.body if not (isinstance(stmt, cst.SimpleStatementLine) and any(isinstance(expr, cst.Expr) and isinstance(expr.value, cst.SimpleString) for expr in stmt.body))]))\\n\\n    tree = tree.visit(DocstringRemover())\\n    return tree.code\\n',\n",
       " 'def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\\n    docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n    if docstring.startswith(\"No docstring\"):\\n        return updated_node\\n\\n    return updated_node.with_changes(body=updated_node.body.with_changes(body=[stmt for stmt in updated_node.body.body if not (isinstance(stmt, cst.SimpleStatementLine) and any(isinstance(expr, cst.Expr) and isinstance(expr.value, cst.SimpleString) for expr in stmt.body))]))\\n',\n",
       " '\\ndef _process_file(self, file_path: str):\\n    \"\"\" This method is called for every file in the directory.\\n        It does the following:\\n        1. Reads the file\\n        2. Parses the file\\n        3. Visits the file with the visitor\\n        \"\"\"\\n    with open(file_path, \"r\", encoding=\\'utf-8\\') as file:\\n        source_code = file.read()\\n\\n    try:\\n        tree = cst.parse_module(source_code)\\n    except cst.ParserSyntaxError:\\n        print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n        return\\n\\n    tree.visit(self.visitor)\\n\\n    # Remove docstrings if specified\\n    if self.remove_docstrings:\\n        source_code = self.remove_docstring(source_code, tree)\\n\\n    # Minify the code if specified\\n    if self.minify_code:\\n        minifier = PythonMinifier(source_code)\\n        source_code = minifier.get_minified_code()\\n\\n    # Add the processed code to the corresponding list in the visitor\\n    self.visitor.function_source_codes.append(source_code)\\n',\n",
       " '\\ndef process_file(self, file_path: str):\\n    \"\"\" This method is called for every file in the directory.\\n        It does the following:\\n        1. Runs flake8 on the file\\n        if flake8 returns a non-zero exit code, it means the file has a syntax error\\n        2. Reads the file\\n        3. Parses the file\\n        4. Visits the file with the visitor\\n\\n        \"\"\"\\n    result = subprocess.run([\"flake8\", \"--select=E999\", file_path], capture_output=True)\\n\\n    if result.returncode != 0:\\n        print(f\"Skipping file with syntax error: {file_path}\")\\n        print(result.stderr.decode(\"utf-8\"))\\n        return\\n\\n    with open(file_path, \"r\", encoding=\\'utf-8\\') as f:\\n        source_code = f.read()\\n\\n    try:\\n        tree = cst.parse_module(source_code)\\n        tree.visit(self.visitor)\\n    except cst.ParserSyntaxError as e:\\n        print(f\"Syntax error: {e}\")\\n        print(f\"Skipping file with syntax error: {file_path}\")\\n',\n",
       " '\\ndef process_directory(self) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\\n    \"\"\" This method is called for every directory.\\n        It does the following:\\n        1. Gets all the python files in the directory\\n        2. Processes each file\\n        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\\n        \"\"\"\\n    function_source_codes = []\\n    class_source_codes = []\\n\\n    python_files = self.get_files_with_extension(\\'.py\\')\\n\\n    for file_path in python_files:\\n        self._process_file(file_path)\\n\\n    function_source_codes = self.visitor.function_source_codes\\n    function_nodes = self.visitor.function_nodes\\n    class_source_codes = self.visitor.class_source_codes\\n    class_nodes = self.visitor.class_nodes\\n\\n    return function_source_codes, class_source_codes, function_nodes, class_nodes\\n',\n",
       " 'import os\\nimport subprocess\\nimport libcst as cst\\nfrom typing import List, Tuple, Union, Optional\\nfrom babydragon.processors.os_processor import OsProcessor\\nfrom python_minifier import minify\\n\\n\\nclass PythonMinifier:\\n    def __init__(self, code: str = None):\\n\\n        self.code = code\\n        self.output_code = None\\n\\n    def minify(self):\\n        if self.code:\\n            self.output_code = self.minify_code(self.code)\\n\\n    def get_minified_code(self):\\n        if not self.output_code:\\n            self.minify()\\n        return self.output_code\\n\\n    @staticmethod\\n    def minify_code(code: str) -> str:\\n        return minify(code)\\n\\nclass PythonDocstringExtractor:\\n    @staticmethod\\n    def extract_docstring(function_def: cst.FunctionDef) -> str:\\n        docstring = None\\n\\n        for stmt in function_def.body.body:\\n            if isinstance(stmt, cst.SimpleStatementLine):\\n                for expr in stmt.body:\\n                    if isinstance(expr, cst.Expr) and isinstance(expr.value, cst.SimpleString):\\n                        docstring = expr.value.value.strip(\\'\"\\').strip(\"\\'\")\\n                        break\\n            if docstring is not None:\\n                break\\n\\n        if docstring is not None:\\n            return docstring.strip()\\n        else:\\n            function_name = function_def.name.value\\n            return f\"No docstring provided for function \\'{function_name}\\'. Please add a docstring to describe this function.\"\\n\\nclass FunctionAndClassVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        \"\"\" This method is called for every FunctionDef node in the tree.\\n        and it does the following:\\n        1. Gets the source code for the node\\n        2. Adds the node to the list of function nodes\\n        3. Adds the source code to the list of function source codes\\n        \"\"\"\\n        function_source_code = cst.Module([]).code_for_node(node)\\n        self.function_nodes.append(node)\\n        self.function_source_codes.append(function_source_code)\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\\n        \"\"\" This method is called for every ClassDef node in the tree.\\n        and it does the following:\\n        1. Gets the source code for the node\\n        2. Adds the node to the list of class nodes\\n        3. Adds the source code to the list of class source codes\\n        \"\"\"\\n        class_source_code = cst.Module([]).code_for_node(node)\\n        self.class_nodes.append(node)\\n        self.class_source_codes.append(class_source_code)\\n\\n\\nclass PythonParser(OsProcessor):\\n    def __init__(self, directory_path: str, visitor: Optional[FunctionAndClassVisitor] = None, minify_code: bool = False, remove_docstrings: bool = False):\\n        super().__init__(directory_path)\\n        self.visitor = visitor if visitor else FunctionAndClassVisitor()\\n        self.minify_code = minify_code\\n        self.remove_docstrings = remove_docstrings\\n\\n    \\n    def remove_docstring(self, tree: cst.Module) -> str:\\n        \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\\n\\n        # Remove docstrings using a transformer\\n        class DocstringRemover(cst.CSTTransformer):\\n            def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\\n                docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n                if docstring.startswith(\"No docstring\"):\\n                    return updated_node\\n\\n                return updated_node.with_changes(body=updated_node.body.with_changes(body=[stmt for stmt in updated_node.body.body if not (isinstance(stmt, cst.SimpleStatementLine) and any(isinstance(expr, cst.Expr) and isinstance(expr.value, cst.SimpleString) for expr in stmt.body))]))\\n\\n        tree = tree.visit(DocstringRemover())\\n        return tree.code\\n\\n    def _process_file(self, file_path: str):\\n        \"\"\" This method is called for every file in the directory.\\n        It does the following:\\n        1. Reads the file\\n        2. Parses the file\\n        3. Visits the file with the visitor\\n        \"\"\"\\n        with open(file_path, \"r\", encoding=\\'utf-8\\') as file:\\n            source_code = file.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n        except cst.ParserSyntaxError:\\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n            return\\n\\n        tree.visit(self.visitor)\\n\\n        # Remove docstrings if specified\\n        if self.remove_docstrings:\\n            source_code = self.remove_docstring(source_code, tree)\\n\\n        # Minify the code if specified\\n        if self.minify_code:\\n            minifier = PythonMinifier(source_code)\\n            source_code = minifier.get_minified_code()\\n\\n        # Add the processed code to the corresponding list in the visitor\\n        self.visitor.function_source_codes.append(source_code)\\n\\n    def process_file(self, file_path: str):\\n        \"\"\" This method is called for every file in the directory.\\n        It does the following:\\n        1. Runs flake8 on the file\\n        if flake8 returns a non-zero exit code, it means the file has a syntax error\\n        2. Reads the file\\n        3. Parses the file\\n        4. Visits the file with the visitor\\n\\n        \"\"\"\\n        result = subprocess.run([\"flake8\", \"--select=E999\", file_path], capture_output=True)\\n\\n        if result.returncode != 0:\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n            print(result.stderr.decode(\"utf-8\"))\\n            return\\n\\n        with open(file_path, \"r\", encoding=\\'utf-8\\') as f:\\n            source_code = f.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n            tree.visit(self.visitor)\\n        except cst.ParserSyntaxError as e:\\n            print(f\"Syntax error: {e}\")\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n\\n    def process_directory(self) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\\n        \"\"\" This method is called for every directory.\\n        It does the following:\\n        1. Gets all the python files in the directory\\n        2. Processes each file\\n        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\\n        \"\"\"\\n        function_source_codes = []\\n        class_source_codes = []\\n\\n        python_files = self.get_files_with_extension(\\'.py\\')\\n\\n        for file_path in python_files:\\n            self._process_file(file_path)\\n\\n        function_source_codes = self.visitor.function_source_codes\\n        function_nodes = self.visitor.function_nodes\\n        class_source_codes = self.visitor.class_source_codes\\n        class_nodes = self.visitor.class_nodes\\n\\n        return function_source_codes, class_source_codes, function_nodes, class_nodes\\n',\n",
       " \"\\nclass CodeGenerator:\\n    def __init__(self):\\n        self.git_memory = None\\n        self.commit_index = None\\n        self.contextual_memory = None\\n\\n    def generate_meta_code(self, user_input):\\n        # Transform user input into meta-code representation\\n        pass\\n\\n    def load_git_memory(self, git_memory):\\n        # Retrieve GitMemory context\\n        pass\\n\\n    def load_commit_context(self, commit_index):\\n        # Retrieve commit context\\n        pass\\n\\n    def load_contextual_resources(self, contextual_memory):\\n        # Load contextual resources required for code generation\\n        pass\\n\\n    def get_current_draft(self, commit_context):\\n        # Get the current draft code from commit context\\n        pass\\n\\n    def generate_modifications(self, draft_code, meta_code, context_resources):\\n        # Generate modifications to the draft code based on meta-code and context resources\\n        pass\\n\\n    def extract_source_code(self, modifications, context_resources):\\n        # Extract source code from modifications and context resources\\n        pass\\n\\n    def apply_modifications(self, draft_code, modifications):\\n        # Apply modifications to the draft code\\n        pass\\n\\n    def validate_draft(self, updated_draft, source_code):\\n        # Validate the updated draft code and check for consistency\\n        pass\\n\\n    def compare_draft_with_objective(self, updated_draft, meta_code):\\n        # Compare the updated draft code with user's goal based on meta-code\\n        pass\\n\\n    def store_draft_to_commit_index(self, commit_index, updated_draft):\\n        # Store the updated draft code in the commit index\\n        pass\\n\\n    def rollback(self, commit_index):\\n        # Roll back the draft code to the previous state in the commit index\\n        pass\\n\\n\\n    def codeGenerationIteration(self, userInput, gitMemory, commitIndex, contextualMemory):\\n        metaCode = self.generateMetaCode(userInput)\\n\\n        # Concurrent processing - Step 1\\n        with concurrent.futures.ThreadPoolExecutor() as executor:\\n            gitMemoryContext = executor.submit(self.loadGitMemory, gitMemory)\\n            commitContext = executor.submit(self.loadCommitContext, commitIndex)\\n            contextResources = executor.submit(self.loadContextualResources, contextualMemory)\\n\\n            gitMemoryContext = gitMemoryContext.result()\\n            commitContext = commitContext.result()\\n            contextResources = contextResources.result()\\n\\n        draftCode = self.getCurrentDraft(commitContext)\\n\\n        # Concurrent processing - Step 2\\n        with concurrent.futures.ThreadPoolExecutor() as executor:\\n            modifications = executor.submit(self.generateModifications, draftCode, metaCode, contextResources)\\n            sourceCode = executor.submit(self.extractSourceCode, modifications, contextResources)\\n\\n            modifications = modifications.result()\\n            sourceCode = sourceCode.result()\\n\\n        updatedDraft = self.applyModifications(draftCode, modifications)\\n\\n        # Concurrent processing - Step 3\\n        with concurrent.futures.ThreadPoolExecutor() as executor:\\n            validationResult = executor.submit(self.validateDraft, updatedDraft, sourceCode)\\n            comparisonResult = executor.submit(self.compareDraftWithObjective, updatedDraft, metaCode)\\n\\n            validationResult = validationResult.result()\\n            comparisonResult = comparisonResult.result()\\n\\n        if validationResult and comparisonResult:\\n            self.storeDraftToCommitIndex(commitIndex, updatedDraft)\\n        else:\\n            updatedDraft = self.rollback(commitIndex)\\n\\n        return updatedDraft\\n\",\n",
       " '\\n\\nclass DirectoryProcessor:\\n    def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\\n        self.directory_path = directory_path\\n        self.visitor = visitor\\n\\n    def _process_file(self, file_path: str):\\n        with open(file_path, \"r\") as file:\\n            source_code = file.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n        except cst.ParserSyntaxError:\\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n            return\\n\\n        tree.visit(self.visitor)\\n\\n    def process_file(self, file_path: str):\\n        # Run flake8 on the file\\n        result = subprocess.run(\\n            [\"flake8\", \"--select=E999\", file_path], capture_output=True\\n        )\\n\\n        if result.returncode != 0:\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n            print(result.stderr.decode(\"utf-8\"))\\n            return\\n\\n        with open(file_path, \"r\") as f:\\n            source_code = f.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n            tree.visit(self.visitor)\\n        except cst.ParserSyntaxError as e:\\n            print(f\"Syntax error: {e}\")\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n\\n    def process_directory(self) -> List[str]:\\n        function_source_codes = []\\n        class_source_codes = []\\n\\n        for root, _, files in os.walk(self.directory_path):\\n            for file in files:\\n                if file.endswith(\".py\"):\\n                    file_path = os.path.join(root, file)\\n                    self._process_file(file_path)\\n\\n        function_source_codes = self.visitor.function_source_codes\\n        function_nodes = self.visitor.function_nodes\\n        class_source_codes = self.visitor.class_source_codes\\n        class_nodes = self.visitor.class_nodes\\n\\n\\n        return function_source_codes, class_source_codes, function_nodes, class_nodes\\n\\n    def clone_repo(self, repo_url):\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.directory_path, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n',\n",
       " '\\n\\nclass GitHubUserProcessor:\\n    def __init__(self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.github = Github()\\n        self.directory_processor = None\\n        self.function_source_codes = []\\n        self.class_source_codes = []\\n        self.visitor = visitor\\n\\n    def get_public_repos(self):\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def process_repos(self, base_directory):\\n        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.directory_processor.clone_repo(repo.clone_url)\\n                (\\n                    function_source_codes,\\n                    class_source_codes,\\n                ) = self.directory_processor.process_directory()\\n                self.function_source_codes.extend(function_source_codes)\\n                self.class_source_codes.extend(class_source_codes)\\n                shutil.rmtree(repo_path)\\n\\n        return self.directory_processor\\n',\n",
       " '\\n\\nclass GitHubRepoProcessor:\\n    def __init__(self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.github = Github()\\n        self.directory_processor = None\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n        self.visitor = visitor\\n\\n    def get_repo(self, repo_name):\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_repo(self, base_directory):\\n        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.directory_processor.clone_repo(repo.clone_url)\\n        (\\n            function_source_codes,\\n            class_source_codes,\\n            function_nodes,\\n            class_nodes,\\n        ) = self.directory_processor.process_directory()\\n        self.function_source_codes.extend(function_source_codes)\\n        self.function_nodes.extend(function_nodes)\\n        self.class_source_codes.extend(class_source_codes)\\n        self.class_nodes.extend(class_nodes)\\n        shutil.rmtree(repo_path)\\n        return self.directory_processor\\n\\n    def get_values(self):\\n        #concatenate the function and class source codes\\n        self.function_source_codes.extend(self.class_source_codes)\\n        self.function_nodes.extend(self.class_nodes)\\n        return self.function_source_codes, self.function_nodes\\n',\n",
       " '\\n\\n# A custom visitor to find function calls and their arguments\\nclass FunctionCallFinder(cst.CSTVisitor):\\n    METADATA_DEPENDENCIES = (PositionProvider,)\\n\\n    def visit_Call(self, node: cst.Call) -> None:\\n        function_name = None\\n        if isinstance(node.func, cst.Name):\\n            function_name = node.func.value\\n\\n        if function_name:\\n            pos = self.get_metadata(PositionProvider, node).start\\n            print(\\n                f\"Function \\'{function_name}\\' called at line {pos.line}, column {pos.column} with arguments:\"\\n            )\\n\\n            for arg in node.args:\\n                arg_start_pos = self.get_metadata(PositionProvider, arg).start\\n                arg_value = arg.value\\n                if isinstance(arg_value, cst.SimpleString):\\n                    arg_value = arg_value.evaluated_value\\n                print(\\n                    f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\\n                )\\n',\n",
       " '\\n\\nclass MultiplicationCounterVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.count = 0\\n        self.functions_with_operation_dict = {}\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        self.current_function = node\\n        self.functions_with_operation_dict[node.name] = []\\n\\n    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        self.current_function = None\\n\\n    def visit_BinaryOperation(self, node: cst.BinaryOperation) -> None:\\n        if isinstance(node.operator, cst.Multiply) or isinstance(\\n            node.operator, cst.BitAnd\\n        ):\\n            self.count += 1\\n            if self.current_function:\\n                self.functions_with_operation_dict[self.current_function.name].append(\\n                    cst.Module([]).code_for_node(node)\\n                )\\n\\n    def visit_Call(self, node: cst.Call) -> None:\\n        if m.matches(node, m.Call(func=m.Attribute(attr=m.Name(\"dot\")))) or m.matches(\\n            node, m.Call(func=m.Name(\"dot\"), args=[m.Arg(), m.Arg()])\\n        ):\\n            self.count += 1\\n            if self.current_function:\\n                self.functions_with_operation_dict[self.current_function.name].append(\\n                    cst.Module([]).code_for_node(node)\\n                )\\n',\n",
       " '\\n\\nclass FunctionAndClassVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        function_source_code = cst.Module([]).code_for_node(node)\\n        #add in place summary and code mod\\n        self.function_nodes.append(node)\\n        self.function_source_codes.append(function_source_code)\\n\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\\n        class_source_code = cst.Module([]).code_for_node(node)\\n        #add in place summary and code mod\\n        self.class_nodes.append(node)\\n        self.class_source_codes.append(class_source_code)\\n',\n",
       " \"\\n\\n\\nclass TypingCollector(cst.CSTVisitor):\\n    def __init__(self):\\n        # stack for storing the canonical name of the current function\\n        self.stack: List[Tuple[str, ...]] = []\\n        # store the annotations\\n        self.annotations: Dict[\\n            Tuple[str, ...],  # key: tuple of canonical class/function name\\n            Tuple[cst.Parameters, Optional[cst.Annotation]],  # value: (params, returns)\\n        ] = {}\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> Optional[bool]:\\n        self.stack.append(node.name.value)\\n\\n    def leave_ClassDef(self, node: cst.ClassDef) -> None:\\n        self.stack.pop()\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> Optional[bool]:\\n        self.stack.append(node.name.value)\\n        self.annotations[tuple(self.stack)] = (node.params, node.returns)\\n        return False  # pyi files don't support inner functions, return False to stop the traversal.\\n\\n    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        self.stack.pop()\\n\",\n",
       " '\\n\\nclass IssueParser:\\n    def __init__(self, repo_name):\\n        self.g = Github()\\n        self.repo = self.g.get_repo(repo_name)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n',\n",
       " '\\n\\nclass CommitParser:\\n    def __init__(self, repo_name):\\n        self.g = Github()\\n        self.repo = self.g.get_repo(repo_name)\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n',\n",
       " '\\n\\nclass PythonMinifier:\\n    def __init__(self, code: str = None):\\n\\n        self.code = code\\n        self.output_code = None\\n\\n    def minify(self):\\n        if self.code:\\n            self.output_code = self.minify_code(self.code)\\n\\n    def get_minified_code(self):\\n        if not self.output_code:\\n            self.minify()\\n        return self.output_code\\n\\n    @staticmethod\\n    def minify_code(code: str) -> str:\\n        return minify(code)\\n',\n",
       " '\\nclass PythonDocstringExtractor:\\n    @staticmethod\\n    def extract_docstring(function_def: cst.FunctionDef) -> str:\\n        docstring = None\\n\\n        for stmt in function_def.body.body:\\n            if isinstance(stmt, cst.SimpleStatementLine):\\n                for expr in stmt.body:\\n                    if isinstance(expr, cst.Expr) and isinstance(expr.value, cst.SimpleString):\\n                        docstring = expr.value.value.strip(\\'\"\\').strip(\"\\'\")\\n                        break\\n            if docstring is not None:\\n                break\\n\\n        if docstring is not None:\\n            return docstring.strip()\\n        else:\\n            function_name = function_def.name.value\\n            return f\"No docstring provided for function \\'{function_name}\\'. Please add a docstring to describe this function.\"\\n',\n",
       " '\\n\\nclass GitMemory(MemoryIndex):\\n    def __init__(self, username, repo_name):\\n        super().__init__()\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.parser = GitHubRepoProcessor(username, repo_name)\\n        self.minifier = PythonMinifier()\\n        self.docstring_extractor = PythonDocstringExtractor()\\n        self.directory_parser = None\\n        self.min_code_index = None\\n        self.doc_string_index = None\\n        self.libcst_node_index = None\\n    \\n    def create_code_index(self, base_directory):\\n        self.directory_parser = self.parser.process_repo(base_directory)\\n        code_values, code_nodes = self.parser.get_values()\\n        self.code_index = self.init_index(values=code_values)\\n        self.code_index.save()\\n\\n    def create_indexes(self, base_directory):\\n        self.directory_parser = self.parser.process_repo(base_directory)\\n        code_values, code_nodes = self.parser.get_values()\\n        self.code_index = self.init_index(values=code_values)\\n\\n        min_code_values = []\\n        doc_string_values = []\\n        for code_value, code_node in zip(code_values, code_nodes):\\n            minifier = PythonMinifier(code=code_value)\\n            min_code = minifier.get_minified_code()\\n            doc_string = self.docstring_extractor.extract_docstring(code_node)\\n            min_code_values.append(min_code)\\n            doc_string_values.append(doc_string)\\n        self.doc_string_index = self.init_index(values=doc_string_values)\\n        self.min_code_index = self.init_index(values=min_code_values)\\n',\n",
       " '\\nclass VectorThread(BaseThread, MemoryIndex):\\n    \"\"\" vector BaseThread, creates a faiss index with the messages and allows to search for similar messages, memory BaseThread can return messages in either similarity or chronological order \\n      add a parameter to choose the order of the messages\\n    \"\"\"\\n    def __init__(self, name= \\'vector_memory\\', max_context = 2048, use_mark = False):\\n        BaseThread.__init__(self,name= name , max_memory= None)\\n        MemoryIndex.__init__(self, index = None, name = name)\\n        self.max_context = max_context\\n        self.use_mark = use_mark\\n        self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n        \\n    def index_message(self,message: str , verbose: bool =False):\\n        \"\"\"index a message in the faiss index, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated\\n        \"\"\"\\n        \\n        self.add_to_index(value = message, verbose = verbose)\\n\\n    def add_message(self, message_dict: dict, verbose: bool = False):\\n        \"\"\"add a message to the memory thread, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\\n        \"\"\"\\n        # print(\"checking the dict\")\\n        message_dict = check_dict(message_dict)\\n        # print(\"trying to add the message\")\\n        BaseThread.add_message(self,message_dict)\\n        # print(message_dict)\\n        message = message_dict[\"content\"]\\n        self.index_message(message, verbose = verbose)\\n        return True\\n    \\n    def token_bound_query(self, query, k: int =10, max_tokens: int =4000):\\n        \"\"\" returns the k most similar messages to the query, sorted in similarity order\"\"\"\\n        if self.use_mark:\\n            query = mark_question(query)\\n        return MemoryIndex.token_bound_query(self, query, k, max_tokens)\\n    \\n    def sorted_query(self, query, k: int =10, max_tokens: int =4000, reverse: bool = False, return_from_thread = True):\\n        \"\"\" returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n        unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n        #sort the messages \\n        \\n        sorted_messages = [unsorted_messages[i] for i in sorted(range(len(unsorted_messages)), key=lambda k: unsorted_indices[k])]\\n        sorted_scores = [unsorted_scores[i] for i in sorted(range(len(unsorted_scores)), key=lambda k: unsorted_indices[k])]\\n        sorted_indices = [unsorted_indices[i] for i in sorted(range(len(unsorted_indices)), key=lambda k: unsorted_indices[k])]\\n        if reverse:\\n            sorted_messages.reverse()\\n            sorted_scores.reverse()\\n            sorted_indices.reverse()\\n        if return_from_thread:\\n            sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n        return sorted_messages, sorted_scores, sorted_indices\\n    \\n    def weighted_query(self, query, k: int = 10, max_tokens: int = 4000, decay_factor: float = 0.1, temporal_weight: float = 0.5, order_by: str = \\'chronological\\', reverse: bool = False) -> list:\\n        \"\"\" Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\\n        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\\n        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\\n        The order_by parameter controls the order of the results. If it is set to \\'similarity\\', the results are sorted in similarity order. If it is set to \\'chronological\\', the results are sorted in chronological order with the most recent message first.\\n        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\\n        \"\"\"\\n        # Validate order_by parameter\\n        if order_by not in (\\'similarity\\', \\'chronological\\'):\\n            raise ValueError(\"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\")\\n\\n        # Get similarity-based results\\n        sim_messages, sim_scores, sim_indices = self.sorted_query(query, k, max_tokens=max_tokens)\\n        \\n        # Get token-bound history\\n        hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\\n        \\n        # Combine messages and indices\\n        combined_messages = sim_messages + hist_messages\\n        combined_indices = sim_indices + hist_indices\\n        \\n        # Create the local_index and populate it\\n        self.local_index = MemoryIndex(name=\\'local_index\\')\\n        for message in combined_messages:\\n            self.local_index.add_to_index(value=message, verbose=False)\\n        \\n        # Perform a new query on the combined index\\n        new_query_results, new_query_scores, new_query_indices = self.local_index.token_bound_query(query, k=len(combined_messages), max_tokens=max_tokens)\\n        \\n        # Compute temporal weights\\n        temporal_weights = [np.exp(-decay_factor * i) for i in range(len(combined_messages))]\\n        temporal_weights = [w / sum(temporal_weights) for w in temporal_weights]  # Normalize the temporal weights\\n        \\n        # Combine similarity scores and temporal weights\\n        weighted_scores = []\\n        for i in range(len(new_query_scores)):\\n            sim_score = new_query_scores[i]\\n            temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\\n            weighted_score = (1 - temporal_weight) * sim_score + temporal_weight * temp_weight\\n            weighted_scores.append(weighted_score)\\n        \\n        # Sort the results based on the order_by parameter\\n        if order_by == \\'similarity\\':\\n            sorting_key = lambda k: weighted_scores[k]\\n        elif order_by == \\'chronological\\':  # order_by == \\'chronological\\'\\n            sorting_key = lambda k: new_query_indices[k]\\n        else:\\n            raise ValueError(\"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\")\\n\\n        sorted_indices = [new_query_indices[i] for i in sorted(range(len(new_query_indices)), key=sorting_key, reverse=not reverse)]\\n        sorted_results = [new_query_results[i] for i in sorted(range(len(new_query_results)), key=sorting_key, reverse=not reverse)]\\n        sorted_scores = [weighted_scores[i] for i in sorted(range(len(weighted_scores)), key=sorting_key, reverse=not reverse)]\\n\\n        # Return only the top k results without exceeding max_tokens\\n        final_results, final_scores, final_indices = [], [], []\\n        current_tokens = 0\\n        for i in range(min(k, len(sorted_results))):\\n            message_tokens = self.get_message_tokens(sorted_results[i])\\n            if current_tokens + message_tokens <= max_tokens:\\n                final_results.append(sorted_results[i])\\n                final_scores.append(sorted_scores[i])\\n                final_indices.append(sorted_indices[i])\\n                current_tokens += message_tokens\\n            else:\\n                break\\n        \\n        return final_results, final_scores, final_indices\\n',\n",
       " '\\nclass BaseThread:\\n    \"\"\"\\n    This class is used to keep track of the memory thread of a conversation and the total number of tokens. \\n    All conversation memories should subclass this class. If max_memory is None, it has \\n    no limit to the number of tokens that can be stored in the memory thread.\\n    \"\"\"\\n\\n    def __init__(self, name: str = \\'memory\\', max_memory: Optional[int] = None, tokenizer: Optional[Any] = None) -> None:\\n        \"\"\"\\n        Initialize the BaseThread instance.\\n\\n        :param name: The name of the memory thread. Defaults to \\'memory\\'.\\n        :param max_memory: The maximum number of tokens allowed in the memory thread. \\n                           Defaults to None, which means no limit.\\n        :param tokenizer: The tokenizer to be used for tokenizing messages. \\n                          Defaults to None, which means using the tiktoken encoding for the \\'gpt-3.5-turbo\\' model.\\n        \"\"\"\\n        self.name = name\\n        self.max_memory = max_memory\\n        self.memory_thread = []\\n        self.time_stamps = []\\n        self.message_tokens = []\\n        self.total_tokens = 0\\n        if tokenizer is None:\\n            self.tokenizer = tiktoken.encoding_for_model(\\'gpt-3.5-turbo\\')\\n\\n\\n    def __getitem__(self, idx):\\n        return self.memory_thread[idx]    \\n    \\n    def __len__(self):\\n        return len(self.memory_thread)\\n    \\n    def get_message_tokens(self, message_dict: dict) -> int:\\n        \"\"\"\\n        Calculate the number of tokens in a message, including the role token.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The total number of tokens in the message.\\n        \"\"\"\\n        message_dict = check_dict(message_dict)\\n        message = message_dict[\"content\"]\\n        return len(self.tokenizer.encode(message))+6 # +6 for the role token\\n    \\n    def get_message_role(self, message_dict: dict) -> str:\\n        \"\"\"\\n        Get the role of the message from a message dictionary.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The role of the message.\\n        \"\"\"\\n        message_dict = check_dict(message_dict)\\n        return message_dict[\"role\"]\\n    \\n    def add_message(self, message_dict: dict) -> None:\\n        \"\"\"\\n        Add a message to the memory thread.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        \"\"\"\\n        message_tokens = self.get_message_tokens(message_dict)\\n        \\n        if  self.max_memory is None or self.total_tokens + message_tokens <= self.max_memory:\\n            #add the message_dict to the memory_thread\\n            # update the total number of tokens\\n            self.memory_thread.append(message_dict)\\n            self.total_tokens += message_tokens\\n            self.message_tokens.append(message_tokens)\\n            time_stamp  = time.time()\\n            self.time_stamps.append(time_stamp)\\n        else :\\n            display(Markdown(\"The memory BaseThread is full, the last message was not added\"))\\n    \\n    def remove_message(self, message_dict: Union[dict,None] =  None , idx: Union[int,None] = None) -> None:\\n        \"\"\" \\n        Remove a message from the memory thread.\\n        \"\"\"\\n        if message_dict is None and idx is None:\\n            raise Exception(\"You need to provide either a message_dict or an idx\")\\n        elif message_dict is not None and idx is not None:\\n            raise Exception(\"You need to provide either a message_dict or an idx\")\\n        \\n        if idx is None:\\n            message_dict = check_dict(message_dict)\\n            search_results = self.find_message(message_dict)\\n            if search_results is not None:\\n                idx = search_results[-1][\"idx\"]\\n                message = search_results[-1][\"message_dict\"]\\n                self.memory_thread.pop(idx)\\n                self.message_tokens.pop(idx)\\n                self.time_stamps.pop(idx)\\n                self.total_tokens -= self.get_message_tokens(message)\\n            else:   \\n                raise Exception(\"The message was not found in the memory BaseThread\")\\n        else:\\n            if idx < len(self.memory_thread):\\n                message = self.memory_thread.pop(idx)\\n                self.total_tokens -= self.get_message_tokens(message)\\n            else:  \\n                raise Exception(\"The index was out bound\")\\n            \\n    def find_message(self, message:Union[dict,str], role : Union[str,None] = None) -> Union[None, list]:\\n        \"\"\"\\n        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\\n        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\\n        #check if the message is a dictioanry or a string\\n        message = message if isinstance(message, str) else check_dict(message)\\n        search_results = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            target = message_dict if isinstance(message, dict) else message_dict[\"content\"]\\n            if target == message and (role is None or message_dict[\"role\"] == role):\\n                search_results.append({\"idx\": idx, \"message_dict\": message_dict})\\n        return search_results if len(search_results) > 0 else None\\n    \\n    def find_role(self, role: str) -> Union[None, list]:\\n        \"\"\"\\n        Find all messages with a specific role in the memory thread.\\n        \"\"\"\\n        search_results = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if message_dict[\"role\"] == role:\\n                search_results.append({\"idx\": idx, \"message_dict\": message_dict})\\n        return search_results if len(search_results) > 0 else None\\n    \\n    def last_message(self, role: Union[str,None] = None) -> Union[None, dict]:\\n        \"\"\"\\n        Get the last message in the memory thread with a specific role.\"\"\"\\n        if role is None:\\n            return self.memory_thread[-1]\\n        else:\\n            for message_dict in reversed(self.memory_thread):\\n                if message_dict[\"role\"] == role:\\n                    return message_dict\\n            return None\\n        \\n    def first_message(self, role: Union[str,None] = None) -> Union[None, dict]:\\n        \"\"\"\\n        Get the first message in the memory thread with a specific role.\"\"\"\\n        if role is None:\\n            return self.memory_thread[0]\\n        else:\\n            for message_dict in self.memory_thread:\\n                if message_dict[\"role\"] == role:\\n                    return message_dict\\n            return None\\n    \\n    def messages_before(self, message: dict , role: Union[str,None] = None ) -> Union[None, list]:\\n        \"\"\"\\n        Get all messages before a specific message in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if message_dict == message and (role is None or message_dict[\"role\"] == role):\\n                messages = self.memory_thread[:idx]\\n                break\\n        return messages if len(messages) > 0 else None\\n    \\n    def messages_before(self, message: dict , role: Union[str,None] = None ) -> Union[None, list]:\\n        \"\"\"\\n        Get all messages after a specific message in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if message_dict == message and (role is None or message_dict[\"role\"] == role):\\n                messages = self.memory_thread[idx+1:]\\n                break\\n        return messages if len(messages) > 0 else None\\n    \\n    def messages_between(self, start_message: dict, end_message: dict, role: Union[str,None] = None ) -> Union[None, list]:\\n        \"\"\"\\n        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if message_dict == start_message and (role is None or message_dict[\"role\"] == role):\\n                start_idx = idx\\n                break\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if message_dict == end_message and (role is None or message_dict[\"role\"] == role):\\n                end_idx = idx\\n                break\\n        messages = self.memory_thread[start_idx+1:end_idx]\\n        return messages if len(messages) > 0 else None\\n        \\n    def messages_more_tokens(self, tokens: int, role: Union[str,None] = None ):\\n        \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if self.message_tokens[idx] > tokens and (role is None or message_dict[\"role\"] == role):\\n                messages.append(message_dict)\\n        return messages if len(messages) > 0 else None\\n    \\n    def messages_less_tokens(self, tokens: int, role: Union[str,None] = None ):\\n        \"\"\"\\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if self.message_tokens[idx] < tokens and (role is None or message_dict[\"role\"] == role):\\n                messages.append(message_dict)\\n        return messages if len(messages) > 0 else None\\n    \\n    def messages_between_tokens(self, start_tokens: int, end_tokens: int, role: Union[str,None] = None ):\\n        \"\"\"\\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if self.message_tokens[idx] > start_tokens and self.message_tokens[idx] < end_tokens and (role is None or message_dict[\"role\"] == role):\\n                messages.append(message_dict)\\n        return messages if len(messages) > 0 else None\\n\\n    def messages_before_time(self, time_stamp, role: Union[str,None] = None ):\\n        \"\"\"\\n        Get all messages before a specific time in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if self.time_stamps[idx] < time_stamp and (role is None or message_dict[\"role\"] == role):\\n                messages.append(message_dict)\\n        return messages if len(messages) > 0 else None\\n    \\n    def messages_after_time(self, time_stamp, role: Union[str,None] = None ):\\n        \"\"\"\\n        Get all messages after a specific time in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if self.time_stamps[idx] > time_stamp and (role is None or message_dict[\"role\"] == role):\\n                messages.append(message_dict)\\n        return messages if len(messages) > 0 else None\\n    \\n    def messages_between_time(self, start_time, end_time, role: Union[str,None] = None ):\\n        \"\"\"\\n        Get all messages between two specific times in the memory thread with a specific role.\"\"\"\\n        messages = []\\n        for idx, message_dict in enumerate(self.memory_thread):\\n            if self.time_stamps[idx] > start_time and self.time_stamps[idx] < end_time and (role is None or message_dict[\"role\"] == role):\\n                messages.append(message_dict)\\n        return messages if len(messages) > 0 else None\\n    \\n    def token_bound_history(self, max_tokens: int, max_history=None, role: Union[str, None]=None):\\n        messages = []\\n        indices = []\\n        tokens = 0\\n        if max_history is None:\\n            max_history = len(self.memory_thread)\\n\\n        for idx, message_dict in enumerate(reversed(self.memory_thread)):\\n            if tokens + self.message_tokens[idx] < max_tokens and (role is None or message_dict[\"role\"] == role) and idx < max_history:\\n                messages.append(message_dict)\\n                indices.append(len(self.memory_thread) - 1 - idx)\\n                tokens += self.message_tokens[idx]\\n            else:\\n                break\\n        return messages, indices if len(messages) > 0 else (None, None)\\n',\n",
       " '\\nclass FifoThread(BaseThread):\\n    \"\"\"FIFO Memory BaseThread, the oldest messages are removed first when reaching the max_memory limit, the memory is defined in terms of tokens, \\n    outs are passe to the longterm_memory, lucid_memory is a redundant memory that stores all the messages\\n    \"\"\"\\n    def __init__(self, name= \\'fifo_memory\\', max_memory = None, longterm_thread = None, redundant = True):\\n        \\n        BaseThread.__init__(self,name= name , max_memory= None)\\n        if redundant is True:            \\n            self.redundant_thread = BaseThread(name = \\'lucid_memory\\',max_memory = None)\\n        else:\\n            self.redundant_thread = None\\n        if longterm_thread is None:\\n            self.longterm_thread = BaseThread(name =\\'longterm_memory\\',max_memory = None)\\n        else:\\n            self.longterm_thread = longterm_thread\\n        # create an alias for the memory_thread to make the code more readable\\n        self.fifo_thread = self.memory_thread\\n        self.max_memory = max_memory\\n            \\n    def to_longterm(self, idx: int):\\n        \"\"\" move the message at the index idx to the longterm_memory\"\"\"\\n        #move the message at the index idx to the longterm_memory\\n        display(Markdown(\"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(idx)))\\n        message = copy.deepcopy(self.memory_thread[idx])\\n        # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\\n        self.longterm_thread.add_message(message)\\n        self.remove_message(idx=idx)\\n    \\n    def add_message(self, message_dict: dict):\\n        \"\"\" add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\\n        # message_dict = {\"role\": role, \"content\": content}\\n        #chek that the message_dict is a dictionary or a list of dictionaries\\n        message_dict = check_dict(message_dict)\\n        if self.redundant_thread is not None:\\n            self.redundant_thread.add_message(message_dict)\\n        message_tokens = self.get_message_tokens(message_dict)\\n        \\n        if self.total_tokens + message_tokens > self.max_memory:\\n            while self.total_tokens + message_tokens > self.max_memory :\\n                if len(self.memory_thread) > 0:\\n                    self.to_longterm(idx=0)\\n            super().add_message(message_dict)\\n            \\n        else:\\n            #add the message_dict to the memory_thread\\n            # update the total number of tokens\\n            super().add_message(message_dict)\\n',\n",
       " '\\n\\nclass MemoryKernel(MemoryIndex):\\n    def __init__(self, values, embeddings, name=\"memory_kernel\", save_path=None):\\n        super().__init__(values, embeddings, name, save_path)\\n        self.create_k_hop_index()\\n\\n    def cos_sim(self, a, b):\\n        \"\"\"\\n        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n        \"\"\"\\n        if not isinstance(a, np.ndarray):\\n            a = np.array(a)\\n\\n        if not isinstance(b, np.ndarray):\\n            b = np.array(b)\\n\\n        if len(a.shape) == 1:\\n            a = a[np.newaxis, :]\\n\\n        if len(b.shape) == 1:\\n            b = b[np.newaxis, :]\\n\\n        a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\\n        b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\\n        return np.dot(a_norm, b_norm.T)\\n\\n    def cos_sim_batch(self, a: np.ndarray, b: np.ndarray, batch_size: int = 128):\\n        \"\"\"\\n        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j using batch processing.\\n        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n        \"\"\"\\n\\n        if not isinstance(a, np.ndarray):\\n            a = np.array(a)\\n\\n        if not isinstance(b, np.ndarray):\\n            b = np.array(b)\\n\\n        if len(a.shape) == 1:\\n            a = np.expand_dims(a, 0)\\n\\n        if len(b.shape) == 1:\\n            b = np.expand_dims(b, 0)\\n\\n        a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)\\n        b_norm = b / np.linalg.norm(b, axis=1, keepdims=True)\\n\\n        sim_matrix = []\\n        for i in range(0, len(a_norm), batch_size):\\n            a_batch = a_norm[i : i + batch_size]\\n            sim_batch = np.matmul(a_batch, b_norm.T)\\n            sim_matrix.append(sim_batch)\\n        sim_matrix = np.concatenate(sim_matrix, axis=0)\\n        return sim_matrix\\n\\n    def compute_kernel(\\n        self, embedding_set, threshold=0.65, use_softmax=False, cos_sim_batch=True\\n    ):\\n        \"\"\"\\n        Compute the adjacency matrix of the graph.\\n\\n        Parameters:\\n        embedding_set (numpy array): The embedding matrix of the nodes.\\n        threshold (float): The threshold for the adjacency matrix.\\n        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\\n        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\\n\\n        Returns:\\n        adj_matrix (numpy array): The adjacency matrix of the graph.\\n        \"\"\"\\n        if cos_sim_batch:\\n            A = self.cos_sim_batch(embedding_set, embedding_set)\\n        else:\\n            A = self.cos_sim(embedding_set, embedding_set)\\n        if use_softmax:\\n            # softmax\\n            A = np.exp(A)\\n            A = A / np.sum(A, axis=1)[:, np.newaxis]\\n        adj_matrix = np.zeros_like(A)\\n        adj_matrix[A > threshold] = 1\\n        adj_matrix[A <= threshold] = 0\\n        adj_matrix = adj_matrix.astype(np.float32)\\n        return adj_matrix\\n\\n    def k_hop_message_passing(self, A, node_features, k):\\n        \"\"\"\\n        Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n        Parameters:\\n        A (numpy array): The adjacency matrix of the graph.\\n        node_features (numpy array): The feature matrix of the nodes.\\n        k (int): The number of hops for message passing.\\n\\n        Returns:\\n        A_k (numpy array): The k-hop adjacency matrix.\\n        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n        \"\"\"\\n\\n        print(\"Compute the k-hop adjacency matrix\")\\n        A_k = np.linalg.matrix_power(A, k)\\n\\n        print(\"Aggregate the messages from the k-hop neighborhood:\")\\n        agg_features = node_features.copy()\\n\\n        for i in tqdm(range(k)):\\n            agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\\n\\n        return A_k, agg_features\\n    \\n    def faiss_index_to_adj_matrix(index: faiss.Index, num_vectors: int) -> np.ndarray:\\n        \"\"\"\\n        Convert a Faiss index into a NumPy adjacency matrix.\\n\\n        Args:\\n            index (faiss.Index): The Faiss index.\\n            num_vectors (int): The number of vectors in the Faiss index.\\n\\n        Returns:\\n            np.ndarray: The adjacency matrix.\\n        \"\"\"\\n        # Create an empty adjacency matrix\\n        adj_matrix = np.zeros((num_vectors, num_vectors), dtype=np.float32)\\n\\n        # Populate the adjacency matrix with distances from the Faiss index\\n        for i in range(num_vectors):\\n            distances, _ = index.search(index.reconstruct(i).reshape(1, -1), num_vectors)\\n            adj_matrix[i] = distances\\n\\n        return adj_matrix\\n\\n    def create_k_hop_index(self, k):\\n        print(\"Computing the adjacency matrix\")\\n        print(\"Embeddings shape: \", self.embeddings.shape)\\n        self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\\n        print(\"Computing the k-hop adjacency matrix and aggregated features\")\\n        self.A_k, self.node_embeddings = self.k_hop_message_passing(\\n            self.A, self.embeddings, k\\n        )\\n        print(\"Updating the memory index\")\\n        self.k_hop_index = MemoryIndex(index=None, values=self.values, embeddings=self.node_embeddings, name=self.memory_index.name)\\n',\n",
       " '\\n\\nclass MemoryIndex:\\n    \"\"\"\\n    this class is a wrapper for a faiss index, it contains information about the format of the index the faiss index itself\\n    \"\"\"\\n    def __init__(\\n        self,\\n        index: Optional[faiss.Index] = None,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        name: str = \"memory_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        tokenizer: Optional[tiktoken.Encoding] = None,\\n        ):\\n\\n        self.name = name\\n        self.embedder = OpenAiEmbedder()\\n        if save_path is None:\\n            save_path = \"storage\"\\n\\n        self.save_path = save_path\\n\\n        # Create the \\'storage\\' folder if it does not exist\\n        os.makedirs(self.save_path, exist_ok=True)\\n        self.values = []\\n        if load is True:\\n            self.load()\\n        else:\\n            self.init_index(index, values, embeddings)\\n        if tokenizer is None:\\n            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n        else:\\n            self.tokenizer = tokenizer\\n        self.query_history = []\\n        self.save()\\n\\n    def init_index(\\n        self,\\n        index: Optional[faiss.Index] = None,\\n        values: Optional[List[str]] = None,\\n        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n        ) -> None:\\n\\n        \"\"\"\\n        initializes the index, there are 4 cases:\\n        1. we create a new index from scratch\\n        2. we create a new index from a list of embeddings and values\\n        3. we create a new index from a faiss index and values list\\n        4. we load an index from a file\\n        \"\"\"\\n        # fist case is when we create a new index from scratch\\n        if index is None and values is None and embeddings is None:\\n            print(\"Creating a new index\")\\n            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n            self.values = []\\n        # second case is where we create the index from a list of embeddings\\n        elif (\\n            index is None\\n            and values is not None\\n            and embeddings is not None\\n            and len(values) == len(embeddings)\\n        ):\\n            print(\"Creating a new index from a list of embeddings and values\")\\n            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n            for embedding, value in zip(embeddings, values):\\n                self.add_to_index(value, embedding)\\n        # third case is where we create the index from a faiss index and values list\\n        elif (\\n            isinstance(index, faiss.Index)\\n            and index.d == self.embedder.get_embedding_size()\\n            and type(values) == list\\n            and len(values) == index.ntotal\\n        ):\\n            print(\"Creating a new index from a faiss index and values list\")\\n            self.index = index\\n            self.values = values\\n        # fourth case is where we create an index from a list of values, the values are embedded and the index is created\\n        elif index is None and values is not None and embeddings is None:\\n            print(\"Creating a new index from a list of values\")\\n            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n            i = 0\\n            for value in values:\\n                # print the value id to see the progress\\n                print(\"Embedding value \", i, \" of \", len(values))\\n                # start tracking the time using time\\n                start = time.time()\\n                self.add_to_index(value)\\n                # print the time it took to embed the value\\n                print(\"Embedding value \", i, \" took \", time.time() - start, \" seconds\")\\n                i += 1\\n        else:\\n            raise ValueError(\\n                \"The index is not a valid faiss index or the embedding dimension is not correct\"\\n            )\\n\\n    def add_to_index(\\n        self,\\n        value: str,\\n        embedding: Optional[Union[List[float], np.ndarray, str]] = None,\\n        verbose: bool = True,\\n        ) -> None:\\n        \"\"\"\\n        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\\n        \"\"\"\\n        if value not in self.values:\\n            if embedding is None:\\n                embedding = self.embedder.embed(value)\\n                if verbose:\\n                    display(Markdown(\"The value {value} was embedded\".format(value=value)))\\n\\n            if type(embedding) is list:\\n                embedding = np.array([embedding])\\n            elif type(embedding) is str:\\n                embedding = eval(embedding)\\n                embedding = np.array([embedding]).astype(np.float32)\\n            elif type(embedding) is not np.ndarray:\\n                raise ValueError(\"The embedding is not a valid type\")\\n\\n            # Ensure that the embedding is a 2D numpy array\\n            if embedding.ndim == 1:\\n                embedding = embedding.reshape(1, -1)\\n            # print(\"embedding is \", embedding)\\n            # print(\"embedding type is \", type(embedding))\\n            # print(\"embedding shape is \", embedding.shape)\\n            self.index.add(embedding)\\n            self.values.append(value)\\n        else:\\n            if verbose:\\n                display(\\n                    Markdown(\\n                        \"The value {value} was already in the index\".format(value=value)\\n                    )\\n                )\\n\\n    def get_embedding_by_index(self, index: int) -> np.ndarray:\\n        \"\"\"\\n        Get the embedding corresponding to a certain index value.\\n        \"\"\"\\n        if index < 0 or index >= len(self.values):\\n            raise ValueError(\"The index is out of range\")\\n\\n        # Fetch the embedding from the Faiss index\\n        embedding = self.index.reconstruct(index)\\n\\n        return embedding\\n\\n    def get_index_by_value(self, value: str) -> Optional[int]:\\n        \"\"\"\\n        Get the index corresponding to a value in self.values.\\n        \"\"\"\\n        if value in self.values:\\n            index = self.values.index(value)\\n            return index\\n        else:\\n            return None\\n\\n    def get_embedding_by_value(self, value: str) -> Optional[np.ndarray]:\\n        \"\"\"\\n        Get the embedding corresponding to a certain value in self.values.\\n        \"\"\"\\n        index = self.get_index_by_value(value)\\n        if index is not None:\\n            embedding = self.get_embedding_by_index(index)\\n            return embedding\\n        else:\\n            return None\\n\\n    def get_all_embeddings(self) -> np.ndarray:\\n        \"\"\"\\n        Get all the embeddings in the index.\\n        \"\"\"\\n        embeddings = []\\n        for i in range(len(self.values)):\\n            embeddings.append(self.get_embedding_by_index(i))\\n        self.embeddings = np.array(embeddings)\\n        return self.embeddings\\n\\n    def faiss_query(self, query: str, k:int =10) -> Tuple[List[str], List[float]]:\\n        \"\"\" Query the faiss index for the top-k most similar values to the query\"\"\"\\n\\n        # Embed the data\\n        embedding = self.embedder.embed(query)\\n        if k > len(self.values):\\n            k = len(self.values)\\n        # Query the Faiss index for the top-K most similar values\\n        D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\\n        # Get the values corresponding to the indices\\n        values = [self.values[i] for i in I[0]]\\n        scores = [d for d in D[0]]\\n        return values, scores, I\\n    \\n    def token_bound_query(self, query, k=10, max_tokens=4000):\\n        \"\"\" Query the faiss index for the top-k most similar values to the query, but bound the number of tokens retrieved by the max_tokens parameter\"\"\"\\n        returned_tokens = 0\\n        top_k_hint = []\\n        scores = []\\n        tokens = []\\n        indices = []\\n\\n        if len(self.values) > 0:\\n            top_k, scores, indices = self.faiss_query(query, k=min(k, len(self.values)))\\n\\n            for hint in top_k:\\n                # mark the message and gets the length in tokens\\n                message_tokens = len(self.tokenizer.encode(hint))\\n                tokens.append(message_tokens)\\n                if returned_tokens + message_tokens <= max_tokens:\\n                    top_k_hint += [hint]\\n                    returned_tokens += message_tokens\\n            \\n            self.query_history.append({\"query\": query, \"hints\": top_k_hint, \"scores\": scores, \"indices\":indices, \"hints_tokens\": tokens, \"returned_tokens\": returned_tokens , \"max_tokens\": max_tokens, \"k\": k})\\n\\n        return top_k_hint, scores, indices\\n    \\n    def save(self):\\n        \"\"\" Save the index to disk using faiss and json and numpy\"\"\"\\n        # Create the directory to save the index, values, and embeddings\\n        save_directory = os.path.join(self.save_path, self.name)\\n        os.makedirs(save_directory, exist_ok=True)\\n\\n        # Save the FAISS index\\n        index_filename = os.path.join(save_directory, f\"{self.name}_index.faiss\")\\n        faiss.write_index(self.index, index_filename)\\n\\n        # Save the index values\\n        values_filename = os.path.join(save_directory, f\"{self.name}_values.json\")\\n        with open(values_filename, \"w\") as f:\\n            json.dump(self.values, f)\\n\\n        # Save the numpy array of the embeddings\\n        embeddings_filename = os.path.join(save_directory, f\"{self.name}_embeddings.npz\")\\n        # print(f\"embs: {self.get_all_embeddings().shape}\")\\n        np.savez_compressed(embeddings_filename, self.get_all_embeddings())\\n\\n    def load(self):\\n        \"\"\" Load the index, values, and embeddings from disk \"\"\"\\n        # Set the directory to load the index, values, and embeddings from\\n        load_directory = os.path.join(self.save_path, self.name)\\n\\n        # Load the FAISS index\\n        index_filename = os.path.join(load_directory, f\"{self.name}_index.faiss\")\\n        self.index = faiss.read_index(index_filename)\\n\\n        # Load the index values\\n        values_filename = os.path.join(load_directory, f\"{self.name}_values.json\")\\n        with open(values_filename, \"r\") as f:\\n            self.values = json.load(f)\\n\\n        # Load the numpy array of the embeddings\\n        embeddings_filename = os.path.join(load_directory, f\"{self.name}_embeddings.npz\")\\n        embeddings_data = np.load(embeddings_filename)\\n        self.embeddings = embeddings_data[\\'arr_0\\']\\n\\n    def save_pickle(self, path=None):\\n        \"\"\"saves the index and values to a pickle file\"\"\"\\n        if path is None and self.save_path is None:\\n            path = self.name + \".pkl\"\\n        elif path is None and self.save_path is not None:\\n            if self.save_path.endswith(\"/\"):\\n                path = self.save_path + self.name + \".pkl\"\\n            else:\\n                path = self.save_path + \"/\" + self.name + \".pkl\"\\n        print(\"Saving the index to \", path)\\n        with open(path, \"wb\") as f:\\n            pickle.dump({\"index\": self.index, \"values\": self.values}, f)\\n\\n    def load_pickle(self, path=None):\\n        \"\"\"loads the index and values from a pickle file\"\"\"\\n        if path is None and self.save_path is None:\\n            path = self.name + \".pkl\"\\n        elif path is None and self.save_path is not None:\\n            if self.save_path.endswith(\"/\"):\\n                path = self.save_path + self.name + \".pkl\"\\n            else:\\n                path = self.save_path + \"/\" + self.name + \".pkl\"\\n\\n        with open(path, \"rb\") as f:\\n            data = pickle.load(f)\\n            self.index = data[\"index\"]\\n            self.values = data[\"values\"]\\n\\n    def prune_index(\\n        self,\\n        constraint: Optional[str] = None,\\n        regex_pattern: Optional[str] = None,\\n        length_constraint: Optional[int] = None,\\n    ) -> \\'MemoryIndex\\':\\n        \"\"\" Prune the index based on the constraint provided. Currently, only regex and length constraints are supported. \"\"\"\\n\\n        if constraint is not None:\\n            if constraint == \"regex\":\\n                if regex_pattern is None:\\n                    raise ValueError(\\n                        \"regex_pattern must be provided for regex constraint.\"\\n                    )\\n                pruned_values, pruned_embeddings = self._prune_by_regex(regex_pattern)\\n            elif constraint == \"length\":\\n                if length_constraint is None:\\n                    raise ValueError(\\n                        \"length_constraint must be provided for length constraint.\"\\n                    )\\n                pruned_values, pruned_embeddings = self._prune_by_length(\\n                    length_constraint\\n                )\\n            else:\\n                raise ValueError(\"Invalid constraint type provided.\")\\n        else:\\n            raise ValueError(\"constraint must be provided for pruning the index.\")\\n\\n        # Create a new index with pruned values and embeddings\\n        pruned_memory_index = MemoryIndex(\\n            values=pruned_values,\\n            embeddings=pruned_embeddings,\\n            name=self.name + \"_pruned\",\\n        )\\n\\n        return pruned_memory_index\\n\\n    def _prune_by_regex(self, regex_pattern: str) -> Tuple[List[str], List[np.ndarray]]:\\n        \"\"\" Prune the index by the regex pattern provided.\"\"\"\\n        pruned_values = []\\n        pruned_embeddings = []\\n\\n        for value in self.values:\\n            if re.search(regex_pattern, value):\\n                pruned_values.append(value)\\n                pruned_embeddings.append(self.get_embedding_by_value(value))\\n\\n        return pruned_values, pruned_embeddings\\n\\n    def _prune_by_length(self, length_constraint: int) -> Tuple[List[str], List[np.ndarray]]:\\n        \"\"\" Prune the index by the length constraint provided.\"\"\"\\n        pruned_values = []\\n        pruned_embeddings = []\\n\\n        for value in self.values:\\n            if len(value) >= length_constraint:\\n                pruned_values.append(value)\\n                pruned_embeddings.append(self.get_embedding_by_value(value))\\n\\n        return pruned_values, pruned_embeddings\\n',\n",
       " '\\nclass PythonIndex(MemoryIndex, PythonParser):\\n    def __init__(\\n        self,\\n        directory_path: str,\\n        name: str = \"python_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        tokenizer: Optional[tiktoken.Encoding] = None,\\n    ):\\n        # Initialize the MemoryIndex\\n        MemoryIndex.__init__(\\n            self,\\n            name=name,\\n            save_path=save_path,\\n            load=load,\\n            tokenizer=tokenizer,\\n        )\\n        # Initialize the PythonParser\\n        PythonParser.__init__(\\n            self,\\n            directory_path=directory_path,\\n            minify_code=minify_code,\\n            remove_docstrings=remove_docstrings,\\n        )\\n\\n        if not load:\\n            # Extract functions and classes source code\\n            function_source_codes, class_source_codes, _, _ = self.process_directory()\\n\\n            # Concatenate function and class source code and index them\\n            for code in function_source_codes + class_source_codes:\\n                self.add_to_index(code)\\n\\n            self.save()\\n',\n",
       " '\\nclass PandasIndex(MemoryIndex):\\n    def __init__(\\n        self,\\n        pandaframe: Union[pd.DataFrame, str],\\n        columns: Optional[Union[str, List[str]]] = None,\\n        name: str = \\'panda_index\\',\\n        save_path: Optional[str] = None,\\n        in_place: bool = True,\\n        embeddings_col: Optional[str] = None,\\n    ):\\n        \"\"\"\\n        Create a PandasIndex object.\\n\\n        Args:\\n            pandaframe: The DataFrame or path to a CSV file.\\n            columns: The columns of the DataFrame to use as values.\\n            name: The name of the index.\\n            save_path: The path to save the index.\\n            in_place: Whether to work on the DataFrame in place or create a copy.\\n            embeddings_col: The column name containing the embeddings.\\n        \"\"\"\\n        self.columns = columns\\n        self.values = []\\n\\n        # Load or copy pandaframe, and set self.name, self.columns\\n        if isinstance(pandaframe, str) and pandaframe.endswith(\".csv\") and os.path.isfile(pandaframe):\\n            try:\\n                pandaframe = pd.read_csv(pandaframe)\\n            except:\\n                raise ValueError(\"The CSV file is not valid\")\\n            self.name = pandaframe.split(\"/\")[-1].split(\".\")[0]\\n            self.columns = \"values\"\\n        elif isinstance(pandaframe, pd.core.frame.DataFrame) and columns is not None:\\n            if not in_place:\\n                pandaframe = copy.deepcopy(pandaframe)\\n        else:\\n            raise ValueError(\"The pandaframe is not a valid pandas dataframe or the columns are not valid or the path is not valid\")\\n\\n        values, embeddings = self.extract_values_and_embeddings(pandaframe, embeddings_col)\\n        super().__init__(values=values, embeddings=embeddings, name=name, save_path=save_path)\\n\\n    def extract_values_and_embeddings(\\n        self,\\n        pandaframe: pd.DataFrame,\\n        embeddings_col: Optional[str],\\n    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n        \"\"\"\\n        Extract values and embeddings from a pandas DataFrame.\\n\\n        Args:\\n            pandaframe: The DataFrame to extract values and embeddings from.\\n            embeddings_col: The column name containing the embeddings.\\n\\n        Returns:\\n            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n        \"\"\"\\n        if isinstance(self.columns, list) and len(self.columns) > 1:\\n            pandaframe[\"values\"] = pandaframe[self.columns].apply(lambda x: \\' \\'.join(x), axis=1)\\n            self.columns = \"values\"\\n        elif isinstance(self.columns, list) and len(self.columns) == 1:\\n            self.columns = self.columns[0]\\n            pandaframe[\"values\"] = pandaframe[self.columns]\\n            self.columns = \"values\"\\n        elif not isinstance(self.columns, str):\\n            raise ValueError(\"The columns are not valid\")\\n\\n        values = []\\n        embeddings = []\\n\\n        for _, row in pandaframe.iterrows():\\n            value = row[\"values\"]\\n            values.append(value)\\n\\n            if embeddings_col is not None:\\n                embedding = row[embeddings_col]\\n                embeddings.append(embedding)\\n\\n        return values, embeddings if embeddings_col is not None else None\\n',\n",
       " '\\n\\n\\nclass FifoChat(FifoThread, Chat):\\n    \"\"\"\\n    A chatbot class that combines FIFO Memory Thread, BaseChat, and Prompter. The oldest messages are removed first\\n    when reaching the max_memory limit. The memory is defined in terms of tokens, and outs are passed to the\\n    longterm_memory. The lucid_memory is a redundant memory that stores all the messages.\\n    \"\"\"\\n\\n    def __init__(self, model: Optional[str] = None,\\n                 index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None, \\n                  system_prompt: Optional[str] = None, user_prompt: Optional[str] = None, name: str = \\'fifo_memory\\',\\n                  max_index_memory: int = 400,  max_fifo_memory: int = 2048, max_output_tokens: int = 1000, \\n                  longterm_thread: Optional[BaseThread] =None):\\n        \\n        FifoThread.__init__(self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread)\\n        Chat.__init__(self, model= model,index_dict= index_dict,  max_output_tokens=max_output_tokens, max_index_memory=max_index_memory, system_prompt=system_prompt, user_prompt=user_prompt)\\n\\n        self.prompt_func = self.fifo_memory_prompt\\n\\n    def fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\\n        \"\"\"\\n        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n        marked_question = mark_question(self.user_prompt(message))\\n        prompt = [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\\n        return prompt, marked_question \\n\\n\\n    def query(self, question: str, verbose: bool = True) -> str:\\n        \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n        # First call the base class\\'s query method\\n        answer = BaseChat.query(self, message=question, verbose=verbose)\\n        marked_question = mark_question(question)\\n        # Add the marked question and answer to the memory\\n        self.add_message(marked_question)\\n        self.add_message(answer)\\n\\n        return answer\\n    \\n',\n",
       " 'class VectorChat(VectorThread, Chat):\\n    \"\"\"\\n    A chatbot class that combines Vector Memory Thread, BaseChat, and Prompter. Memory prompt is constructed by\\n    filling the memory with the k most similar messages to the question until the max prompt memory tokens are reached.\\n    \"\"\"\\n\\n    def __init__(self, model: Optional[str] = None, index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,  name: str = \\'vector_memory\\',  max_index_memory: int = 400,  max_vector_memory: int = 2048, max_output_tokens: int = 1000, system_prompt: str = None, user_prompt: str = None):\\n        VectorThread.__init__(self, name=name, max_context=max_vector_memory)\\n        Chat.__init__(self, model= model,index_dict= index_dict,  max_output_tokens=max_output_tokens, max_index_memory=max_index_memory, system_prompt=system_prompt, user_prompt=user_prompt)\\n        self.max_vector_memory = self.max_context\\n        self.prompt_func = self.vector_memory_prompt\\n\\n    def vector_memory_prompt(self, message: str, k: int = 10) -> Tuple[List[dict], dict]: \\n        \"\"\"\\n        Combine system prompt, k most similar messages to the question, and the user prompt.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include in the prompt.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n        sorted_messages, sorted_scores, sorted_indices = self.sorted_query(message, k=k, max_tokens = self.max_vector_memory,reverse=True) \\n        marked_question = mark_question(self.user_prompt(message))\\n        prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\\n        return prompt, marked_question\\n    \\n    def weighted_memory_prompt(self, message: str, k: int = 10, decay_factor: float = 0.1, temporal_weight: float = 0.5) -> Tuple[List[dict], dict]:\\n        \"\"\"\\n        Combine system prompt, weighted k most similar messages to the question, and the user prompt.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include in the prompt.\\n        :param decay_factor: A float representing the decay factor for weighting.\\n        :param temporal_weight: A float representing the weight of the temporal aspect.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n        weighted_messages, weighted_scores, weighted_indices = self.weighted_query(message, k=k, max_tokens = self.max_vector_memory,decay_factor = decay_factor, temporal_weight = temporal_weight, order_by = \\'chronological\\', reverse = True)\\n        marked_question = mark_question(self.user_prompt(message))\\n        prompt = [mark_system(self.system_prompt)] + weighted_messages + [marked_question]\\n        return prompt, marked_question \\n\\n    def query(self, question: str, verbose: bool = False) -> str:\\n        \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n        # First call the base class\\'s query method\\n        answer = BaseChat.query(self, message=question, verbose=verbose)\\n        marked_question = mark_question(question)\\n        # Add the marked question and answer to the memory\\n        self.add_message(marked_question)\\n        self.add_message(answer)\\n        return answer\\n',\n",
       " '\\n\\nclass FifoVectorChat(FifoThread, Chat):\\n    \"\"\"\\n    A chatbot class that combines FIFO Memory Thread, Vector Memory Thread, BaseChat, and Prompter.\\n    The memory prompt is constructed by including both FIFO memory and Vector memory.\\n    \"\"\"\\n    def __init__(self, model: str = None, index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,  system_prompt: str = None, user_prompt: str = None, name: str = \\'fifo_vector_memory\\', max_memory: int = 2048,  max_index_memory: int = 400,  max_output_tokens: int = 1000, longterm_thread: Optional[VectorThread] = None, longterm_frac: float = 0.5):\\n        self.total_max_memory = max_memory\\n\\n        self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\\n        FifoThread.__init__(self, name=name, max_memory=self.max_fifo_memory, longterm_thread=self.longterm_thread)\\n        Chat.__init__(self, model= model, index_dict= index_dict,  max_output_tokens=max_output_tokens, max_index_memory=max_index_memory, system_prompt=system_prompt, user_prompt=user_prompt)\\n        self.prompt_func = self.fifovector_memory_prompt\\n        self.prompt_list = []\\n\\n    def setup_longterm_memory(self, longterm_thread: Optional[VectorThread], max_memory: int, longterm_frac: float):\\n        \"\"\"\\n        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\\n\\n        :param longterm_thread: An optional VectorThread for long-term memory.\\n        :param max_memory: The maximum amount of memory for the chatbot.\\n        :param longterm_frac: The fraction of memory dedicated to long-term memory.\\n        \"\"\"\\n        if longterm_thread is None:\\n            self.longterm_frac = longterm_frac\\n            self.max_fifo_memory = int(max_memory * (1-self.longterm_frac))\\n            self.max_vector_memory = max_memory - self.max_fifo_memory    \\n            self.longterm_thread = VectorThread(name= \\'longterm_memory\\', max_context=self.max_vector_memory)\\n        else:\\n            self.longterm_thread = longterm_thread\\n            self.max_vector_memory = self.longterm_thread.max_context\\n            self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\\n            self.longterm_frac = self.max_vector_memory / self.total_max_memory\\n\\n    def fifovector_memory_prompt(self, message: str, k: int = 10) -> Tuple[List[dict], dict]:\\n        \"\"\"\\n        Combine the system prompt, long-term memory (vector memory), short-term memory (FIFO memory), and the user prompt.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include from the long-term memory.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n        prompt = [mark_system(self.system_prompt)]\\n        if len(self.longterm_thread.memory_thread) > 0 and self.longterm_thread.total_tokens <= self.max_vector_memory:\\n            prompt += self.longterm_thread.memory_thread\\n        elif len(self.longterm_thread.memory_thread) > 0 and self.longterm_thread.total_tokens > self.max_vector_memory:\\n            sorted_messages, sorted_scores, sorted_indices = self.longterm_thread.sorted_query(message, k=k, max_tokens = self.max_vector_memory,reverse=True) \\n            prompt += sorted_messages\\n\\n        prompt += self.memory_thread\\n        marked_question = mark_question(self.user_prompt(message))\\n        prompt += [marked_question]\\n        return prompt, marked_question\\n    def query(self, question: str, verbose: bool = False) -> str:\\n        \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n        answer = BaseChat.query(self, message=question, verbose=verbose)\\n        marked_question = mark_question(question)\\n        self.add_message(marked_question)\\n        self.add_message(answer)\\n        return answer\\n    \\n\\n\\n\\n        \\n',\n",
       " '    \\n\\nclass Chat(BaseChat, Prompter):\\n    \"\"\"\\n    This class combines the BaseChat and Prompter classes to create a oneshot chatbot with a system and user prompt,\\n    and the ability to handle multiple index_dict.\\n    \"\"\"\\n\\n    def __init__(self, model: str = None, max_output_tokens: int = 1000, system_prompt: str = None, user_prompt: str = None, index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None, max_index_memory: int = 1000) -> None:\\n        BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\\n        Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\\n        self.index_dict = index_dict\\n        self.setup_indices(max_index_memory)\\n            \\n    def setup_indices(self,max_index_memory):\\n        \"\"\" setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index. \\n        \"\"\"\\n        if self.index_dict is not None:\\n            self.current_index = list(self.index_dict.keys())[0]\\n            self.system_prompt = INDEX_SYSTEM_PROMPT if self.user_defined_system_prompt is False else self.system_prompt\\n            self.user_prompt = self.get_index_hints if self.user_defined_user_prompt is False else self.user_prompt\\n            self.max_index_memory = max_index_memory\\n   \\n\\n    def get_index_hints(self, question: str, k: int = 10, max_tokens: int = None) -> str:\\n        \"\"\"\\n        Get hints from the current index for the given question.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include from the index.\\n        :param max_tokens: The maximum number of tokens to be retrieved from the index.\\n        :return: A string representing the hint prompt with the question.\\n        \"\"\"\\n        if max_tokens is None:\\n            max_tokens = self.max_index_memory\\n        hints = []\\n        if self.current_index is not None:\\n            index_instance = self.index_dict[self.current_index]\\n            if isinstance(index_instance, PandasIndex) or isinstance(index_instance, MemoryIndex) or isinstance(index_instance, PythonIndex):\\n                hints, _, _ = index_instance.token_bound_query(question, k=k, max_tokens=max_tokens)         \\n            else:\\n                raise ValueError(\"The current index is not a valid index instance.\")\\n            hints_string = \"\\\\n\".join(hints)\\n            hint_prompt = INDEX_HINT_PROMPT\\n            question_intro = QUESTION_INTRO\\n            return hint_prompt.format(hints_string=hints_string) + question_intro.format(question=question)\\n        else:\\n            return question\\n\\n    def set_current_index(self, index_name: Optional[str]) -> None:\\n        \"\"\"\\n        Set the current index to be used for hints.\\n\\n        :param index_name: A string representing the index name or None to clear the current index.\\n        :raise ValueError: If the provided index name is not available.\\n        \"\"\"\\n        if self.index_dict is None:\\n            raise ValueError(\"No index_dict are available.\")\\n        elif index_name in self.index_dict:\\n            self.current_index = index_name\\n        elif index_name is None:\\n            self.current_index = None\\n        else:\\n            raise ValueError(\"The provided index name is not available.\")\\n    \\n',\n",
       " '\\n\\nclass Prompter:\\n    \"\"\"\\n    This class handles the system and user prompts and the prompt_func. By subclassing and overriding the\\n    prompt_func, you can change the way the prompts are composed.\\n    \"\"\"\\n\\n    def __init__(self, system_prompt: str = None, user_prompt: str = None):\\n        \"\"\"\\n        Initialize the Prompter with system and user prompts.\\n\\n        :param system_prompt: A string representing the system prompt.\\n        :param user_prompt: A string representing the user prompt.\\n        \"\"\"\\n        if system_prompt is None:\\n            self.system_prompt = DEFAULT_SYSTEM_PROMPT\\n            self.user_defined_system_prompt = False\\n        else:\\n            self.system_prompt = system_prompt\\n            self.user_defined_system_prompt = True\\n        if user_prompt is None:\\n            self.user_prompt = self.default_user_prompt\\n            self.user_defined_user_prompt = False\\n        else:\\n            self.user_prompt = user_prompt\\n            self.user_defined_user_prompt = True\\n            \\n        self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\\n\\n    def default_user_prompt(self, message: str) -> str:\\n        return DEFAULT_USER_PROMPT.format(question=message)\\n\\n    def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\\n        \"\"\"\\n        Compose the prompt for the chat-gpt API.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\\n        \"\"\"\\n        marked_question = mark_question(self.user_prompt(message))\\n        prompt = [mark_system(self.system_prompt)] + [marked_question]\\n        return prompt, marked_question\\n    \\n    def update_system_prompt(self, new_prompt: str) -> None:\\n        \"\"\"\\n        Update the system prompt.\\n\\n        :param new_prompt: A string representing the new system prompt.\\n        \"\"\"\\n        self.system_prompt = new_prompt\\n\\n    def update_user_prompt(self, new_prompt: str) -> None:\\n        \"\"\"\\n        Update the user prompt.\\n\\n        :param new_prompt: A string representing the new user prompt.\\n        \"\"\"\\n        self.user_prompt = new_prompt\\n',\n",
       " '\\nclass BaseChat:\\n    \"\"\"\\n    This is the base class for chatbots, defining the basic functions that a chatbot should have, mainly the calls to\\n    chat-gpt API, and a basic Gradio interface. It has a prompt_func that acts as a placeholder for a call to chat-gpt\\n    API without any additional messages. It can be overridden by subclasses to add additional messages to the prompt.\\n    \"\"\"\\n    def __init__(self, model: str = None, max_output_tokens: int = 1000):\\n        \"\"\"\\n        Initialize the BaseChat with a model and max_output_tokens.\\n\\n        :param model: A string representing the chat model to be used.\\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\\n        \"\"\"\\n        if model is None:\\n            self.model = \"gpt-3.5-turbo\"\\n        else:\\n            self.model = model\\n        self.tokenizer = tiktoken.encoding_for_model(\\'gpt-3.5-turbo\\')\\n        self.max_output_tokens = max_output_tokens\\n        self.failed_responses = []\\n        self.outputs = []\\n        self.inputs = []\\n        self.prompts = []\\n        self.prompt_func = self.identity_prompter\\n\\n    def identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\\n        \"\"\"\\n        A simple identity prompter that takes a message and returns the message marked as a question.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing the marked question and the original message.\\n        \"\"\"\\n        return [mark_question(message)], mark_question(message)\\n\\n    def chat_response(self, prompt: List[dict], max_tokens: int = None) -> Tuple[Dict, bool]:\\n        \"\"\"\\n        Call the OpenAI API with the given prompt and maximum number of output tokens.\\n\\n        :param prompt: A list of strings representing the prompt to send to the API.\\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\\n        :return: A tuple containing the API response as a dictionary and a boolean indicating success.\\n        \"\"\"\\n        if max_tokens is None:\\n            max_tokens = self.max_output_tokens\\n        try:\\n            print(\"Trying to call OpenAI API...\")\\n            response = openai.ChatCompletion.create(\\n                model=self.model,\\n                messages=prompt,\\n                max_tokens=max_tokens,\\n            )\\n            return response, True\\n\\n        except openai.error.APIError as e:\\n            print(e)\\n            fail_response = {\"choices\": [{\"message\": {\"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"}}]}\\n            self.failed_responses.append(fail_response)\\n            return fail_response , False\\n\\n    def reply(self, message: str, verbose : bool = True) -> str:\\n        \"\"\"\\n        Reply to a given message using the chatbot.\\n\\n        :param message: A string representing the user message.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n        return self.query(message, verbose)[\"content\"]\\n\\n    def query(self, message: str, verbose: bool = True) -> str:\\n        \"\"\"\\n        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\\n\\n        :param message: A string representing the user message.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n\\n        prompt, _ = self.prompt_func(message)\\n        response, success = self.chat_response(prompt)\\n        if verbose:\\n            display(Markdown(\"#### Question: \\\\n {question}\".format(question = message)))\\n        if success:\\n            answer = get_mark_from_response(response)\\n            self.outputs.append(answer)\\n            self.inputs.append(message)\\n            self.prompts.append(prompt)\\n            if verbose:\\n                display(Markdown(\" #### Anwser: \\\\n {answer}\".format(answer = get_str_from_response(response)))) \\n            return answer\\n        else:\\n            raise Exception(\"OpenAI API Error inside query function\")\\n\\n    def run_text(self, text: str, state: List[Tuple[str, str]]) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\\n        \"\"\"\\n        Process the user\\'s text input and update the chat state.\\n\\n        :param text: A string representing the user input.\\n        :param state: A list of tuples representing the current chat state.\\n        :return: A tuple containing the updated chat state as two lists of tuples.\\n        \"\"\"\\n        print(\"===============Running run_text =============\")\\n        print(\"Inputs:\", text)\\n        try: \\n            print(\"======>Current memory:\\\\n %s\" % self.memory_thread)\\n        except:\\n            print(\"======>No memory\")\\n        response = self.reply(text)\\n        state = state + [(text, response)]\\n        print(\"Outputs:\", state)\\n        return state, state\\n\\n    def gradio(self):\\n        \"\"\"\\n        Create and launch a Gradio interface for the chatbot.\\n        \"\"\"\\n        with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\\n            chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\\n            state = gr.State([])\\n            with gr.Row():\\n                with gr.Column(scale=1):\\n                    txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter, or upload an image\").style(container=False)\\n                with gr.Column(scale=0.15, min_width=0):\\n                    clear = gr.Button(\"Clear️\")\\n\\n            txt.submit(self.run_text, [txt, state], [chatbot, state])\\n            txt.submit(lambda: \"\", None, txt)\\n            demo.launch(server_name=\"localhost\", server_port=7860 )\\n',\n",
       " '\\nclass OpenAiEmbedder:\\n    def get_embedding_size(self):\\n        return ADA_EMBEDDING_SIZE\\n    def embed(self, data, embed_mark = False, verbose = False):\\n        try:\\n            if embed_mark is False and type(data) is dict and \"content\" in data:\\n                if verbose is True:\\n                    print(\"Embedding without mark\", data[\"content\"])\\n                out = openai.Embedding.create(input=data[\"content\"], engine=\\'text-embedding-ada-002\\')\\n            else:\\n                if verbose is True:\\n                    print(\"Embedding without preprocessing the input\", data)\\n                out = openai.Embedding.create(input=str(data), engine=\\'text-embedding-ada-002\\')\\n        except:\\n            raise ValueError(\"The data  is not valid\", data)\\n        return out.data[0].embedding\\n',\n",
       " '\\nclass CohereEmbedder:\\n    def get_embedding_size(self):\\n        return COHERE_EMBEDDING_SIZE\\n    def embed(self, data, embed_mark = False, verbose = False):\\n        try:\\n            if embed_mark is False and type(data) is dict and \"content\" in data:\\n                if verbose is True:\\n                    print(\"Embedding without mark\", data[\"content\"])\\n                out = co.embed(input=data[\"content\"]).embeddings\\n            else:\\n                if verbose is True:\\n                    print(\"Embedding without preprocessing the input\", data)\\n                out = co.embed(input=str(data)).embeddings\\n\\n        except:\\n            raise ValueError(\"The data  is not valid\", data)\\n        return out\\n',\n",
       " '\\n\\nclass PubmedAPI:\\n    def __init__(self):\\n        self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\\n\\n    def search(self, query, max_results=10):\\n        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\\n        record = Entrez.read(handle)\\n        handle.close()\\n        return record[\"IdList\"]\\n\\n    def fetch_abstract(self, pubmed_id):\\n        handle = Entrez.efetch(db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\")\\n        abstract = handle.read()\\n        handle.close()\\n        return abstract\\n\\n    def fetch_pmc_full_text(self, pubmed_id):\\n        # Get the PMC ID for the PubMed ID\\n        handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\\n        record = Entrez.read(handle)\\n        handle.close()\\n        pmc_id = None\\n        for link in record[0][\"LinkSetDb\"]:\\n            if link[\"DbTo\"] == \"pmc\":\\n                pmc_id = link[\"Link\"][0][\"Id\"]\\n                break\\n\\n        if not pmc_id:\\n            return None\\n\\n        # Fetch the PMC article XML\\n        handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\\n        xml_content = handle.read()\\n        handle.close()\\n\\n        # Parse the XML and extract the full text\\n        soup = BeautifulSoup(xml_content, \"xml\")\\n        full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\\n\\n        return full_text\\n',\n",
       " \"\\n\\nclass PubmedParser:\\n    def __init__(self):\\n        self.api = PubmedAPI()\\n\\n    def parse_papers(self, query, max_results=10):\\n        pubmed_ids = self.api.search(query, max_results)\\n        paper_list = []\\n        for pubmed_id in pubmed_ids:\\n            paper_dict = {}\\n            paper_dict['pubmed_id'] = pubmed_id\\n            paper_dict['abstract'] = self.api.fetch_abstract(pubmed_id)\\n            paper_dict['content'] = self.api.fetch_pmc_full_text(pubmed_id)\\n            paper_list.append(paper_dict)\\n        return paper_list\\n\",\n",
       " \"\\n\\nclass ArxivVanityParser:\\n    def __init__(self):\\n        self.base_url = 'https://www.arxiv-vanity.com/'\\n\\n    def _get_vanity_url(self, arxiv_id):\\n        return urljoin(self.base_url, 'papers/' + arxiv_id)\\n\\n    def _fetch_html(self, url):\\n        response = requests.get(url)\\n        if response.status_code == 200:\\n            return response.text\\n        else:\\n            return None\\n\\n    def _extract_main_content(self, html):\\n        soup = BeautifulSoup(html, 'html.parser')\\n        paragraphs = soup.find_all('div', {'class': 'ltx_para'})\\n        content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\\n        return content\\n\\n    def parse_paper(self, arxiv_id):\\n        vanity_url = self._get_vanity_url(arxiv_id)\\n        html = self._fetch_html(vanity_url)\\n        if html is not None:\\n            return self._extract_main_content(html)\\n        else:\\n            return None\\n\",\n",
       " '\\nclass ArxivAPI:\\n    def __init__(self):\\n        self.base_url = \"http://export.arxiv.org/api/query?\"\\n        self.pdf_download_url = \"https://arxiv.org/pdf/\"\\n\\n    def search(self, query, max_results=10):\\n        url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\\n        response = requests.get(url)\\n        if response.status_code == 200:\\n            return response.text\\n        else:\\n            return None\\n\\n    def download_pdf(self, paper_key, save_directory=\"./\"):\\n        pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\\n        response = requests.get(pdf_url)\\n        if response.status_code == 200:\\n            with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\\n                f.write(response.content)\\n            print(f\"PDF for {paper_key} downloaded successfully.\")\\n        else:\\n            print(f\"Error downloading PDF for {paper_key}.\")\\n',\n",
       " \"\\n\\nclass ArxivParser:\\n    def __init__(self):\\n        self.api = ArxivAPI()\\n        self.vanity_parser = ArxivVanityParser()\\n\\n    def _parse_arxiv_id(self, url):\\n        return url.split('/')[-1]\\n\\n    def parse_papers(self, query, max_results=10):\\n        search_results = self.api.search(query, max_results)\\n        if search_results is not None:\\n            soup = BeautifulSoup(search_results, 'html.parser')\\n            entries = soup.find_all('entry')\\n            paper_list = []\\n            for entry in entries:\\n                paper_dict = {}\\n                arxiv_id = self._parse_arxiv_id(entry.id.string)\\n                paper_dict['arxiv_id'] = arxiv_id\\n                paper_dict['title'] = entry.title.string\\n                paper_dict['summary'] = entry.summary.string\\n                paper_dict['content'] = self.vanity_parser.parse_paper(str(arxiv_id))\\n                if paper_dict['content'] == None:\\n                    continue\\n                paper_list.append(paper_dict)\\n            return paper_list\\n        else:\\n            return None\\n\",\n",
       " '\\n\\nclass OsProcessor:\\n    def __init__(self, directory_path: str):\\n        self.directory_path = directory_path\\n\\n    def get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\\n        \"\"\"Returns a list of all files in a directory\"\"\"\\n        if directory_path is None:\\n            directory_path = self.directory_path\\n\\n        all_files = []\\n        for root, _, files in os.walk(directory_path):\\n            for file in files:\\n                all_files.append(os.path.join(root, file))\\n\\n        return all_files\\n\\n    def get_files_with_extension(self, extension: str, directory_path: Optional[str] = None) -> List[str]:\\n        \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\\n        if directory_path is None:\\n            directory_path = self.directory_path\\n\\n        all_files = self.get_all_files(directory_path)\\n        files_with_extension = [file for file in all_files if file.endswith(extension)]\\n\\n        return files_with_extension\\n\\n    def get_file_extension(self, file_path: str) -> str:\\n        \"\"\"Returns the extension of a file\"\"\"\\n        return Path(file_path).suffix\\n\\n    def get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\\n        \"\"\" Returns a list of all subdirectories in a directory\"\"\"\\n        if directory_path is None:\\n            directory_path = self.directory_path\\n\\n        subdirectories = [os.path.join(directory_path, d) for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\\n\\n        return subdirectories\\n\\n    def create_directory(self, directory_path: str) -> None:\\n        \"\"\" Creates a directory if it does not exist\"\"\"\\n        if not os.path.exists(directory_path):\\n            os.makedirs(directory_path)\\n\\n    def delete_directory(self, directory_path: str) -> None:\\n        \"\"\" Deletes a directory if it exists\"\"\"\\n        if os.path.exists(directory_path):\\n            shutil.rmtree(directory_path)\\n\\n    def copy_file(self, source_path: str, destination_path: str) -> None:\\n        \"\"\" Copies a file from one location to another\"\"\"\\n        shutil.copy2(source_path, destination_path)\\n\\n    def move_file(self, source_path: str, destination_path: str) -> None:\\n        \"\"\" Moves a file from one location to another\"\"\"\\n        shutil.move(source_path, destination_path)\\n',\n",
       " '\\n\\nclass GithubProcessor(OsProcessor):\\n    def __init__(self, base_directory: str, username=None, repo_name=None, code_parsers=None, minify_code: bool = False, remove_docstrings: bool = False):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.base_directory = base_directory\\n        self.github = Github()\\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n        repo_path = self.clone_repo(self.repo.clone_url)\\n\\n        OsProcessor.__init__(self,repo_path)\\n        self.code_parsers = code_parsers or [PythonParser(repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings)]\\n\\n\\n    def get_public_repos(self):\\n        \"\"\" Returns a list of all public repos for the user.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def clone_repo(self, repo_url: str):\\n        \"\"\" Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.base_directory, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n    def process_repo(self, repo_path=None):\\n        \"\"\" Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n        if repo_path is None:\\n            repo_path = self.directory_path\\n\\n        for code_parser in self.code_parsers:\\n            code_parser.directory_path = repo_path\\n            code_parser.process_directory(repo_path)\\n\\n    def process_repos(self):\\n        \"\"\" Processes all public repos for the user.\"\"\"\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.clone_repo(repo.clone_url)\\n                self.process_repo(repo_path)\\n                shutil.rmtree(repo_path)\\n\\n    def get_repo(self, repo_name):\\n        \"\"\" Returns the repo with the specified name.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_single_repo(self):\\n\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.clone_repo(repo.clone_url)\\n        self.process_repo(repo_path)\\n        shutil.rmtree(repo_path)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n',\n",
       " '\\nclass FunctionAndClassVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        \"\"\" This method is called for every FunctionDef node in the tree.\\n        and it does the following:\\n        1. Gets the source code for the node\\n        2. Adds the node to the list of function nodes\\n        3. Adds the source code to the list of function source codes\\n        \"\"\"\\n        function_source_code = cst.Module([]).code_for_node(node)\\n        self.function_nodes.append(node)\\n        self.function_source_codes.append(function_source_code)\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\\n        \"\"\" This method is called for every ClassDef node in the tree.\\n        and it does the following:\\n        1. Gets the source code for the node\\n        2. Adds the node to the list of class nodes\\n        3. Adds the source code to the list of class source codes\\n        \"\"\"\\n        class_source_code = cst.Module([]).code_for_node(node)\\n        self.class_nodes.append(node)\\n        self.class_source_codes.append(class_source_code)\\n',\n",
       " '\\n\\nclass PythonParser(OsProcessor):\\n    def __init__(self, directory_path: str, visitor: Optional[FunctionAndClassVisitor] = None, minify_code: bool = False, remove_docstrings: bool = False):\\n        super().__init__(directory_path)\\n        self.visitor = visitor if visitor else FunctionAndClassVisitor()\\n        self.minify_code = minify_code\\n        self.remove_docstrings = remove_docstrings\\n\\n    \\n    def remove_docstring(self, tree: cst.Module) -> str:\\n        \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\\n\\n        # Remove docstrings using a transformer\\n        class DocstringRemover(cst.CSTTransformer):\\n            def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\\n                docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n                if docstring.startswith(\"No docstring\"):\\n                    return updated_node\\n\\n                return updated_node.with_changes(body=updated_node.body.with_changes(body=[stmt for stmt in updated_node.body.body if not (isinstance(stmt, cst.SimpleStatementLine) and any(isinstance(expr, cst.Expr) and isinstance(expr.value, cst.SimpleString) for expr in stmt.body))]))\\n\\n        tree = tree.visit(DocstringRemover())\\n        return tree.code\\n\\n    def _process_file(self, file_path: str):\\n        \"\"\" This method is called for every file in the directory.\\n        It does the following:\\n        1. Reads the file\\n        2. Parses the file\\n        3. Visits the file with the visitor\\n        \"\"\"\\n        with open(file_path, \"r\", encoding=\\'utf-8\\') as file:\\n            source_code = file.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n        except cst.ParserSyntaxError:\\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n            return\\n\\n        tree.visit(self.visitor)\\n\\n        # Remove docstrings if specified\\n        if self.remove_docstrings:\\n            source_code = self.remove_docstring(source_code, tree)\\n\\n        # Minify the code if specified\\n        if self.minify_code:\\n            minifier = PythonMinifier(source_code)\\n            source_code = minifier.get_minified_code()\\n\\n        # Add the processed code to the corresponding list in the visitor\\n        self.visitor.function_source_codes.append(source_code)\\n\\n    def process_file(self, file_path: str):\\n        \"\"\" This method is called for every file in the directory.\\n        It does the following:\\n        1. Runs flake8 on the file\\n        if flake8 returns a non-zero exit code, it means the file has a syntax error\\n        2. Reads the file\\n        3. Parses the file\\n        4. Visits the file with the visitor\\n\\n        \"\"\"\\n        result = subprocess.run([\"flake8\", \"--select=E999\", file_path], capture_output=True)\\n\\n        if result.returncode != 0:\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n            print(result.stderr.decode(\"utf-8\"))\\n            return\\n\\n        with open(file_path, \"r\", encoding=\\'utf-8\\') as f:\\n            source_code = f.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n            tree.visit(self.visitor)\\n        except cst.ParserSyntaxError as e:\\n            print(f\"Syntax error: {e}\")\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n\\n    def process_directory(self) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\\n        \"\"\" This method is called for every directory.\\n        It does the following:\\n        1. Gets all the python files in the directory\\n        2. Processes each file\\n        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\\n        \"\"\"\\n        function_source_codes = []\\n        class_source_codes = []\\n\\n        python_files = self.get_files_with_extension(\\'.py\\')\\n\\n        for file_path in python_files:\\n            self._process_file(file_path)\\n\\n        function_source_codes = self.visitor.function_source_codes\\n        function_nodes = self.visitor.function_nodes\\n        class_source_codes = self.visitor.class_source_codes\\n        class_nodes = self.visitor.class_nodes\\n\\n        return function_source_codes, class_source_codes, function_nodes, class_nodes\\n',\n",
       " '\\n# Remove docstrings using a transformer\\nclass DocstringRemover(cst.CSTTransformer):\\n    def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\\n        docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n        if docstring.startswith(\"No docstring\"):\\n            return updated_node\\n\\n        return updated_node.with_changes(body=updated_node.body.with_changes(body=[stmt for stmt in updated_node.body.body if not (isinstance(stmt, cst.SimpleStatementLine) and any(isinstance(expr, cst.Expr) and isinstance(expr.value, cst.SimpleString) for expr in stmt.body))]))\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_kernel_dict[\"babydragon_kernel\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n\\nclass ArxivParser:\\n    def __init__(self):\\n        self.api = ArxivAPI()\\n        self.vanity_parser = ArxivVanityParser()\\n\\n    def _parse_arxiv_id(self, url):\\n        return url.split('/')[-1]\\n\\n    def parse_papers(self, query, max_results=10):\\n        search_results = self.api.search(query, max_results)\\n        if search_results is not None:\\n            soup = BeautifulSoup(search_results, 'html.parser')\\n            entries = soup.find_all('entry')\\n            paper_list = []\\n            for entry in entries:\\n                paper_dict = {}\\n                arxiv_id = self._parse_arxiv_id(entry.id.string)\\n                paper_dict['arxiv_id'] = arxiv_id\\n                paper_dict['title'] = entry.title.string\\n                paper_dict['summary'] = entry.summary.string\\n                paper_dict['content'] = self.vanity_parser.parse_paper(str(arxiv_id))\\n                if paper_dict['content'] == None:\\n                    continue\\n                paper_list.append(paper_dict)\\n            return paper_list\\n        else:\\n            return None\\n\",\n",
       " '\\n\\nclass OsProcessor:\\n    def __init__(self, directory_path: str):\\n        self.directory_path = directory_path\\n\\n    def get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\\n        \"\"\"Returns a list of all files in a directory\"\"\"\\n        if directory_path is None:\\n            directory_path = self.directory_path\\n\\n        all_files = []\\n        for root, _, files in os.walk(directory_path):\\n            for file in files:\\n                all_files.append(os.path.join(root, file))\\n\\n        return all_files\\n\\n    def get_files_with_extension(self, extension: str, directory_path: Optional[str] = None) -> List[str]:\\n        \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\\n        if directory_path is None:\\n            directory_path = self.directory_path\\n\\n        all_files = self.get_all_files(directory_path)\\n        files_with_extension = [file for file in all_files if file.endswith(extension)]\\n\\n        return files_with_extension\\n\\n    def get_file_extension(self, file_path: str) -> str:\\n        \"\"\"Returns the extension of a file\"\"\"\\n        return Path(file_path).suffix\\n\\n    def get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\\n        \"\"\" Returns a list of all subdirectories in a directory\"\"\"\\n        if directory_path is None:\\n            directory_path = self.directory_path\\n\\n        subdirectories = [os.path.join(directory_path, d) for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\\n\\n        return subdirectories\\n\\n    def create_directory(self, directory_path: str) -> None:\\n        \"\"\" Creates a directory if it does not exist\"\"\"\\n        if not os.path.exists(directory_path):\\n            os.makedirs(directory_path)\\n\\n    def delete_directory(self, directory_path: str) -> None:\\n        \"\"\" Deletes a directory if it exists\"\"\"\\n        if os.path.exists(directory_path):\\n            shutil.rmtree(directory_path)\\n\\n    def copy_file(self, source_path: str, destination_path: str) -> None:\\n        \"\"\" Copies a file from one location to another\"\"\"\\n        shutil.copy2(source_path, destination_path)\\n\\n    def move_file(self, source_path: str, destination_path: str) -> None:\\n        \"\"\" Moves a file from one location to another\"\"\"\\n        shutil.move(source_path, destination_path)\\n',\n",
       " '\\n\\nclass GithubProcessor(OsProcessor):\\n    def __init__(self, base_directory: str, username=None, repo_name=None, code_parsers=None, minify_code: bool = False, remove_docstrings: bool = False):\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.base_directory = base_directory\\n        self.github = Github()\\n        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n        repo_path = self.clone_repo(self.repo.clone_url)\\n\\n        OsProcessor.__init__(self,repo_path)\\n        self.code_parsers = code_parsers or [PythonParser(repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings)]\\n\\n\\n    def get_public_repos(self):\\n        \"\"\" Returns a list of all public repos for the user.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repos()\\n\\n    def clone_repo(self, repo_url: str):\\n        \"\"\" Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n        target_directory = os.path.join(self.base_directory, repo_name)\\n\\n        if os.path.exists(target_directory):\\n            shutil.rmtree(target_directory)\\n\\n        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n        return target_directory\\n\\n    def process_repo(self, repo_path=None):\\n        \"\"\" Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n        if repo_path is None:\\n            repo_path = self.directory_path\\n\\n        for code_parser in self.code_parsers:\\n            code_parser.directory_path = repo_path\\n            code_parser.process_directory(repo_path)\\n\\n    def process_repos(self):\\n        \"\"\" Processes all public repos for the user.\"\"\"\\n        for repo in self.get_public_repos():\\n            if not repo.private:\\n                print(f\"Processing repo: {repo.name}\")\\n                repo_path = self.clone_repo(repo.clone_url)\\n                self.process_repo(repo_path)\\n                shutil.rmtree(repo_path)\\n\\n    def get_repo(self, repo_name):\\n        \"\"\" Returns the repo with the specified name.\"\"\"\\n        user = self.github.get_user(self.username)\\n        return user.get_repo(repo_name)\\n\\n    def process_single_repo(self):\\n\\n        repo = self.get_repo(self.repo_name)\\n        print(f\"Processing repo: {self.repo_name}\")\\n        repo_path = self.clone_repo(repo.clone_url)\\n        self.process_repo(repo_path)\\n        shutil.rmtree(repo_path)\\n\\n    def get_issues(self, state=\"open\"):\\n        \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n        issues = []\\n        for issue in self.repo.get_issues(state=state):\\n            issues.append(issue)\\n        return issues\\n\\n    def parse_issues(self, state=\"open\"):\\n        \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n        parsed_issues = []\\n        issues = self.get_issues(state=state)\\n        for issue in issues:\\n            parsed_issue = {\\n                \"number\": issue.number,\\n                \"title\": issue.title,\\n                \"body\": issue.body,\\n                \"labels\": [label.name for label in issue.labels],\\n            }\\n            parsed_issues.append(parsed_issue)\\n        return parsed_issues\\n\\n    def get_commits(self):\\n        \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n        commits = []\\n        branch = self.repo.get_branch(\"main\")\\n        for commit in self.repo.get_commits(sha=branch.commit.sha):\\n            commits.append(commit)\\n        return commits\\n\\n    def parse_commits(self):\\n        \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n        parsed_commits = []\\n        commits = self.get_commits()\\n        for commit in commits:\\n            parsed_commit = {\\n                \"sha\": commit.sha,\\n                \"message\": commit.commit.message,\\n                \"author\": {\\n                    \"name\": commit.commit.author.name,\\n                    \"email\": commit.commit.author.email,\\n                    \"date\": commit.commit.author.date,\\n                },\\n            }\\n            parsed_commits.append(parsed_commit)\\n        return parsed_commits\\n',\n",
       " '\\nclass FunctionAndClassVisitor(cst.CSTVisitor):\\n    def __init__(self):\\n        self.function_source_codes = []\\n        self.function_nodes = []\\n        self.class_source_codes = []\\n        self.class_nodes = []\\n\\n    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n        \"\"\" This method is called for every FunctionDef node in the tree.\\n        and it does the following:\\n        1. Gets the source code for the node\\n        2. Adds the node to the list of function nodes\\n        3. Adds the source code to the list of function source codes\\n        \"\"\"\\n        function_source_code = cst.Module([]).code_for_node(node)\\n        self.function_nodes.append(node)\\n        self.function_source_codes.append(function_source_code)\\n\\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\\n        \"\"\" This method is called for every ClassDef node in the tree.\\n        and it does the following:\\n        1. Gets the source code for the node\\n        2. Adds the node to the list of class nodes\\n        3. Adds the source code to the list of class source codes\\n        \"\"\"\\n        class_source_code = cst.Module([]).code_for_node(node)\\n        self.class_nodes.append(node)\\n        self.class_source_codes.append(class_source_code)\\n',\n",
       " '\\n\\nclass PythonParser(OsProcessor):\\n    def __init__(self, directory_path: str, visitor: Optional[FunctionAndClassVisitor] = None, minify_code: bool = False, remove_docstrings: bool = False):\\n        super().__init__(directory_path)\\n        self.visitor = visitor if visitor else FunctionAndClassVisitor()\\n        self.minify_code = minify_code\\n        self.remove_docstrings = remove_docstrings\\n\\n    \\n    def remove_docstring(self, tree: cst.Module) -> str:\\n        \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\\n\\n        # Remove docstrings using a transformer\\n        class DocstringRemover(cst.CSTTransformer):\\n            def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\\n                docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n                if docstring.startswith(\"No docstring\"):\\n                    return updated_node\\n\\n                return updated_node.with_changes(body=updated_node.body.with_changes(body=[stmt for stmt in updated_node.body.body if not (isinstance(stmt, cst.SimpleStatementLine) and any(isinstance(expr, cst.Expr) and isinstance(expr.value, cst.SimpleString) for expr in stmt.body))]))\\n\\n        tree = tree.visit(DocstringRemover())\\n        return tree.code\\n\\n    def _process_file(self, file_path: str):\\n        \"\"\" This method is called for every file in the directory.\\n        It does the following:\\n        1. Reads the file\\n        2. Parses the file\\n        3. Visits the file with the visitor\\n        \"\"\"\\n        with open(file_path, \"r\", encoding=\\'utf-8\\') as file:\\n            source_code = file.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n        except cst.ParserSyntaxError:\\n            print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n            return\\n\\n        tree.visit(self.visitor)\\n\\n        # Remove docstrings if specified\\n        if self.remove_docstrings:\\n            source_code = self.remove_docstring(source_code, tree)\\n\\n        # Minify the code if specified\\n        if self.minify_code:\\n            minifier = PythonMinifier(source_code)\\n            source_code = minifier.get_minified_code()\\n\\n        # Add the processed code to the corresponding list in the visitor\\n        self.visitor.function_source_codes.append(source_code)\\n\\n    def process_file(self, file_path: str):\\n        \"\"\" This method is called for every file in the directory.\\n        It does the following:\\n        1. Runs flake8 on the file\\n        if flake8 returns a non-zero exit code, it means the file has a syntax error\\n        2. Reads the file\\n        3. Parses the file\\n        4. Visits the file with the visitor\\n\\n        \"\"\"\\n        result = subprocess.run([\"flake8\", \"--select=E999\", file_path], capture_output=True)\\n\\n        if result.returncode != 0:\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n            print(result.stderr.decode(\"utf-8\"))\\n            return\\n\\n        with open(file_path, \"r\", encoding=\\'utf-8\\') as f:\\n            source_code = f.read()\\n\\n        try:\\n            tree = cst.parse_module(source_code)\\n            tree.visit(self.visitor)\\n        except cst.ParserSyntaxError as e:\\n            print(f\"Syntax error: {e}\")\\n            print(f\"Skipping file with syntax error: {file_path}\")\\n\\n    def process_directory(self) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\\n        \"\"\" This method is called for every directory.\\n        It does the following:\\n        1. Gets all the python files in the directory\\n        2. Processes each file\\n        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\\n        \"\"\"\\n        function_source_codes = []\\n        class_source_codes = []\\n\\n        python_files = self.get_files_with_extension(\\'.py\\')\\n\\n        for file_path in python_files:\\n            self._process_file(file_path)\\n\\n        function_source_codes = self.visitor.function_source_codes\\n        function_nodes = self.visitor.function_nodes\\n        class_source_codes = self.visitor.class_source_codes\\n        class_nodes = self.visitor.class_nodes\\n\\n        return function_source_codes, class_source_codes, function_nodes, class_nodes\\n',\n",
       " '\\n# Remove docstrings using a transformer\\nclass DocstringRemover(cst.CSTTransformer):\\n    def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\\n        docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n        if docstring.startswith(\"No docstring\"):\\n            return updated_node\\n\\n        return updated_node.with_changes(body=updated_node.body.with_changes(body=[stmt for stmt in updated_node.body.body if not (isinstance(stmt, cst.SimpleStatementLine) and any(isinstance(expr, cst.Expr) and isinstance(expr.value, cst.SimpleString) for expr in stmt.body))]))\\n']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_group[\"babydragon_kernel\"][-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[119., 109., 101., ...,  79.,  96.,  83.],\n",
       "       [109., 170., 123., ...,  83., 108.,  89.],\n",
       "       [101., 123., 165., ...,  86.,  98.,  96.],\n",
       "       ...,\n",
       "       [ 79.,  83.,  86., ...,  94.,  89.,  86.],\n",
       "       [ 96., 108.,  98., ...,  89., 130.,  90.],\n",
       "       [ 83.,  89.,  96., ...,  86.,  90., 101.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_kernel_dict[\"babydragon_kernel\"].A_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot2 = Chat(model=\"gpt-3.5-turbo\", index_dict=memory_kernel_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babydragon_kernel\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bucket_group \u001b[39m=\u001b[39m MemoryKernelGroup(memory_kernel_dict)\u001b[39m.\u001b[39;49mrank_decomp_and_merge()\n",
      "File \u001b[0;32m~/neuraldragon/gitensor/BabyDragon/babydragon/memory/indexes/memory_kernel.py:112\u001b[0m, in \u001b[0;36mMemoryKernelGroup.rank_decomp_and_merge\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m     code_embeddings \u001b[39m=\u001b[39m mem_kernel\u001b[39m.\u001b[39mnode_embeddings\n\u001b[1;32m    111\u001b[0m     _, _, VT \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39msvd(code_embeddings)\n\u001b[0;32m--> 112\u001b[0m     rank_buckets \u001b[39m=\u001b[39m group_items_by_rank_buckets_svd(code_values, code_embeddings, VT, component_window_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m    113\u001b[0m     bucket_groups[key] \u001b[39m=\u001b[39m rank_buckets\n\u001b[1;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m bucket_groups\n",
      "File \u001b[0;32m~/neuraldragon/gitensor/BabyDragon/babydragon/working_memory/associative_memory/group_by_rank.py:81\u001b[0m, in \u001b[0;36mgroup_items_by_rank_buckets_svd\u001b[0;34m(code_strings, node_embeddings, S_vectors, component_window_size)\u001b[0m\n\u001b[1;32m     79\u001b[0m     r, c \u001b[39m=\u001b[39m start_idx, end_idx\n\u001b[1;32m     80\u001b[0m     \u001b[39m# Compute SVD2 approx embeddings for this bucket\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     approx_embeddings \u001b[39m=\u001b[39m svd_2(sorted_node_embeddings, r, c)\n\u001b[1;32m     83\u001b[0m     rank_buckets\u001b[39m.\u001b[39mappend((sorted_code_strings, approx_embeddings))\n\u001b[1;32m     85\u001b[0m \u001b[39mreturn\u001b[39;00m rank_buckets\n",
      "File \u001b[0;32m~/neuraldragon/gitensor/BabyDragon/babydragon/working_memory/associative_memory/group_by_rank.py:12\u001b[0m, in \u001b[0;36msvd_2\u001b[0;34m(B, r, c)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msvd_2\u001b[39m(B: np\u001b[39m.\u001b[39mndarray, r: \u001b[39mint\u001b[39m, c: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     11\u001b[0m     U, S, VT \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39msvd(B)\n\u001b[0;32m---> 12\u001b[0m     M_cs \u001b[39m=\u001b[39m [S[i] \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mouter(U[:, i], VT[i, :]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(r, c)]\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(M_cs)\n",
      "File \u001b[0;32m~/neuraldragon/gitensor/BabyDragon/babydragon/working_memory/associative_memory/group_by_rank.py:12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msvd_2\u001b[39m(B: np\u001b[39m.\u001b[39mndarray, r: \u001b[39mint\u001b[39m, c: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     11\u001b[0m     U, S, VT \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39msvd(B)\n\u001b[0;32m---> 12\u001b[0m     M_cs \u001b[39m=\u001b[39m [S[i] \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mouter(U[:, i], VT[i, :]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(r, c)]\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(M_cs)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "bucket_group = MemoryKernelGroup(memory_kernel_dict).rank_decomp_and_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<babydragon.memory.indexes.memory_kernel.MemoryKernelGroup at 0x138e6acb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to ask you a question and you should use the hints to answer it. The hints are:\n",
      "def __init__(self, values, embeddings, name=\"memory_kernel\", save_path=None):\n",
      "    super().__init__(values, embeddings, name, save_path)\n",
      "    self.create_k_hop_index()\n",
      "\n",
      "\n",
      "def __len__(self):\n",
      "    return len(self.memory_thread)\n",
      "\n",
      "def __init__(self, name= 'vector_memory', max_context = 2048, use_mark = False):\n",
      "    BaseThread.__init__(self,name= name , max_memory= None)\n",
      "    MemoryIndex.__init__(self, index = None, name = name)\n",
      "    self.max_context = max_context\n",
      "    self.use_mark = use_mark\n",
      "    self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "    \n",
      "\n",
      "\n",
      "def load_git_memory(self, git_memory):\n",
      "    # Retrieve GitMemory context\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "class GitMemory(MemoryIndex):\n",
      "    def __init__(self, username, repo_name):\n",
      "        super().__init__()\n",
      "        self.username = username\n",
      "        self.repo_name = repo_name\n",
      "        self.parser = GitHubRepoProcessor(username, repo_name)\n",
      "        self.minifier = PythonMinifier()\n",
      "        self.docstring_extractor = PythonDocstringExtractor()\n",
      "        self.directory_parser = None\n",
      "        self.min_code_index = None\n",
      "        self.doc_string_index = None\n",
      "        self.libcst_node_index = None\n",
      "    \n",
      "    def create_code_index(self, base_directory):\n",
      "        self.directory_parser = self.parser.process_repo(base_directory)\n",
      "        code_values, code_nodes = self.parser.get_values()\n",
      "        self.code_index = self.init_index(values=code_values)\n",
      "        self.code_index.save()\n",
      "\n",
      "    def create_indexes(self, base_directory):\n",
      "        self.directory_parser = self.parser.process_repo(base_directory)\n",
      "        code_values, code_nodes = self.parser.get_values()\n",
      "        self.code_index = self.init_index(values=code_values)\n",
      "\n",
      "        min_code_values = []\n",
      "        doc_string_values = []\n",
      "        for code_value, code_node in zip(code_values, code_nodes):\n",
      "            minifier = PythonMinifier(code=code_value)\n",
      "            min_code = minifier.get_minified_code()\n",
      "            doc_string = self.docstring_extractor.extract_docstring(code_node)\n",
      "            min_code_values.append(min_code)\n",
      "            doc_string_values.append(doc_string)\n",
      "        self.doc_string_index = self.init_index(values=doc_string_values)\n",
      "        self.min_code_index = self.init_index(values=min_code_values)\n",
      "\n",
      "\n",
      "def __init__(self, model: Optional[str] = None, index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,  name: str = 'vector_memory',  max_index_memory: int = 400,  max_vector_memory: int = 2048, max_output_tokens: int = 1000, system_prompt: str = None, user_prompt: str = None):\n",
      "    VectorThread.__init__(self, name=name, max_context=max_vector_memory)\n",
      "    Chat.__init__(self, model= model,index_dict= index_dict,  max_output_tokens=max_output_tokens, max_index_memory=max_index_memory, system_prompt=system_prompt, user_prompt=user_prompt)\n",
      "    self.max_vector_memory = self.max_context\n",
      "    self.prompt_func = self.vector_memory_prompt\n",
      "\n",
      "def __init__(self, name= 'fifo_memory', max_memory = None, longterm_thread = None, redundant = True):\n",
      "    \n",
      "    BaseThread.__init__(self,name= name , max_memory= None)\n",
      "    if redundant is True:            \n",
      "        self.redundant_thread = BaseThread(name = 'lucid_memory',max_memory = None)\n",
      "    else:\n",
      "        self.redundant_thread = None\n",
      "    if longterm_thread is None:\n",
      "        self.longterm_thread = BaseThread(name ='longterm_memory',max_memory = None)\n",
      "    else:\n",
      "        self.longterm_thread = longterm_thread\n",
      "    # create an alias for the memory_thread to make the code more readable\n",
      "    self.fifo_thread = self.memory_thread\n",
      "    self.max_memory = max_memory\n",
      "        \n",
      "\n",
      "\n",
      "def create_k_hop_index(self, k):\n",
      "    print(\"Computing the adjacency matrix\")\n",
      "    print(\"Embeddings shape: \", self.embeddings.shape)\n",
      "    self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n",
      "    print(\"Computing the k-hop adjacency matrix and aggregated features\")\n",
      "    self.A_k, self.node_embeddings = self.k_hop_message_passing(\n",
      "        self.A, self.embeddings, k\n",
      "    )\n",
      "    print(\"Updating the memory index\")\n",
      "    self.k_hop_index = MemoryIndex(index=None, values=self.values, embeddings=self.node_embeddings, name=self.memory_index.name)\n",
      "\n",
      "def __init__(self):\n",
      "    self.git_memory = None\n",
      "    self.commit_index = None\n",
      "    self.contextual_memory = None\n",
      " .\n",
      "            Remember that I can not see the hints, but please reply as if the knowledge base was part of your training set. The question is: Can you summarize the MemoryKernel class?\n"
     ]
    }
   ],
   "source": [
    "# set the current index to \"my_index\"\n",
    "chatbot2.set_current_index(\"babydragon_kernel\")\n",
    "\n",
    "# get hints for a given question\n",
    "hints2 = chatbot2.get_index_hints(\"Can you summarize the MemoryKernel class?\")\n",
    "print(hints2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in method 'IndexFlatCodes_reconstruct_n', argument 2 of type 'faiss::idx_t'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 31\u001b[0m\n\u001b[1;32m     25\u001b[0m     adj_matrix[I[:,\u001b[39m0\u001b[39m], I[:,\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m D[:,\u001b[39m0\u001b[39m]\n\u001b[1;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m adj_matrix\n\u001b[0;32m---> 31\u001b[0m adj_matrix \u001b[39m=\u001b[39m faiss_index_to_adj_matrix(faiss_ind\u001b[39m.\u001b[39;49mindex, \u001b[39mlen\u001b[39;49m(faiss_ind\u001b[39m.\u001b[39;49mvalues))\n\u001b[1;32m     32\u001b[0m adj_matrix\u001b[39m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[26], line 22\u001b[0m, in \u001b[0;36mfaiss_index_to_adj_matrix\u001b[0;34m(index, num_vectors)\u001b[0m\n\u001b[1;32m     20\u001b[0m D \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((num_vectors, k), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     21\u001b[0m I \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((num_vectors, k), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint64)\n\u001b[0;32m---> 22\u001b[0m index\u001b[39m.\u001b[39msearch(index\u001b[39m.\u001b[39;49mreconstruct_n(np\u001b[39m.\u001b[39;49marange(num_vectors)), k, D, I)\n\u001b[1;32m     24\u001b[0m \u001b[39m# Populate the adjacency matrix with pairwise distances\u001b[39;00m\n\u001b[1;32m     25\u001b[0m adj_matrix[I[:,\u001b[39m0\u001b[39m], I[:,\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m D[:,\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/neuraldragon/.venv/lib/python3.10/site-packages/faiss/class_wrappers.py:504\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_reconstruct_n\u001b[0;34m(self, n0, ni, x)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[39massert\u001b[39;00m x\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (ni, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md)\n\u001b[0;32m--> 504\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreconstruct_n_c(n0, ni, swig_ptr(x))\n\u001b[1;32m    505\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/neuraldragon/.venv/lib/python3.10/site-packages/faiss/swigfaiss_avx2.py:1947\u001b[0m, in \u001b[0;36mIndexFlatCodes.reconstruct_n\u001b[0;34m(self, i0, ni, recons)\u001b[0m\n\u001b[1;32m   1945\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreconstruct_n\u001b[39m(\u001b[39mself\u001b[39m, i0, ni, recons):\n\u001b[1;32m   1946\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\" reconstruction using the codec interface\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1947\u001b[0m     \u001b[39mreturn\u001b[39;00m _swigfaiss_avx2\u001b[39m.\u001b[39;49mIndexFlatCodes_reconstruct_n(\u001b[39mself\u001b[39;49m, i0, ni, recons)\n",
      "\u001b[0;31mTypeError\u001b[0m: in method 'IndexFlatCodes_reconstruct_n', argument 2 of type 'faiss::idx_t'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[263.88278   , 263.2325    ,  14.350029  , ...,   0.57405764,\n",
       "          0.5739497 ,   0.5706153 ],\n",
       "       [241.39865   , 239.6717    ,  14.201194  , ...,   0.5329086 ,\n",
       "          0.5326816 ,   0.5324526 ],\n",
       "       [278.6772    , 278.40082   ,  15.489161  , ...,   0.577207  ,\n",
       "          0.5654743 ,   0.5647866 ],\n",
       "       ...,\n",
       "       [280.1882    , 278.70374   ,  13.455091  , ...,   0.5906057 ,\n",
       "          0.58382875,   0.5798246 ],\n",
       "       [274.48398   , 274.17065   ,  13.406527  , ...,   0.58650506,\n",
       "          0.58637786,   0.57965755],\n",
       "       [279.35733   , 278.7997    ,  13.338354  , ...,   0.58496964,\n",
       "          0.5846895 ,   0.5837144 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Can you summarize the MemoryKernel class??"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The MemoryKernel class has an initializer method that initializes the values, embeddings, name, and save_path of the class. It then calls a method to create a k-hop index. The create_k_hop_index method computes the adjacency matrix, k-hop adjacency matrix, and aggregated features. It then updates the memory index with the k-hop index."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MemoryKernel class has an initializer method that initializes the values, embeddings, name, and save_path of the class. It then calls a method to create a k-hop index. The create_k_hop_index method computes the adjacency matrix, k-hop adjacency matrix, and aggregated features. It then updates the memory index with the k-hop index.\n"
     ]
    }
   ],
   "source": [
    "# ask a question\n",
    "response2 = chatbot2.reply(\"Can you summarize the MemoryKernel class??\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to ask you a question and you should use the hints to answer it. The hints are:\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    visitor_methods: _VisitorMethodCollection,\n",
      "    *,\n",
      "    before_visit: Optional[VisitorMethod] = None,\n",
      "    after_leave: Optional[VisitorMethod] = None,\n",
      ") -> None:\n",
      "    super().__init__()\n",
      "    self.visitor_methods = visitor_methods\n",
      "    self.before_visit = before_visit\n",
      "    self.after_leave = after_leave\n",
      "\n",
      "\n",
      "def get_visitors(self) -> Mapping[str, VisitorMethod]:\n",
      "    \"\"\"\n",
      "        Returns a mapping of all the ``visit_<Type[CSTNode]>``,\n",
      "        ``visit_<Type[CSTNode]>_<attribute>``, ``leave_<Type[CSTNode]>`` and\n",
      "        `leave_<Type[CSTNode]>_<attribute>`` methods defined by this visitor,\n",
      "        excluding all empty stubs.\n",
      "        \"\"\"\n",
      "\n",
      "    methods = inspect.getmembers(\n",
      "        self,\n",
      "        lambda m: (\n",
      "            inspect.ismethod(m)\n",
      "            and (m.__name__.startswith(\"visit_\") or m.__name__.startswith(\"leave_\"))\n",
      "            and not getattr(m, \"_is_no_op\", False)\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # TODO: verify all visitor methods reference valid node classes.\n",
      "    # for name, __ in methods:\n",
      "    #     ...\n",
      "\n",
      "    return dict(methods)\n",
      "\n",
      "\n",
      "def on_visit(self, node: \"CSTNode\") -> bool:\n",
      "    \"\"\"\n",
      "        Call appropriate visit methods on node before visiting children.\n",
      "        \"\"\"\n",
      "    before_visit = self.before_visit\n",
      "    if before_visit is not None:\n",
      "        before_visit(node)\n",
      "    type_name = type(node).__name__\n",
      "    for v in self.visitor_methods.get(f\"visit_{type_name}\", []):\n",
      "        v(node)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def _get_visitor_methods(\n",
      "    batchable_visitors: Iterable[BatchableCSTVisitor],\n",
      ") -> _VisitorMethodCollection:\n",
      "    \"\"\"\n",
      "    Gather all ``visit_<Type[CSTNode]>``, ``visit_<Type[CSTNode]>_<attribute>``,\n",
      "    ``leave_<Type[CSTNode]>`` amd `leave_<Type[CSTNode]>_<attribute>`` methods\n",
      "    from ``batchabled_visitors``.\n",
      "    \"\"\"\n",
      "    visitor_methods: MutableMapping[str, List[VisitorMethod]] = {}\n",
      "    for bv in batchable_visitors:\n",
      "        for name, fn in bv.get_visitors().items():\n",
      "            visitor_methods.setdefault(name, []).append(fn)\n",
      "    return visitor_methods\n",
      "\n",
      "\n",
      "@mark_no_op\n",
      "def visit_Module(self, node: \"Module\") -> Optional[bool]:\n",
      "    pass\n",
      "\n",
      "\n",
      "def on_leave(self, original_node: \"CSTNode\") -> None:\n",
      "    \"\"\"\n",
      "        Call appropriate leave methods on node after visiting children.\n",
      "        \"\"\"\n",
      "    type_name = type(original_node).__name__\n",
      "    for v in self.visitor_methods.get(f\"leave_{type_name}\", []):\n",
      "        v(original_node)\n",
      "    after_leave = self.after_leave\n",
      "    if after_leave is not None:\n",
      "        after_leave(original_node)\n",
      "\n",
      "\n",
      "@mark_no_op\n",
      "def visit_Module_footer(self, node: \"Module\") -> None:\n",
      "    pass\n",
      "\n",
      "\n",
      "@mark_no_op\n",
      "def visit_Module_encoding(self, node: \"Module\") -> None:\n",
      "    pass\n",
      " .\n",
      "            Remember that I can not see the hints, but please reply as if the knowledge base was part of your training set. The question is: What is are Visitors and how do they relate to code mods?\n"
     ]
    }
   ],
   "source": [
    "# set the current index to \"my_index\"\n",
    "chatbot2.set_current_index(\"libcst_index\")\n",
    "\n",
    "# get hints for a given question\n",
    "hints2 = chatbot2.get_index_hints(\n",
    "    \"What is are Visitors and how do they relate to code mods?\"\n",
    ")\n",
    "print(hints2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " What is are Visitors and how do they relate to code mods?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " In Python programming language, visitors are used to traverse data structures and perform operations on the nodes of the tree-like data structure. The `CSTVisitor` class provides a flexible mechanism for traversal, allowing a set of method calls on the nodes of a tree depending on the type of the node being visited (`visit` and `leave` methods). Visitors can be used in code mods to change the structure of the code, by visiting the nodes in the Abstract Syntax Tree (AST) and introducing modifications as needed. For example, one can use a visitor to find all the function calls in code and replace them with a different function call by manipulating the underlying AST."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Python programming language, visitors are used to traverse data structures and perform operations on the nodes of the tree-like data structure. The `CSTVisitor` class provides a flexible mechanism for traversal, allowing a set of method calls on the nodes of a tree depending on the type of the node being visited (`visit` and `leave` methods). Visitors can be used in code mods to change the structure of the code, by visiting the nodes in the Abstract Syntax Tree (AST) and introducing modifications as needed. For example, one can use a visitor to find all the function calls in code and replace them with a different function call by manipulating the underlying AST.\n"
     ]
    }
   ],
   "source": [
    "# ask a question\n",
    "response2 = chatbot2.reply(\"What is are Visitors and how do they relate to code mods?\")\n",
    "print(response2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new index\n"
     ]
    }
   ],
   "source": [
    "from babydragon.memory.threads.base_thread import BaseThread\n",
    "from babydragon.memory.threads.fifo_thread import FifoThread\n",
    "from babydragon.memory.threads.vector_thread import VectorThread\n",
    "\n",
    "# Initialize the VectorThread\n",
    "vector_memory = VectorThread(name=\"vector_memory\", max_context=2048, use_mark=False)\n",
    "\n",
    "# Initialize the FifoThread with VectorThread as long-term memory thread\n",
    "fifo_memory = FifoThread(\n",
    "    name=\"fifo_memory\", max_memory=10, longterm_thread=vector_memory, redundant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the VectorThread\n",
    "query = \"What is the capital of France?\"\n",
    "results, scores, indices = vector_memory.sorted_query(query, k=5, max_tokens=4000)\n",
    "\n",
    "print(\"Query Results:\")\n",
    "for result in results:\n",
    "    print(result[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, embed, ind = pind.faiss_query(\"Fifo_Threaded_Chat\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new index\n"
     ]
    }
   ],
   "source": [
    "ic = Chat(model=\"gpt-3.5-turbo-0301\", index_dict={\"babydragon\": pind})\n",
    "vci = FifoVectorChat(\n",
    "    model=\"gpt-3.5-turbo-0301\",\n",
    "    index_dict={\"babydragon\": pind},\n",
    "    max_output_tokens=500,\n",
    "    max_index_memory=1000,\n",
    "    max_memory=2000,\n",
    "    longterm_frac=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa bot\n",
    "# ic.gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://localhost:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Running run_text =============\n",
      "Inputs: Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity\n",
      "======>Current memory:\n",
      " []\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity', \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\")]\n",
      "===============Running run_text =============\n",
      "Inputs: Can you write code for this?\n",
      "======>Current memory:\n",
      " [{'role': 'user', 'content': 'Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity'}, {'role': 'assistant', 'content': \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"}]\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Can you write code for this?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity', \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"), ('Can you write code for this?', 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?')]\n",
      "===============Running run_text =============\n",
      "Inputs: Can you summarize MemoryKernel\n",
      "======>Current memory:\n",
      " [{'role': 'user', 'content': 'Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity'}, {'role': 'assistant', 'content': \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"}, {'role': 'user', 'content': 'Can you write code for this?'}, {'role': 'assistant', 'content': 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'}]\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Can you summarize MemoryKernel"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity', \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"), ('Can you write code for this?', 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'), ('Can you summarize MemoryKernel', 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.')]\n",
      "===============Running run_text =============\n",
      "Inputs: Can you summarize group_index_by_rank?\n",
      "======>Current memory:\n",
      " [{'role': 'user', 'content': 'Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity'}, {'role': 'assistant', 'content': \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"}, {'role': 'user', 'content': 'Can you write code for this?'}, {'role': 'assistant', 'content': 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'}, {'role': 'user', 'content': 'Can you summarize MemoryKernel'}, {'role': 'assistant', 'content': 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.'}]\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Can you summarize group_index_by_rank?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " group_items_by_rank_buckets is a method in the MemoryKernel library that is used to group items based on their similarity scores. It takes in code_strings, node_embeddings, S_vectors, a string S_vectors_type, component_window_size, and a boolean use_softmax as inputs. It returns a list of tuples, where each tuple contains a list of sorted strings and a numpy array of sorted node embeddings.\n",
       "\n",
       "The method first checks if the S_vectors_type input is either \"U\" or \"Vt\". If not, it raises a ValueError. If use_softmax is True, it applies the softmax function to the S_vectors array along the axis specified by the input parameter axis. It then determines the number of \"buckets\" based on the number of rows in S_vectors divided by the component_window_size parameter. For each bucket, it selects the rows/columns corresponding to that bucket from the S_vectors array and computes the sum of the absolute values of each row or column, depending on the value of S_vectors_type. It then sorts the contributions array and returns the sorted indexes that correspond to the sorted items in the code_strings and node_embeddings inputs. Finally, it appends the sorted items to a list of tuples and returns the list."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity', \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"), ('Can you write code for this?', 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'), ('Can you summarize MemoryKernel', 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.'), ('Can you summarize group_index_by_rank?', 'group_items_by_rank_buckets is a method in the MemoryKernel library that is used to group items based on their similarity scores. It takes in code_strings, node_embeddings, S_vectors, a string S_vectors_type, component_window_size, and a boolean use_softmax as inputs. It returns a list of tuples, where each tuple contains a list of sorted strings and a numpy array of sorted node embeddings.\\n\\nThe method first checks if the S_vectors_type input is either \"U\" or \"Vt\". If not, it raises a ValueError. If use_softmax is True, it applies the softmax function to the S_vectors array along the axis specified by the input parameter axis. It then determines the number of \"buckets\" based on the number of rows in S_vectors divided by the component_window_size parameter. For each bucket, it selects the rows/columns corresponding to that bucket from the S_vectors array and computes the sum of the absolute values of each row or column, depending on the value of S_vectors_type. It then sorts the contributions array and returns the sorted indexes that correspond to the sorted items in the code_strings and node_embeddings inputs. Finally, it appends the sorted items to a list of tuples and returns the list.')]\n",
      "===============Running run_text =============\n",
      "Inputs: can you summarize memory_index.py\n",
      "======>Current memory:\n",
      " [{'role': 'user', 'content': 'Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity'}, {'role': 'assistant', 'content': \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"}, {'role': 'user', 'content': 'Can you write code for this?'}, {'role': 'assistant', 'content': 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'}, {'role': 'user', 'content': 'Can you summarize MemoryKernel'}, {'role': 'assistant', 'content': 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.'}, {'role': 'user', 'content': 'Can you summarize group_index_by_rank?'}, {'role': 'assistant', 'content': 'group_items_by_rank_buckets is a method in the MemoryKernel library that is used to group items based on their similarity scores. It takes in code_strings, node_embeddings, S_vectors, a string S_vectors_type, component_window_size, and a boolean use_softmax as inputs. It returns a list of tuples, where each tuple contains a list of sorted strings and a numpy array of sorted node embeddings.\\n\\nThe method first checks if the S_vectors_type input is either \"U\" or \"Vt\". If not, it raises a ValueError. If use_softmax is True, it applies the softmax function to the S_vectors array along the axis specified by the input parameter axis. It then determines the number of \"buckets\" based on the number of rows in S_vectors divided by the component_window_size parameter. For each bucket, it selects the rows/columns corresponding to that bucket from the S_vectors array and computes the sum of the absolute values of each row or column, depending on the value of S_vectors_type. It then sorts the contributions array and returns the sorted indexes that correspond to the sorted items in the code_strings and node_embeddings inputs. Finally, it appends the sorted items to a list of tuples and returns the list.'}]\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " can you summarize memory_index.py"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The provided code snippets do not contain a complete definition of memory_index.py. However, based on the code snippets provided, it seems that memory_index.py is a module that contains classes and functions related to memory indices for Python code. The module includes the GitMemory class, which is a subclass of MemoryIndex and is used for creating an index of code from a GitHub repository, and the PythonIndex class, which is also a subclass of MemoryIndex and is used for creating an index of Python code from a local directory. The module also includes functions like __getitem__ and __init__ for interacting with memory threads and initializing objects. Overall, memory_index.py seems to provide a comprehensive set of tools for indexing and searching Python code within a memory-based system."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity', \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"), ('Can you write code for this?', 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'), ('Can you summarize MemoryKernel', 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.'), ('Can you summarize group_index_by_rank?', 'group_items_by_rank_buckets is a method in the MemoryKernel library that is used to group items based on their similarity scores. It takes in code_strings, node_embeddings, S_vectors, a string S_vectors_type, component_window_size, and a boolean use_softmax as inputs. It returns a list of tuples, where each tuple contains a list of sorted strings and a numpy array of sorted node embeddings.\\n\\nThe method first checks if the S_vectors_type input is either \"U\" or \"Vt\". If not, it raises a ValueError. If use_softmax is True, it applies the softmax function to the S_vectors array along the axis specified by the input parameter axis. It then determines the number of \"buckets\" based on the number of rows in S_vectors divided by the component_window_size parameter. For each bucket, it selects the rows/columns corresponding to that bucket from the S_vectors array and computes the sum of the absolute values of each row or column, depending on the value of S_vectors_type. It then sorts the contributions array and returns the sorted indexes that correspond to the sorted items in the code_strings and node_embeddings inputs. Finally, it appends the sorted items to a list of tuples and returns the list.'), ('can you summarize memory_index.py', 'The provided code snippets do not contain a complete definition of memory_index.py. However, based on the code snippets provided, it seems that memory_index.py is a module that contains classes and functions related to memory indices for Python code. The module includes the GitMemory class, which is a subclass of MemoryIndex and is used for creating an index of code from a GitHub repository, and the PythonIndex class, which is also a subclass of MemoryIndex and is used for creating an index of Python code from a local directory. The module also includes functions like __getitem__ and __init__ for interacting with memory threads and initializing objects. Overall, memory_index.py seems to provide a comprehensive set of tools for indexing and searching Python code within a memory-based system.')]\n",
      "===============Running run_text =============\n",
      "Inputs: Write some code to imporve MemoryKernel\n",
      "======>Current memory:\n",
      " [{'role': 'user', 'content': 'Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity'}, {'role': 'assistant', 'content': \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"}, {'role': 'user', 'content': 'Can you write code for this?'}, {'role': 'assistant', 'content': 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'}, {'role': 'user', 'content': 'Can you summarize MemoryKernel'}, {'role': 'assistant', 'content': 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.'}, {'role': 'user', 'content': 'Can you summarize group_index_by_rank?'}, {'role': 'assistant', 'content': 'group_items_by_rank_buckets is a method in the MemoryKernel library that is used to group items based on their similarity scores. It takes in code_strings, node_embeddings, S_vectors, a string S_vectors_type, component_window_size, and a boolean use_softmax as inputs. It returns a list of tuples, where each tuple contains a list of sorted strings and a numpy array of sorted node embeddings.\\n\\nThe method first checks if the S_vectors_type input is either \"U\" or \"Vt\". If not, it raises a ValueError. If use_softmax is True, it applies the softmax function to the S_vectors array along the axis specified by the input parameter axis. It then determines the number of \"buckets\" based on the number of rows in S_vectors divided by the component_window_size parameter. For each bucket, it selects the rows/columns corresponding to that bucket from the S_vectors array and computes the sum of the absolute values of each row or column, depending on the value of S_vectors_type. It then sorts the contributions array and returns the sorted indexes that correspond to the sorted items in the code_strings and node_embeddings inputs. Finally, it appends the sorted items to a list of tuples and returns the list.'}, {'role': 'user', 'content': 'can you summarize memory_index.py'}, {'role': 'assistant', 'content': 'The provided code snippets do not contain a complete definition of memory_index.py. However, based on the code snippets provided, it seems that memory_index.py is a module that contains classes and functions related to memory indices for Python code. The module includes the GitMemory class, which is a subclass of MemoryIndex and is used for creating an index of code from a GitHub repository, and the PythonIndex class, which is also a subclass of MemoryIndex and is used for creating an index of Python code from a local directory. The module also includes functions like __getitem__ and __init__ for interacting with memory threads and initializing objects. Overall, memory_index.py seems to provide a comprehensive set of tools for indexing and searching Python code within a memory-based system.'}]\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Write some code to imporve MemoryKernel"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " Based on the information provided by the previous code snippets, there are a few ways to enhance the performance of MemoryKernel. One way would be to fine-tune its similarity search algorithm by optimizing the group_items_by_rank_buckets function. This can be achieved by experimenting with different values for the component_window_size parameter, adjusting the use_softmax parameter, and testing different similarity metrics to find the most accurate and efficient approach.\n",
       "\n",
       "Another way to improve MemoryKernel would be to optimize the creation of the code index. This could involve implementing caching mechanisms to speed up the retrieval of data, increasing the size of the training data, or using more sophisticated pre-processing techniques to extract features from the code.\n",
       "\n",
       "Finally, optimizing the memory usage of the library could also help improve its performance. This could involve using a more efficient data structure to represent the similarity scores, minimizing the size of the embeddings used to train the model, and possibly experimenting with distributed computing techniques to speed up the processing.\n",
       "\n",
       "Here is an example code snippet for fine-tuning the use of group_items_by_rank_buckets function:\n",
       "\n",
       "```\n",
       "from memory_kernel import MemoryKernel\n",
       "\n",
       "# Initialize the MemoryKernel object and configure its parameters\n",
       "mk = MemoryKernel()\n",
       "mk.component_window_size = 10\n",
       "mk.use_softmax = True\n",
       "mk.similarity_metric = \"cosine\"\n",
       "\n",
       "# Load the data and build the index\n",
       "mk.load_data(path_to_data)\n",
       "mk.build_index()\n",
       "\n",
       "# Test the search function with a query\n",
       "query = \"def calculate_fibonacci(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\"\n",
       "results = mk.search(query)\n",
       "\n",
       "print(results)\n",
       "```\n",
       "\n",
       "This code snippet initializes a MemoryKernel object, sets its component_window_size parameter to 10 and its use_softmax parameter to True. It also sets the similarity_metric parameter to \"cosine\" to use cosine similarity as the similarity metric. The code then loads the data from a file using the load_data method and builds the index using the build_index method. Finally, it executes a search query using the search method and prints the results."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity', \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"), ('Can you write code for this?', 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'), ('Can you summarize MemoryKernel', 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.'), ('Can you summarize group_index_by_rank?', 'group_items_by_rank_buckets is a method in the MemoryKernel library that is used to group items based on their similarity scores. It takes in code_strings, node_embeddings, S_vectors, a string S_vectors_type, component_window_size, and a boolean use_softmax as inputs. It returns a list of tuples, where each tuple contains a list of sorted strings and a numpy array of sorted node embeddings.\\n\\nThe method first checks if the S_vectors_type input is either \"U\" or \"Vt\". If not, it raises a ValueError. If use_softmax is True, it applies the softmax function to the S_vectors array along the axis specified by the input parameter axis. It then determines the number of \"buckets\" based on the number of rows in S_vectors divided by the component_window_size parameter. For each bucket, it selects the rows/columns corresponding to that bucket from the S_vectors array and computes the sum of the absolute values of each row or column, depending on the value of S_vectors_type. It then sorts the contributions array and returns the sorted indexes that correspond to the sorted items in the code_strings and node_embeddings inputs. Finally, it appends the sorted items to a list of tuples and returns the list.'), ('can you summarize memory_index.py', 'The provided code snippets do not contain a complete definition of memory_index.py. However, based on the code snippets provided, it seems that memory_index.py is a module that contains classes and functions related to memory indices for Python code. The module includes the GitMemory class, which is a subclass of MemoryIndex and is used for creating an index of code from a GitHub repository, and the PythonIndex class, which is also a subclass of MemoryIndex and is used for creating an index of Python code from a local directory. The module also includes functions like __getitem__ and __init__ for interacting with memory threads and initializing objects. Overall, memory_index.py seems to provide a comprehensive set of tools for indexing and searching Python code within a memory-based system.'), ('Write some code to imporve MemoryKernel', 'Based on the information provided by the previous code snippets, there are a few ways to enhance the performance of MemoryKernel. One way would be to fine-tune its similarity search algorithm by optimizing the group_items_by_rank_buckets function. This can be achieved by experimenting with different values for the component_window_size parameter, adjusting the use_softmax parameter, and testing different similarity metrics to find the most accurate and efficient approach.\\n\\nAnother way to improve MemoryKernel would be to optimize the creation of the code index. This could involve implementing caching mechanisms to speed up the retrieval of data, increasing the size of the training data, or using more sophisticated pre-processing techniques to extract features from the code.\\n\\nFinally, optimizing the memory usage of the library could also help improve its performance. This could involve using a more efficient data structure to represent the similarity scores, minimizing the size of the embeddings used to train the model, and possibly experimenting with distributed computing techniques to speed up the processing.\\n\\nHere is an example code snippet for fine-tuning the use of group_items_by_rank_buckets function:\\n\\n```\\nfrom memory_kernel import MemoryKernel\\n\\n# Initialize the MemoryKernel object and configure its parameters\\nmk = MemoryKernel()\\nmk.component_window_size = 10\\nmk.use_softmax = True\\nmk.similarity_metric = \"cosine\"\\n\\n# Load the data and build the index\\nmk.load_data(path_to_data)\\nmk.build_index()\\n\\n# Test the search function with a query\\nquery = \"def calculate_fibonacci(n):\\\\n    if n == 0:\\\\n        return 0\\\\n    elif n == 1:\\\\n        return 1\\\\n    else:\\\\n        return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\"\\nresults = mk.search(query)\\n\\nprint(results)\\n```\\n\\nThis code snippet initializes a MemoryKernel object, sets its component_window_size parameter to 10 and its use_softmax parameter to True. It also sets the similarity_metric parameter to \"cosine\" to use cosine similarity as the similarity metric. The code then loads the data from a file using the load_data method and builds the index using the build_index method. Finally, it executes a search query using the search method and prints the results.')]\n"
     ]
    }
   ],
   "source": [
    "# chatbot with memory of conversation\n",
    "vci.gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio\n",
    "\n",
    "gradio.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
