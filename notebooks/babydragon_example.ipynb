{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Callable, List, Optional\n",
    "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
    "from babydragon.memory.kernels.memory_kernel import MemoryKernel\n",
    "from babydragon.memory.indexes.python_index import PythonIndex\n",
    "from babydragon.chat.memory_chat import FifoChat\n",
    "import babydragon\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babydragon_path = os.path.dirname(os.path.abspath(babydragon.__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babyindex= PythonIndex(babydragon_path, name=\"babyd_index_parallel\", minify_code=False, load = True, max_workers=16, backup= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babyindex.faiss_query(\"open ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babykernel = MemoryKernel(babyindex, name=\"babyd_kernel_parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = FifoChat(model= \"gpt-3.5-turbo\", index_dict = {\"babyindex\":babyindex}, name=\"babyd_chatbot\", max_fifo_memory=1000, max_index_memory = 2500, max_output_tokens= 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.reply(\"What is a MemoryThread?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babydragon.tasks.llm_task import LLMReader, LLMWriter\n",
    "from babydragon.chat.chat import Chat\n",
    "\n",
    "system_prompt = \"You are a helfpul summarizer. The user will input some code from the babydragon package, you have to write a summary of what the code does for future documentation use.\\n\\n\"\n",
    "\n",
    "\n",
    "def summary_prompt(paragraph):\n",
    "    return f\"Summarize the following paragraph:\\n\\n{paragraph}\\n\\nSummary:\"\n",
    "\n",
    "\n",
    "summarizer = Chat(system_prompt=system_prompt, user_prompt=summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_index = babyindex\n",
    "path = [[x] for x in range(len(target_index.values))]\n",
    "# path = [list(range(len(target_index.values)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_task = LLMWriter(\n",
    "    index=target_index,\n",
    "    path=path,\n",
    "    chatbot=summarizer,\n",
    "    max_workers=12,\n",
    "    task_id=\"summary_babydragon\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " There is no paragraph provided to summarize. Please provide the necessary input."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " from typing import Optional\n",
       "\n",
       "import tiktoken\n",
       "import os\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "from babydragon.processors.parsers.python_parser import PythonParser\n",
       "\n",
       "\n",
       "class PythonIndex(MemoryIndex, PythonParser):\n",
       "    def __init__(\n",
       "        self,\n",
       "        directory_path: str,\n",
       "        name: str = \"python_index\",\n",
       "        save_path: Optional[str] = None,\n",
       "        load: bool = False,\n",
       "        minify_code: bool = False,\n",
       "        remove_docstrings: bool = False,\n",
       "        tokenizer: Optional[tiktoken.Encoding] = None,\n",
       "        max_workers: int = 1,\n",
       "        backup: bool = False,\n",
       "    ):\n",
       "        # # Initialize the MemoryIndex\n",
       "        # MemoryIndex.__init__(\n",
       "        #     self,\n",
       "        #     name=name,\n",
       "        #     save_path=save_path,\n",
       "        #     load=load,\n",
       "        #     tokenizer=tokenizer,\n",
       "        #     max_workers=max_workers,\n",
       "        #     backup\n",
       "        # )\n",
       "        # Initialize the PythonParser\n",
       "        PythonParser.__init__(\n",
       "            self,\n",
       "            directory_path=directory_path,\n",
       "            minify_code=minify_code,\n",
       "            remove_docstrings=remove_docstrings,\n",
       "        )\n",
       "        #check if load folder exists\n",
       "        if save_path is None:\n",
       "            save_path = \"storage\"\n",
       "        load_directory = os.path.join(save_path, name)\n",
       "        loadcheck = not load or not os.path.exists(load_directory)\n",
       "        if load and not os.path.exists(load_directory):\n",
       "            print(\"No python-index found even if load=True, indexing from scratch\")\n",
       "        if loadcheck:\n",
       "            # Extract functions and classes source code\n",
       "            function_source_codes, class_source_codes, _, _ = self.process_directory()\n",
       "            print(\n",
       "                \"Indexing {} functions and {} classes\".format(\n",
       "                    len(function_source_codes), len(class_source_codes)\n",
       "                )\n",
       "            )\n",
       "            # Concatenate function and class source code and index them\n",
       "            codes = function_source_codes + class_source_codes\n",
       "            load = False\n",
       "\n",
       "         # Initialize the MemoryIndex\n",
       "        MemoryIndex.__init__(\n",
       "            self,\n",
       "            name=name,\n",
       "            values=codes if loadcheck else None,\n",
       "            save_path=save_path,\n",
       "            load=load,\n",
       "            tokenizer=tokenizer,\n",
       "            max_workers=max_workers,\n",
       "            backup=backup,\n",
       "        )"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code is importing necessary modules from the babydragon package and defining a class called PythonIndex. This class takes in various parameters such as directory path, minimum number of workers, and backup status. It initializes a MemoryIndex and PythonParser, and processes the directory to extract function and class source codes. It then concatenates the source codes and indexes them. The indexed values are saved and loaded if specified."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 67 executed in 4.56 seconds.\n",
      "Sub-task 67 results saved in 0.00 seconds.\n",
      "Sub-task 68 executed in 0.00 seconds.\n",
      "Sub-task 68 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def create_paths(\n",
       "    self, embeddings: np.ndarray, num_clusters: int\n",
       ") -> List[List[int]]:\n",
       "    raise NotImplementedError\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function named \"create_paths\" takes in embeddings as a numpy array and a number of clusters as an integer, and is expected to return a list of lists of integers. However, the implementation of this function is not yet provided and raises a \"NotImplementedError\" when called."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 69 executed in 4.00 seconds.\n",
      "Sub-task 69 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def create_paths(\n",
       "    self, embeddings: np.ndarray, num_clusters: int\n",
       ") -> List[List[int]]:\n",
       "    clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n",
       "    cluster_assignments = clusterer.fit_predict(embeddings)\n",
       "    paths = [[] for _ in range(num_clusters)]\n",
       "    for i, cluster in enumerate(cluster_assignments):\n",
       "        paths[cluster].append(i)\n",
       "    paths = [path for path in paths if path]\n",
       "    return paths\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function called \"create_paths\" that takes an array of embeddings and a number of clusters as input and returns a list of lists containing cluster assignments. It first creates a clusterer object using the HDBSCAN algorithm and uses it to assign each embedding to a cluster. Then, it creates an empty list of paths for each cluster and appends the indices of embeddings belonging to each cluster to the corresponding path list. Finally, it removes any empty path lists and returns the remaining non-empty path lists."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 70 executed in 5.40 seconds.\n",
      "Sub-task 70 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def create_paths(\n",
       "    self, A: np.ndarray, num_clusters: int\n",
       ") -> List[List[int]]:\n",
       "    n_samples = A.shape[0]\n",
       "    n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\n",
       "    spectral_clustering = SpectralClustering(\n",
       "        n_clusters=num_clusters,\n",
       "        affinity=\"precomputed\",\n",
       "        n_neighbors=n_neighbors,\n",
       "        random_state=42,\n",
       "    )\n",
       "    cluster_assignments = spectral_clustering.fit_predict(A)\n",
       "    paths = [[] for _ in range(num_clusters)]\n",
       "    for i, cluster in enumerate(cluster_assignments):\n",
       "        paths[cluster].append(i)\n",
       "    paths = [path for path in paths if path]\n",
       "    return paths\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function that takes a numpy array A and an integer num_clusters as input and returns a list of paths. It sets the number of neighbors for SpectralClustering to the minimum of the number of samples minus one and 10. It uses SpectralClustering to assign clusters to the data points and creates a list of paths for each cluster that contains the indices of the points assigned to that cluster. The function returns a list of non-empty paths."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 71 executed in 3.20 seconds.\n",
      "Sub-task 71 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "\n",
       "def calc_shgo_mode(scores: List[float]) -> float:\n",
       "    def objective(x):\n",
       "        return -estimate_pdf(scores)(x)\n",
       "\n",
       "    bounds = [(min(scores), max(scores))]\n",
       "    result = scipy.optimize.shgo(objective, bounds)\n",
       "    return result.x\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function, `calc_shgo_mode`, that takes a list of numbers and calculates the mode using the shgo optimization algorithm from the Scipy library. The function first defines another function called `objective` that estimates the probability density function of the scores and uses it to find the mode. The mode is returned as a float value."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " from typing import List\n",
       "\n",
       "import hdbscan\n",
       "import numpy as np\n",
       "from sklearn.cluster import SpectralClustering\n",
       "\n",
       "\n",
       "class ClusterPaths:\n",
       "    def create_paths(\n",
       "        self, embeddings: np.ndarray, num_clusters: int\n",
       "    ) -> List[List[int]]:\n",
       "        raise NotImplementedError\n",
       "\n",
       "\n",
       "class HDBSCANPaths(ClusterPaths):\n",
       "    def create_paths(\n",
       "        self, embeddings: np.ndarray, num_clusters: int\n",
       "    ) -> List[List[int]]:\n",
       "        clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n",
       "        cluster_assignments = clusterer.fit_predict(embeddings)\n",
       "        paths = [[] for _ in range(num_clusters)]\n",
       "        for i, cluster in enumerate(cluster_assignments):\n",
       "            paths[cluster].append(i)\n",
       "        paths = [path for path in paths if path]\n",
       "        return paths\n",
       "\n",
       "\n",
       "class SpectralClusteringPaths(ClusterPaths):\n",
       "    def create_paths(\n",
       "        self, A: np.ndarray, num_clusters: int\n",
       "    ) -> List[List[int]]:\n",
       "        n_samples = A.shape[0]\n",
       "        n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\n",
       "        spectral_clustering = SpectralClustering(\n",
       "            n_clusters=num_clusters,\n",
       "            affinity=\"precomputed\",\n",
       "            n_neighbors=n_neighbors,\n",
       "            random_state=42,\n",
       "        )\n",
       "        cluster_assignments = spectral_clustering.fit_predict(A)\n",
       "        paths = [[] for _ in range(num_clusters)]\n",
       "        for i, cluster in enumerate(cluster_assignments):\n",
       "            paths[cluster].append(i)\n",
       "        paths = [path for path in paths if path]\n",
       "        return paths\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines two classes, HDBSCANPaths and SpectralClusteringPaths, that inherit from the abstract class ClusterPaths. Both classes have a method create_paths that takes in embeddings or an adjacency matrix, and uses either HDBSCAN or SpectralClustering algorithm to cluster them into num_clusters clusters. The method returns a list of cluster assignments where each index in the list corresponds to a cluster and contains a list of indices that belong to that cluster."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 72 executed in 3.98 seconds.\n",
      "Sub-task 72 results saved in 0.00 seconds.\n",
      "Sub-task 73 executed in 0.00 seconds.\n",
      "Sub-task 73 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def objective(x):\n",
       "    return -estimate_pdf(scores)(x)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The given code defines a function named \"objective\" that takes a variable \"x\" as input and returns the negative estimated probability density function of \"scores\" at point \"x\"."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 74 executed in 0.50 seconds.\n",
      "Sub-task 74 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...\n",
      "RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "\n",
       "def estimate_pdf(scores: List[float]) -> callable:\n",
       "    pdf = scipy.stats.gaussian_kde(scores)\n",
       "    return pdf\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function called \"estimate_pdf\" that takes in a list of float numbers called \"scores,\" uses the \"gaussian_kde\" function from the \"scipy.stats\" package to estimate the probability density function (PDF) based on the input scores, and returns the estimated PDF as a callable."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 75 executed in 4.92 seconds.\n",
      "Sub-task 75 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "\n",
       "def sort_paths_by_mode_distance(\n",
       "    paths, memory_kernel, distance_metric: str = \"cosine\"\n",
       ") -> List[List[int]]:\n",
       "    sorted_paths = []\n",
       "    for i, path in enumerate(paths):\n",
       "        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
       "        cluster_embeddings = np.array(cluster_embeddings)\n",
       "        cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
       "        if distance_metric == \"cosine\" or distance_metric == \"guassian\":\n",
       "            scores = [\n",
       "                (i, cosine(cluster_mean, emb))\n",
       "                for i, emb in zip(path, cluster_embeddings)\n",
       "            ]\n",
       "        elif distance_metric == \"euclidean\":\n",
       "            scores = [\n",
       "                (i, np.linalg.norm(cluster_mean - emb))\n",
       "                for i, emb in zip(path, cluster_embeddings)\n",
       "            ]\n",
       "        score_values = [score for _, score in scores]  # Extract score values\n",
       "        mu = calc_shgo_mode(score_values)\n",
       "        sigma = np.std(score_values)\n",
       "        if distance_metric == \"guassian\":\n",
       "            scores = [\n",
       "                (i, np.exp(-((x - mu) ** 2) / (2 * sigma**2))) for i, x in scores\n",
       "            ]\n",
       "        # Sort path by score\n",
       "        sorted_path_and_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
       "        sorted_path = [x[0] for x in sorted_path_and_scores]\n",
       "        sorted_paths.append(sorted_path)\n",
       "    return sorted_paths\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The 'sort_paths_by_mode_distance' function sorts a list of node paths based on their similarity score calculated using the mean of their embeddings and a specified distance metric. The sorted paths are returned as a list. If the distance metric is \"gaussian\", scores are normalized using a gaussian function."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 76 executed in 4.09 seconds.\n",
      "Sub-task 76 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...\n",
      "RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "\n",
       "def sort_paths_by_kernel_density(\n",
       "    paths, memory_kernel, distance_metric: str = \"cosine\"\n",
       ") -> List[List[int]]:\n",
       "    sorted_paths = []\n",
       "    for i, path in enumerate(paths):\n",
       "        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
       "        cluster_embeddings = np.array(cluster_embeddings)\n",
       "        cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
       "        if distance_metric == \"cosine\":\n",
       "            scores = [\n",
       "                (i, cosine(cluster_mean, emb))\n",
       "                for i, emb in zip(path, cluster_embeddings)\n",
       "            ]\n",
       "        elif distance_metric == \"euclidean\":\n",
       "            scores = [\n",
       "                (i, np.linalg.norm(cluster_mean - emb))\n",
       "                for i, emb in zip(path, cluster_embeddings)\n",
       "            ]\n",
       "        score_values = [score for _, score in scores]  # Extract score values\n",
       "\n",
       "        # Estimate PDF using Kernel Density Estimation\n",
       "        kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(\n",
       "            np.array(score_values).reshape(-1, 1)\n",
       "        )\n",
       "        kde_scores = [kde.score_samples([[x]])[0] for _, x in scores]\n",
       "\n",
       "        # Sort path by score\n",
       "        sorted_path_and_scores = sorted(\n",
       "            zip(path, kde_scores), key=lambda x: x[1], reverse=True\n",
       "        )\n",
       "        sorted_path = [x[0] for x in sorted_path_and_scores]\n",
       "        sorted_paths.append(sorted_path)\n",
       "    return sorted_paths\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function `sort_paths_by_kernel_density` takes in a list of paths, a memory kernel, and a distance metric, and sorts the paths based on the kernel density estimated using the given distance metric. The function calculates the mean of the node embeddings in each path, computes the distance of each node's embedding from the path mean using the given distance metric, estimates the kernel density of the distances using Gaussian Kernel Density Estimation, and sorts the paths based on the density scores in descending order. The sorted paths are returned as a list of lists."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 77 executed in 5.26 seconds.\n",
      "Sub-task 77 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " from typing import List\n",
       "\n",
       "import numpy as np\n",
       "import scipy\n",
       "from scipy.spatial.distance import cosine\n",
       "from sklearn.neighbors import KernelDensity\n",
       "\n",
       "\n",
       "def calc_shgo_mode(scores: List[float]) -> float:\n",
       "    def objective(x):\n",
       "        return -estimate_pdf(scores)(x)\n",
       "\n",
       "    bounds = [(min(scores), max(scores))]\n",
       "    result = scipy.optimize.shgo(objective, bounds)\n",
       "    return result.x\n",
       "\n",
       "\n",
       "def estimate_pdf(scores: List[float]) -> callable:\n",
       "    pdf = scipy.stats.gaussian_kde(scores)\n",
       "    return pdf\n",
       "\n",
       "\n",
       "def sort_paths_by_mode_distance(\n",
       "    paths, memory_kernel, distance_metric: str = \"cosine\"\n",
       ") -> List[List[int]]:\n",
       "    sorted_paths = []\n",
       "    for i, path in enumerate(paths):\n",
       "        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
       "        cluster_embeddings = np.array(cluster_embeddings)\n",
       "        cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
       "        if distance_metric == \"cosine\" or distance_metric == \"guassian\":\n",
       "            scores = [\n",
       "                (i, cosine(cluster_mean, emb))\n",
       "                for i, emb in zip(path, cluster_embeddings)\n",
       "            ]\n",
       "        elif distance_metric == \"euclidean\":\n",
       "            scores = [\n",
       "                (i, np.linalg.norm(cluster_mean - emb))\n",
       "                for i, emb in zip(path, cluster_embeddings)\n",
       "            ]\n",
       "        score_values = [score for _, score in scores]  # Extract score values\n",
       "        mu = calc_shgo_mode(score_values)\n",
       "        sigma = np.std(score_values)\n",
       "        if distance_metric == \"guassian\":\n",
       "            scores = [\n",
       "                (i, np.exp(-((x - mu) ** 2) / (2 * sigma**2))) for i, x in scores\n",
       "            ]\n",
       "        # Sort path by score\n",
       "        sorted_path_and_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
       "        sorted_path = [x[0] for x in sorted_path_and_scores]\n",
       "        sorted_paths.append(sorted_path)\n",
       "    return sorted_paths\n",
       "\n",
       "\n",
       "def sort_paths_by_kernel_density(\n",
       "    paths, memory_kernel, distance_metric: str = \"cosine\"\n",
       ") -> List[List[int]]:\n",
       "    sorted_paths = []\n",
       "    for i, path in enumerate(paths):\n",
       "        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
       "        cluster_embeddings = np.array(cluster_embeddings)\n",
       "        cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
       "        if distance_metric == \"cosine\":\n",
       "            scores = [\n",
       "                (i, cosine(cluster_mean, emb))\n",
       "                for i, emb in zip(path, cluster_embeddings)\n",
       "            ]\n",
       "        elif distance_metric == \"euclidean\":\n",
       "            scores = [\n",
       "                (i, np.linalg.norm(cluster_mean - emb))\n",
       "                for i, emb in zip(path, cluster_embeddings)\n",
       "            ]\n",
       "        score_values = [score for _, score in scores]  # Extract score values\n",
       "\n",
       "        # Estimate PDF using Kernel Density Estimation\n",
       "        kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(\n",
       "            np.array(score_values).reshape(-1, 1)\n",
       "        )\n",
       "        kde_scores = [kde.score_samples([[x]])[0] for _, x in scores]\n",
       "\n",
       "        # Sort path by score\n",
       "        sorted_path_and_scores = sorted(\n",
       "            zip(path, kde_scores), key=lambda x: x[1], reverse=True\n",
       "        )\n",
       "        sorted_path = [x[0] for x in sorted_path_and_scores]\n",
       "        sorted_paths.append(sorted_path)\n",
       "    return sorted_paths\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code includes functions for calculating the mode of a distribution of scores, estimating a probability density function using Gaussian KDE, and sorting paths based on the mode distance or kernel density of the path scores. The functions take in a list of scores and a distance metric, and make use of several Python packages including numpy, scipy, and scikit-learn. The sorted paths are returned as a list of lists of integers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 78 executed in 2.11 seconds.\n",
      "Sub-task 78 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def __init__(\n",
       "    self,\n",
       "    mem_index: MemoryIndex,\n",
       "    name: str = \"memory_kernel\",\n",
       "    k: int = 2,\n",
       "    save_path: str = None,\n",
       "):\n",
       "    \"\"\"\n",
       "        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\n",
       "\n",
       "        Args:\n",
       "            mem_index (MemoryIndex): A MemoryIndex instance.\n",
       "            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\n",
       "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
       "            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\n",
       "        \"\"\"\n",
       "    super().__init__(\n",
       "        index=mem_index.index,\n",
       "        values=mem_index.values,\n",
       "        embeddings=mem_index.embeddings,\n",
       "        name=name,\n",
       "        save_path=save_path,\n",
       "    )\n",
       "    self.k = k\n",
       "    if len(self.values) > 0: \n",
       "        self.create_k_hop_index(k=k)\n",
       "    else:\n",
       "        raise ValueError(\"The input MemoryIndex is empty. Please check the input MemoryIndex.\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code initializes the MemoryKernel class with a MemoryIndex instance, a name, k value, and save path. The k value determines the number of hops for message passing. If the values are not empty, it creates a k-hop index, otherwise, it raises a value error."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 79 executed in 0.37 seconds.\n",
      "Sub-task 79 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def cos_sim(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
       "    \"\"\"\n",
       "        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
       "        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
       "        \"\"\"\n",
       "    if not isinstance(a, np.ndarray):\n",
       "        a = np.array(a)\n",
       "\n",
       "    if not isinstance(b, np.ndarray):\n",
       "        b = np.array(b)\n",
       "\n",
       "    if len(a.shape) == 1:\n",
       "        a = a[np.newaxis, :]\n",
       "\n",
       "    if len(b.shape) == 1:\n",
       "        b = b[np.newaxis, :]\n",
       "\n",
       "    a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\n",
       "    b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\n",
       "    return np.dot(a_norm, b_norm.T)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n",
      "RateLimiter: Waiting for 3.00 seconds before next call.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The `cos_sim` function takes in two numpy arrays and computes the cosine similarity between all pairs of vectors in the arrays. It returns a matrix with cosine similarity values for each pair of vectors. The function also handles cases where the input arrays are not numpy arrays or are one-dimensional arrays."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 80 executed in 4.39 seconds.\n",
      "Sub-task 80 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def compute_kernel(\n",
       "    self,\n",
       "    embedding_set: np.ndarray,\n",
       "    threshold: float = 0.65,\n",
       "    use_softmax: bool = False,\n",
       ") -> np.ndarray:\n",
       "\n",
       "    \"\"\"\n",
       "        Compute the adjacency matrix of the graph.\n",
       "\n",
       "        Parameters:\n",
       "        embedding_set (numpy array): The embedding matrix of the nodes.\n",
       "        threshold (float): The threshold for the adjacency matrix.\n",
       "        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\n",
       "        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\n",
       "\n",
       "        Returns:\n",
       "        adj_matrix (numpy array): The adjacency matrix of the graph.\n",
       "        \"\"\"\n",
       "\n",
       "    A = self.cos_sim(embedding_set, embedding_set)\n",
       "    if use_softmax:\n",
       "        # softmax\n",
       "        A = np.exp(A)\n",
       "        A = A / np.sum(A, axis=1)[:, np.newaxis]\n",
       "    adj_matrix = np.zeros_like(A)\n",
       "    adj_matrix[A > threshold] = 1\n",
       "    adj_matrix[A <= threshold] = 0\n",
       "    adj_matrix = adj_matrix.astype(np.float32)\n",
       "    return adj_matrix\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The compute_kernel function in the babydragon package computes the adjacency matrix of a graph using the cosine similarity of the embedding matrix of its nodes. The function takes in parameters such as a threshold value, the option to use softmax when computing the adjacency matrix, and whether to use batch processing for computing the cosine similarity. The output of the function is the adjacency matrix as a numpy array."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 81 executed in 3.22 seconds.\n",
      "Sub-task 81 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def gen_gse_embeddings(\n",
       "    self, A: np.ndarray, embeddings: np.ndarray, m: int = 7\n",
       ") -> np.ndarray:\n",
       "    \"\"\"\n",
       "        Generate Graph Sylvester Embeddings.\n",
       "\n",
       "        Args:\n",
       "            A (np.ndarray): The adjacency matrix of the graph.\n",
       "            embeddings (np.ndarray): The original node embeddings.\n",
       "            m (int, optional): The number of spectral scales. Defaults to 7.\n",
       "\n",
       "        Returns:\n",
       "            np.ndarray: The generated Graph Sylvester Embeddings.\n",
       "        \"\"\"\n",
       "    V = list(range(len(embeddings)))\n",
       "    W = A\n",
       "\n",
       "    G = (V, W)\n",
       "    ts = np.linspace(0, 1, m)  # equally spaced scales\n",
       "\n",
       "    gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\n",
       "    return gse_embeddings\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code generates Graph Sylvester Embeddings from the adjacency matrix and original node embeddings of a graph. The number of spectral scales can be specified and the function returns the generated embeddings."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def k_hop_message_passing(\n",
       "    self, A: np.ndarray, node_features: np.ndarray, k: int\n",
       ") -> Tuple[np.ndarray, np.ndarray]:\n",
       "    \"\"\"\n",
       "        Compute the k-hop adjacency matrix and aggregated features using message passing.\n",
       "\n",
       "        Parameters:\n",
       "        A (numpy array): The adjacency matrix of the graph.\n",
       "        node_features (numpy array): The feature matrix of the nodes.\n",
       "        k (int): The number of hops for message passing.\n",
       "\n",
       "        Returns:\n",
       "        A_k (numpy array): The k-hop adjacency matrix.\n",
       "        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n",
       "        \"\"\"\n",
       "\n",
       "    print(\"Compute the k-hop adjacency matrix\")\n",
       "    A_k = np.linalg.matrix_power(A, k)\n",
       "\n",
       "    print(\"Aggregate the messages from the k-hop neighborhood:\")\n",
       "    agg_features = node_features.copy()\n",
       "\n",
       "    for i in tqdm(range(k)):\n",
       "        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n",
       "\n",
       "    return A_k, agg_features\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The `k_hop_message_passing` function computes the k-hop adjacency matrix and aggregated features using message passing, where k represents the number of hops. It takes in the adjacency matrix A, node feature matrix node_features and k as inputs. It returns the k-hop adjacency matrix A and the aggregated feature matrix for each node in the k-hop neighborhood. The function involves matrix computations to calculate A_k and aggregating messages from the k-hop neighborhood to compute agg_features."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 82 executed in 5.82 seconds.\n",
      "Sub-task 82 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def graph_sylvester_embedding(self, G: Tuple, m: int, ts: np.ndarray) -> np.ndarray:\n",
       "    \"\"\"\n",
       "        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\n",
       "\n",
       "        Args:\n",
       "            G (Tuple): A tuple containing the graph's vertices (V) and weights (W).\n",
       "            m (int): The number of singular values to consider.\n",
       "            ts (np.ndarray): The spectral scales.\n",
       "\n",
       "        Returns:\n",
       "            np.ndarray: The node_embeddings matrix.\n",
       "        \"\"\"\n",
       "    V, W = G\n",
       "    n = len(V)\n",
       "    D_BE = np.diag(W.sum(axis=1))\n",
       "    L_BE = np.identity(n) - np.dot(\n",
       "        np.diag(1 / np.sqrt(D_BE.diagonal())),\n",
       "        np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n",
       "    )\n",
       "\n",
       "    A = W\n",
       "    B = L_BE\n",
       "    C = np.identity(n)\n",
       "    X = solve_sylvester(A, B, C)\n",
       "\n",
       "    U, S, _ = svd(X, full_matrices=False)\n",
       "    U_m = U[:, :m]\n",
       "    S_m = S[:m]\n",
       "\n",
       "    node_embeddings = np.zeros((n, m))\n",
       "\n",
       "    for i in range(n):\n",
       "        for s in range(m):\n",
       "            # Spectral kernel descriptor\n",
       "            node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n",
       "\n",
       "    return node_embeddings\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The `graph_sylvester_embedding` function in the BabyDragon package calculates the spectral kernel descriptor or Spectral Graph Wavelet descriptor for a given graph. It takes in a tuple containing the graph's vertices and weights, the number of singular values to consider, and the spectral scales. It returns the node embeddings matrix for the graph. The function first computes the Laplacian matrix for the graph, then calculates the singular value decomposition of a Sylvester equation to obtain the node embeddings matrix. Finally, it computes the spectral kernel descriptor using the calculated singular values and spectral scales."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 83 executed in 2.83 seconds.\n",
      "Sub-task 83 results saved in 0.00 seconds.\n",
      "Sub-task 84 executed in 0.00 seconds.\n",
      "Sub-task 84 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...\n",
      "RateLimiter: Waiting for 3.00 seconds before next call.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def create_k_hop_index(self, k: int = 2):\n",
       "    \"\"\"\n",
       "        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\n",
       "        aggregated features, and updating the memory index.\n",
       "\n",
       "        Args:\n",
       "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
       "        \"\"\"\n",
       "    self.k = k\n",
       "    print(\"Computing the adjacency matrix\")\n",
       "    print(\"Embeddings shape: \", self.embeddings.shape)\n",
       "    self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n",
       "    print(\"Computing the k-hop adjacency matrix and aggregated features\")\n",
       "    self.A_k, self.node_embeddings = self.k_hop_message_passing(\n",
       "        self.A, self.embeddings, k\n",
       "    )\n",
       "    print(\"Updating the memory index\")\n",
       "    self.k_hop_index = MemoryIndex(name=self.name)\n",
       "    self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a method that creates a k-hop index. It does this by computing the adjacency matrix, k-hop adjacency matrix, aggregated features, and updating the memory index. The number of hops for message passing can be set with an optional argument. The method prints out updates on its progress as it computes the various matrices and updates the index."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 85 executed in 2.49 seconds.\n",
      "Sub-task 85 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "@classmethod\n",
       "def from_task_results(cls, task_memory_index):\n",
       "    new_memory_kernel = cls(mem_index=task_memory_index)\n",
       "\n",
       "    # Create a new index for the new MemoryKernel\n",
       "    new_memory_kernel.create_k_hop_index()\n",
       "\n",
       "    return new_memory_kernel\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a class method that creates a new MemoryKernel object, initializes it with a task memory index, creates a new k-hop index for the object, and then returns the new object."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 86 executed in 0.92 seconds.\n",
      "Sub-task 86 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def __init__(\n",
       "    self,\n",
       "    memory_kernel_dict: Dict[str, MemoryKernel],\n",
       "    name: str = \"memory_kernel_group\",\n",
       "):\n",
       "    \"\"\"\n",
       "        Initialize the MultiKernel with a dictionary of MemoryKernel instances.\n",
       "\n",
       "        Args:\n",
       "            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\n",
       "            name (str, optional): The name of the MultiKernel. Defaults to \"memory_kernel_group\".\n",
       "        \"\"\"\n",
       "    self.memory_kernel_dict = memory_kernel_dict\n",
       "    self.path_group = {}\n",
       "    self.name = name\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a class with an initialization function that takes in a dictionary of MemoryKernel instances and a name parameter. It sets the dictionary, creates an empty dictionary called 'path_group', and assigns the name value if provided."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " from typing import Tuple\n",
       "\n",
       "import numpy as np\n",
       "from numpy.linalg import svd\n",
       "from scipy.linalg import solve_sylvester\n",
       "from tqdm import tqdm\n",
       "\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "\n",
       "\n",
       "class MemoryKernel(MemoryIndex):\n",
       "    def __init__(\n",
       "        self,\n",
       "        mem_index: MemoryIndex,\n",
       "        name: str = \"memory_kernel\",\n",
       "        k: int = 2,\n",
       "        save_path: str = None,\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\n",
       "\n",
       "        Args:\n",
       "            mem_index (MemoryIndex): A MemoryIndex instance.\n",
       "            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\n",
       "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
       "            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\n",
       "        \"\"\"\n",
       "        super().__init__(\n",
       "            index=mem_index.index,\n",
       "            values=mem_index.values,\n",
       "            embeddings=mem_index.embeddings,\n",
       "            name=name,\n",
       "            save_path=save_path,\n",
       "        )\n",
       "        self.k = k\n",
       "        if len(self.values) > 0: \n",
       "            self.create_k_hop_index(k=k)\n",
       "        else:\n",
       "            raise ValueError(\"The input MemoryIndex is empty. Please check the input MemoryIndex.\")\n",
       "\n",
       "    def cos_sim(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
       "        \"\"\"\n",
       "        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
       "        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
       "        \"\"\"\n",
       "        if not isinstance(a, np.ndarray):\n",
       "            a = np.array(a)\n",
       "\n",
       "        if not isinstance(b, np.ndarray):\n",
       "            b = np.array(b)\n",
       "\n",
       "        if len(a.shape) == 1:\n",
       "            a = a[np.newaxis, :]\n",
       "\n",
       "        if len(b.shape) == 1:\n",
       "            b = b[np.newaxis, :]\n",
       "\n",
       "        a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\n",
       "        b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\n",
       "        return np.dot(a_norm, b_norm.T)\n",
       "\n",
       "    def compute_kernel(\n",
       "        self,\n",
       "        embedding_set: np.ndarray,\n",
       "        threshold: float = 0.65,\n",
       "        use_softmax: bool = False,\n",
       "    ) -> np.ndarray:\n",
       "\n",
       "        \"\"\"\n",
       "        Compute the adjacency matrix of the graph.\n",
       "\n",
       "        Parameters:\n",
       "        embedding_set (numpy array): The embedding matrix of the nodes.\n",
       "        threshold (float): The threshold for the adjacency matrix.\n",
       "        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\n",
       "        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\n",
       "\n",
       "        Returns:\n",
       "        adj_matrix (numpy array): The adjacency matrix of the graph.\n",
       "        \"\"\"\n",
       "\n",
       "        A = self.cos_sim(embedding_set, embedding_set)\n",
       "        if use_softmax:\n",
       "            # softmax\n",
       "            A = np.exp(A)\n",
       "            A = A / np.sum(A, axis=1)[:, np.newaxis]\n",
       "        adj_matrix = np.zeros_like(A)\n",
       "        adj_matrix[A > threshold] = 1\n",
       "        adj_matrix[A <= threshold] = 0\n",
       "        adj_matrix = adj_matrix.astype(np.float32)\n",
       "        return adj_matrix\n",
       "\n",
       "    def k_hop_message_passing(\n",
       "        self, A: np.ndarray, node_features: np.ndarray, k: int\n",
       "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
       "        \"\"\"\n",
       "        Compute the k-hop adjacency matrix and aggregated features using message passing.\n",
       "\n",
       "        Parameters:\n",
       "        A (numpy array): The adjacency matrix of the graph.\n",
       "        node_features (numpy array): The feature matrix of the nodes.\n",
       "        k (int): The number of hops for message passing.\n",
       "\n",
       "        Returns:\n",
       "        A_k (numpy array): The k-hop adjacency matrix.\n",
       "        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n",
       "        \"\"\"\n",
       "\n",
       "        print(\"Compute the k-hop adjacency matrix\")\n",
       "        A_k = np.linalg.matrix_power(A, k)\n",
       "\n",
       "        print(\"Aggregate the messages from the k-hop neighborhood:\")\n",
       "        agg_features = node_features.copy()\n",
       "\n",
       "        for i in tqdm(range(k)):\n",
       "            agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n",
       "\n",
       "        return A_k, agg_features\n",
       "\n",
       "    def graph_sylvester_embedding(self, G: Tuple, m: int, ts: np.ndarray) -> np.ndarray:\n",
       "        \"\"\"\n",
       "        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\n",
       "\n",
       "        Args:\n",
       "            G (Tuple): A tuple containing the graph's vertices (V) and weights (W).\n",
       "            m (int): The number of singular values to consider.\n",
       "            ts (np.ndarray): The spectral scales.\n",
       "\n",
       "        Returns:\n",
       "            np.ndarray: The node_embeddings matrix.\n",
       "        \"\"\"\n",
       "        V, W = G\n",
       "        n = len(V)\n",
       "        D_BE = np.diag(W.sum(axis=1))\n",
       "        L_BE = np.identity(n) - np.dot(\n",
       "            np.diag(1 / np.sqrt(D_BE.diagonal())),\n",
       "            np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n",
       "        )\n",
       "\n",
       "        A = W\n",
       "        B = L_BE\n",
       "        C = np.identity(n)\n",
       "        X = solve_sylvester(A, B, C)\n",
       "\n",
       "        U, S, _ = svd(X, full_matrices=False)\n",
       "        U_m = U[:, :m]\n",
       "        S_m = S[:m]\n",
       "\n",
       "        node_embeddings = np.zeros((n, m))\n",
       "\n",
       "        for i in range(n):\n",
       "            for s in range(m):\n",
       "                # Spectral kernel descriptor\n",
       "                node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n",
       "\n",
       "        return node_embeddings\n",
       "\n",
       "    def gen_gse_embeddings(\n",
       "        self, A: np.ndarray, embeddings: np.ndarray, m: int = 7\n",
       "    ) -> np.ndarray:\n",
       "        \"\"\"\n",
       "        Generate Graph Sylvester Embeddings.\n",
       "\n",
       "        Args:\n",
       "            A (np.ndarray): The adjacency matrix of the graph.\n",
       "            embeddings (np.ndarray): The original node embeddings.\n",
       "            m (int, optional): The number of spectral scales. Defaults to 7.\n",
       "\n",
       "        Returns:\n",
       "            np.ndarray: The generated Graph Sylvester Embeddings.\n",
       "        \"\"\"\n",
       "        V = list(range(len(embeddings)))\n",
       "        W = A\n",
       "\n",
       "        G = (V, W)\n",
       "        ts = np.linspace(0, 1, m)  # equally spaced scales\n",
       "\n",
       "        gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\n",
       "        return gse_embeddings\n",
       "\n",
       "    def create_k_hop_index(self, k: int = 2):\n",
       "        \"\"\"\n",
       "        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\n",
       "        aggregated features, and updating the memory index.\n",
       "\n",
       "        Args:\n",
       "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
       "        \"\"\"\n",
       "        self.k = k\n",
       "        print(\"Computing the adjacency matrix\")\n",
       "        print(\"Embeddings shape: \", self.embeddings.shape)\n",
       "        self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n",
       "        print(\"Computing the k-hop adjacency matrix and aggregated features\")\n",
       "        self.A_k, self.node_embeddings = self.k_hop_message_passing(\n",
       "            self.A, self.embeddings, k\n",
       "        )\n",
       "        print(\"Updating the memory index\")\n",
       "        self.k_hop_index = MemoryIndex(name=self.name)\n",
       "        self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\n",
       "\n",
       "    @classmethod\n",
       "    def from_task_results(cls, task_memory_index):\n",
       "        new_memory_kernel = cls(mem_index=task_memory_index)\n",
       "\n",
       "        # Create a new index for the new MemoryKernel\n",
       "        new_memory_kernel.create_k_hop_index()\n",
       "\n",
       "        return new_memory_kernel\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a class called MemoryKernel that inherits from the MemoryIndex class. The MemoryKernel class computes adjacency matrices, k-hop adjacency matrices, and aggregated features. It also generates Graph Sylvester Embeddings and computes the Spectral Graph Wavelet descriptors for a given graph. The class allows the creation of a k-hop index, which updates the memory index. Additionally, it initializes the MemoryKernel class with a MemoryIndex instance, a name, a k value, and a save path."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 87 executed in 7.84 seconds.\n",
      "Sub-task 87 results saved in 0.00 seconds.\n",
      "Sub-task 88 executed in 0.00 seconds.\n",
      "Sub-task 88 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def __init__(\n",
       "    self,\n",
       "    memory_kernel_dict: Dict[str, MemoryKernel],\n",
       "    name: str = \"memory_kernel_group\",\n",
       "):\n",
       "    super().__init__(memory_kernel_dict, name)\n",
       "    self.cluster_paths = HDBSCANPaths()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines an initialization function with arguments 'memory_kernel_dict' as a dictionary and 'name' as a string with default value 'memory_kernel_group'. It calls the parent class initialization function with the aforementioned arguments and assigns an instance variable 'cluster_paths' as an object of the HDBSCANPaths class."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 89 executed in 2.99 seconds.\n",
      "Sub-task 89 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def __init__(\n",
       "    self,\n",
       "    memory_kernel_dict: Dict[str, MemoryKernel],\n",
       "    name: str = \"memory_kernel_group\",\n",
       "):\n",
       "    super().__init__(memory_kernel_dict, name)\n",
       "    self.cluster_paths = SpectralClusteringPaths()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This is a constructor method for a class that takes a dictionary of memory kernels as input and assigns a name to it. It inherits from another class and instantiates a SpectralClusteringPaths object."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def generate_path_groups(self, num_clusters: int = None) -> None:\n",
       "    path_group = {}\n",
       "    for k, v in self.memory_kernel_dict.items():\n",
       "        embeddings = v.node_embeddings\n",
       "        if num_clusters is None:\n",
       "            num_clusters = int(np.sqrt(len(embeddings)))\n",
       "        paths = self.cluster_paths.create_paths(embeddings, num_clusters)\n",
       "        path_group[k] = paths\n",
       "    self.path_group = path_group\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code generates path groups by clustering node embeddings using a memory kernel dictionary. It creates a dictionary of paths for each cluster, and assigns it to a variable called \"path_group\". The number of clusters can either be specified by the user or automatically determined based on the length of the embeddings. Finally, the path group dictionary is assigned to the class attribute \"path_group\"."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 90 executed in 5.64 seconds.\n",
      "Sub-task 90 results saved in 0.00 seconds.\n",
      "Sub-task 91 executed in 0.00 seconds.\n",
      "Sub-task 91 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def generate_path_groups(self, num_clusters: int = None) -> None:\n",
       "    path_group = {}\n",
       "    for k, v in self.memory_kernel_dict.items():\n",
       "        A_k = v.A_k\n",
       "        if num_clusters is None:\n",
       "            num_clusters = int(np.sqrt(len(A_k)))\n",
       "        paths = self.cluster_paths.create_paths(A_k, num_clusters)\n",
       "        path_group[k] = paths\n",
       "    self.path_group = path_group\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code is a method that generates path clusters based on the memory kernel dictionary elements, which contain matrices A_k. It creates path groups based on the number of clusters specified, or by default, based on the square root of the length of each A_k matrix. The method returns no output, but updates the path_group attribute of the class instance it is called upon."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 92 executed in 4.37 seconds.\n",
      "Sub-task 92 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " from typing import Dict\n",
       "\n",
       "import numpy as np\n",
       "\n",
       "from babydragon.chat.chat import Chat\n",
       "from babydragon.memory.kernels.kernel_clustering import (\n",
       "    HDBSCANPaths, SpectralClusteringPaths)\n",
       "from babydragon.memory.kernels.memory_kernel import MemoryKernel\n",
       "from babydragon.tasks.llm_task import LLMWriter\n",
       "\n",
       "\n",
       "class MultiKernel(MemoryKernel):\n",
       "    def __init__(\n",
       "        self,\n",
       "        memory_kernel_dict: Dict[str, MemoryKernel],\n",
       "        name: str = \"memory_kernel_group\",\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Initialize the MultiKernel with a dictionary of MemoryKernel instances.\n",
       "\n",
       "        Args:\n",
       "            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\n",
       "            name (str, optional): The name of the MultiKernel. Defaults to \"memory_kernel_group\".\n",
       "        \"\"\"\n",
       "        self.memory_kernel_dict = memory_kernel_dict\n",
       "        self.path_group = {}\n",
       "        self.name = name\n",
       "\n",
       "\n",
       "class HDBSCANMultiKernel(MultiKernel):\n",
       "    def __init__(\n",
       "        self,\n",
       "        memory_kernel_dict: Dict[str, MemoryKernel],\n",
       "        name: str = \"memory_kernel_group\",\n",
       "    ):\n",
       "        super().__init__(memory_kernel_dict, name)\n",
       "        self.cluster_paths = HDBSCANPaths()\n",
       "\n",
       "    def generate_path_groups(self, num_clusters: int = None) -> None:\n",
       "        path_group = {}\n",
       "        for k, v in self.memory_kernel_dict.items():\n",
       "            embeddings = v.node_embeddings\n",
       "            if num_clusters is None:\n",
       "                num_clusters = int(np.sqrt(len(embeddings)))\n",
       "            paths = self.cluster_paths.create_paths(embeddings, num_clusters)\n",
       "            path_group[k] = paths\n",
       "        self.path_group = path_group\n",
       "\n",
       "\n",
       "class SpectralClusteringMultiKernel(MultiKernel):\n",
       "    def __init__(\n",
       "        self,\n",
       "        memory_kernel_dict: Dict[str, MemoryKernel],\n",
       "        name: str = \"memory_kernel_group\",\n",
       "    ):\n",
       "        super().__init__(memory_kernel_dict, name)\n",
       "        self.cluster_paths = SpectralClusteringPaths()\n",
       "\n",
       "    def generate_path_groups(self, num_clusters: int = None) -> None:\n",
       "        path_group = {}\n",
       "        for k, v in self.memory_kernel_dict.items():\n",
       "            A_k = v.A_k\n",
       "            if num_clusters is None:\n",
       "                num_clusters = int(np.sqrt(len(A_k)))\n",
       "            paths = self.cluster_paths.create_paths(A_k, num_clusters)\n",
       "            path_group[k] = paths\n",
       "        self.path_group = path_group\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code is a part of the babydragon package and contains classes for multi-kernel memory computation. The classes include HDBSCANMultiKernel and SpectralClusteringMultiKernel. The MultiKernel class initializes a memory kernel with a dictionary of MemoryKernel instances, whereas the other two classes extend the MultiKernel class and define their own path groups. The generate_path_groups method generates the path groups for each memory kernel in the dictionary and stores them in self.path_group."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 93 executed in 4.43 seconds.\n",
      "Sub-task 93 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def __init__(self, memory_kernel_group: MultiKernel):\n",
       "    self.memory_kernel_group = memory_kernel_group\n",
       "    self.memory_kernel_dict = memory_kernel_group.memory_kernel_dict\n",
       "    self.memory_kernel_group.generate_path_groups()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code initializes an object with a memory kernel group parameter of type MultiKernel. It then sets the object's memory kernel group and memory kernel dictionary attributes to the ones passed through the parameter. Finally, it calls the generate_path_groups() method of the memory kernel group object."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 94 executed in 0.28 seconds.\n",
      "Sub-task 94 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def plot_embeddings_with_path(self, embeddings, title, paths):\n",
       "    tsne = TSNE(n_components=2, random_state=42)\n",
       "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
       "\n",
       "    plt.figure(figsize=(10, 8))\n",
       "    colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n",
       "    for i, path in enumerate(paths):\n",
       "        path_embeddings = reduced_embeddings[path]\n",
       "        plt.scatter(\n",
       "            path_embeddings[:, 0],\n",
       "            path_embeddings[:, 1],\n",
       "            color=colors[i],\n",
       "            label=f\"Cluster {i}\",\n",
       "        )\n",
       "        for j in range(len(path) - 1):\n",
       "            plt.plot(\n",
       "                [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n",
       "                [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n",
       "                color=colors[i],\n",
       "            )\n",
       "    plt.title(title)\n",
       "    plt.legend()\n",
       "    plt.show()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code is a method that takes in embeddings, a title, and a list of paths. It applies the t-SNE dimensionality reduction technique to the embeddings and plots the resulting reduced embeddings in a scatter plot. Each path is plotted in a different color with the adjacent points connected by a line. The method returns a plot with a legend and the specified title."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 95 executed in 4.85 seconds.\n",
      "Sub-task 95 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def visualize_paths(self):\n",
       "    #loop through memory kernels and print path_group\n",
       "    for key, kernel in self.memory_kernel_dict.items():\n",
       "        print(f\"Kernel: {key}\")\n",
       "        paths = self.memory_kernel_group.path_group[key]\n",
       "        print(f\"Path Group: {paths}\")\n",
       "        node_embeddings = kernel.node_embeddings\n",
       "        self.plot_embeddings_with_path(\n",
       "            node_embeddings, f\"Node Embeddings for {key}\", paths\n",
       "        )\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code visualizes paths by looping through memory kernels and printing the path group and kernel for each item in the memory kernel dictionary. It also retrieves node embeddings for the kernel and plots them with the corresponding paths."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 96 executed in 0.15 seconds.\n",
      "Sub-task 96 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def plot_singular_values(self):\n",
       "    #loop through memory kernels and print path_group\n",
       "    for key, kernel in self.memory_kernel_dict.items():\n",
       "        print(f\"Kernel: {key}\")\n",
       "        A_k = kernel.A_k\n",
       "        U, S, V = np.linalg.svd(A_k)\n",
       "        plt.plot(S)\n",
       "        plt.show()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function \"plot_singular_values\" is defined to plot the singular values of a matrix A_k obtained by computing the singular value decomposition of the memory kernel's A_k matrix using the NumPy linalg svd method. The code loops through all the memory kernels in a dictionary, prints the name of the kernel, and then plots the singular values of the obtained matrix A_k using the matplotlib plot function. The resulting plot is then displayed using the show function."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 97 executed in 7.50 seconds.\n",
      "Sub-task 97 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " from matplotlib import pyplot as plt\n",
       "import matplotlib.cm as cm\n",
       "import numpy as np\n",
       "from babydragon.memory.kernels.multi_kernel import MultiKernel\n",
       "from sklearn.manifold import TSNE\n",
       "\n",
       "\n",
       "class MultiKernelVisualization:\n",
       "    def __init__(self, memory_kernel_group: MultiKernel):\n",
       "        self.memory_kernel_group = memory_kernel_group\n",
       "        self.memory_kernel_dict = memory_kernel_group.memory_kernel_dict\n",
       "        self.memory_kernel_group.generate_path_groups()\n",
       "\n",
       "    def plot_embeddings_with_path(self, embeddings, title, paths):\n",
       "        tsne = TSNE(n_components=2, random_state=42)\n",
       "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
       "\n",
       "        plt.figure(figsize=(10, 8))\n",
       "        colors = cm.rainbow(np.linspace(0, 1, len(paths)))\n",
       "        for i, path in enumerate(paths):\n",
       "            path_embeddings = reduced_embeddings[path]\n",
       "            plt.scatter(\n",
       "                path_embeddings[:, 0],\n",
       "                path_embeddings[:, 1],\n",
       "                color=colors[i],\n",
       "                label=f\"Cluster {i}\",\n",
       "            )\n",
       "            for j in range(len(path) - 1):\n",
       "                plt.plot(\n",
       "                    [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\n",
       "                    [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\n",
       "                    color=colors[i],\n",
       "                )\n",
       "        plt.title(title)\n",
       "        plt.legend()\n",
       "        plt.show()\n",
       "\n",
       "    def visualize_paths(self):\n",
       "        #loop through memory kernels and print path_group\n",
       "        for key, kernel in self.memory_kernel_dict.items():\n",
       "            print(f\"Kernel: {key}\")\n",
       "            paths = self.memory_kernel_group.path_group[key]\n",
       "            print(f\"Path Group: {paths}\")\n",
       "            node_embeddings = kernel.node_embeddings\n",
       "            self.plot_embeddings_with_path(\n",
       "                node_embeddings, f\"Node Embeddings for {key}\", paths\n",
       "            )\n",
       "    def plot_singular_values(self):\n",
       "        #loop through memory kernels and print path_group\n",
       "        for key, kernel in self.memory_kernel_dict.items():\n",
       "            print(f\"Kernel: {key}\")\n",
       "            A_k = kernel.A_k\n",
       "            U, S, V = np.linalg.svd(A_k)\n",
       "            plt.plot(S)\n",
       "            plt.show()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code imports libraries such as pyplot, cm, and numpy, and uses functions from MultiKernel and TSNE. It defines a class MultiKernelVisualization with methods to plot embeddings and paths, visualize path groups, and plot singular values. The paths are generated from memory kernels and the embeddings are reduced using TSNE. The method visualize_paths loops through memory kernels and prints the path group and node embeddings, and the method plot_singular_values plots the singular values for each kernel."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 98 executed in 1.94 seconds.\n",
      "Sub-task 98 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def __getitem__(self, idx):\n",
       "    return self.memory_thread[idx]\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a __getitem__ method that takes an index as input and returns the item at that index from a memory_thread."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def __len__(self):\n",
       "    return len(self.memory_thread)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The given code defines a `__len__()` function within a class. This function returns the length of the `memory_thread` attribute of the class."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def __init__(\n",
       "    self,\n",
       "    name: str = \"memory\",\n",
       "    max_memory: Optional[int] = None,\n",
       "    tokenizer: Optional[Any] = None,\n",
       ") -> None:\n",
       "    \"\"\"\n",
       "        Initialize the BaseThread instance.\n",
       "\n",
       "        :param name: The name of the memory thread. Defaults to 'memory'.\n",
       "        :param max_memory: The maximum number of tokens allowed in the memory thread.\n",
       "                           Defaults to None, which means no limit.\n",
       "        :param tokenizer: The tokenizer to be used for tokenizing messages.\n",
       "                          Defaults to None, which means using the tiktoken encoding for the 'gpt-3.5-turbo' model.\n",
       "        \"\"\"\n",
       "    self.name = name\n",
       "    self.max_memory = max_memory\n",
       "    self.memory_thread = []\n",
       "    self.time_stamps = []\n",
       "    self.message_tokens = []\n",
       "    self.total_tokens = 0\n",
       "    if tokenizer is None:\n",
       "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This is a constructor method for a class that initializes a thread for storing messages. It takes in parameters such as the name of the thread, maximum number of tokens allowed, and the tokenizer to be used for tokenizing messages. If no tokenizer is provided, it defaults to using the tiktoken encoding for the 'gpt-3.5-turbo' model. The method sets the instance variables for the thread's name, maximum memory, thread list, time stamps, message tokens, total tokens and assigns the default tokenizer, if necessary."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 99 executed in 5.74 seconds.\n",
      "Sub-task 99 results saved in 0.00 seconds.\n",
      "Sub-task 100 executed in 0.00 seconds.\n",
      "Sub-task 100 results saved in 0.00 seconds.\n",
      "Sub-task 101 executed in 0.00 seconds.\n",
      "Sub-task 101 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def reset_memory(self) -> None:\n",
       "    \"\"\"\n",
       "        Reset the memory thread.\n",
       "        \"\"\"\n",
       "    self.memory_thread = []\n",
       "    self.time_stamps = []\n",
       "    self.message_tokens = []\n",
       "    self.total_tokens = 0\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code defines a function called \"reset_memory\" which resets the memory thread by clearing out the memory_thread list, time_stamps list, message_tokens list, and setting the total_tokens to 0."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 102 executed in 4.17 seconds.\n",
      "Sub-task 102 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def get_message_tokens(self, message_dict: dict) -> int:\n",
       "    \"\"\"\n",
       "        Calculate the number of tokens in a message, including the role token.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        :return: The total number of tokens in the message.\n",
       "        \"\"\"\n",
       "    message_dict = check_dict(message_dict)\n",
       "    message = message_dict[\"content\"]\n",
       "    return len(self.tokenizer.encode(message)) + 6  # +6 for the role token\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a method called \"get_message_tokens\" that takes in a dictionary representing a message and calculates the total number of tokens in it, including a role token. It uses the \"tokenizer\" attribute of the class to encode the message and adds 6 to the total number of tokens for the role token."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 103 executed in 3.07 seconds.\n",
      "Sub-task 103 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def get_message_role(self, message_dict: dict) -> str:\n",
       "    \"\"\"\n",
       "        Get the role of the message from a message dictionary.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        :return: The role of the message.\n",
       "        \"\"\"\n",
       "    message_dict = check_dict(message_dict)\n",
       "    return message_dict[\"role\"]\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This is a method in the babydragon package that retrieves the role of a message from a dictionary that contains the message's role and content. The dictionary must be passed as an argument to the method, which then checks the validity of the dictionary before returning the role string. The method name is \"get_message_role\" and it takes a self parameter."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 104 executed in 3.89 seconds.\n",
      "Sub-task 104 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...\n",
      "RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def add_message(self, message_dict: dict) -> None:\n",
       "    \"\"\"\n",
       "        Add a message to the memory thread.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        \"\"\"\n",
       "    message_tokens = self.get_message_tokens(message_dict)\n",
       "\n",
       "    if (\n",
       "        self.max_memory is None\n",
       "        or self.total_tokens + message_tokens <= self.max_memory\n",
       "    ):\n",
       "        # add the message_dict to the memory_thread\n",
       "        # update the total number of tokens\n",
       "        self.memory_thread.append(message_dict)\n",
       "        self.total_tokens += message_tokens\n",
       "        self.message_tokens.append(message_tokens)\n",
       "        time_stamp = time.time()\n",
       "        self.time_stamps.append(time_stamp)\n",
       "    else:\n",
       "        display(\n",
       "            Markdown(\n",
       "                \"The memory BaseThread is full, the last message was not added\"\n",
       "            )\n",
       "        )\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The 'add_message' function adds a message to the memory thread, with the message being a dictionary containing the role and content of the message. It computes the message_tokens using the 'get_message_tokens' function, which is used to check whether the max memory has exceeded or not. If the max memory has not been exceeded, then the message_dict is appended to the memory_thread, while the total_tokens, message_tokens, and time_stamps are updated. If the memory_base is full, then the last message will not be added."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 105 executed in 5.71 seconds.\n",
      "Sub-task 105 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def remove_message(\n",
       "    self, message_dict: Union[dict, None] = None, idx: Union[int, None] = None\n",
       ") -> None:\n",
       "    \"\"\"\n",
       "        Remove a message from the memory thread.\n",
       "        \"\"\"\n",
       "    if message_dict is None and idx is None:\n",
       "        raise Exception(\"You need to provide either a message_dict or an idx\")\n",
       "    elif message_dict is not None and idx is not None:\n",
       "        raise Exception(\"You need to provide either a message_dict or an idx\")\n",
       "\n",
       "    if idx is None:\n",
       "        message_dict = check_dict(message_dict)\n",
       "        search_results = self.find_message(message_dict)\n",
       "        if search_results is not None:\n",
       "            idx = search_results[-1][\"idx\"]\n",
       "            message = search_results[-1][\"message_dict\"]\n",
       "            self.memory_thread.pop(idx)\n",
       "            self.message_tokens.pop(idx)\n",
       "            self.time_stamps.pop(idx)\n",
       "            self.total_tokens -= self.get_message_tokens(message)\n",
       "        else:\n",
       "            raise Exception(\"The message was not found in the memory BaseThread\")\n",
       "    else:\n",
       "        if idx < len(self.memory_thread):\n",
       "            message = self.memory_thread.pop(idx)\n",
       "            self.total_tokens -= self.get_message_tokens(message)\n",
       "        else:\n",
       "            raise Exception(\"The index was out bound\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The remove_message function in the babydragon package allows a user to remove a specific message from the memory thread by providing either a message dictionary or an index. If a message dictionary is provided, the function will search for the message and remove it from the thread, while also reducing the total token count. If an index is provided, the function will directly remove the message at that index and update the total token count. If the message or index cannot be found, an exception is raised."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 106 executed in 2.32 seconds.\n",
      "Sub-task 106 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def find_message(\n",
       "    self, message: Union[dict, str], role: Union[str, None] = None\n",
       ") -> Union[None, list]:\n",
       "    \"\"\"\n",
       "        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\n",
       "        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\n",
       "    # check if the message is a dictioanry or a string\n",
       "    message = message if isinstance(message, str) else check_dict(message)\n",
       "    search_results = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        target = (\n",
       "            message_dict if isinstance(message, dict) else message_dict[\"content\"]\n",
       "        )\n",
       "        if target == message and (role is None or message_dict[\"role\"] == role):\n",
       "            search_results.append({\"idx\": idx, \"message_dict\": message_dict})\n",
       "    return search_results if len(search_results) > 0 else None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The `find_message` function searches for messages in a memory thread. It can accept a dictionary or a string as a message and search for an exact match or a string within the content. The function will return a list of search results if any matches are found, or `None` if there are no matches. The search can also be filtered by the role of the message."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 107 executed in 1.32 seconds.\n",
      "Sub-task 107 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def find_role(self, role: str) -> Union[None, list]:\n",
       "    \"\"\"\n",
       "        Find all messages with a specific role in the memory thread.\n",
       "        \"\"\"\n",
       "    search_results = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if message_dict[\"role\"] == role:\n",
       "            search_results.append({\"idx\": idx, \"message_dict\": message_dict})\n",
       "    return search_results if len(search_results) > 0 else None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This piece of code is a method that takes in a role as input and returns a list of message dictionaries that match that role from a memory thread. If there are no matches, it will return None."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 108 executed in 0.68 seconds.\n",
      "Sub-task 108 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def last_message(self, role: Union[str, None] = None) -> Union[None, dict]:\n",
       "    \"\"\"\n",
       "        Get the last message in the memory thread with a specific role.\"\"\"\n",
       "    if role is None:\n",
       "        return self.memory_thread[-1]\n",
       "    else:\n",
       "        for message_dict in reversed(self.memory_thread):\n",
       "            if message_dict[\"role\"] == role:\n",
       "                return message_dict\n",
       "        return None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This function in the babydragon package allows one to retrieve the last message that was saved in a memory thread. It can search for a message with a specific role, or simply return the last message in the memory thread if no role is specified. The function returns a dictionary containing information about the selected message, or None if no message is found."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 109 executed in 5.67 seconds.\n",
      "Sub-task 109 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def first_message(self, role: Union[str, None] = None) -> Union[None, dict]:\n",
       "    \"\"\"\n",
       "        Get the first message in the memory thread with a specific role.\"\"\"\n",
       "    if role is None:\n",
       "        return self.memory_thread[0]\n",
       "    else:\n",
       "        for message_dict in self.memory_thread:\n",
       "            if message_dict[\"role\"] == role:\n",
       "                return message_dict\n",
       "        return None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code is a method in a class that retrieves the first message in a memory thread based on a specified role. It takes in a role parameter which is optional and can be a string or None, and returns a dictionary of the first message found with the specified role, or the first message if no role is specified. If no message is found, it returns None."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 110 executed in 3.71 seconds.\n",
      "Sub-task 110 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def messages_before(\n",
       "    self, message: dict, role: Union[str, None] = None\n",
       ") -> Union[None, list]:\n",
       "    \"\"\"\n",
       "        Get all messages before a specific message in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if message_dict == message and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            messages = self.memory_thread[:idx]\n",
       "            break\n",
       "    return messages if len(messages) > 0 else None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a method called `messages_before` that retrieves all messages that were sent before a specific message in a memory thread that has a specific role. The method takes two parameters - the message to search for and the role to filter by (optional). The output is a list of messages that meet the specified criteria. If there are no messages that meet the criteria, the code returns None."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 111 executed in 2.07 seconds.\n",
      "Sub-task 111 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def messages_before(\n",
       "    self, message: dict, role: Union[str, None] = None\n",
       ") -> Union[None, list]:\n",
       "    \"\"\"\n",
       "        Get all messages after a specific message in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if message_dict == message and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            messages = self.memory_thread[idx + 1 :]\n",
       "            break\n",
       "    return messages if len(messages) > 0 else None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The given code is a method named \"messages_before\" from within a class. It takes in a message dictionary and a role as input parameters and returns a list of all messages that come after the given message in a memory thread with a matching role. If no messages are found, it returns None."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 112 executed in 1.62 seconds.\n",
      "Sub-task 112 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def messages_between(\n",
       "    self, start_message: dict, end_message: dict, role: Union[str, None] = None\n",
       ") -> Union[None, list]:\n",
       "    \"\"\"\n",
       "        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if message_dict == start_message and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            start_idx = idx\n",
       "            break\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if message_dict == end_message and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            end_idx = idx\n",
       "            break\n",
       "    messages = self.memory_thread[start_idx + 1 : end_idx]\n",
       "    return messages if len(messages) > 0 else None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The given code defines a method named \"messages_between\" which takes two messages, start_message and end_message, and a role as parameters. It retrieves all the messages present between these start and end messages. The messages are selected based on the role parameter. If the role is not provided, the method retrieves all the messages irrespective of their roles. The method returns the selected messages in a list, or None if no messages were found."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 113 executed in 4.21 seconds.\n",
      "Sub-task 113 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def messages_more_tokens(self, tokens: int, role: Union[str, None] = None):\n",
       "    \"\"\"\n",
       "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if self.message_tokens[idx] > tokens and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "    return messages if len(messages) > 0 else None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The `messages_more_tokens` function retrieves all messages from a memory thread that have more tokens than a specified number and a specific role, if provided. The function returns a list of message dictionaries that match the criteria or a `None` value if no messages are found."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 114 executed in 1.14 seconds.\n",
      "Sub-task 114 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def messages_less_tokens(self, tokens: int, role: Union[str, None] = None):\n",
       "    \"\"\"\n",
       "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if self.message_tokens[idx] < tokens and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "    return messages if len(messages) > 0 else None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a method that retrieves messages from a memory thread that have a specific role and a number of tokens below a certain limit. The method returns a list of the retrieved messages if any exist, otherwise, it returns None."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 115 executed in 2.54 seconds.\n",
      "Sub-task 115 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def messages_between_tokens(\n",
       "    self, start_tokens: int, end_tokens: int, role: Union[str, None] = None\n",
       "):\n",
       "    \"\"\"\n",
       "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if (\n",
       "            self.message_tokens[idx] > start_tokens\n",
       "            and self.message_tokens[idx] < end_tokens\n",
       "            and (role is None or message_dict[\"role\"] == role)\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "    return messages if len(messages) > 0 else None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function \"messages_between_tokens\" in the babydragon package retrieves all messages in a memory thread that have less tokens than a specified number and a specified role. It returns a list of these messages, or None if no messages match the criteria."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 116 executed in 3.39 seconds.\n",
      "Sub-task 116 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def messages_before_time(self, time_stamp, role: Union[str, None] = None):\n",
       "    \"\"\"\n",
       "        Get all messages before a specific time in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if self.time_stamps[idx] < time_stamp and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "    return messages if len(messages) > 0 else None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The above code is a method in the babydragon package that retrieves a list of all messages stored in the memory thread that were posted before a specified time, with an optional filter for messages posted by a particular role. If there are relevant messages, the method returns them, otherwise it returns None."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 117 executed in 3.65 seconds.\n",
      "Sub-task 117 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def messages_after_time(self, time_stamp, role: Union[str, None] = None):\n",
       "    \"\"\"\n",
       "        Get all messages after a specific time in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if self.time_stamps[idx] > time_stamp and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "    return messages if len(messages) > 0 else None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code is a method that retrieves all the messages that come after a specific time in the memory thread. It accepts an optional role parameter and returns a list of message dictionary objects filtered based on the provided criteria."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 118 executed in 3.32 seconds.\n",
      "Sub-task 118 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def messages_between_time(\n",
       "    self, start_time, end_time, role: Union[str, None] = None\n",
       "):\n",
       "    \"\"\"\n",
       "        Get all messages between two specific times in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if (\n",
       "            self.time_stamps[idx] > start_time\n",
       "            and self.time_stamps[idx] < end_time\n",
       "            and (role is None or message_dict[\"role\"] == role)\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "    return messages if len(messages) > 0 else None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code is defining a function called `messages_between_time()` that takes in a start time, end time, and an optional role parameter. The function retrieves all messages that were sent between the specified start and end times in a memory thread. If a role parameter is provided, the function filters messages based on their role. The function returns a list of messages or None if no messages were found."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 119 executed in 4.02 seconds.\n",
      "Sub-task 119 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def __init__(\n",
       "    self, name=\"fifo_memory\", max_memory=None, longterm_thread=None, redundant=True\n",
       "):\n",
       "\n",
       "    BaseThread.__init__(self, name=name, max_memory=None)\n",
       "    if redundant is True:\n",
       "        self.redundant_thread = BaseThread(name=\"lucid_memory\", max_memory=None)\n",
       "    else:\n",
       "        self.redundant_thread = None\n",
       "    if longterm_thread is None:\n",
       "        self.longterm_thread = BaseThread(name=\"longterm_memory\", max_memory=None)\n",
       "    else:\n",
       "        self.longterm_thread = longterm_thread\n",
       "    # create an alias for the memory_thread to make the code more readable\n",
       "    self.fifo_thread = self.memory_thread\n",
       "    self.max_memory = max_memory\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code initializes a memory system with several threads. It has options for redundant threads and a long-term memory thread. It creates an alias for the memory_thread to make the code more readable. The max_memory variable can also be set."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def token_bound_history(\n",
       "    self, max_tokens: int, max_history=None, role: Union[str, None] = None\n",
       "):\n",
       "    messages = []\n",
       "    indices = []\n",
       "    tokens = 0\n",
       "    if max_history is None:\n",
       "        max_history = len(self.memory_thread)\n",
       "\n",
       "    for idx, message_dict in enumerate(reversed(self.memory_thread)):\n",
       "        if (\n",
       "            tokens + self.message_tokens[idx] < max_tokens\n",
       "            and (role is None or message_dict[\"role\"] == role)\n",
       "            and idx < max_history\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "            indices.append(len(self.memory_thread) - 1 - idx)\n",
       "            tokens += self.message_tokens[idx]\n",
       "        else:\n",
       "            break\n",
       "    return messages, indices if len(messages) > 0 else (None, None)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code is defining a function called 'token_bound_history' that takes in three parameters: 'max_tokens', 'max_history', and 'role'. It initializes an empty list and an empty indices list. It then loops through a reversed 'memory_thread' list and checks if the sum of 'tokens' and 'message_tokens' is less than 'max_tokens', and if the 'role' matches with the 'message_dict'. If both conditions satisfy, the message and its index are appended to 'messages' and 'indices' list respectively. The loop stops when either of the conditions fail. The function returns 'messages' and 'indices', but if there are no messages, it returns 'None' and 'None'."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 120 executed in 7.73 seconds.\n",
      "Sub-task 120 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " import time\n",
       "from typing import Any, Dict, Optional, Union\n",
       "\n",
       "import tiktoken\n",
       "from IPython.display import Markdown, display\n",
       "\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "from babydragon.utils.oai import check_dict, mark_question\n",
       "\n",
       "\n",
       "class BaseThread:\n",
       "    \"\"\"\n",
       "    This class is used to keep track of the memory thread of a conversation and the total number of tokens.\n",
       "    All conversation memories should subclass this class. If max_memory is None, it has\n",
       "    no limit to the number of tokens that can be stored in the memory thread.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        name: str = \"memory\",\n",
       "        max_memory: Optional[int] = None,\n",
       "        tokenizer: Optional[Any] = None,\n",
       "    ) -> None:\n",
       "        \"\"\"\n",
       "        Initialize the BaseThread instance.\n",
       "\n",
       "        :param name: The name of the memory thread. Defaults to 'memory'.\n",
       "        :param max_memory: The maximum number of tokens allowed in the memory thread.\n",
       "                           Defaults to None, which means no limit.\n",
       "        :param tokenizer: The tokenizer to be used for tokenizing messages.\n",
       "                          Defaults to None, which means using the tiktoken encoding for the 'gpt-3.5-turbo' model.\n",
       "        \"\"\"\n",
       "        self.name = name\n",
       "        self.max_memory = max_memory\n",
       "        self.memory_thread = []\n",
       "        self.time_stamps = []\n",
       "        self.message_tokens = []\n",
       "        self.total_tokens = 0\n",
       "        if tokenizer is None:\n",
       "            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
       "\n",
       "    def __getitem__(self, idx):\n",
       "        return self.memory_thread[idx]\n",
       "\n",
       "    def __len__(self):\n",
       "        return len(self.memory_thread)\n",
       "\n",
       "    def reset_memory(self) -> None:\n",
       "        \"\"\"\n",
       "        Reset the memory thread.\n",
       "        \"\"\"\n",
       "        self.memory_thread = []\n",
       "        self.time_stamps = []\n",
       "        self.message_tokens = []\n",
       "        self.total_tokens = 0\n",
       "\n",
       "    def get_message_tokens(self, message_dict: dict) -> int:\n",
       "        \"\"\"\n",
       "        Calculate the number of tokens in a message, including the role token.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        :return: The total number of tokens in the message.\n",
       "        \"\"\"\n",
       "        message_dict = check_dict(message_dict)\n",
       "        message = message_dict[\"content\"]\n",
       "        return len(self.tokenizer.encode(message)) + 6  # +6 for the role token\n",
       "\n",
       "    def get_message_role(self, message_dict: dict) -> str:\n",
       "        \"\"\"\n",
       "        Get the role of the message from a message dictionary.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        :return: The role of the message.\n",
       "        \"\"\"\n",
       "        message_dict = check_dict(message_dict)\n",
       "        return message_dict[\"role\"]\n",
       "\n",
       "    def add_message(self, message_dict: dict) -> None:\n",
       "        \"\"\"\n",
       "        Add a message to the memory thread.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        \"\"\"\n",
       "        message_tokens = self.get_message_tokens(message_dict)\n",
       "\n",
       "        if (\n",
       "            self.max_memory is None\n",
       "            or self.total_tokens + message_tokens <= self.max_memory\n",
       "        ):\n",
       "            # add the message_dict to the memory_thread\n",
       "            # update the total number of tokens\n",
       "            self.memory_thread.append(message_dict)\n",
       "            self.total_tokens += message_tokens\n",
       "            self.message_tokens.append(message_tokens)\n",
       "            time_stamp = time.time()\n",
       "            self.time_stamps.append(time_stamp)\n",
       "        else:\n",
       "            display(\n",
       "                Markdown(\n",
       "                    \"The memory BaseThread is full, the last message was not added\"\n",
       "                )\n",
       "            )\n",
       "\n",
       "    def remove_message(\n",
       "        self, message_dict: Union[dict, None] = None, idx: Union[int, None] = None\n",
       "    ) -> None:\n",
       "        \"\"\"\n",
       "        Remove a message from the memory thread.\n",
       "        \"\"\"\n",
       "        if message_dict is None and idx is None:\n",
       "            raise Exception(\"You need to provide either a message_dict or an idx\")\n",
       "        elif message_dict is not None and idx is not None:\n",
       "            raise Exception(\"You need to provide either a message_dict or an idx\")\n",
       "\n",
       "        if idx is None:\n",
       "            message_dict = check_dict(message_dict)\n",
       "            search_results = self.find_message(message_dict)\n",
       "            if search_results is not None:\n",
       "                idx = search_results[-1][\"idx\"]\n",
       "                message = search_results[-1][\"message_dict\"]\n",
       "                self.memory_thread.pop(idx)\n",
       "                self.message_tokens.pop(idx)\n",
       "                self.time_stamps.pop(idx)\n",
       "                self.total_tokens -= self.get_message_tokens(message)\n",
       "            else:\n",
       "                raise Exception(\"The message was not found in the memory BaseThread\")\n",
       "        else:\n",
       "            if idx < len(self.memory_thread):\n",
       "                message = self.memory_thread.pop(idx)\n",
       "                self.total_tokens -= self.get_message_tokens(message)\n",
       "            else:\n",
       "                raise Exception(\"The index was out bound\")\n",
       "\n",
       "    def find_message(\n",
       "        self, message: Union[dict, str], role: Union[str, None] = None\n",
       "    ) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\n",
       "        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\n",
       "        # check if the message is a dictioanry or a string\n",
       "        message = message if isinstance(message, str) else check_dict(message)\n",
       "        search_results = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            target = (\n",
       "                message_dict if isinstance(message, dict) else message_dict[\"content\"]\n",
       "            )\n",
       "            if target == message and (role is None or message_dict[\"role\"] == role):\n",
       "                search_results.append({\"idx\": idx, \"message_dict\": message_dict})\n",
       "        return search_results if len(search_results) > 0 else None\n",
       "\n",
       "    def find_role(self, role: str) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Find all messages with a specific role in the memory thread.\n",
       "        \"\"\"\n",
       "        search_results = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict[\"role\"] == role:\n",
       "                search_results.append({\"idx\": idx, \"message_dict\": message_dict})\n",
       "        return search_results if len(search_results) > 0 else None\n",
       "\n",
       "    def last_message(self, role: Union[str, None] = None) -> Union[None, dict]:\n",
       "        \"\"\"\n",
       "        Get the last message in the memory thread with a specific role.\"\"\"\n",
       "        if role is None:\n",
       "            return self.memory_thread[-1]\n",
       "        else:\n",
       "            for message_dict in reversed(self.memory_thread):\n",
       "                if message_dict[\"role\"] == role:\n",
       "                    return message_dict\n",
       "            return None\n",
       "\n",
       "    def first_message(self, role: Union[str, None] = None) -> Union[None, dict]:\n",
       "        \"\"\"\n",
       "        Get the first message in the memory thread with a specific role.\"\"\"\n",
       "        if role is None:\n",
       "            return self.memory_thread[0]\n",
       "        else:\n",
       "            for message_dict in self.memory_thread:\n",
       "                if message_dict[\"role\"] == role:\n",
       "                    return message_dict\n",
       "            return None\n",
       "\n",
       "    def messages_before(\n",
       "        self, message: dict, role: Union[str, None] = None\n",
       "    ) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Get all messages before a specific message in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict == message and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages = self.memory_thread[:idx]\n",
       "                break\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_before(\n",
       "        self, message: dict, role: Union[str, None] = None\n",
       "    ) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Get all messages after a specific message in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict == message and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages = self.memory_thread[idx + 1 :]\n",
       "                break\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_between(\n",
       "        self, start_message: dict, end_message: dict, role: Union[str, None] = None\n",
       "    ) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict == start_message and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                start_idx = idx\n",
       "                break\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict == end_message and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                end_idx = idx\n",
       "                break\n",
       "        messages = self.memory_thread[start_idx + 1 : end_idx]\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_more_tokens(self, tokens: int, role: Union[str, None] = None):\n",
       "        \"\"\"\n",
       "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if self.message_tokens[idx] > tokens and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_less_tokens(self, tokens: int, role: Union[str, None] = None):\n",
       "        \"\"\"\n",
       "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if self.message_tokens[idx] < tokens and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_between_tokens(\n",
       "        self, start_tokens: int, end_tokens: int, role: Union[str, None] = None\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if (\n",
       "                self.message_tokens[idx] > start_tokens\n",
       "                and self.message_tokens[idx] < end_tokens\n",
       "                and (role is None or message_dict[\"role\"] == role)\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_before_time(self, time_stamp, role: Union[str, None] = None):\n",
       "        \"\"\"\n",
       "        Get all messages before a specific time in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if self.time_stamps[idx] < time_stamp and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_after_time(self, time_stamp, role: Union[str, None] = None):\n",
       "        \"\"\"\n",
       "        Get all messages after a specific time in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if self.time_stamps[idx] > time_stamp and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_between_time(\n",
       "        self, start_time, end_time, role: Union[str, None] = None\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Get all messages between two specific times in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if (\n",
       "                self.time_stamps[idx] > start_time\n",
       "                and self.time_stamps[idx] < end_time\n",
       "                and (role is None or message_dict[\"role\"] == role)\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def token_bound_history(\n",
       "        self, max_tokens: int, max_history=None, role: Union[str, None] = None\n",
       "    ):\n",
       "        messages = []\n",
       "        indices = []\n",
       "        tokens = 0\n",
       "        if max_history is None:\n",
       "            max_history = len(self.memory_thread)\n",
       "\n",
       "        for idx, message_dict in enumerate(reversed(self.memory_thread)):\n",
       "            if (\n",
       "                tokens + self.message_tokens[idx] < max_tokens\n",
       "                and (role is None or message_dict[\"role\"] == role)\n",
       "                and idx < max_history\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "                indices.append(len(self.memory_thread) - 1 - idx)\n",
       "                tokens += self.message_tokens[idx]\n",
       "            else:\n",
       "                break\n",
       "        return messages, indices if len(messages) > 0 else (None, None)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code imports various modules and defines a class called BaseThread, which is used to handle an organized memory thread of a conversation. The class has methods to add, remove, and search for messages in the memory thread based on various parameters such as token count, role, and time stamps. The class also has a method for resetting the memory thread and a method for getting the token-bound history of the conversation. The constructor of the BaseThread class takes in optional parameters for the name of the memory thread, maximum number of memory tokens, and a tokenizer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 121 executed in 1.66 seconds.\n",
      "Sub-task 121 results saved in 0.00 seconds.\n",
      "Sub-task 122 executed in 0.00 seconds.\n",
      "Sub-task 122 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def to_longterm(self, idx: int):\n",
       "    \"\"\"move the message at the index idx to the longterm_memory\"\"\"\n",
       "    # move the message at the index idx to the longterm_memory\n",
       "    display(\n",
       "        Markdown(\n",
       "            \"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(\n",
       "                idx\n",
       "            )\n",
       "        )\n",
       "    )\n",
       "    message = copy.deepcopy(self.memory_thread[idx])\n",
       "    # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\n",
       "    self.longterm_thread.add_message(message)\n",
       "    self.remove_message(idx=idx)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The above code defines a method called \"to_longterm\", which moves a message indexed by \"idx\" from the memory thread to the longterm memory. A message is first copied and added to the longterm memory, and then removed from the memory thread. The method also displays a message indicating that the message has been moved to the longterm memory."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 123 executed in 1.71 seconds.\n",
      "Sub-task 123 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " import copy\n",
       "\n",
       "from IPython.display import Markdown, display\n",
       "\n",
       "from babydragon.memory.threads.base_thread import BaseThread\n",
       "from babydragon.utils.oai import check_dict\n",
       "\n",
       "\n",
       "class FifoThread(BaseThread):\n",
       "    \"\"\"FIFO Memory BaseThread, the oldest messages are removed first when reaching the max_memory limit, the memory is defined in terms of tokens,\n",
       "    outs are passe to the longterm_memory, lucid_memory is a redundant memory that stores all the messages\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self, name=\"fifo_memory\", max_memory=None, longterm_thread=None, redundant=True\n",
       "    ):\n",
       "\n",
       "        BaseThread.__init__(self, name=name, max_memory=None)\n",
       "        if redundant is True:\n",
       "            self.redundant_thread = BaseThread(name=\"lucid_memory\", max_memory=None)\n",
       "        else:\n",
       "            self.redundant_thread = None\n",
       "        if longterm_thread is None:\n",
       "            self.longterm_thread = BaseThread(name=\"longterm_memory\", max_memory=None)\n",
       "        else:\n",
       "            self.longterm_thread = longterm_thread\n",
       "        # create an alias for the memory_thread to make the code more readable\n",
       "        self.fifo_thread = self.memory_thread\n",
       "        self.max_memory = max_memory\n",
       "\n",
       "    def to_longterm(self, idx: int):\n",
       "        \"\"\"move the message at the index idx to the longterm_memory\"\"\"\n",
       "        # move the message at the index idx to the longterm_memory\n",
       "        display(\n",
       "            Markdown(\n",
       "                \"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(\n",
       "                    idx\n",
       "                )\n",
       "            )\n",
       "        )\n",
       "        message = copy.deepcopy(self.memory_thread[idx])\n",
       "        # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\n",
       "        self.longterm_thread.add_message(message)\n",
       "        self.remove_message(idx=idx)\n",
       "\n",
       "    def add_message(self, message_dict: dict):\n",
       "        \"\"\"add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\n",
       "        # message_dict = {\"role\": role, \"content\": content}\n",
       "        # chek that the message_dict is a dictionary or a list of dictionaries\n",
       "        message_dict = check_dict(message_dict)\n",
       "        if self.redundant_thread is not None:\n",
       "            self.redundant_thread.add_message(message_dict)\n",
       "        message_tokens = self.get_message_tokens(message_dict)\n",
       "\n",
       "        if self.total_tokens + message_tokens > self.max_memory:\n",
       "            while self.total_tokens + message_tokens > self.max_memory:\n",
       "                if len(self.memory_thread) > 0:\n",
       "                    self.to_longterm(idx=0)\n",
       "            super().add_message(message_dict)\n",
       "\n",
       "        else:\n",
       "            # add the message_dict to the memory_thread\n",
       "            # update the total number of tokens\n",
       "            super().add_message(message_dict)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code defines a class called FifoThread that's a memory management thread using a first-in-first-out principle for messages. Messages can be stored in a long-term memory or a redundant memory. If the memory is full, the oldest messages are removed until there is enough space to add a new message. The class also has a method to move messages to long-term memory. The code uses the copy and IPython.display libraries and imports classes from the babydragon package."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def add_message(self, message_dict: dict):\n",
       "    \"\"\"add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\n",
       "    # message_dict = {\"role\": role, \"content\": content}\n",
       "    # chek that the message_dict is a dictionary or a list of dictionaries\n",
       "    message_dict = check_dict(message_dict)\n",
       "    if self.redundant_thread is not None:\n",
       "        self.redundant_thread.add_message(message_dict)\n",
       "    message_tokens = self.get_message_tokens(message_dict)\n",
       "\n",
       "    if self.total_tokens + message_tokens > self.max_memory:\n",
       "        while self.total_tokens + message_tokens > self.max_memory:\n",
       "            if len(self.memory_thread) > 0:\n",
       "                self.to_longterm(idx=0)\n",
       "        super().add_message(message_dict)\n",
       "\n",
       "    else:\n",
       "        # add the message_dict to the memory_thread\n",
       "        # update the total number of tokens\n",
       "        super().add_message(message_dict)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code is part of the functionality of the memory_thread class in the babydragon package. It adds messages in the form of a dictionary to the memory_thread, following the FIFO principle. If the memory_thread is full, the oldest message is removed from the thread to make space for the new message. If still not enough space is available, the oldest messages continue to be removed until there is sufficient space. The code also checks that the input message is a dictionary or a list of dictionaries. If there is a redundant_thread, the same message is added to it as well. Finally, the method updates the total number of tokens in the thread."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 124 executed in 8.35 seconds.\n",
      "Sub-task 124 results saved in 0.00 seconds.\n",
      "Sub-task 125 executed in 0.00 seconds.\n",
      "Sub-task 125 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\n",
       "    BaseThread.__init__(self, name=name, max_memory=None)\n",
       "    MemoryIndex.__init__(self, index=None, name=name)\n",
       "    self.max_context = max_context\n",
       "    self.use_mark = use_mark\n",
       "    self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a class called \"vector_memory\". It has initialization parameters like name, max_context, and use_mark. It inherits from two other classes, \"BaseThread\" and \"MemoryIndex\". It also initializes a \"local_index\" attribute which uses the Faiss library to create an index for a given embedding size."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 126 executed in 0.49 seconds.\n",
      "Sub-task 126 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def index_message(self, message: str, verbose: bool = False):\n",
       "    \"\"\"index a message in the faiss index, the message is embedded and added to the index\n",
       "        self.values and self.embeddings and self.index are updated\n",
       "        \"\"\"\n",
       "\n",
       "    self.add_to_index(value=message, verbose=verbose)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a method called \"index_message\" that adds a message to a faiss index by embedding it and updating the index. It calls another function called \"add_to_index\" to accomplish this task. The method takes two parameters, the message to be indexed and a boolean to control the verbosity of the logging."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 127 executed in 3.70 seconds.\n",
      "Sub-task 127 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...\n",
      "RateLimiter: Waiting for 3.00 seconds before next call.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def add_message(self, message_dict: dict, verbose: bool = False):\n",
       "    \"\"\"add a message to the memory thread, the message is embedded and added to the index\n",
       "        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\n",
       "        \"\"\"\n",
       "    # print(\"checking the dict\")\n",
       "    message_dict = check_dict(message_dict)\n",
       "    # print(\"trying to add the message\")\n",
       "    BaseThread.add_message(self, message_dict)\n",
       "    # print(message_dict)\n",
       "    message = message_dict[\"content\"]\n",
       "    self.index_message(message, verbose=verbose)\n",
       "    return True\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function called \"add_message\" that adds a message to a memory thread. The message is embedded and added to the index. If \"use_mark\" is set to False, only the content of the message is embedded. The function also updates self.values and self.embeddings, then returns True."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 128 executed in 2.07 seconds.\n",
      "Sub-task 128 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\n",
       "    \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\n",
       "    if self.use_mark:\n",
       "        query = mark_question(query)\n",
       "    return MemoryIndex.token_bound_query(self, query, k, max_tokens)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function called 'token_bound_query' that takes in a query, an integer value for k (defaults to 10) and an integer value for max_tokens (defaults to 4000). The function sorts the k most similar messages to the query in similarity order. It includes a check to use 'mark_question' function in case 'use_mark' is True. Finally, it returns the result of applying 'MemoryIndex.token_bound_query' function to query, k and max_tokens."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 129 executed in 6.16 seconds.\n",
      "Sub-task 129 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def sorted_query(\n",
       "    self,\n",
       "    query,\n",
       "    k: int = 10,\n",
       "    max_tokens: int = 4000,\n",
       "    reverse: bool = False,\n",
       "    return_from_thread=True,\n",
       "):\n",
       "    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\n",
       "        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\n",
       "        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\n",
       "        \"\"\"\n",
       "    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(\n",
       "        query, k, max_tokens=max_tokens\n",
       "    )\n",
       "    # sort the messages\n",
       "\n",
       "    sorted_messages = [\n",
       "        unsorted_messages[i]\n",
       "        for i in sorted(\n",
       "            range(len(unsorted_messages)), key=lambda k: unsorted_indices[k]\n",
       "        )\n",
       "    ]\n",
       "    sorted_scores = [\n",
       "        unsorted_scores[i]\n",
       "        for i in sorted(\n",
       "            range(len(unsorted_scores)), key=lambda k: unsorted_indices[k]\n",
       "        )\n",
       "    ]\n",
       "    sorted_indices = [\n",
       "        unsorted_indices[i]\n",
       "        for i in sorted(\n",
       "            range(len(unsorted_indices)), key=lambda k: unsorted_indices[k]\n",
       "        )\n",
       "    ]\n",
       "    if reverse:\n",
       "        sorted_messages.reverse()\n",
       "        sorted_scores.reverse()\n",
       "        sorted_indices.reverse()\n",
       "    if return_from_thread:\n",
       "        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\n",
       "    return sorted_messages, sorted_scores, sorted_indices\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The \"sorted_query\" function in the babydragon package returns k most similar messages to a query in chronological order with the most recent message first. The messages can be returned either from memory thread or from the index and can be sorted in reverse chronological order if specified. It sorts the messages and their respective scores and indices based on the chronological order. The function returns the sorted messages, scores, and indices."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 130 executed in 1.10 seconds.\n",
      "Sub-task 130 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def get_embedding_size(self):\n",
       "    return ADA_EMBEDDING_SIZE\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code is defining a function called \"get_embedding_size\" which returns a constant value called \"ADA_EMBEDDING_SIZE\"."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " from typing import Optional\n",
       "\n",
       "import faiss\n",
       "import numpy as np\n",
       "\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "from babydragon.memory.threads.base_thread import BaseThread\n",
       "from babydragon.utils.oai import check_dict, mark_question\n",
       "\n",
       "\n",
       "class VectorThread(BaseThread, MemoryIndex):\n",
       "    \"\"\"vector BaseThread, creates a faiss index with the messages and allows to search for similar messages, memory BaseThread can return messages in either similarity or chronological order\n",
       "    add a parameter to choose the order of the messages\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\n",
       "        BaseThread.__init__(self, name=name, max_memory=None)\n",
       "        MemoryIndex.__init__(self, index=None, name=name)\n",
       "        self.max_context = max_context\n",
       "        self.use_mark = use_mark\n",
       "        self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
       "\n",
       "    def index_message(self, message: str, verbose: bool = False):\n",
       "        \"\"\"index a message in the faiss index, the message is embedded and added to the index\n",
       "        self.values and self.embeddings and self.index are updated\n",
       "        \"\"\"\n",
       "\n",
       "        self.add_to_index(value=message, verbose=verbose)\n",
       "\n",
       "    def add_message(self, message_dict: dict, verbose: bool = False):\n",
       "        \"\"\"add a message to the memory thread, the message is embedded and added to the index\n",
       "        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\n",
       "        \"\"\"\n",
       "        # print(\"checking the dict\")\n",
       "        message_dict = check_dict(message_dict)\n",
       "        # print(\"trying to add the message\")\n",
       "        BaseThread.add_message(self, message_dict)\n",
       "        # print(message_dict)\n",
       "        message = message_dict[\"content\"]\n",
       "        self.index_message(message, verbose=verbose)\n",
       "        return True\n",
       "\n",
       "    def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\n",
       "        \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\n",
       "        if self.use_mark:\n",
       "            query = mark_question(query)\n",
       "        return MemoryIndex.token_bound_query(self, query, k, max_tokens)\n",
       "\n",
       "    def sorted_query(\n",
       "        self,\n",
       "        query,\n",
       "        k: int = 10,\n",
       "        max_tokens: int = 4000,\n",
       "        reverse: bool = False,\n",
       "        return_from_thread=True,\n",
       "    ):\n",
       "        \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\n",
       "        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\n",
       "        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\n",
       "        \"\"\"\n",
       "        unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(\n",
       "            query, k, max_tokens=max_tokens\n",
       "        )\n",
       "        # sort the messages\n",
       "\n",
       "        sorted_messages = [\n",
       "            unsorted_messages[i]\n",
       "            for i in sorted(\n",
       "                range(len(unsorted_messages)), key=lambda k: unsorted_indices[k]\n",
       "            )\n",
       "        ]\n",
       "        sorted_scores = [\n",
       "            unsorted_scores[i]\n",
       "            for i in sorted(\n",
       "                range(len(unsorted_scores)), key=lambda k: unsorted_indices[k]\n",
       "            )\n",
       "        ]\n",
       "        sorted_indices = [\n",
       "            unsorted_indices[i]\n",
       "            for i in sorted(\n",
       "                range(len(unsorted_indices)), key=lambda k: unsorted_indices[k]\n",
       "            )\n",
       "        ]\n",
       "        if reverse:\n",
       "            sorted_messages.reverse()\n",
       "            sorted_scores.reverse()\n",
       "            sorted_indices.reverse()\n",
       "        if return_from_thread:\n",
       "            sorted_messages = [self.memory_thread[i] for i in sorted_indices]\n",
       "        return sorted_messages, sorted_scores, sorted_indices\n",
       "\n",
       "    def weighted_query(\n",
       "        self,\n",
       "        query,\n",
       "        k: int = 10,\n",
       "        max_tokens: int = 4000,\n",
       "        decay_factor: float = 0.1,\n",
       "        temporal_weight: float = 0.5,\n",
       "        order_by: str = \"chronological\",\n",
       "        reverse: bool = False,\n",
       "    ) -> list:\n",
       "        \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\n",
       "        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\n",
       "        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\n",
       "        The order_by parameter controls the order of the results. If it is set to 'similarity', the results are sorted in similarity order. If it is set to 'chronological', the results are sorted in chronological order with the most recent message first.\n",
       "        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\n",
       "        \"\"\"\n",
       "        # Validate order_by parameter\n",
       "        if order_by not in (\"similarity\", \"chronological\"):\n",
       "            raise ValueError(\n",
       "                \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
       "            )\n",
       "\n",
       "        # Get similarity-based results\n",
       "        sim_messages, sim_scores, sim_indices = self.sorted_query(\n",
       "            query, k, max_tokens=max_tokens\n",
       "        )\n",
       "\n",
       "        # Get token-bound history\n",
       "        hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\n",
       "\n",
       "        # Combine messages and indices\n",
       "        combined_messages = sim_messages + hist_messages\n",
       "        combined_indices = sim_indices + hist_indices\n",
       "\n",
       "        # Create the local_index and populate it\n",
       "        self.local_index = MemoryIndex(name=\"local_index\")\n",
       "        for message in combined_messages:\n",
       "            self.local_index.add_to_index(value=message, verbose=False)\n",
       "\n",
       "        # Perform a new query on the combined index\n",
       "        (\n",
       "            new_query_results,\n",
       "            new_query_scores,\n",
       "            new_query_indices,\n",
       "        ) = self.local_index.token_bound_query(\n",
       "            query, k=len(combined_messages), max_tokens=max_tokens\n",
       "        )\n",
       "\n",
       "        # Compute temporal weights\n",
       "        temporal_weights = [\n",
       "            np.exp(-decay_factor * i) for i in range(len(combined_messages))\n",
       "        ]\n",
       "        temporal_weights = [\n",
       "            w / sum(temporal_weights) for w in temporal_weights\n",
       "        ]  # Normalize the temporal weights\n",
       "\n",
       "        # Combine similarity scores and temporal weights\n",
       "        weighted_scores = []\n",
       "        for i in range(len(new_query_scores)):\n",
       "            sim_score = new_query_scores[i]\n",
       "            temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\n",
       "            weighted_score = (\n",
       "                1 - temporal_weight\n",
       "            ) * sim_score + temporal_weight * temp_weight\n",
       "            weighted_scores.append(weighted_score)\n",
       "\n",
       "        # Sort the results based on the order_by parameter\n",
       "        if order_by == \"similarity\":\n",
       "            sorting_key = lambda k: weighted_scores[k]\n",
       "        elif order_by == \"chronological\":  # order_by == 'chronological'\n",
       "            sorting_key = lambda k: new_query_indices[k]\n",
       "        else:\n",
       "            raise ValueError(\n",
       "                \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
       "            )\n",
       "\n",
       "        sorted_indices = [\n",
       "            new_query_indices[i]\n",
       "            for i in sorted(\n",
       "                range(len(new_query_indices)), key=sorting_key, reverse=not reverse\n",
       "            )\n",
       "        ]\n",
       "        sorted_results = [\n",
       "            new_query_results[i]\n",
       "            for i in sorted(\n",
       "                range(len(new_query_results)), key=sorting_key, reverse=not reverse\n",
       "            )\n",
       "        ]\n",
       "        sorted_scores = [\n",
       "            weighted_scores[i]\n",
       "            for i in sorted(\n",
       "                range(len(weighted_scores)), key=sorting_key, reverse=not reverse\n",
       "            )\n",
       "        ]\n",
       "\n",
       "        # Return only the top k results without exceeding max_tokens\n",
       "        final_results, final_scores, final_indices = [], [], []\n",
       "        current_tokens = 0\n",
       "        for i in range(min(k, len(sorted_results))):\n",
       "            message_tokens = self.get_message_tokens(sorted_results[i])\n",
       "            if current_tokens + message_tokens <= max_tokens:\n",
       "                final_results.append(sorted_results[i])\n",
       "                final_scores.append(sorted_scores[i])\n",
       "                final_indices.append(sorted_indices[i])\n",
       "                current_tokens += message_tokens\n",
       "            else:\n",
       "                break\n",
       "\n",
       "        return final_results, final_scores, final_indices\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code contains a class called VectorThread, which combines and implements functionalities of two other classes. The class creates a faiss index of messages and allows to search for similar messages. The messages can be returned either in similarity or chronological order with the most recent message first. The returned messages can be from either the memory thread or the index. The results can be weighted based on both similarity scores and temporal weights, with the temporal weights computed using an exponential decay function. The order of returned messages can be controlled using a parameter."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def weighted_query(\n",
       "    self,\n",
       "    query,\n",
       "    k: int = 10,\n",
       "    max_tokens: int = 4000,\n",
       "    decay_factor: float = 0.1,\n",
       "    temporal_weight: float = 0.5,\n",
       "    order_by: str = \"chronological\",\n",
       "    reverse: bool = False,\n",
       ") -> list:\n",
       "    \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\n",
       "        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\n",
       "        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\n",
       "        The order_by parameter controls the order of the results. If it is set to 'similarity', the results are sorted in similarity order. If it is set to 'chronological', the results are sorted in chronological order with the most recent message first.\n",
       "        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\n",
       "        \"\"\"\n",
       "    # Validate order_by parameter\n",
       "    if order_by not in (\"similarity\", \"chronological\"):\n",
       "        raise ValueError(\n",
       "            \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
       "        )\n",
       "\n",
       "    # Get similarity-based results\n",
       "    sim_messages, sim_scores, sim_indices = self.sorted_query(\n",
       "        query, k, max_tokens=max_tokens\n",
       "    )\n",
       "\n",
       "    # Get token-bound history\n",
       "    hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\n",
       "\n",
       "    # Combine messages and indices\n",
       "    combined_messages = sim_messages + hist_messages\n",
       "    combined_indices = sim_indices + hist_indices\n",
       "\n",
       "    # Create the local_index and populate it\n",
       "    self.local_index = MemoryIndex(name=\"local_index\")\n",
       "    for message in combined_messages:\n",
       "        self.local_index.add_to_index(value=message, verbose=False)\n",
       "\n",
       "    # Perform a new query on the combined index\n",
       "    (\n",
       "        new_query_results,\n",
       "        new_query_scores,\n",
       "        new_query_indices,\n",
       "    ) = self.local_index.token_bound_query(\n",
       "        query, k=len(combined_messages), max_tokens=max_tokens\n",
       "    )\n",
       "\n",
       "    # Compute temporal weights\n",
       "    temporal_weights = [\n",
       "        np.exp(-decay_factor * i) for i in range(len(combined_messages))\n",
       "    ]\n",
       "    temporal_weights = [\n",
       "        w / sum(temporal_weights) for w in temporal_weights\n",
       "    ]  # Normalize the temporal weights\n",
       "\n",
       "    # Combine similarity scores and temporal weights\n",
       "    weighted_scores = []\n",
       "    for i in range(len(new_query_scores)):\n",
       "        sim_score = new_query_scores[i]\n",
       "        temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\n",
       "        weighted_score = (\n",
       "            1 - temporal_weight\n",
       "        ) * sim_score + temporal_weight * temp_weight\n",
       "        weighted_scores.append(weighted_score)\n",
       "\n",
       "    # Sort the results based on the order_by parameter\n",
       "    if order_by == \"similarity\":\n",
       "        sorting_key = lambda k: weighted_scores[k]\n",
       "    elif order_by == \"chronological\":  # order_by == 'chronological'\n",
       "        sorting_key = lambda k: new_query_indices[k]\n",
       "    else:\n",
       "        raise ValueError(\n",
       "            \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
       "        )\n",
       "\n",
       "    sorted_indices = [\n",
       "        new_query_indices[i]\n",
       "        for i in sorted(\n",
       "            range(len(new_query_indices)), key=sorting_key, reverse=not reverse\n",
       "        )\n",
       "    ]\n",
       "    sorted_results = [\n",
       "        new_query_results[i]\n",
       "        for i in sorted(\n",
       "            range(len(new_query_results)), key=sorting_key, reverse=not reverse\n",
       "        )\n",
       "    ]\n",
       "    sorted_scores = [\n",
       "        weighted_scores[i]\n",
       "        for i in sorted(\n",
       "            range(len(weighted_scores)), key=sorting_key, reverse=not reverse\n",
       "        )\n",
       "    ]\n",
       "\n",
       "    # Return only the top k results without exceeding max_tokens\n",
       "    final_results, final_scores, final_indices = [], [], []\n",
       "    current_tokens = 0\n",
       "    for i in range(min(k, len(sorted_results))):\n",
       "        message_tokens = self.get_message_tokens(sorted_results[i])\n",
       "        if current_tokens + message_tokens <= max_tokens:\n",
       "            final_results.append(sorted_results[i])\n",
       "            final_scores.append(sorted_scores[i])\n",
       "            final_indices.append(sorted_indices[i])\n",
       "            current_tokens += message_tokens\n",
       "        else:\n",
       "            break\n",
       "\n",
       "    return final_results, final_scores, final_indices\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The `weighted_query` function returns the `k` most similar messages to a given query. The results are sorted either by similarity or chronologically, with the temporal weights and similarity scores combined. The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The order_by parameter controls the order of the results. If it is set to 'similarity', the results are sorted in similarity order. If it is set to 'chronological', the results are sorted in chronological order with the most recent message first. The function performs a new query on a combined index of similarity-based and token-bound history messages and returns only the top `k` results without exceeding the `max_tokens` parameter."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 131 executed in 8.11 seconds.\n",
      "Sub-task 131 results saved in 0.00 seconds.\n",
      "Sub-task 132 executed in 0.00 seconds.\n",
      "Sub-task 132 results saved in 0.00 seconds.\n",
      "Sub-task 133 executed in 0.00 seconds.\n",
      "Sub-task 133 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def embed(self, data, embed_mark=False, verbose=False):\n",
       "\n",
       "    if embed_mark is False and type(data) is dict and \"content\" in data:\n",
       "        if verbose is True:\n",
       "            print(\"Embedding without mark\", data[\"content\"])\n",
       "        out = openai.Embedding.create(\n",
       "            input=data[\"content\"], engine=\"text-embedding-ada-002\"\n",
       "        )\n",
       "    else:\n",
       "        if len(str(data)) > MAX_CONTEXT_LENGTH:\n",
       "            data = str(data)[:MAX_CONTEXT_LENGTH]\n",
       "        if verbose is True:\n",
       "            print(\"Embedding without preprocessing the input\", data)\n",
       "        out = openai.Embedding.create(\n",
       "            input=str(data), engine=\"text-embedding-ada-002\"\n",
       "        )\n",
       "    return out.data[0].embedding\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This is a method called \"embed\" from the open source babydragon package that takes in three parameters: data, embed_mark, and verbose. It checks the type of the data input and creates an embedding using the openai.Embedding API. If embed_mark is False and the type of data is a dictionary with the key \"content\", then the content is embedded. Otherwise, the data is converted to a string and embedded. If verbose is True, the method prints out messages indicating whether or not the embedding was done with or without preprocessing. Finally, the method returns the embedding result of the input data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 134 executed in 6.85 seconds.\n",
      "Sub-task 134 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "\n",
       "def parse_and_embed_functions(input_str: str) -> List[np.ndarray]:\n",
       "    # Parse the input string with libcst\n",
       "    module = cst.parse_module(input_str)\n",
       "\n",
       "    # Find all the functions in the module and embed them separately\n",
       "    embeddings = []\n",
       "    for node in module.body:\n",
       "\n",
       "        if isinstance(node, cst.FunctionDef) or isinstance(node, cst.ClassDef):\n",
       "            func_str = cst.Module(body=[node]).code\n",
       "            print(\"Function string\", func_str)\n",
       "            embedding = openai.Embedding.create(\n",
       "                input=str(func_str)[:MAX_CONTEXT_LENGTH],\n",
       "                engine=\"text-embedding-ada-002\",\n",
       "            )\n",
       "            if embedding is not None:\n",
       "                embeddings.append(embedding.data[0].embedding)\n",
       "\n",
       "    avg_embedding = avg_embeddings(embeddings)\n",
       "    print(avg_embedding.shape)\n",
       "    return avg_embedding\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function that takes a string as input, parses it using the libcst library, and finds all the functions and classes in the input string. For each function or class, it creates a string representation, embeds it using OpenAI's text-embedding-ada-002 engine, and adds the resulting embedding to a list. The function then calculates the average embedding of all the embeddings in the list and returns it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 135 executed in 0.43 seconds.\n",
      "Sub-task 135 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "\n",
       "def avg_embeddings(embeddings: List[np.ndarray]) -> np.ndarray:\n",
       "    print(\"Embeddings len\", len(embeddings))\n",
       "    # convert embeddings to numpy array\n",
       "    embeddings = np.array(embeddings)\n",
       "    print(\"Embedding Matrix Shape\", embeddings.shape)\n",
       "    return np.array([np.sum(embeddings.T, axis=1)]).astype(np.float32)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function called \"avg_embeddings\" that takes a list of numpy arrays as input. It converts the input into a numpy array, calculates the sum of each column in the array, and returns the resulting array as a single row matrix in the form of an np.ndarray. Some print statements are included for debugging purposes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 136 executed in 1.74 seconds.\n",
      "Sub-task 136 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def get_embedding_size(self):\n",
       "    return COHERE_EMBEDDING_SIZE\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code returns the constant value COHERE_EMBEDDING_SIZE when the method get_embedding_size is called."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " from typing import List\n",
       "\n",
       "import libcst as cst\n",
       "import numpy as np\n",
       "import openai\n",
       "\n",
       "ADA_EMBEDDING_SIZE = 1536\n",
       "MAX_CONTEXT_LENGTH = 8100\n",
       "\n",
       "\n",
       "class OpenAiEmbedder:\n",
       "    def get_embedding_size(self):\n",
       "        return ADA_EMBEDDING_SIZE\n",
       "\n",
       "    def embed(self, data, embed_mark=False, verbose=False):\n",
       "\n",
       "        if embed_mark is False and type(data) is dict and \"content\" in data:\n",
       "            if verbose is True:\n",
       "                print(\"Embedding without mark\", data[\"content\"])\n",
       "            out = openai.Embedding.create(\n",
       "                input=data[\"content\"], engine=\"text-embedding-ada-002\"\n",
       "            )\n",
       "        else:\n",
       "            if len(str(data)) > MAX_CONTEXT_LENGTH:\n",
       "                data = str(data)[:MAX_CONTEXT_LENGTH]\n",
       "            if verbose is True:\n",
       "                print(\"Embedding without preprocessing the input\", data)\n",
       "            out = openai.Embedding.create(\n",
       "                input=str(data), engine=\"text-embedding-ada-002\"\n",
       "            )\n",
       "        return out.data[0].embedding\n",
       "\n",
       "\n",
       "def parse_and_embed_functions(input_str: str) -> List[np.ndarray]:\n",
       "    # Parse the input string with libcst\n",
       "    module = cst.parse_module(input_str)\n",
       "\n",
       "    # Find all the functions in the module and embed them separately\n",
       "    embeddings = []\n",
       "    for node in module.body:\n",
       "\n",
       "        if isinstance(node, cst.FunctionDef) or isinstance(node, cst.ClassDef):\n",
       "            func_str = cst.Module(body=[node]).code\n",
       "            print(\"Function string\", func_str)\n",
       "            embedding = openai.Embedding.create(\n",
       "                input=str(func_str)[:MAX_CONTEXT_LENGTH],\n",
       "                engine=\"text-embedding-ada-002\",\n",
       "            )\n",
       "            if embedding is not None:\n",
       "                embeddings.append(embedding.data[0].embedding)\n",
       "\n",
       "    avg_embedding = avg_embeddings(embeddings)\n",
       "    print(avg_embedding.shape)\n",
       "    return avg_embedding\n",
       "\n",
       "\n",
       "def avg_embeddings(embeddings: List[np.ndarray]) -> np.ndarray:\n",
       "    print(\"Embeddings len\", len(embeddings))\n",
       "    # convert embeddings to numpy array\n",
       "    embeddings = np.array(embeddings)\n",
       "    print(\"Embedding Matrix Shape\", embeddings.shape)\n",
       "    return np.array([np.sum(embeddings.T, axis=1)]).astype(np.float32)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code uses the OpenAI API to embed text data, specifically functions and classes in Python code. It does this by parsing the input string with the libcst library and then embedding each function or class separately. The embeddings are then averaged to produce a final embedding for the set of functions/classes. The code also includes methods to limit the length of the input string and to print debugging information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 137 executed in 3.52 seconds.\n",
      "Sub-task 137 results saved in 0.00 seconds.\n",
      "Sub-task 138 executed in 0.00 seconds.\n",
      "Sub-task 138 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def embed(self, data, embed_mark=False, verbose=False):\n",
       "    try:\n",
       "        if embed_mark is False and type(data) is dict and \"content\" in data:\n",
       "            if verbose is True:\n",
       "                print(\"Embedding without mark\", data[\"content\"])\n",
       "            out = co.embed(input=data[\"content\"]).embeddings\n",
       "        else:\n",
       "            if verbose is True:\n",
       "                print(\"Embedding without preprocessing the input\", data)\n",
       "            out = co.embed(input=str(data)).embeddings\n",
       "\n",
       "    except:\n",
       "        raise ValueError(\"The data  is not valid\", data)\n",
       "    return out\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The \"embed\" function takes in data and can embed it using a pre-trained model. If the input is a dictionary with a \"content\" key, the function will embed the value of the \"content\" key. If not, it will embed the value of the input parameter. The function can also output verbose print statements and will raise an error if the input data is not valid."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 139 executed in 6.36 seconds.\n",
      "Sub-task 139 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def get_embedding_size(self):\n",
       "    return 356\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The function \"get_embedding_size\" returns the integer value 356."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " import cohere as co\n",
       "\n",
       "COHERE_EMBEDDING_SIZE = 512\n",
       "\n",
       "\n",
       "class CohereEmbedder:\n",
       "    def get_embedding_size(self):\n",
       "        return COHERE_EMBEDDING_SIZE\n",
       "\n",
       "    def embed(self, data, embed_mark=False, verbose=False):\n",
       "        try:\n",
       "            if embed_mark is False and type(data) is dict and \"content\" in data:\n",
       "                if verbose is True:\n",
       "                    print(\"Embedding without mark\", data[\"content\"])\n",
       "                out = co.embed(input=data[\"content\"]).embeddings\n",
       "            else:\n",
       "                if verbose is True:\n",
       "                    print(\"Embedding without preprocessing the input\", data)\n",
       "                out = co.embed(input=str(data)).embeddings\n",
       "\n",
       "        except:\n",
       "            raise ValueError(\"The data  is not valid\", data)\n",
       "        return out\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code imports the cohere package and defines a constant for the size of embeddings. It also defines a class for embedding data using cohere. The `get_embedding_size()` method returns the size of the embeddings. The `embed()` method takes input data and embeds it using cohere, and can optionally preprocess the input data and/or print verbose output."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 140 executed in 2.36 seconds.\n",
      "Sub-task 140 results saved in 0.00 seconds.\n",
      "Sub-task 141 executed in 0.00 seconds.\n",
      "Sub-task 141 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " import math\n",
       "\n",
       "import numpy as np\n",
       "from sentence_transformers import SentenceTransformer\n",
       "\n",
       "\n",
       "class SBERTEmbedder:\n",
       "    def get_embedding_size(self):\n",
       "        return 356\n",
       "\n",
       "    def embed(\n",
       "        data,\n",
       "        key=\"content\",\n",
       "        model_name=\"all-MiniLM-L6-v2\",\n",
       "        cores=1,\n",
       "        gpu=False,\n",
       "        batch_size=128,\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n",
       "        \"\"\"\n",
       "        print(\"Embedding data\")\n",
       "        model = SentenceTransformer(model_name)\n",
       "        print(\"Model loaded\")\n",
       "\n",
       "        sentences = data[key].tolist()\n",
       "        unique_sentences = data[key].unique()\n",
       "        print(\"Unique sentences\", len(unique_sentences))\n",
       "\n",
       "        if cores == 1:\n",
       "            embeddings = model.encode(\n",
       "                unique_sentences, show_progress_bar=True, batch_size=batch_size\n",
       "            )\n",
       "        else:\n",
       "            devices = [\"cpu\"] * cores\n",
       "            if gpu:\n",
       "                devices = None  # use all CUDA devices\n",
       "\n",
       "            # Start the multi-process pool on multiple devices\n",
       "            print(\"Multi-process pool starting\")\n",
       "            pool = model.start_multi_process_pool(devices)\n",
       "            print(\"Multi-process pool started\")\n",
       "\n",
       "            chunk_size = math.ceil(len(unique_sentences) / cores)\n",
       "\n",
       "            # Compute the embeddings using the multi-process pool\n",
       "            embeddings = model.encode_multi_process(\n",
       "                unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size\n",
       "            )\n",
       "            model.stop_multi_process_pool(pool)\n",
       "\n",
       "        print(\"Embeddings computed\")\n",
       "\n",
       "        mapping = {\n",
       "            sentence: embedding\n",
       "            for sentence, embedding in zip(unique_sentences, embeddings)\n",
       "        }\n",
       "        embeddings = np.array([mapping[sentence] for sentence in sentences])\n",
       "\n",
       "        return embeddings\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code defines a SBERTEmbedder class that uses the SentenceTransformer module to embed sentences or text through a MiniLM language model, which uses mean pooling. It includes methods to get the size of the embedding, compute the embeddings of the data, and an option to use multi-processing for faster computation. The resulting embeddings are mapped to their respective sentences and returned as a numpy array."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def embed(\n",
       "    data,\n",
       "    key=\"content\",\n",
       "    model_name=\"all-MiniLM-L6-v2\",\n",
       "    cores=1,\n",
       "    gpu=False,\n",
       "    batch_size=128,\n",
       "):\n",
       "    \"\"\"\n",
       "        Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n",
       "        \"\"\"\n",
       "    print(\"Embedding data\")\n",
       "    model = SentenceTransformer(model_name)\n",
       "    print(\"Model loaded\")\n",
       "\n",
       "    sentences = data[key].tolist()\n",
       "    unique_sentences = data[key].unique()\n",
       "    print(\"Unique sentences\", len(unique_sentences))\n",
       "\n",
       "    if cores == 1:\n",
       "        embeddings = model.encode(\n",
       "            unique_sentences, show_progress_bar=True, batch_size=batch_size\n",
       "        )\n",
       "    else:\n",
       "        devices = [\"cpu\"] * cores\n",
       "        if gpu:\n",
       "            devices = None  # use all CUDA devices\n",
       "\n",
       "        # Start the multi-process pool on multiple devices\n",
       "        print(\"Multi-process pool starting\")\n",
       "        pool = model.start_multi_process_pool(devices)\n",
       "        print(\"Multi-process pool started\")\n",
       "\n",
       "        chunk_size = math.ceil(len(unique_sentences) / cores)\n",
       "\n",
       "        # Compute the embeddings using the multi-process pool\n",
       "        embeddings = model.encode_multi_process(\n",
       "            unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size\n",
       "        )\n",
       "        model.stop_multi_process_pool(pool)\n",
       "\n",
       "    print(\"Embeddings computed\")\n",
       "\n",
       "    mapping = {\n",
       "        sentence: embedding\n",
       "        for sentence, embedding in zip(unique_sentences, embeddings)\n",
       "    }\n",
       "    embeddings = np.array([mapping[sentence] for sentence in sentences])\n",
       "\n",
       "    return embeddings\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The embed function in the babydragon package can be used to embed sentences or text using the MiniLM language model, which uses mean pooling. The function takes in data, a key, a model name, the number of cores, whether to use the GPU, and a batch size. The function prints out progress updates, loads the SentenceTransformer model, identifies unique sentences, and computes the embeddings. If multiple cores are specified, the function starts a multi-process pool, computes the embeddings in parallel, and then stops the pool. Finally, the function maps each sentence to its corresponding embedding and returns the embeddings."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 142 executed in 9.66 seconds.\n",
      "Sub-task 142 results saved in 0.00 seconds.\n",
      "Sub-task 143 executed in 0.00 seconds.\n",
      "Sub-task 143 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def get_public_repos(self):\n",
       "    \"\"\"Returns a list of all public repos for the user.\"\"\"\n",
       "    user = self.github.get_user(self.username)\n",
       "    return user.get_repos()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a method `get_public_repos` that uses the `github` library to retrieve a list of all the public repositories associated with a user's account and returns this list."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def __init__(\n",
       "    self,\n",
       "    base_directory: str,\n",
       "    username=None,\n",
       "    repo_name=None,\n",
       "    code_parsers=None,\n",
       "    minify_code: bool = False,\n",
       "    remove_docstrings: bool = False,\n",
       "):\n",
       "    self.username = username\n",
       "    self.repo_name = repo_name\n",
       "    self.base_directory = base_directory\n",
       "    self.github = Github()\n",
       "    self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\n",
       "    repo_path = self.clone_repo(self.repo.clone_url)\n",
       "\n",
       "    OsProcessor.__init__(self, repo_path)\n",
       "    self.code_parsers = code_parsers or [\n",
       "        PythonParser(\n",
       "            repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\n",
       "        )\n",
       "    ]\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code defines an initializer function that takes in several parameters, including a base directory, username, repository name, and code parsers. It sets these inputs as instance variables and initializes a Github object and a repository object using the username and repo name parameters. It also clones the repository and uses the OsProcessor to initialize the base path. Finally, it sets a default code parser as the PythonParser with options to minify code and remove docstrings if desired."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 144 executed in 5.03 seconds.\n",
      "Sub-task 144 results saved in 0.00 seconds.\n",
      "Sub-task 145 executed in 0.00 seconds.\n",
      "Sub-task 145 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def process_repo(self, repo_path=None):\n",
       "    \"\"\"Processes the repo at the specified path.\n",
       "        If no path is specified, the repo at self.directory_path is processed.\n",
       "        Returns the list of parsed functions and classes.\"\"\"\n",
       "    if repo_path is None:\n",
       "        repo_path = self.directory_path\n",
       "\n",
       "    for code_parser in self.code_parsers:\n",
       "        code_parser.directory_path = repo_path\n",
       "        code_parser.process_directory(repo_path)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The `process_repo` method processes a code repository by running parsers on it. If no repo path is given it uses the directory_path attribute of the object. It then sets the directory_path attribute of each parser to the repo path before calling the `process_directory` method on each code parser. The method returns a list of parsed functions and classes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def clone_repo(self, repo_url: str):\n",
       "    \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\n",
       "    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
       "    target_directory = os.path.join(self.base_directory, repo_name)\n",
       "\n",
       "    if os.path.exists(target_directory):\n",
       "        shutil.rmtree(target_directory)\n",
       "\n",
       "    subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
       "\n",
       "    return target_directory\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function called \"clone_repo\" that takes in a repository URL as a string, clones the repository at that URL, and returns the path to the cloned repository. The function first extracts the name of the repository from the URL and sets the target directory as a combination of a base directory and the repository name. If the target directory already exists, it is deleted to ensure a fresh clone. The function then uses the subprocess module to run a \"git clone\" command, passing in the repository URL and target directory as arguments, which clones the repository to the target directory. Finally, the function returns the path to the cloned repository."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 146 executed in 7.07 seconds.\n",
      "Sub-task 146 results saved in 0.00 seconds.\n",
      "Sub-task 147 executed in 0.00 seconds.\n",
      "Sub-task 147 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def process_repos(self):\n",
       "    \"\"\"Processes all public repos for the user.\"\"\"\n",
       "    for repo in self.get_public_repos():\n",
       "        if not repo.private:\n",
       "            print(f\"Processing repo: {repo.name}\")\n",
       "            repo_path = self.clone_repo(repo.clone_url)\n",
       "            self.process_repo(repo_path)\n",
       "            shutil.rmtree(repo_path)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function called \"process_repos\" that processes all public repositories for the user. It loops through each repository, checks if it is private, prints the repository name, clones the repository, processes it, and then deletes the cloned repository."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 148 executed in 0.82 seconds.\n",
      "Sub-task 148 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def get_repo(self, repo_name):\n",
       "    \"\"\"Returns the repo with the specified name.\"\"\"\n",
       "    user = self.github.get_user(self.username)\n",
       "    return user.get_repo(repo_name)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a method `get_repo` that takes a repository name as input. It retrieves the user using the Github API using `self.username` and returns the repository with the specified name using `get_repo` method on the user object."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 149 executed in 3.82 seconds.\n",
      "Sub-task 149 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def process_single_repo(self):\n",
       "\n",
       "    repo = self.get_repo(self.repo_name)\n",
       "    print(f\"Processing repo: {self.repo_name}\")\n",
       "    repo_path = self.clone_repo(repo.clone_url)\n",
       "    self.process_repo(repo_path)\n",
       "    shutil.rmtree(repo_path)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The given code defines a function that processes a single repository. It retrieves repository details using the provided repository name, clones the repository, processes it and deletes the cloned repository after processing. It also prints a message stating the name of the repository it processed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 150 executed in 2.49 seconds.\n",
      "Sub-task 150 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def get_issues(self, state=\"open\"):\n",
       "    \"\"\"\n",
       "        Returns a list of all issues in the repo with the specified state.\n",
       "        \"\"\"\n",
       "    issues = []\n",
       "    for issue in self.repo.get_issues(state=state):\n",
       "        issues.append(issue)\n",
       "    return issues\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code is a method in the babydragon package that retrieves a list of issues related to a repository, and allows for filtering by their state (open or closed)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 151 executed in 1.77 seconds.\n",
      "Sub-task 151 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def parse_issues(self, state=\"open\"):\n",
       "    \"\"\"\n",
       "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
       "        Each dict contains the issue number, title, body, and labels.\n",
       "        \"\"\"\n",
       "    parsed_issues = []\n",
       "    issues = self.get_issues(state=state)\n",
       "    for issue in issues:\n",
       "        parsed_issue = {\n",
       "            \"number\": issue.number,\n",
       "            \"title\": issue.title,\n",
       "            \"body\": issue.body,\n",
       "            \"labels\": [label.name for label in issue.labels],\n",
       "        }\n",
       "        parsed_issues.append(parsed_issue)\n",
       "    return parsed_issues\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function that takes in a state parameter (with \"open\" being the default value) and parses all issues in the repository with that state. It returns a list of dictionaries containing the issue number, title, body, and labels for each parsed issue. It achieves this by calling another function to get the issues with the specified state and then iterating through each issue to extract the desired information and append it to the list of parsed issues."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 152 executed in 6.54 seconds.\n",
      "Sub-task 152 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def get_commits(self):\n",
       "    \"\"\"\n",
       "        Returns a list of all commits in the main branch of the repository.\n",
       "        \"\"\"\n",
       "    commits = []\n",
       "    branch = self.repo.get_branch(\"main\")\n",
       "    for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
       "        commits.append(commit)\n",
       "    return commits\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code is a method that retrieves a list of all commits in the main branch of a git repository using the babydragon package. Returned as a list, it uses a for loop to iterate through the commits and appends each commit to the list."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 153 executed in 0.35 seconds.\n",
      "Sub-task 153 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def parse_commits(self):\n",
       "    \"\"\"\n",
       "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
       "        Each dict contains the commit sha, commit message, and author information.\n",
       "        \"\"\"\n",
       "    parsed_commits = []\n",
       "    commits = self.get_commits()\n",
       "    for commit in commits:\n",
       "        parsed_commit = {\n",
       "            \"sha\": commit.sha,\n",
       "            \"message\": commit.commit.message,\n",
       "            \"author\": {\n",
       "                \"name\": commit.commit.author.name,\n",
       "                \"email\": commit.commit.author.email,\n",
       "                \"date\": commit.commit.author.date,\n",
       "            },\n",
       "        }\n",
       "        parsed_commits.append(parsed_commit)\n",
       "    return parsed_commits\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code parses the commits in the main branch of a repository and returns a list of dictionaries containing the commit SHA, message, and author information (name, email, date)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 154 executed in 1.88 seconds.\n",
      "Sub-task 154 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...\n",
      "RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " import os\n",
       "import shutil\n",
       "import subprocess\n",
       "from typing import List\n",
       "\n",
       "from github import Github\n",
       "\n",
       "from babydragon.processors.os_processor import OsProcessor\n",
       "from babydragon.processors.parsers.python_parser import PythonParser\n",
       "\n",
       "\n",
       "class GithubProcessor(OsProcessor):\n",
       "    def __init__(\n",
       "        self,\n",
       "        base_directory: str,\n",
       "        username=None,\n",
       "        repo_name=None,\n",
       "        code_parsers=None,\n",
       "        minify_code: bool = False,\n",
       "        remove_docstrings: bool = False,\n",
       "    ):\n",
       "        self.username = username\n",
       "        self.repo_name = repo_name\n",
       "        self.base_directory = base_directory\n",
       "        self.github = Github()\n",
       "        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\n",
       "        repo_path = self.clone_repo(self.repo.clone_url)\n",
       "\n",
       "        OsProcessor.__init__(self, repo_path)\n",
       "        self.code_parsers = code_parsers or [\n",
       "            PythonParser(\n",
       "                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\n",
       "            )\n",
       "        ]\n",
       "\n",
       "    def get_public_repos(self):\n",
       "        \"\"\"Returns a list of all public repos for the user.\"\"\"\n",
       "        user = self.github.get_user(self.username)\n",
       "        return user.get_repos()\n",
       "\n",
       "    def clone_repo(self, repo_url: str):\n",
       "        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\n",
       "        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
       "        target_directory = os.path.join(self.base_directory, repo_name)\n",
       "\n",
       "        if os.path.exists(target_directory):\n",
       "            shutil.rmtree(target_directory)\n",
       "\n",
       "        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
       "\n",
       "        return target_directory\n",
       "\n",
       "    def process_repo(self, repo_path=None):\n",
       "        \"\"\"Processes the repo at the specified path.\n",
       "        If no path is specified, the repo at self.directory_path is processed.\n",
       "        Returns the list of parsed functions and classes.\"\"\"\n",
       "        if repo_path is None:\n",
       "            repo_path = self.directory_path\n",
       "\n",
       "        for code_parser in self.code_parsers:\n",
       "            code_parser.directory_path = repo_path\n",
       "            code_parser.process_directory(repo_path)\n",
       "\n",
       "    def process_repos(self):\n",
       "        \"\"\"Processes all public repos for the user.\"\"\"\n",
       "        for repo in self.get_public_repos():\n",
       "            if not repo.private:\n",
       "                print(f\"Processing repo: {repo.name}\")\n",
       "                repo_path = self.clone_repo(repo.clone_url)\n",
       "                self.process_repo(repo_path)\n",
       "                shutil.rmtree(repo_path)\n",
       "\n",
       "    def get_repo(self, repo_name):\n",
       "        \"\"\"Returns the repo with the specified name.\"\"\"\n",
       "        user = self.github.get_user(self.username)\n",
       "        return user.get_repo(repo_name)\n",
       "\n",
       "    def process_single_repo(self):\n",
       "\n",
       "        repo = self.get_repo(self.repo_name)\n",
       "        print(f\"Processing repo: {self.repo_name}\")\n",
       "        repo_path = self.clone_repo(repo.clone_url)\n",
       "        self.process_repo(repo_path)\n",
       "        shutil.rmtree(repo_path)\n",
       "\n",
       "    def get_issues(self, state=\"open\"):\n",
       "        \"\"\"\n",
       "        Returns a list of all issues in the repo with the specified state.\n",
       "        \"\"\"\n",
       "        issues = []\n",
       "        for issue in self.repo.get_issues(state=state):\n",
       "            issues.append(issue)\n",
       "        return issues\n",
       "\n",
       "    def parse_issues(self, state=\"open\"):\n",
       "        \"\"\"\n",
       "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
       "        Each dict contains the issue number, title, body, and labels.\n",
       "        \"\"\"\n",
       "        parsed_issues = []\n",
       "        issues = self.get_issues(state=state)\n",
       "        for issue in issues:\n",
       "            parsed_issue = {\n",
       "                \"number\": issue.number,\n",
       "                \"title\": issue.title,\n",
       "                \"body\": issue.body,\n",
       "                \"labels\": [label.name for label in issue.labels],\n",
       "            }\n",
       "            parsed_issues.append(parsed_issue)\n",
       "        return parsed_issues\n",
       "\n",
       "    def get_commits(self):\n",
       "        \"\"\"\n",
       "        Returns a list of all commits in the main branch of the repository.\n",
       "        \"\"\"\n",
       "        commits = []\n",
       "        branch = self.repo.get_branch(\"main\")\n",
       "        for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
       "            commits.append(commit)\n",
       "        return commits\n",
       "\n",
       "    def parse_commits(self):\n",
       "        \"\"\"\n",
       "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
       "        Each dict contains the commit sha, commit message, and author information.\n",
       "        \"\"\"\n",
       "        parsed_commits = []\n",
       "        commits = self.get_commits()\n",
       "        for commit in commits:\n",
       "            parsed_commit = {\n",
       "                \"sha\": commit.sha,\n",
       "                \"message\": commit.commit.message,\n",
       "                \"author\": {\n",
       "                    \"name\": commit.commit.author.name,\n",
       "                    \"email\": commit.commit.author.email,\n",
       "                    \"date\": commit.commit.author.date,\n",
       "                },\n",
       "            }\n",
       "            parsed_commits.append(parsed_commit)\n",
       "        return parsed_commits\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code is a collection of functions for processing Github repositories. It uses the Github API and various code parsers to clone a repo, parse its code (e.g. minify it), and extract information such as issues and commits. It also provides functions for processing all public repos for a user and for parsing issues and commits in a single repo."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 155 executed in 5.69 seconds.\n",
      "Sub-task 155 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def __init__(self, directory_path: str):\n",
       "    self.directory_path = directory_path\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines an instance method __init__ with an argument directory_path of string data type. The method initializes an object attribute of the instance variable directory_path with the value passed to it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 156 executed in 0.46 seconds.\n",
      "Sub-task 156 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\n",
       "    \"\"\"Returns a list of all files in a directory\"\"\"\n",
       "    if directory_path is None:\n",
       "        directory_path = self.directory_path\n",
       "\n",
       "    all_files = []\n",
       "    for root, _, files in os.walk(directory_path):\n",
       "        for file in files:\n",
       "            all_files.append(os.path.join(root, file))\n",
       "\n",
       "    return all_files\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This method returns a list of all files in a directory given its path. If no path is provided, it defaults to the directory path that was previously set. It utilizes the `os.walk` method to iterate through all files in the directory and adds them to a list, which is then returned."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 157 executed in 4.76 seconds.\n",
      "Sub-task 157 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def get_file_extension(self, file_path: str) -> str:\n",
       "    \"\"\"Returns the extension of a file\"\"\"\n",
       "    return Path(file_path).suffix\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code is a method to get the extension of a file, given the file path. It takes a file path string as input, uses the Path function to extract the extension, and returns it as a string."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\n",
       "    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\n",
       "    if directory_path is None:\n",
       "        directory_path = self.directory_path\n",
       "\n",
       "    subdirectories = [\n",
       "        os.path.join(directory_path, d)\n",
       "        for d in os.listdir(directory_path)\n",
       "        if os.path.isdir(os.path.join(directory_path, d))\n",
       "    ]\n",
       "\n",
       "    return subdirectories\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The \"get_subdirectories\" function from the babydragon package takes an optional directory path and returns a list of all subdirectories in that directory. If no directory path is provided, it falls back to a default directory path. The function utilizes the \"os\" module to identify and filter out subdirectories from a list of files and returns only the subdirectories as a list."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def create_directory(self, directory_path: str) -> None:\n",
       "    \"\"\"Creates a directory if it does not exist\"\"\"\n",
       "    if not os.path.exists(directory_path):\n",
       "        os.makedirs(directory_path)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code defines a function that creates a directory by taking a directory path as input. It uses the os module to check if the directory already exists. If it doesn't exist, it creates the directory using the os.makedirs() method."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def get_files_with_extension(\n",
       "    self, extension: str, directory_path: Optional[str] = None\n",
       ") -> List[str]:\n",
       "    \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\n",
       "    if directory_path is None:\n",
       "        directory_path = self.directory_path\n",
       "\n",
       "    all_files = self.get_all_files(directory_path)\n",
       "    files_with_extension = [file for file in all_files if file.endswith(extension)]\n",
       "\n",
       "    return files_with_extension\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The `get_files_with_extension` method of the babydragon package returns a list of all files in a specified directory with a given file extension. If no directory path is specified, it defaults to the package's directory. The method first retrieves all files in the directory (using the `get_all_files` method), and then filters the list to only include files with the specified extension. The filtered list is then returned."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 158 executed in 12.68 seconds.\n",
      "Sub-task 158 results saved in 0.00 seconds.\n",
      "Sub-task 159 executed in 0.00 seconds.\n",
      "Sub-task 159 results saved in 0.00 seconds.\n",
      "Sub-task 160 executed in 0.00 seconds.\n",
      "Sub-task 160 results saved in 0.00 seconds.\n",
      "Sub-task 161 executed in 0.00 seconds.\n",
      "Sub-task 161 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def delete_directory(self, directory_path: str) -> None:\n",
       "    \"\"\"Deletes a directory if it exists\"\"\"\n",
       "    if os.path.exists(directory_path):\n",
       "        shutil.rmtree(directory_path)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function called \"delete_directory\" that receives a \"directory_path\" parameter as a string. The function checks if the directory path exists and, if it does, it removes it using shutil.rmtree() function. The function returns None."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 162 executed in 1.41 seconds.\n",
      "Sub-task 162 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def copy_file(self, source_path: str, destination_path: str) -> None:\n",
       "    \"\"\"Copies a file from one location to another\"\"\"\n",
       "    shutil.copy2(source_path, destination_path)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function named \"copy_file\" that takes in two parameters, source_path and destination_path as strings. It then copies a file from the source_path to the destination_path using the Python Standard Library module, shutil."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 163 executed in 2.83 seconds.\n",
      "Sub-task 163 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def move_file(self, source_path: str, destination_path: str) -> None:\n",
       "    \"\"\"Moves a file from one location to another\"\"\"\n",
       "    shutil.move(source_path, destination_path)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function called `move_file` that moves a file from a given source path to a destination path, and it uses the `shutil.move` method to perform the file transfer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 164 executed in 3.00 seconds.\n",
      "Sub-task 164 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def __init__(self, repo_name):\n",
       "    self.g = Github()\n",
       "    self.repo = self.g.get_repo(repo_name)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code initializes a Github object and gets a specific repository object based on the repo_name passed as a parameter to the constructor."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " import os\n",
       "import shutil\n",
       "from pathlib import Path\n",
       "from typing import Dict, List, Optional\n",
       "\n",
       "\n",
       "class OsProcessor:\n",
       "    def __init__(self, directory_path: str):\n",
       "        self.directory_path = directory_path\n",
       "\n",
       "    def get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\n",
       "        \"\"\"Returns a list of all files in a directory\"\"\"\n",
       "        if directory_path is None:\n",
       "            directory_path = self.directory_path\n",
       "\n",
       "        all_files = []\n",
       "        for root, _, files in os.walk(directory_path):\n",
       "            for file in files:\n",
       "                all_files.append(os.path.join(root, file))\n",
       "\n",
       "        return all_files\n",
       "\n",
       "    def get_files_with_extension(\n",
       "        self, extension: str, directory_path: Optional[str] = None\n",
       "    ) -> List[str]:\n",
       "        \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\n",
       "        if directory_path is None:\n",
       "            directory_path = self.directory_path\n",
       "\n",
       "        all_files = self.get_all_files(directory_path)\n",
       "        files_with_extension = [file for file in all_files if file.endswith(extension)]\n",
       "\n",
       "        return files_with_extension\n",
       "\n",
       "    def get_file_extension(self, file_path: str) -> str:\n",
       "        \"\"\"Returns the extension of a file\"\"\"\n",
       "        return Path(file_path).suffix\n",
       "\n",
       "    def get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\n",
       "        \"\"\"Returns a list of all subdirectories in a directory\"\"\"\n",
       "        if directory_path is None:\n",
       "            directory_path = self.directory_path\n",
       "\n",
       "        subdirectories = [\n",
       "            os.path.join(directory_path, d)\n",
       "            for d in os.listdir(directory_path)\n",
       "            if os.path.isdir(os.path.join(directory_path, d))\n",
       "        ]\n",
       "\n",
       "        return subdirectories\n",
       "\n",
       "    def create_directory(self, directory_path: str) -> None:\n",
       "        \"\"\"Creates a directory if it does not exist\"\"\"\n",
       "        if not os.path.exists(directory_path):\n",
       "            os.makedirs(directory_path)\n",
       "\n",
       "    def delete_directory(self, directory_path: str) -> None:\n",
       "        \"\"\"Deletes a directory if it exists\"\"\"\n",
       "        if os.path.exists(directory_path):\n",
       "            shutil.rmtree(directory_path)\n",
       "\n",
       "    def copy_file(self, source_path: str, destination_path: str) -> None:\n",
       "        \"\"\"Copies a file from one location to another\"\"\"\n",
       "        shutil.copy2(source_path, destination_path)\n",
       "\n",
       "    def move_file(self, source_path: str, destination_path: str) -> None:\n",
       "        \"\"\"Moves a file from one location to another\"\"\"\n",
       "        shutil.move(source_path, destination_path)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a class `OsProcessor` that provides methods to work with files and directories. It contains functions to get all the files in a directory, get files with a particular extension, get the extension of a file, get all subdirectories in a directory, create and delete directories, and copy or move a file from one location to another. The functions take optional parameters such as the directory path and return lists or None. The required modules for the code include `os`, `shutil`, and `pathlib`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 165 executed in 7.21 seconds.\n",
      "Sub-task 165 results saved in 0.00 seconds.\n",
      "Sub-task 166 executed in 0.00 seconds.\n",
      "Sub-task 166 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " from github import Github\n",
       "\n",
       "\n",
       "class IssueParser:\n",
       "    def __init__(self, repo_name):\n",
       "        self.g = Github()\n",
       "        self.repo = self.g.get_repo(repo_name)\n",
       "\n",
       "    def get_issues(self, state=\"open\"):\n",
       "        \"\"\"\n",
       "        Returns a list of all issues in the repo with the specified state.\n",
       "        \"\"\"\n",
       "        issues = []\n",
       "        for issue in self.repo.get_issues(state=state):\n",
       "            issues.append(issue)\n",
       "        return issues\n",
       "\n",
       "    def parse_issues(self, state=\"open\"):\n",
       "        \"\"\"\n",
       "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
       "        Each dict contains the issue number, title, body, and labels.\n",
       "        \"\"\"\n",
       "        parsed_issues = []\n",
       "        issues = self.get_issues(state=state)\n",
       "        for issue in issues:\n",
       "            parsed_issue = {\n",
       "                \"number\": issue.number,\n",
       "                \"title\": issue.title,\n",
       "                \"body\": issue.body,\n",
       "                \"labels\": [label.name for label in issue.labels],\n",
       "            }\n",
       "            parsed_issues.append(parsed_issue)\n",
       "        return parsed_issues\n",
       "\n",
       "\n",
       "class CommitParser:\n",
       "    def __init__(self, repo_name):\n",
       "        self.g = Github()\n",
       "        self.repo = self.g.get_repo(repo_name)\n",
       "\n",
       "    def get_commits(self):\n",
       "        \"\"\"\n",
       "        Returns a list of all commits in the main branch of the repository.\n",
       "        \"\"\"\n",
       "        commits = []\n",
       "        branch = self.repo.get_branch(\"main\")\n",
       "        for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
       "            commits.append(commit)\n",
       "        return commits\n",
       "\n",
       "    def parse_commits(self):\n",
       "        \"\"\"\n",
       "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
       "        Each dict contains the commit sha, commit message, and author information.\n",
       "        \"\"\"\n",
       "        parsed_commits = []\n",
       "        commits = self.get_commits()\n",
       "        for commit in commits:\n",
       "            parsed_commit = {\n",
       "                \"sha\": commit.sha,\n",
       "                \"message\": commit.commit.message,\n",
       "                \"author\": {\n",
       "                    \"name\": commit.commit.author.name,\n",
       "                    \"email\": commit.commit.author.email,\n",
       "                    \"date\": commit.commit.author.date,\n",
       "                },\n",
       "            }\n",
       "            parsed_commits.append(parsed_commit)\n",
       "        return parsed_commits\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code imports the Github library and defines classes IssueParser and CommitParser. IssueParser gets a list of issues and parses each issue's number, title, body, and labels. CommitParser gets a list of commits and parses each commit's sha, commit message, and author information. Both classes retrieve data from a specified Github repository."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 167 executed in 3.59 seconds.\n",
      "Sub-task 167 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def __init__(self, code: str = None):\n",
       "\n",
       "    self.code = code\n",
       "    self.output_code = None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This is a constructor method for a class, which takes a string parameter called `code`. It initializes two instance variables, `code` and `output_code`. The `code` variable receives the value passed as the parameter, and the `output_code` variable is initialized to `None`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 168 executed in 3.27 seconds.\n",
      "Sub-task 168 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def minify(self):\n",
       "    if self.code:\n",
       "        self.output_code = self.minify_code(self.code)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a `minify` function which takes `self` as an argument. If `self` has a non-empty `code`, then the `minify_code` function is executed on the `code` and the resulting value is stored in `output_code`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 169 executed in 2.25 seconds.\n",
      "Sub-task 169 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def get_minified_code(self):\n",
       "    if not self.output_code:\n",
       "        self.minify()\n",
       "    return self.output_code\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code is a method that retrieves the minified version of a code. If the output code is not already present, it invokes the minify() method to produce it and then returns the minified version."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 170 executed in 1.94 seconds.\n",
      "Sub-task 170 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "@staticmethod\n",
       "def minify_code(code: str) -> str:\n",
       "    return minify(code)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The given code defines a static method named \"minify_code\" which takes in a string argument named \"code\". It then calls another function named \"minify\" with \"code\" as its argument and returns its result, which is also a string."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 171 executed in 3.57 seconds.\n",
      "Sub-task 171 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def __init__(self):\n",
       "    self.function_source_codes = []\n",
       "    self.function_nodes = []\n",
       "    self.class_source_codes = []\n",
       "    self.class_nodes = []\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a class constructor that initializes four empty lists: function_source_codes, function_nodes, class_source_codes, and class_nodes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " @staticmethod\n",
       "def extract_docstring(function_def: cst.FunctionDef) -> str:\n",
       "    docstring = None\n",
       "\n",
       "    for stmt in function_def.body.body:\n",
       "        if isinstance(stmt, cst.SimpleStatementLine):\n",
       "            for expr in stmt.body:\n",
       "                if isinstance(expr, cst.Expr) and isinstance(\n",
       "                    expr.value, cst.SimpleString\n",
       "                ):\n",
       "                    docstring = expr.value.value.strip('\"').strip(\"'\")\n",
       "                    break\n",
       "        if docstring is not None:\n",
       "            break\n",
       "\n",
       "    if docstring is not None:\n",
       "        return docstring.strip()\n",
       "    else:\n",
       "        function_name = function_def.name.value\n",
       "        return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code defines a static method that extracts a docstring from a given function definition. It iterates over the body of the function definition and looks for a simple string expression to extract as the docstring. If a docstring is found, it is returned stripped of any surrounding quotes. If no docstring is found, an error message is returned."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 172 executed in 5.29 seconds.\n",
      "Sub-task 172 results saved in 0.00 seconds.\n",
      "Sub-task 173 executed in 0.00 seconds.\n",
      "Sub-task 173 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "    \"\"\"This method is called for every FunctionDef node in the tree.\n",
       "        and it does the following:\n",
       "        1. Gets the source code for the node\n",
       "        2. Adds the node to the list of function nodes\n",
       "        3. Adds the source code to the list of function source codes\n",
       "        \"\"\"\n",
       "    function_source_code = cst.Module([]).code_for_node(node)\n",
       "    self.function_nodes.append(node)\n",
       "    self.function_source_codes.append(function_source_code)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The given code defines a method called \"visit_FunctionDef\" which is called for each \"FunctionDef\" node in the tree. It performs three tasks by getting the source code for the node, adding the node to the function node list, and adding the source code to the function source code list."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 174 executed in 3.88 seconds.\n",
      "Sub-task 174 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
       "    \"\"\"This method is called for every ClassDef node in the tree.\n",
       "        and it does the following:\n",
       "        1. Gets the source code for the node\n",
       "        2. Adds the node to the list of class nodes\n",
       "        3. Adds the source code to the list of class source codes\n",
       "        \"\"\"\n",
       "    class_source_code = cst.Module([]).code_for_node(node)\n",
       "    self.class_nodes.append(node)\n",
       "    self.class_source_codes.append(class_source_code)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The given code is a method in the babydragon package that is called for every ClassDef node in the tree. It obtains the source code for the node, adds the node to the list of class nodes, and adds the source code to the list of class source codes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 175 executed in 2.97 seconds.\n",
      "Sub-task 175 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def __init__(\n",
       "    self,\n",
       "    directory_path: str,\n",
       "    visitor: Optional[FunctionAndClassVisitor] = None,\n",
       "    minify_code: bool = False,\n",
       "    remove_docstrings: bool = False,\n",
       "):\n",
       "    super().__init__(directory_path)\n",
       "    self.visitor = visitor if visitor else FunctionAndClassVisitor()\n",
       "    self.minify_code = minify_code\n",
       "    self.remove_docstrings = remove_docstrings\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code initializes a class constructor which taking in directory path, optional visitor, and optional bool parameters to minify code and remove docstrings. It inherits a superclass and sets the visitor, minify_code and remove_docstrings attributes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 176 executed in 2.17 seconds.\n",
      "Sub-task 176 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def remove_docstring(self, tree: cst.Module) -> str:\n",
       "    \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\n",
       "\n",
       "    # Remove docstrings using a transformer\n",
       "    class DocstringRemover(cst.CSTTransformer):\n",
       "        def leave_FunctionDef(\n",
       "            self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
       "        ) -> cst.FunctionDef:\n",
       "            docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
       "            if docstring.startswith(\"No docstring\"):\n",
       "                return updated_node\n",
       "\n",
       "            return updated_node.with_changes(\n",
       "                body=updated_node.body.with_changes(\n",
       "                    body=[\n",
       "                        stmt\n",
       "                        for stmt in updated_node.body.body\n",
       "                        if not (\n",
       "                            isinstance(stmt, cst.SimpleStatementLine)\n",
       "                            and any(\n",
       "                                isinstance(expr, cst.Expr)\n",
       "                                and isinstance(expr.value, cst.SimpleString)\n",
       "                                for expr in stmt.body\n",
       "                            )\n",
       "                        )\n",
       "                    ]\n",
       "                )\n",
       "            )\n",
       "\n",
       "    tree = tree.visit(DocstringRemover())\n",
       "    return tree.code\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function that removes docstrings from the given Python code using a transformer. It visits the code tree, removes docstrings from all functions and returns the code without docstrings."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 177 executed in 2.71 seconds.\n",
      "Sub-task 177 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def leave_FunctionDef(\n",
       "    self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
       ") -> cst.FunctionDef:\n",
       "    docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
       "    if docstring.startswith(\"No docstring\"):\n",
       "        return updated_node\n",
       "\n",
       "    return updated_node.with_changes(\n",
       "        body=updated_node.body.with_changes(\n",
       "            body=[\n",
       "                stmt\n",
       "                for stmt in updated_node.body.body\n",
       "                if not (\n",
       "                    isinstance(stmt, cst.SimpleStatementLine)\n",
       "                    and any(\n",
       "                        isinstance(expr, cst.Expr)\n",
       "                        and isinstance(expr.value, cst.SimpleString)\n",
       "                        for expr in stmt.body\n",
       "                    )\n",
       "                )\n",
       "            ]\n",
       "        )\n",
       "    )\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function that takes in a Python FunctionDef node and its updated counterpart as input. It extracts the docstring from the original node and if it starts with the phrase \"No docstring\", it returns the updated node. Otherwise, it modifies the body of the updated node by removing any statements that contain a string literal expression. The modified node is then returned."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 178 executed in 5.43 seconds.\n",
      "Sub-task 178 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def _process_file(self, file_path: str):\n",
       "    \"\"\"This method is called for every file in the directory.\n",
       "        It does the following:\n",
       "        1. Reads the file\n",
       "        2. Parses the file\n",
       "        3. Visits the file with the visitor\n",
       "        \"\"\"\n",
       "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
       "        source_code = file.read()\n",
       "\n",
       "    try:\n",
       "        tree = cst.parse_module(source_code)\n",
       "    except cst.ParserSyntaxError:\n",
       "        print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
       "        return\n",
       "\n",
       "    tree.visit(self.visitor)\n",
       "\n",
       "    # Remove docstrings if specified\n",
       "    if self.remove_docstrings:\n",
       "        source_code = self.remove_docstring(source_code, tree)\n",
       "\n",
       "    # Minify the code if specified\n",
       "    if self.minify_code:\n",
       "        minifier = PythonMinifier(source_code)\n",
       "        source_code = minifier.get_minified_code()\n",
       "\n",
       "    # Add the processed code to the corresponding list in the visitor\n",
       "    self.visitor.function_source_codes.append(source_code)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The given code defines a method named `_process_file` that takes a file path as input. This method reads the file, parses it, and visits it with a visitor. It then removes docstrings and minifies the code if specified. Finally, it adds the processed code to the corresponding list in the visitor."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 179 executed in 2.27 seconds.\n",
      "Sub-task 179 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def process_file(self, file_path: str):\n",
       "    \"\"\"This method is called for every file in the directory.\n",
       "        It does the following:\n",
       "        1. Runs flake8 on the file\n",
       "        if flake8 returns a non-zero exit code, it means the file has a syntax error\n",
       "        2. Reads the file\n",
       "        3. Parses the file\n",
       "        4. Visits the file with the visitor\n",
       "\n",
       "        \"\"\"\n",
       "    result = subprocess.run(\n",
       "        [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
       "    )\n",
       "\n",
       "    if result.returncode != 0:\n",
       "        print(f\"Skipping file with syntax error: {file_path}\")\n",
       "        print(result.stderr.decode(\"utf-8\"))\n",
       "        return\n",
       "\n",
       "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
       "        source_code = f.read()\n",
       "\n",
       "    try:\n",
       "        tree = cst.parse_module(source_code)\n",
       "        tree.visit(self.visitor)\n",
       "    except cst.ParserSyntaxError as e:\n",
       "        print(f\"Syntax error: {e}\")\n",
       "        print(f\"Skipping file with syntax error: {file_path}\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The `process_file` method checks for syntax errors in a file using the `flake8` tool. If no syntax errors are found, it reads the file, parses it, and visits it with a provided visitor. If a syntax error is found, the file is skipped and an error message is printed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 180 executed in 2.85 seconds.\n",
      "Sub-task 180 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def process_directory(\n",
       "    self,\n",
       ") -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\n",
       "    \"\"\"This method is called for every directory.\n",
       "        It does the following:\n",
       "        1. Gets all the python files in the directory\n",
       "        2. Processes each file\n",
       "        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\n",
       "        \"\"\"\n",
       "    function_source_codes = []\n",
       "    class_source_codes = []\n",
       "\n",
       "    python_files = self.get_files_with_extension(\".py\")\n",
       "\n",
       "    for file_path in python_files:\n",
       "        self._process_file(file_path)\n",
       "\n",
       "    function_source_codes = self.visitor.function_source_codes\n",
       "    function_nodes = self.visitor.function_nodes\n",
       "    class_source_codes = self.visitor.class_source_codes\n",
       "    class_nodes = self.visitor.class_nodes\n",
       "\n",
       "    return function_source_codes, class_source_codes, function_nodes, class_nodes\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a method called \"process_directory\" which takes no arguments and returns four lists. The method finds all the python files in a given directory, processes each file to extract relevant information, and outputs lists of function and class source codes and corresponding nodes. The method relies on a \"visitor\" object to process the files and extract the relevant information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 181 executed in 3.43 seconds.\n",
      "Sub-task 181 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def visit_Call(self, node: cst.Call) -> None:\n",
       "    function_name = None\n",
       "    if isinstance(node.func, cst.Name):\n",
       "        function_name = node.func.value\n",
       "\n",
       "    if function_name:\n",
       "        pos = self.get_metadata(PositionProvider, node).start\n",
       "        print(\n",
       "            f\"Function '{function_name}' called at line {pos.line}, column {pos.column} with arguments:\"\n",
       "        )\n",
       "\n",
       "        for arg in node.args:\n",
       "            arg_start_pos = self.get_metadata(PositionProvider, arg).start\n",
       "            arg_value = arg.value\n",
       "            if isinstance(arg_value, cst.SimpleString):\n",
       "                arg_value = arg_value.evaluated_value\n",
       "            print(\n",
       "                f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\n",
       "            )\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " This code defines a function that takes a node object, extracts the name of a function from it, and prints information about its arguments and location within the code. It makes use of metadata and position providers to gather information about the function call and its arguments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " import os\n",
       "import subprocess\n",
       "from typing import List, Optional, Tuple, Union\n",
       "\n",
       "import libcst as cst\n",
       "from python_minifier import minify\n",
       "\n",
       "from babydragon.processors.os_processor import OsProcessor\n",
       "\n",
       "\n",
       "class PythonMinifier:\n",
       "    def __init__(self, code: str = None):\n",
       "\n",
       "        self.code = code\n",
       "        self.output_code = None\n",
       "\n",
       "    def minify(self):\n",
       "        if self.code:\n",
       "            self.output_code = self.minify_code(self.code)\n",
       "\n",
       "    def get_minified_code(self):\n",
       "        if not self.output_code:\n",
       "            self.minify()\n",
       "        return self.output_code\n",
       "\n",
       "    @staticmethod\n",
       "    def minify_code(code: str) -> str:\n",
       "        return minify(code)\n",
       "\n",
       "\n",
       "class PythonDocstringExtractor:\n",
       "    @staticmethod\n",
       "    def extract_docstring(function_def: cst.FunctionDef) -> str:\n",
       "        docstring = None\n",
       "\n",
       "        for stmt in function_def.body.body:\n",
       "            if isinstance(stmt, cst.SimpleStatementLine):\n",
       "                for expr in stmt.body:\n",
       "                    if isinstance(expr, cst.Expr) and isinstance(\n",
       "                        expr.value, cst.SimpleString\n",
       "                    ):\n",
       "                        docstring = expr.value.value.strip('\"').strip(\"'\")\n",
       "                        break\n",
       "            if docstring is not None:\n",
       "                break\n",
       "\n",
       "        if docstring is not None:\n",
       "            return docstring.strip()\n",
       "        else:\n",
       "            function_name = function_def.name.value\n",
       "            return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n",
       "\n",
       "\n",
       "class FunctionAndClassVisitor(cst.CSTVisitor):\n",
       "    def __init__(self):\n",
       "        self.function_source_codes = []\n",
       "        self.function_nodes = []\n",
       "        self.class_source_codes = []\n",
       "        self.class_nodes = []\n",
       "\n",
       "    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "        \"\"\"This method is called for every FunctionDef node in the tree.\n",
       "        and it does the following:\n",
       "        1. Gets the source code for the node\n",
       "        2. Adds the node to the list of function nodes\n",
       "        3. Adds the source code to the list of function source codes\n",
       "        \"\"\"\n",
       "        function_source_code = cst.Module([]).code_for_node(node)\n",
       "        self.function_nodes.append(node)\n",
       "        self.function_source_codes.append(function_source_code)\n",
       "\n",
       "    def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
       "        \"\"\"This method is called for every ClassDef node in the tree.\n",
       "        and it does the following:\n",
       "        1. Gets the source code for the node\n",
       "        2. Adds the node to the list of class nodes\n",
       "        3. Adds the source code to the list of class source codes\n",
       "        \"\"\"\n",
       "        class_source_code = cst.Module([]).code_for_node(node)\n",
       "        self.class_nodes.append(node)\n",
       "        self.class_source_codes.append(class_source_code)\n",
       "\n",
       "\n",
       "class PythonParser(OsProcessor):\n",
       "    def __init__(\n",
       "        self,\n",
       "        directory_path: str,\n",
       "        visitor: Optional[FunctionAndClassVisitor] = None,\n",
       "        minify_code: bool = False,\n",
       "        remove_docstrings: bool = False,\n",
       "    ):\n",
       "        super().__init__(directory_path)\n",
       "        self.visitor = visitor if visitor else FunctionAndClassVisitor()\n",
       "        self.minify_code = minify_code\n",
       "        self.remove_docstrings = remove_docstrings\n",
       "\n",
       "    def remove_docstring(self, tree: cst.Module) -> str:\n",
       "        \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\n",
       "\n",
       "        # Remove docstrings using a transformer\n",
       "        class DocstringRemover(cst.CSTTransformer):\n",
       "            def leave_FunctionDef(\n",
       "                self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
       "            ) -> cst.FunctionDef:\n",
       "                docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
       "                if docstring.startswith(\"No docstring\"):\n",
       "                    return updated_node\n",
       "\n",
       "                return updated_node.with_changes(\n",
       "                    body=updated_node.body.with_changes(\n",
       "                        body=[\n",
       "                            stmt\n",
       "                            for stmt in updated_node.body.body\n",
       "                            if not (\n",
       "                                isinstance(stmt, cst.SimpleStatementLine)\n",
       "                                and any(\n",
       "                                    isinstance(expr, cst.Expr)\n",
       "                                    and isinstance(expr.value, cst.SimpleString)\n",
       "                                    for expr in stmt.body\n",
       "                                )\n",
       "                            )\n",
       "                        ]\n",
       "                    )\n",
       "                )\n",
       "\n",
       "        tree = tree.visit(DocstringRemover())\n",
       "        return tree.code\n",
       "\n",
       "    def _process_file(self, file_path: str):\n",
       "        \"\"\"This method is called for every file in the directory.\n",
       "        It does the following:\n",
       "        1. Reads the file\n",
       "        2. Parses the file\n",
       "        3. Visits the file with the visitor\n",
       "        \"\"\"\n",
       "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
       "            source_code = file.read()\n",
       "\n",
       "        try:\n",
       "            tree = cst.parse_module(source_code)\n",
       "        except cst.ParserSyntaxError:\n",
       "            print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
       "            return\n",
       "\n",
       "        tree.visit(self.visitor)\n",
       "\n",
       "        # Remove docstrings if specified\n",
       "        if self.remove_docstrings:\n",
       "            source_code = self.remove_docstring(source_code, tree)\n",
       "\n",
       "        # Minify the code if specified\n",
       "        if self.minify_code:\n",
       "            minifier = PythonMinifier(source_code)\n",
       "            source_code = minifier.get_minified_code()\n",
       "\n",
       "        # Add the processed code to the corresponding list in the visitor\n",
       "        self.visitor.function_source_codes.append(source_code)\n",
       "\n",
       "    def process_file(self, file_path: str):\n",
       "        \"\"\"This method is called for every file in the directory.\n",
       "        It does the following:\n",
       "        1. Runs flake8 on the file\n",
       "        if flake8 returns a non-zero exit code, it means the file has a syntax error\n",
       "        2. Reads the file\n",
       "        3. Parses the file\n",
       "        4. Visits the file with the visitor\n",
       "\n",
       "        \"\"\"\n",
       "        result = subprocess.run(\n",
       "            [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
       "        )\n",
       "\n",
       "        if result.returncode != 0:\n",
       "            print(f\"Skipping file with syntax error: {file_path}\")\n",
       "            print(result.stderr.decode(\"utf-8\"))\n",
       "            return\n",
       "\n",
       "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
       "            source_code = f.read()\n",
       "\n",
       "        try:\n",
       "            tree = cst.parse_module(source_code)\n",
       "            tree.visit(self.visitor)\n",
       "        except cst.ParserSyntaxError as e:\n",
       "            print(f\"Syntax error: {e}\")\n",
       "            print(f\"Skipping file with syntax error: {file_path}\")\n",
       "\n",
       "    def process_directory(\n",
       "        self,\n",
       "    ) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\n",
       "        \"\"\"This method is called for every directory.\n",
       "        It does the following:\n",
       "        1. Gets all the python files in the directory\n",
       "        2. Processes each file\n",
       "        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\n",
       "        \"\"\"\n",
       "        function_source_codes = []\n",
       "        class_source_codes = []\n",
       "\n",
       "        python_files = self.get_files_with_extension(\".py\")\n",
       "\n",
       "        for file_path in python_files:\n",
       "            self._process_file(file_path)\n",
       "\n",
       "        function_source_codes = self.visitor.function_source_codes\n",
       "        function_nodes = self.visitor.function_nodes\n",
       "        class_source_codes = self.visitor.class_source_codes\n",
       "        class_nodes = self.visitor.class_nodes\n",
       "\n",
       "        return function_source_codes, class_source_codes, function_nodes, class_nodes\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code imports several modules and defines classes and methods to parse and manipulate Python code. The PythonMinifier and PythonDocstringExtractor classes respectively minify and extract docstrings from code. The FunctionAndClassVisitor class visits nodes in the code and collects source code and nodes for functions and classes. The PythonParser class uses the visitor to process files in a directory, including parsing, visiting, removing docstrings (if specified), and minifying (if specified). The processed code is then collected and returned."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 182 executed in 5.57 seconds.\n",
      "Sub-task 182 results saved in 0.00 seconds.\n",
      "Sub-task 183 executed in 0.00 seconds.\n",
      "Sub-task 183 results saved in 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " def __init__(self):\n",
       "    self.count = 0\n",
       "    self.functions_with_operation_dict = {}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a class with an initializer method that initializes two instance variables - count and functions_with_operation_dict - with default values."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 184 executed in 0.88 seconds.\n",
      "Sub-task 184 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "    self.current_function = node\n",
       "    self.functions_with_operation_dict[node.name] = []\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code is defining a function \"visit_FunctionDef\" that takes as input a \"FunctionDef\" node from the \"cst\" module. Within this function, the \"current_function\" attribute of \"self\" is set to the input node, and an empty list is added to a dictionary with the key being the name of the function."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 185 executed in 5.95 seconds.\n",
      "Sub-task 185 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " \n",
       "def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "    self.current_function = None\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The code defines a function named `leave_FunctionDef` which takes a node object of type `cst.FunctionDef` and sets the value of `self.current_function` to `None`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-task 186 executed in 0.41 seconds.\n",
      "Sub-task 186 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...RateLimiter: Waiting for 3.00 seconds before next call.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "babyindex_summary = summary_task.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babyindex_summary.faiss_query(\"OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_summary = FifoChat(model= \"gpt-3.5-turbo\", index_dict = {\"babyindex_summary\":babyindex_summary}, name=\"babyd_chatbot_summary\", max_fifo_memory=1000, max_index_memory = 2500, max_output_tokens= 400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
