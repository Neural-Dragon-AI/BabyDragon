{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from typing import List, Tuple, Union, Optional, Callable, Any, Dict\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from llama_cpp import Llama\n",
    "import polars_distance as pld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 25 key-value pairs and 995 tensors from /Users/tommasofurlanello/Documents/Dev/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = ehartford_dolphin-2.5-mixtral-8x7b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:  898 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 46.22 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = ehartford_dolphin-2.5-mixtral-8x7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.38 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size = 47325.41 MiB, (47325.48 / 98304.00)\n",
      "llm_load_tensors: system memory used  = 47325.04 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: ggml.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/tommasofurlanello/.pyenv/versions/3.12.1/envs/babydragon/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9 (1009)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 103079.22 MB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  4096.00 MiB, (51427.11 / 98304.00)\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, (51427.12 / 98304.00)\n",
      "llama_build_graph: non-view tensors processed: 1124/1124\n",
      "llama_new_context_with_model: compute buffer total size = 2167.22 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  2164.05 MiB, (53591.16 / 98304.00)\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=\"/Users/tommasofurlanello/Documents/Dev/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q8_0.gguf\",chat_format=\"chatml\",n_ctx=0,n_gpu_layers = -1, use_mlock=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2430.92 ms\n",
      "llama_print_timings:      sample time =      25.54 ms /   252 runs   (    0.10 ms per token,  9866.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2430.71 ms /    37 tokens (   65.69 ms per token,    15.22 tokens per second)\n",
      "llama_print_timings:        eval time =   11095.01 ms /   251 runs   (   44.20 ms per token,    22.62 tokens per second)\n",
      "llama_print_timings:       total time =   13958.61 ms\n"
     ]
    }
   ],
   "source": [
    "#the schema of the df is: \n",
    "messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes python packages. GANG GANG\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"What is polars library in python?\"\n",
    "          }\n",
    "      ]\n",
    "results = llm.create_chat_completion(\n",
    "      messages = messages,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__bool__',\n",
       " '__ceil__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__divmod__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__float__',\n",
       " '__floor__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__le__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__mod__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdivmod__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rlshift__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__round__',\n",
       " '__rpow__',\n",
       " '__rrshift__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__trunc__',\n",
       " '__xor__',\n",
       " 'as_integer_ratio',\n",
       " 'bit_count',\n",
       " 'bit_length',\n",
       " 'conjugate',\n",
       " 'denominator',\n",
       " 'from_bytes',\n",
       " 'imag',\n",
       " 'is_integer',\n",
       " 'numerator',\n",
       " 'real',\n",
       " 'to_bytes']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(llm.n_ctx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-4107df95-31b0-4c21-80ff-d59184847316',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1704801133,\n",
       " 'model': '/Users/tommasofurlanello/Documents/Dev/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': ' Polars is a fast, efficient, and easy-to-use data processing library for Python. It provides a DataFrame object similar to pandas but optimized for speed and memory efficiency. Polars uses Rust as its core language, which allows it to perform computations much faster than other Python libraries. The library supports various operations like filtering, groupby, joins, and more on large datasets with ease.'},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 37, 'completion_tokens': 86, 'total_tokens': 123}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(df: pl.DataFrame, embedding_column:str, query_embedding: List[float]):\n",
    "    target_lit = pl.lit(pl.Series(\"query\", [query_embedding]))\n",
    "    df_with_query = df.with_columns(target_lit.alias(\"query\")).cast({\"query\": pl.Array(inner=pl.Float64, width=3)})\n",
    "    df_distance = df_with_query.with_columns(pld.col(embedding_column).dist_arr.euclidean('query').alias('dist'))\n",
    "    return df_distance.sort('dist')\n",
    "\n",
    "\n",
    "class ChatFrame:\n",
    "      chat_schema = {\"role\":pl.Utf8,\n",
    "                   \"content\":pl.Utf8,\n",
    "                   \"time_stamps\":pl.Datetime,\n",
    "                   \"author\":pl.Utf8,\n",
    "                   \"message_id\":pl.Utf8,\n",
    "                   \"chat_id\":pl.Utf8, \n",
    "                   \"prompt\": pl.List(pl.Struct({\"role\":pl.Utf8,\"content\":pl.Utf8})),\n",
    "                   'prompt_tokens': pl.Int64, \n",
    "                   'completion_tokens': pl.Int64, \n",
    "                   'total_tokens': pl.Int64\n",
    "}     \n",
    "      chat_ml_schema =  {\"chat_ml\":pl.Struct({\"role\":pl.Utf8,\"content\":pl.Utf8}),\n",
    "                   \"time_stamps\":pl.Datetime,\n",
    "                   \"author\":pl.Utf8,\n",
    "                   \"message_id\":pl.Utf8,\n",
    "                   \"chat_id\":pl.Utf8, \n",
    "                   'completion_tokens': pl.Int64, \n",
    "}\n",
    "      chat_nested_schema = {\"role\":pl.Utf8,\n",
    "                   \"content\":pl.Utf8,\n",
    "                   \"time_stamps\":pl.Datetime,\n",
    "                   \"author\":pl.Utf8,\n",
    "                   \"message_id\":pl.Utf8,\n",
    "                   \"chat_id\":pl.Utf8, \n",
    "                   \"prompt\": pl.List(pl.Struct({\"role\":pl.Utf8,\"content\":pl.Utf8})),\n",
    "                  \"usage\": pl.Struct({'prompt_tokens': pl.Int64, 'completion_tokens': pl.Int64, 'total_tokens': pl.Int64})\n",
    "}\n",
    "      def __init__(self,llm: Llama):\n",
    "            self.chat_id = str(uuid.uuid4())\n",
    "            # self.messages = chat_ml_messages\n",
    "            # self.time_stamps = self.validate_timestamps(time_stamps)\n",
    "            self.df = pl.DataFrame(schema=self.chat_schema)\n",
    "            self.llm = llm\n",
    "            self.ctx = llm.n_ctx()\n",
    "            self.output_reserved = 1500\n",
    "                        \n",
    "      \n",
    "      def add_from_chat_ml(self,chat_ml_messages: List[Dict[str, str]] , \n",
    "                           time_stamps: Optional[List[datetime]]=None,\n",
    "                           author: Optional[List[str]]=None,):\n",
    "            if time_stamps is None:\n",
    "                  time_stamps = [datetime.now() for _ in range(len(chat_ml_messages))]\n",
    "\n",
    "            message_id = [\"chatml-\" + str(uuid.uuid4()) for _ in range(len(chat_ml_messages))]\n",
    "            if author is None:\n",
    "                  author = [\"import\" for _ in range(len(chat_ml_messages))]\n",
    "            chat_id = [self.chat_id for _ in range(len(chat_ml_messages))]\n",
    "            \n",
    "            message_tokens_len = [len(self.llm.tokenize(bytes(message[\"role\"]+ message[\"content\"], \"utf-8\"))) for message in chat_ml_messages]\n",
    "            \n",
    "\n",
    "            nested_df = pl.DataFrame({\"chat_ml\":chat_ml_messages,\n",
    "                                      \"time_stamps\":time_stamps,\n",
    "                                      \"author\":author,\n",
    "                                      \"message_id\":message_id,\n",
    "                                      \"chat_id\":chat_id,\n",
    "                                      \"completion_tokens\":message_tokens_len,\n",
    "                                    }\n",
    "                                      ,schema=self.chat_ml_schema)\n",
    "            # createa  list of [\"chatml-\" + str(uuid.uuid4())] with the same length as the number of messages\n",
    "            \n",
    "            message_df = nested_df.unnest(\"chat_ml\")\n",
    "            self.df = pl.concat([chat.df,message_df],how=\"diagonal\",)\n",
    "            return message_df                 \n",
    "      \n",
    "      def add_message_from_results_dict(self, results_dict: dict, prompt: List[Dict[str, str]]):\n",
    "            \n",
    "            data = {\"role\":[results_dict[\"choices\"][0][\"message\"][\"role\"]],\n",
    "                        \"content\":[results_dict[\"choices\"][0][\"message\"][\"content\"]],\n",
    "                        \"author\":[results_dict[\"model\"]],\n",
    "                        \"message_id\":[results_dict[\"id\"]],\n",
    "                        \"chat_id\":self.chat_id,\n",
    "                        \"prompt\":[prompt],\n",
    "                        \"usage\":[results_dict[\"usage\"]],\n",
    "                        \"time_stamps\":[datetime.fromtimestamp(results_dict[\"created\"])],\n",
    "            \n",
    "                        }\n",
    "            results_df = pl.DataFrame(data=data,schema=self.chat_nested_schema).unnest(\"usage\")\n",
    "            self.df = pl.concat([self.df,results_df],how=\"vertical\")\n",
    "            return results_df\n",
    "      \n",
    "      def fifo_filter(self, n: int, keep_first: bool = False):\n",
    "            #extracts the last message sent by the user if the user is the last message and append it at the end of the returned df\n",
    "            add_last = False\n",
    "            if list(self.df[-1][\"role\"]) == [\"user\"]:\n",
    "                  df_last_user_message = self.df.filter(pl.col(\"role\") == \"user\")[-1]\n",
    "                  #remove from df the last message sent by the user\n",
    "                  df = self.df.filter(pl.col(\"message_id\") != df_last_user_message[\"message_id\"])\n",
    "                  \n",
    "                  n = n - df_last_user_message[\"completion_tokens\"].sum()\n",
    "                  add_last = True\n",
    "            else:\n",
    "                  df = self.df\n",
    "                  \n",
    "            #starts by removing messages that are sent by the system \n",
    "            df_no_system = df.filter(pl.col(\"role\") != \"system\")\n",
    "            df_system = df.filter(pl.col(\"role\") == \"system\")\n",
    "\n",
    "            \n",
    "            if keep_first:\n",
    "                  df_first = df_no_system.filter(pl.col(\"role\") == \"user\")[0]\n",
    "                  df_system = pl.concat([df_system,df_first],how=\"vertical\")\n",
    "                  df_no_system = df_no_system.filter(pl.col(\"message_id\") != df_first[\"message_id\"])\n",
    "            system_tokens = df_system[\"completion_tokens\"].sum()\n",
    "            n = n - system_tokens\n",
    "            \n",
    "            cumulative_df = df_no_system.with_columns(pl.col(\"completion_tokens\").reverse().cum_sum().reverse().alias(\"cumulative_completion_tokens_from_last_msg\"))\n",
    "            fifo_df = cumulative_df.filter(pl.col(\"cumulative_completion_tokens_from_last_msg\") <= n).select(pl.all().exclude(\"cumulative_completion_tokens_from_last_msg\"))\n",
    "            if add_last:\n",
    "                  fifo_df = pl.concat([fifo_df,df_last_user_message],how=\"vertical\")\n",
    "\n",
    "            return pl.concat([df_system,fifo_df],how=\"vertical\")\n",
    "\n",
    "      \n",
    "      def to_chat_ml(self,df: Optional[pl.DataFrame]=None):\n",
    "            if df is None:\n",
    "                  df = self.df\n",
    "            return df.select(pl.struct([\"role\",\"content\"]).alias(\"chat_ml\")).to_dict(as_series=False)[\"chat_ml\"]\n",
    "      \n",
    "      def reply(self, message: str):\n",
    "            user_as_chat_ml = {\"role\":\"user\",\"content\":message}\n",
    "            self.add_from_chat_ml([user_as_chat_ml],time_stamps=[datetime.now()],author=\"user\")\n",
    "            df_prompts = self.fifo_filter(self.ctx-self.output_reserved,keep_first=True)\n",
    "            prompt = self.to_chat_ml(df_prompts)\n",
    "            results = self.llm.create_chat_completion(\n",
    "                  messages = prompt,\n",
    "                  max_tokens = None,\n",
    "                 \n",
    "            )\n",
    "            self.add_message_from_results_dict(results, prompt=prompt)\n",
    "            return results\n",
    "\n",
    "            \n",
    "chat = ChatFrame(llm)\n",
    "message_df = chat.add_from_chat_ml(messages)\n",
    "results_df = chat.add_message_from_results_dict(results,prompt=messages)\n",
    "new_user_message = {\"role\":\"user\", \"content\":\"What is gang gang?\"}\n",
    "new_user_df = chat.add_from_chat_ml([new_user_message])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2577.84 ms\n",
      "llama_print_timings:      sample time =      23.52 ms /   311 runs   (    0.08 ms per token, 13222.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2336.29 ms /    36 tokens (   64.90 ms per token,    15.41 tokens per second)\n",
      "llama_print_timings:        eval time =   13691.45 ms /   310 runs   (   44.17 ms per token,    22.64 tokens per second)\n",
      "llama_print_timings:       total time =   16430.61 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-dfaefba3-02b1-4402-b12c-c3521d864e05',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1704810095,\n",
       " 'model': '/Users/tommasofurlanello/Documents/Dev/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '\"Gang Gang\" is an internet slang term that has gained popularity in recent years, often used to express excitement or enthusiasm. It can be seen as a modern version of the phrase \"Yahoo!\" or \"Woo-hoo!\". The origin of the term is unclear, but it\\'s believed to have started on social media platforms and online forums.\\n\\nTo concatenate multiple columns into a single column row in Python using pandas, you can use the `apply` function along with the `join` method. Here\\'s an example:\\n\\n```python\\nimport pandas as pd\\n\\n# Create a sample DataFrame\\ndata = {\\'Name\\': [\\'John\\', \\'Jane\\'],\\n        \\'Age\\': [25, 30],\\n        \\'City\\': [\\'New York\\', \\'San Francisco\\']}\\ndf = pd.DataFrame(data)\\n\\n# Concatenate multiple columns into a single column row\\ndf[\\'Concatenated_Column\\'] = df[[\\'Name\\', \\'Age\\', \\'City\\']].apply(lambda x: \\'_\\'.join(x), axis=1)\\n\\nprint(df)\\n```\\n\\nOutput:\\n```\\n   Name  Age         City Concatenated_Column\\n0  John   25   New York     John_25_New York\\n1  Jane   30  San Francisco     Jane_30_San Francisco\\n```'},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 183,\n",
       "  'completion_tokens': 310,\n",
       "  'total_tokens': 493}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.reply(\"And how do I concatenate multiple columns into a single column row?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1,)\n",
      "Series: 'message_id' [str]\n",
      "[\n",
      "\t\"chatml-412af2e…\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2418.72 ms\n",
      "llama_print_timings:      sample time =      10.04 ms /   131 runs   (    0.08 ms per token, 13054.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     502.55 ms /    19 tokens (   26.45 ms per token,    37.81 tokens per second)\n",
      "llama_print_timings:        eval time =    5798.74 ms /   130 runs   (   44.61 ms per token,    22.42 tokens per second)\n",
      "llama_print_timings:       total time =    6467.75 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-80a39be1-1e4c-42eb-9c73-d0406ed56a07',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1704801153,\n",
       " 'model': '/Users/tommasofurlanello/Documents/Dev/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"Polars è una libreria Python veloce, efficiente e facile da usare per il processing dei dati. Fornisce un oggetto DataFrame simile a pandas ma ottimizzato per la velocità e l'efficienza della memoria. Polars utilizza Rust come linguaggio di base, che gli permette di eseguire calcoli molto più velocemente rispetto ad altre librerie Python. La libreria supporta varie operazioni come filtro, groupby, join e altro su grandi dataset con facilità.\"},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 460,\n",
       "  'completion_tokens': 130,\n",
       "  'total_tokens': 590}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.reply(\"puoi tradurla in italiano?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (8, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>role</th><th>content</th><th>time_stamps</th><th>author</th><th>message_id</th><th>chat_id</th><th>prompt</th><th>prompt_tokens</th><th>completion_tokens</th><th>total_tokens</th></tr><tr><td>str</td><td>str</td><td>datetime[μs]</td><td>str</td><td>str</td><td>str</td><td>list[struct[2]]</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;system&quot;</td><td>&quot;You are an ass…</td><td>2024-01-09 12:52:19.894352</td><td>&quot;import&quot;</td><td>&quot;chatml-b8e8ca7…</td><td>&quot;99681ca2-f45c-…</td><td>null</td><td>null</td><td>16</td><td>null</td></tr><tr><td>&quot;user&quot;</td><td>&quot;What is polars…</td><td>2024-01-09 12:52:19.894354</td><td>&quot;import&quot;</td><td>&quot;chatml-412af2e…</td><td>&quot;99681ca2-f45c-…</td><td>null</td><td>null</td><td>10</td><td>null</td></tr><tr><td>&quot;assistant&quot;</td><td>&quot; Polars is a f…</td><td>2024-01-09 12:52:13</td><td>&quot;/Users/tommaso…</td><td>&quot;chatcmpl-4107d…</td><td>&quot;99681ca2-f45c-…</td><td>[{&quot;system&quot;,&quot;You are an assistant who perfectly describes python packages. GANG GANG&quot;}, {&quot;user&quot;,&quot;What is polars library in python?&quot;}]</td><td>37</td><td>86</td><td>123</td></tr><tr><td>&quot;user&quot;</td><td>&quot;What is gang g…</td><td>2024-01-09 12:52:19.895671</td><td>&quot;import&quot;</td><td>&quot;chatml-9b81183…</td><td>&quot;99681ca2-f45c-…</td><td>null</td><td>null</td><td>7</td><td>null</td></tr><tr><td>&quot;user&quot;</td><td>&quot;And how do I c…</td><td>2024-01-09 12:52:19.899078</td><td>&quot;user&quot;</td><td>&quot;chatml-1e988b9…</td><td>&quot;99681ca2-f45c-…</td><td>null</td><td>null</td><td>17</td><td>null</td></tr><tr><td>&quot;assistant&quot;</td><td>&quot; Gang Gang ref…</td><td>2024-01-09 12:52:19</td><td>&quot;/Users/tommaso…</td><td>&quot;chatcmpl-9a069…</td><td>&quot;99681ca2-f45c-…</td><td>[{&quot;system&quot;,&quot;You are an assistant who perfectly describes python packages. GANG GANG&quot;}, {&quot;user&quot;,&quot;What is polars library in python?&quot;}, … {&quot;user&quot;,&quot;And how do I concatenate multiple columns into a single column row?&quot;}]</td><td>159</td><td>282</td><td>441</td></tr><tr><td>&quot;user&quot;</td><td>&quot;puoi tradurla …</td><td>2024-01-09 12:52:33.328149</td><td>&quot;user&quot;</td><td>&quot;chatml-14fd592…</td><td>&quot;99681ca2-f45c-…</td><td>null</td><td>null</td><td>10</td><td>null</td></tr><tr><td>&quot;assistant&quot;</td><td>&quot;Polars è una l…</td><td>2024-01-09 12:52:33</td><td>&quot;/Users/tommaso…</td><td>&quot;chatcmpl-80a39…</td><td>&quot;99681ca2-f45c-…</td><td>[{&quot;system&quot;,&quot;You are an assistant who perfectly describes python packages. GANG GANG&quot;}, {&quot;user&quot;,&quot;What is polars library in python?&quot;}, … {&quot;user&quot;,&quot;puoi tradurla in italiano?&quot;}]</td><td>460</td><td>130</td><td>590</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (8, 10)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ role      ┆ content   ┆ time_stam ┆ author    ┆ … ┆ prompt    ┆ prompt_to ┆ completio ┆ total_to │\n",
       "│ ---       ┆ ---       ┆ ps        ┆ ---       ┆   ┆ ---       ┆ kens      ┆ n_tokens  ┆ kens     │\n",
       "│ str       ┆ str       ┆ ---       ┆ str       ┆   ┆ list[stru ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆ datetime[ ┆           ┆   ┆ ct[2]]    ┆ i64       ┆ i64       ┆ i64      │\n",
       "│           ┆           ┆ μs]       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ system    ┆ You are   ┆ 2024-01-0 ┆ import    ┆ … ┆ null      ┆ null      ┆ 16        ┆ null     │\n",
       "│           ┆ an        ┆ 9 12:52:1 ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ assistant ┆ 9.894352  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ who       ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ perfect…  ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ user      ┆ What is   ┆ 2024-01-0 ┆ import    ┆ … ┆ null      ┆ null      ┆ 10        ┆ null     │\n",
       "│           ┆ polars    ┆ 9 12:52:1 ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ library   ┆ 9.894354  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ in        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ python…   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ assistant ┆ Polars is ┆ 2024-01-0 ┆ /Users/to ┆ … ┆ [{\"system ┆ 37        ┆ 86        ┆ 123      │\n",
       "│           ┆ a fast,   ┆ 9         ┆ mmasofurl ┆   ┆ \",\"You    ┆           ┆           ┆          │\n",
       "│           ┆ efficient ┆ 12:52:13  ┆ anello/Do ┆   ┆ are an    ┆           ┆           ┆          │\n",
       "│           ┆ , an…     ┆           ┆ cumen…    ┆   ┆ assistant ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ …         ┆           ┆           ┆          │\n",
       "│ user      ┆ What is   ┆ 2024-01-0 ┆ import    ┆ … ┆ null      ┆ null      ┆ 7         ┆ null     │\n",
       "│           ┆ gang      ┆ 9 12:52:1 ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ gang?     ┆ 9.895671  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ user      ┆ And how   ┆ 2024-01-0 ┆ user      ┆ … ┆ null      ┆ null      ┆ 17        ┆ null     │\n",
       "│           ┆ do I conc ┆ 9 12:52:1 ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ atenate   ┆ 9.899078  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ multipl…  ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ assistant ┆ Gang Gang ┆ 2024-01-0 ┆ /Users/to ┆ … ┆ [{\"system ┆ 159       ┆ 282       ┆ 441      │\n",
       "│           ┆ refers to ┆ 9         ┆ mmasofurl ┆   ┆ \",\"You    ┆           ┆           ┆          │\n",
       "│           ┆ the       ┆ 12:52:19  ┆ anello/Do ┆   ┆ are an    ┆           ┆           ┆          │\n",
       "│           ┆ popular…  ┆           ┆ cumen…    ┆   ┆ assistant ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ …         ┆           ┆           ┆          │\n",
       "│ user      ┆ puoi      ┆ 2024-01-0 ┆ user      ┆ … ┆ null      ┆ null      ┆ 10        ┆ null     │\n",
       "│           ┆ tradurla  ┆ 9 12:52:3 ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ in        ┆ 3.328149  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ italiano? ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ assistant ┆ Polars è  ┆ 2024-01-0 ┆ /Users/to ┆ … ┆ [{\"system ┆ 460       ┆ 130       ┆ 590      │\n",
       "│           ┆ una       ┆ 9         ┆ mmasofurl ┆   ┆ \",\"You    ┆           ┆           ┆          │\n",
       "│           ┆ libreria  ┆ 12:52:33  ┆ anello/Do ┆   ┆ are an    ┆           ┆           ┆          │\n",
       "│           ┆ Python    ┆           ┆ cumen…    ┆   ┆ assistant ┆           ┆           ┆          │\n",
       "│           ┆ vel…      ┆           ┆           ┆   ┆ …         ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>role</th><th>content</th><th>time_stamps</th><th>author</th><th>message_id</th><th>chat_id</th><th>prompt</th><th>prompt_tokens</th><th>completion_tokens</th><th>total_tokens</th></tr><tr><td>str</td><td>str</td><td>datetime[μs]</td><td>str</td><td>str</td><td>str</td><td>list[struct[2]]</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;system&quot;</td><td>&quot;You are an ass…</td><td>2024-01-09 12:52:19.894352</td><td>&quot;import&quot;</td><td>&quot;chatml-b8e8ca7…</td><td>&quot;99681ca2-f45c-…</td><td>null</td><td>null</td><td>16</td><td>null</td></tr><tr><td>&quot;user&quot;</td><td>&quot;What is polars…</td><td>2024-01-09 12:52:19.894354</td><td>&quot;import&quot;</td><td>&quot;chatml-412af2e…</td><td>&quot;99681ca2-f45c-…</td><td>null</td><td>null</td><td>10</td><td>null</td></tr><tr><td>&quot;assistant&quot;</td><td>&quot; Polars is a f…</td><td>2024-01-09 12:52:13</td><td>&quot;/Users/tommaso…</td><td>&quot;chatcmpl-4107d…</td><td>&quot;99681ca2-f45c-…</td><td>[{&quot;system&quot;,&quot;You are an assistant who perfectly describes python packages. GANG GANG&quot;}, {&quot;user&quot;,&quot;What is polars library in python?&quot;}]</td><td>37</td><td>86</td><td>123</td></tr><tr><td>&quot;user&quot;</td><td>&quot;What is gang g…</td><td>2024-01-09 12:52:19.895671</td><td>&quot;import&quot;</td><td>&quot;chatml-9b81183…</td><td>&quot;99681ca2-f45c-…</td><td>null</td><td>null</td><td>7</td><td>null</td></tr><tr><td>&quot;user&quot;</td><td>&quot;And how do I c…</td><td>2024-01-09 12:52:19.899078</td><td>&quot;user&quot;</td><td>&quot;chatml-1e988b9…</td><td>&quot;99681ca2-f45c-…</td><td>null</td><td>null</td><td>17</td><td>null</td></tr><tr><td>&quot;assistant&quot;</td><td>&quot; Gang Gang ref…</td><td>2024-01-09 12:52:19</td><td>&quot;/Users/tommaso…</td><td>&quot;chatcmpl-9a069…</td><td>&quot;99681ca2-f45c-…</td><td>[{&quot;system&quot;,&quot;You are an assistant who perfectly describes python packages. GANG GANG&quot;}, {&quot;user&quot;,&quot;What is polars library in python?&quot;}, … {&quot;user&quot;,&quot;And how do I concatenate multiple columns into a single column row?&quot;}]</td><td>159</td><td>282</td><td>441</td></tr><tr><td>&quot;user&quot;</td><td>&quot;puoi tradurla …</td><td>2024-01-09 12:52:33.328149</td><td>&quot;user&quot;</td><td>&quot;chatml-14fd592…</td><td>&quot;99681ca2-f45c-…</td><td>null</td><td>null</td><td>10</td><td>null</td></tr><tr><td>&quot;assistant&quot;</td><td>&quot;Polars è una l…</td><td>2024-01-09 12:52:33</td><td>&quot;/Users/tommaso…</td><td>&quot;chatcmpl-80a39…</td><td>&quot;99681ca2-f45c-…</td><td>[{&quot;system&quot;,&quot;You are an assistant who perfectly describes python packages. GANG GANG&quot;}, {&quot;user&quot;,&quot;What is polars library in python?&quot;}, … {&quot;user&quot;,&quot;puoi tradurla in italiano?&quot;}]</td><td>460</td><td>130</td><td>590</td></tr><tr><td>&quot;system&quot;</td><td>&quot;You are an ass…</td><td>2024-01-09 12:52:19.894352</td><td>&quot;import&quot;</td><td>&quot;chatml-b8e8ca7…</td><td>&quot;99681ca2-f45c-…</td><td>null</td><td>null</td><td>16</td><td>null</td></tr><tr><td>&quot;user&quot;</td><td>&quot;What is polars…</td><td>2024-01-09 12:52:19.894354</td><td>&quot;import&quot;</td><td>&quot;chatml-412af2e…</td><td>&quot;99681ca2-f45c-…</td><td>null</td><td>null</td><td>10</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 10)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ role      ┆ content   ┆ time_stam ┆ author    ┆ … ┆ prompt    ┆ prompt_to ┆ completio ┆ total_to │\n",
       "│ ---       ┆ ---       ┆ ps        ┆ ---       ┆   ┆ ---       ┆ kens      ┆ n_tokens  ┆ kens     │\n",
       "│ str       ┆ str       ┆ ---       ┆ str       ┆   ┆ list[stru ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆ datetime[ ┆           ┆   ┆ ct[2]]    ┆ i64       ┆ i64       ┆ i64      │\n",
       "│           ┆           ┆ μs]       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ system    ┆ You are   ┆ 2024-01-0 ┆ import    ┆ … ┆ null      ┆ null      ┆ 16        ┆ null     │\n",
       "│           ┆ an        ┆ 9 12:52:1 ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ assistant ┆ 9.894352  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ who       ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ perfect…  ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ user      ┆ What is   ┆ 2024-01-0 ┆ import    ┆ … ┆ null      ┆ null      ┆ 10        ┆ null     │\n",
       "│           ┆ polars    ┆ 9 12:52:1 ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ library   ┆ 9.894354  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ in        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ python…   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ assistant ┆ Polars is ┆ 2024-01-0 ┆ /Users/to ┆ … ┆ [{\"system ┆ 37        ┆ 86        ┆ 123      │\n",
       "│           ┆ a fast,   ┆ 9         ┆ mmasofurl ┆   ┆ \",\"You    ┆           ┆           ┆          │\n",
       "│           ┆ efficient ┆ 12:52:13  ┆ anello/Do ┆   ┆ are an    ┆           ┆           ┆          │\n",
       "│           ┆ , an…     ┆           ┆ cumen…    ┆   ┆ assistant ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ …         ┆           ┆           ┆          │\n",
       "│ user      ┆ What is   ┆ 2024-01-0 ┆ import    ┆ … ┆ null      ┆ null      ┆ 7         ┆ null     │\n",
       "│           ┆ gang      ┆ 9 12:52:1 ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ gang?     ┆ 9.895671  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ user      ┆ puoi      ┆ 2024-01-0 ┆ user      ┆ … ┆ null      ┆ null      ┆ 10        ┆ null     │\n",
       "│           ┆ tradurla  ┆ 9 12:52:3 ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ in        ┆ 3.328149  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ italiano? ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ assistant ┆ Polars è  ┆ 2024-01-0 ┆ /Users/to ┆ … ┆ [{\"system ┆ 460       ┆ 130       ┆ 590      │\n",
       "│           ┆ una       ┆ 9         ┆ mmasofurl ┆   ┆ \",\"You    ┆           ┆           ┆          │\n",
       "│           ┆ libreria  ┆ 12:52:33  ┆ anello/Do ┆   ┆ are an    ┆           ┆           ┆          │\n",
       "│           ┆ Python    ┆           ┆ cumen…    ┆   ┆ assistant ┆           ┆           ┆          │\n",
       "│           ┆ vel…      ┆           ┆           ┆   ┆ …         ┆           ┆           ┆          │\n",
       "│ system    ┆ You are   ┆ 2024-01-0 ┆ import    ┆ … ┆ null      ┆ null      ┆ 16        ┆ null     │\n",
       "│           ┆ an        ┆ 9 12:52:1 ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ assistant ┆ 9.894352  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ who       ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ perfect…  ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ user      ┆ What is   ┆ 2024-01-0 ┆ import    ┆ … ┆ null      ┆ null      ┆ 10        ┆ null     │\n",
       "│           ┆ polars    ┆ 9 12:52:1 ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ library   ┆ 9.894354  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ in        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ python…   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.concat([chat.df,message_df],how=\"diagonal\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_ml': [{'role': 'system',\n",
       "   'content': 'You are an assistant who perfectly describes python packages. GANG GANG'},\n",
       "  {'role': 'user', 'content': 'What is polars library in python?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': ' Polars is a fast, efficient, and easy-to-use data processing library for Python. It provides a DataFrame object similar to pandas but optimized for speed and memory efficiency. Polars uses Rust as its core language, which allows it to perform computations much faster than other Python libraries. The library supports various operations like filtering, groupby, joins, and more on large datasets with ease.'},\n",
       "  {'role': 'user', 'content': 'What is gang gang?'},\n",
       "  {'role': 'user',\n",
       "   'content': 'And how do I concatenate multiple columns into a single column row?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': ' Gang Gang refers to the popular internet slang used by fans of the television show \"The Office\" as a catchphrase for camaraderie and teamwork. It has since been adopted by various online communities as a symbol of unity and support.\\n\\nTo concatenate multiple columns into a single column row in Python, you can use the `apply` function along with the `join` method from the pandas library. Here\\'s an example:\\n\\n```python\\nimport pandas as pd\\n\\n# Create a sample DataFrame\\ndata = {\\'Name\\': [\\'John\\', \\'Jane\\'], \\'Age\\': [25, 30], \\'City\\': [\\'New York\\', \\'San Francisco\\']}\\ndf = pd.DataFrame(data)\\n\\n# Concatenate multiple columns into a single column row\\ndf[\\'Concatenated_Column\\'] = df[[\\'Name\\', \\'Age\\', \\'City\\']].apply(lambda x: \\'_\\'.join(x), axis=1)\\n\\nprint(df)\\n```\\n\\nOutput:\\n```\\n   Name  Age         City Concatenated_Column\\n0  John   25    New York       John_25_New York\\n1  Jane   30  San Francisco       Jane_30_San Francisco\\n```'},\n",
       "  {'role': 'user', 'content': 'puoi tradurla in italiano?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"Polars è una libreria Python veloce, efficiente e facile da usare per il processing dei dati. Fornisce un oggetto DataFrame simile a pandas ma ottimizzato per la velocità e l'efficienza della memoria. Polars utilizza Rust come linguaggio di base, che gli permette di eseguire calcoli molto più velocemente rispetto ad altre librerie Python. La libreria supporta varie operazioni come filtro, groupby, join e altro su grandi dataset con facilità.\"}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.df.select(pl.struct([pl.col(\"role\"),pl.col(\"content\")]).alias(\"chat_ml\")).to_dict(as_series=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "String(4c9003e8-0527-4f1c-81a8-92b7f620aa6e)"
      ],
      "text/plain": [
       "<Expr ['String(4c9003e8-0527-4f1c-81a8…'] at 0x11C9A5C10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = str(uuid.uuid4())\n",
    "print(type(id))\n",
    "pl.lit(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df_single = pl.DataFrame(data={\"chat_id\": [1]})\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_single \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_id\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m df_single\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df_single = pl.DataFrame(data={\"chat_id\": [1]})\n",
    "df_single = df.with_columns(pl.lit(1).alias(\"chat_id\"))\n",
    "df_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3724.28 ms\n",
      "llama_print_timings:      sample time =       8.34 ms /   103 runs   (    0.08 ms per token, 12347.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1077.72 ms /    23 tokens (   46.86 ms per token,    21.34 tokens per second)\n",
      "llama_print_timings:        eval time =   12324.44 ms /   102 runs   (  120.83 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =   13537.53 ms\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-2e9e5958-e306-43f1-b92c-d304c817da98',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1704751139,\n",
       " 'model': '/Users/tommasofurlanello/Documents/Dev/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \" Polars is a fast, efficient, and easy-to-use data processing library for Python. It provides high-performance DataFrame operations, similar to pandas, but with optimized memory usage and faster execution times. Polars is built on top of the Rust programming language, which allows it to leverage the power of Rust's performance and safety features. The library includes support for reading and writing various file formats, handling missing data, groupby operations, joins, and more.\"},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 37, 'completion_tokens': 102, 'total_tokens': 139}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'id': 'chatcmpl-eab5813e-3c8a-474b-bbee-636b9385d955',\n",
    " 'object': 'chat.completion',\n",
    " 'created': 1704751049,\n",
    " 'model': '/Users/tommasofurlanello/Documents/Dev/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q8_0.gguf',\n",
    " 'choices': [{'index': 0,\n",
    "   'message': {'role': 'assistant',\n",
    "    'content': ' The Polars library in Python is a high-performance DataFrame library that provides fast and efficient data processing capabilities. It is designed to handle large datasets with ease, offering features such as lazy evaluation, parallel execution, and memory optimization. Polars supports various data types, including integers, floats, strings, booleans, and timestamps, making it suitable for a wide range of applications in data science, machine learning, and analytics.'},\n",
    "   'finish_reason': 'stop'}],\n",
    " 'usage': {'prompt_tokens': 33, 'completion_tokens': 92, 'total_tokens': 125}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Polars is a fast, efficient, and easy-to-use data processing library for Python. It provides high-performance DataFrame operations, similar to pandas, but with optimized memory usage and faster execution times. Polars is built on top of the Rust programming language, which allows it to leverage the power of Rust's performance and safety features. The library includes support for reading and writing various file formats, handling missing data, groupby operations, joins, and more.\"]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[results_dict[\"choices\"][0][\"message\"][\"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an assistant who perfectly describes python packages.'},\n",
       " {'role': 'user', 'content': 'What is polars library in python?'}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('', Struct({'role': String, 'content': String}))])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_series = pl.Series(messages)\n",
    "messages_series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>prompt:</th></tr><tr><td>list[struct[2]]</td></tr></thead><tbody><tr><td>[{&quot;system&quot;,&quot;You are an assistant who perfectly describes python packages.&quot;}, {&quot;user&quot;,&quot;What is polars library in python?&quot;}]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 1)\n",
       "┌───────────────────────────────────┐\n",
       "│ prompt:                           │\n",
       "│ ---                               │\n",
       "│ list[struct[2]]                   │\n",
       "╞═══════════════════════════════════╡\n",
       "│ [{\"system\",\"You are an assistant… │\n",
       "└───────────────────────────────────┘"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = {\"prompt:\": pl.List(pl.Struct({\"role\":pl.Utf8,\"content\":pl.Utf8})),\n",
    "          \"role\":pl.Utf8,\n",
    "            \"content\":pl.Utf8,\n",
    "            \"author\":pl.Utf8,\n",
    "            \"message_id\":pl.Utf8,}\n",
    "data = {\"prompt:\": [messages_series],\n",
    "        \"role\"}\n",
    "df = pl.DataFrame(data=data,schema=schema)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>role</th><th>content</th><th>author</th><th>message_id</th><th>chat_id</th><th>prompt</th><th>usage</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>list[struct[2]]</td><td>struct[3]</td></tr></thead><tbody><tr><td>&quot;assistant&quot;</td><td>&quot; Polars is a f…</td><td>&quot;/Users/tommaso…</td><td>&quot;chatcmpl-2e9e5…</td><td>null</td><td>[{&quot;system&quot;,&quot;You are an assistant who perfectly describes python packages.&quot;}, {&quot;user&quot;,&quot;What is polars library in python?&quot;}]</td><td>{37,102,139}</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 7)\n",
       "┌───────────┬───────────────┬───────────────┬──────────────┬─────────┬──────────────┬──────────────┐\n",
       "│ role      ┆ content       ┆ author        ┆ message_id   ┆ chat_id ┆ prompt       ┆ usage        │\n",
       "│ ---       ┆ ---           ┆ ---           ┆ ---          ┆ ---     ┆ ---          ┆ ---          │\n",
       "│ str       ┆ str           ┆ str           ┆ str          ┆ str     ┆ list[struct[ ┆ struct[3]    │\n",
       "│           ┆               ┆               ┆              ┆         ┆ 2]]          ┆              │\n",
       "╞═══════════╪═══════════════╪═══════════════╪══════════════╪═════════╪══════════════╪══════════════╡\n",
       "│ assistant ┆ Polars is a   ┆ /Users/tommas ┆ chatcmpl-2e9 ┆ null    ┆ [{\"system\",\" ┆ {37,102,139} │\n",
       "│           ┆ fast,         ┆ ofurlanello/D ┆ e5958-e306-4 ┆         ┆ You are an   ┆              │\n",
       "│           ┆ efficient,    ┆ ocumen…       ┆ 3f1-b92c…    ┆         ┆ assistant…   ┆              │\n",
       "│           ┆ an…           ┆               ┆              ┆         ┆              ┆              │\n",
       "└───────────┴───────────────┴───────────────┴──────────────┴─────────┴──────────────┴──────────────┘"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_schema = {\"role\":pl.Utf8,\n",
    "                   \"content\":pl.Utf8,\n",
    "                   \"author\":pl.Utf8,\n",
    "                   \"message_id\":pl.Utf8,\n",
    "                   \"chat_id\":pl.Utf8, \n",
    "                   \"author\":pl.Utf8,\n",
    "                   \"prompt\": pl.List(pl.Struct({\"role\":role_dtype,\"content\":pl.Utf8})),\n",
    "                   \"usage\": pl.Struct({'prompt_tokens': pl.Int64, 'completion_tokens': pl.Int64, 'total_tokens': pl.Int64})\n",
    "}\n",
    "def add_message_from_results_dict(self, results_dict: dict, prompt: List[Dict[str, str]]):\n",
    "    data = {\"role\":[results_dict[\"choices\"][0][\"message\"][\"role\"]],\n",
    "            \"content\":[results_dict[\"choices\"][0][\"message\"][\"content\"]],\n",
    "            \"author\":[results_dict[\"model\"]],\n",
    "            \"message_id\":[results_dict[\"id\"]],\n",
    "            \"chat_id\":self.chat_id,\n",
    "            \"prompt\":[prompt],\n",
    "            \"usage\":[results_dict[\"usage\"]]\n",
    "    \n",
    "            }\n",
    "    df = pl.DataFrame(data=data,schema=chat_schema)\n",
    "    self.df = pl.concat([self.df,df],how=\"vertical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babydragon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
