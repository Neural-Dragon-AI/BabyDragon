{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import inspect\n",
    "import re\n",
    "\n",
    "\n",
    "def type_mapping(dtype):\n",
    "    if dtype == float:\n",
    "        return \"number\"\n",
    "    elif dtype == int:\n",
    "        return \"integer\"\n",
    "    elif dtype == str:\n",
    "        return \"string\"\n",
    "    elif dtype == bool:\n",
    "        return \"boolean\"\n",
    "    elif dtype == list:\n",
    "        return \"list\"\n",
    "    elif dtype == dict:\n",
    "        return \"dict\"\n",
    "    else:\n",
    "        return \"string\"\n",
    "\n",
    "\n",
    "def extract_params(doc_str: str):\n",
    "    # split doc string by newline, skipping empty lines\n",
    "    params_str = [line for line in doc_str.split(\"\\n\") if line.strip()]\n",
    "    params = {}\n",
    "    for line in params_str:\n",
    "        # we only look at lines starting with ':param'\n",
    "        if line.strip().startswith(':param'):\n",
    "            param_match = re.findall(r'(?<=:param )\\w+', line)\n",
    "            if param_match:\n",
    "                param_name = param_match[0]\n",
    "                desc_match = line.replace(f\":param {param_name}:\", \"\").strip()\n",
    "                # if there is a description, store it\n",
    "                if desc_match:\n",
    "                    params[param_name] = desc_match\n",
    "    return params\n",
    "\n",
    "\n",
    "def func_to_json(func):\n",
    "    # Check if the function is a functools.partial\n",
    "    if isinstance(func, functools.partial) or isinstance(func, functools.partialmethod):\n",
    "        fixed_args = func.keywords\n",
    "        _func = func.func\n",
    "        if isinstance(func, functools.partial) and (fixed_args is None or fixed_args == {}):\n",
    "            fixed_args = dict(zip(func.func.__code__.co_varnames, func.args))\n",
    "    else:\n",
    "        fixed_args = {}\n",
    "        _func = func\n",
    "\n",
    "    # first we get function name\n",
    "    func_name = _func.__name__\n",
    "    # then we get the function annotations\n",
    "    argspec = inspect.getfullargspec(_func)\n",
    "    # get the function docstring\n",
    "    func_doc = inspect.getdoc(_func) or \"\"\n",
    "    # parse the docstring to get the description\n",
    "    func_description = ''.join([line for line in func_doc.split(\"\\n\") if not line.strip().startswith(':')])\n",
    "    # parse the docstring to get the descriptions for each parameter in dict format\n",
    "    param_details = extract_params(func_doc) if func_doc else {}\n",
    "    # attach parameter types to params and exclude fixed args\n",
    "    # get params\n",
    "    params = {}\n",
    "    for param_name in argspec.args:\n",
    "        if param_name not in fixed_args.keys():\n",
    "            params[param_name] = {\n",
    "                \"description\": param_details.get(param_name) or \"\",\n",
    "                \"type\": type_mapping(argspec.annotations.get(param_name, type(None)))\n",
    "            }\n",
    "    # calculate required parameters excluding fixed args\n",
    "    # _required = [arg for arg in argspec.args if arg not in fixed_args]\n",
    "    _required = [i for i in argspec.args if i not in fixed_args.keys()]\n",
    "    if inspect.getfullargspec(_func).defaults:\n",
    "        _required = [argspec.args[i] for i, a in enumerate(argspec.args) if\n",
    "                     argspec.args[i] not in inspect.getfullargspec(_func).defaults and argspec.args[\n",
    "                         i] not in fixed_args.keys()]\n",
    "    # then return everything in dict\n",
    "    return {\n",
    "        \"name\": func_name,\n",
    "        \"description\": func_description,\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": params\n",
    "        },\n",
    "        \"required\": _required\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "import openai\n",
    "\n",
    "sys_msg = \"\"\"Assistant is a large language model trained by OpenAI.\n",
    "\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussion on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        openai_api_key: str,\n",
    "        model_name: str = 'gpt-4-0613',\n",
    "        functions: Optional[list] = None\n",
    "    ):\n",
    "        openai.api_key = openai_api_key\n",
    "        self.model_name = model_name\n",
    "        self.functions = self._parse_functions(functions)\n",
    "        self.func_mapping = self._create_func_mapping(functions)\n",
    "        self.chat_history = [{'role': 'system', 'content': sys_msg}]\n",
    "\n",
    "    def _parse_functions(self, functions: Optional[list]) -> Optional[list]:\n",
    "        if functions is None:\n",
    "            return None\n",
    "        return [func_to_json(func) for func in functions]\n",
    "\n",
    "    def _create_func_mapping(self, functions: Optional[list]) -> dict:\n",
    "        if functions is None:\n",
    "            return {}\n",
    "        return {func.__name__: func for func in functions}\n",
    "\n",
    "    def _create_chat_completion(\n",
    "        self, messages: list, use_functions: bool=True\n",
    "    ) -> openai.ChatCompletion:\n",
    "        if use_functions and self.functions:\n",
    "            res = openai.ChatCompletion.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                functions=self.functions\n",
    "            )\n",
    "        else:\n",
    "            res = openai.ChatCompletion.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages\n",
    "            )\n",
    "        return res\n",
    "\n",
    "    def _generate_response(self) -> openai.ChatCompletion:\n",
    "        while True:\n",
    "            print('.', end='')\n",
    "            res = self._create_chat_completion(\n",
    "                self.chat_history + self.internal_thoughts\n",
    "            )\n",
    "            finish_reason = res.choices[0].finish_reason\n",
    "\n",
    "            if finish_reason == 'stop' or len(self.internal_thoughts) > 3:\n",
    "                # create the final answer\n",
    "                final_thought = self._final_thought_answer()\n",
    "                final_res = self._create_chat_completion(\n",
    "                    self.chat_history + [final_thought],\n",
    "                    use_functions=False\n",
    "                )\n",
    "                return final_res\n",
    "            elif finish_reason == 'function_call':\n",
    "                self._handle_function_call(res)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected finish reason: {finish_reason}\")\n",
    "\n",
    "    def _handle_function_call(self, res: openai.ChatCompletion):\n",
    "        self.internal_thoughts.append(res.choices[0].message.to_dict())\n",
    "        func_name = res.choices[0].message.function_call.name\n",
    "        args_str = res.choices[0].message.function_call.arguments\n",
    "        result = self._call_function(func_name, args_str)\n",
    "        res_msg = {'role': 'assistant', 'content': (f\"The answer is {result}.\")}\n",
    "        self.internal_thoughts.append(res_msg)\n",
    "\n",
    "    def _call_function(self, func_name: str, args_str: str):\n",
    "        args = json.loads(args_str)\n",
    "        func = self.func_mapping[func_name]\n",
    "        res = func(**args)\n",
    "        return res\n",
    "    \n",
    "    def _final_thought_answer(self):\n",
    "        thoughts = (\"To answer the question I will use these step by step instructions.\"\n",
    "                    \"\\n\\n\")\n",
    "        for thought in self.internal_thoughts:\n",
    "            if 'function_call' in thought.keys():\n",
    "                thoughts += (f\"I will use the {thought['function_call']['name']} \"\n",
    "                             \"function to calculate the answer with arguments \"\n",
    "                             + thought['function_call']['arguments'] + \".\\n\\n\")\n",
    "            else:\n",
    "                thoughts += thought[\"content\"] + \"\\n\\n\"\n",
    "        self.final_thought = {\n",
    "            'role': 'assistant',\n",
    "            'content': (f\"{thoughts} Based on the above, I will now answer the \"\n",
    "                        \"question, this message will only be seen by me so answer with \"\n",
    "                        \"the assumption with that the user has not seen this message.\")\n",
    "        }\n",
    "        return self.final_thought\n",
    "\n",
    "    def ask(self, query: str) -> openai.ChatCompletion:\n",
    "        self.internal_thoughts = []\n",
    "        self.chat_history.append({'role': 'user', 'content': query})\n",
    "        res = self._generate_response()\n",
    "        self.chat_history.append(res.choices[0].message.to_dict())\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic Python Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"hello_world\",\n",
      "    \"description\": \"\",\n",
      "    \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "            \"name\": {\n",
      "                \"description\": \"The name of the person\",\n",
      "                \"type\": \"string\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"required\": [\n",
      "        \"name\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Import or paste the code containing the func_to_json function here\n",
    "# ...\n",
    "\n",
    "def hello_world(name: str) -> str:\n",
    "    \"\"\"\n",
    "    :param name: The name of the person\n",
    "    \"\"\"\n",
    "    return f\"Hello, {name}\"\n",
    "\n",
    "json_repr = func_to_json(hello_world)\n",
    "print(json.dumps(json_repr, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to Look For:\n",
    " - The func_to_json function should generate a JSON object that correctly describes the hello_world function, including the parameter name and type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Function with Multiple Types of Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"calculate_area\",\n",
      "    \"description\": \"\",\n",
      "    \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "            \"length\": {\n",
      "                \"description\": \"The length of the shape\",\n",
      "                \"type\": \"number\"\n",
      "            },\n",
      "            \"width\": {\n",
      "                \"description\": \"The width of the shape\",\n",
      "                \"type\": \"number\"\n",
      "            },\n",
      "            \"is_3D\": {\n",
      "                \"description\": \"Whether the shape is 3D\",\n",
      "                \"type\": \"boolean\"\n",
      "            },\n",
      "            \"height\": {\n",
      "                \"description\": \"The height of the shape, only relevant if is_3D is True\",\n",
      "                \"type\": \"number\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"required\": [\n",
      "        \"length\",\n",
      "        \"width\",\n",
      "        \"is_3D\",\n",
      "        \"height\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def calculate_area(length: float, width: float, is_3D: bool = False, height: float = 1.0):\n",
    "    \"\"\"\n",
    "    :param length: The length of the shape\n",
    "    :param width: The width of the shape\n",
    "    :param is_3D: Whether the shape is 3D\n",
    "    :param height: The height of the shape, only relevant if is_3D is True\n",
    "    \"\"\"\n",
    "    if is_3D:\n",
    "        return length * width * height\n",
    "    else:\n",
    "        return length * width\n",
    "\n",
    "json_repr = func_to_json(calculate_area)\n",
    "print(json.dumps(json_repr, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to Look For:\n",
    "  - The JSON object should include all parameters, their types, and which ones are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functools.partial(<function calculate_area at 0x112b93ac0>, length=10, width=5)\n",
      "{\n",
      "    \"name\": \"calculate_area\",\n",
      "    \"description\": \"\",\n",
      "    \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "            \"is_3D\": {\n",
      "                \"description\": \"Whether the shape is 3D\",\n",
      "                \"type\": \"boolean\"\n",
      "            },\n",
      "            \"height\": {\n",
      "                \"description\": \"The height of the shape, only relevant if is_3D is True\",\n",
      "                \"type\": \"number\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"required\": [\n",
      "        \"is_3D\",\n",
      "        \"height\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "partial_func = partial(calculate_area, length=10, width=5)\n",
    "print(partial_func)\n",
    "json_repr = func_to_json(partial_func)\n",
    "print(json.dumps(json_repr, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to Look For:\n",
    "  - The JSON object should recognize that length and width are fixed (pre-filled) arguments and not include them as required parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Agent with an API key and default model name\n",
    "agent_1 = Agent(openai_api_key=openai.api_key)\n",
    "\n",
    "# Ask a simple question\n",
    "response_1 = agent_1.ask(\"What is the capital of France?\")\n",
    "print(response_1.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..The product of 6 and 9 is 54.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    return a * b\n",
    "\n",
    "# Initialize the Agent with custom functions\n",
    "agent_2 = Agent(openai_api_key=openai.api_key, functions=[multiply])\n",
    "\n",
    "# Ask a question that utilizes custom function\n",
    "response_2 = agent_2.ask(\"Multiply 6 by 9.\")\n",
    "print(response_2.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def select_best_context(question: str, potential_contexts: list) -> str:\n",
    "    \"\"\"\n",
    "    Given a question and list of potential contexts, returns the best context.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Add question to potential_contexts for vectorization\n",
    "    docs = [question] + potential_contexts\n",
    "    doc_vectors = vectorizer.fit_transform(docs)\n",
    "    \n",
    "    # Compute cosine similarities between question and potential contexts\n",
    "    cosine_similarities = cosine_similarity(doc_vectors[0:1], doc_vectors[1:]).flatten()\n",
    "    \n",
    "    # Get the index of the most similar context\n",
    "    most_similar_index = np.argmax(cosine_similarities)\n",
    "    \n",
    "    return potential_contexts[most_similar_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "Invalid schema for function 'select_best_context': 'list' is not valid under any of the given schemas",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m agent_with_feedback \u001b[39m=\u001b[39m Agent(openai_api_key\u001b[39m=\u001b[39mopenai\u001b[39m.\u001b[39mapi_key, functions\u001b[39m=\u001b[39m[select_best_context])\n\u001b[1;32m      3\u001b[0m \u001b[39m# Ask a question that requires a contextual answer\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m response \u001b[39m=\u001b[39m agent_with_feedback\u001b[39m.\u001b[39;49mask(\u001b[39m\"\u001b[39;49m\u001b[39mWhat\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms the significance of this group theory?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent)\n",
      "Cell \u001b[0;32mIn[8], line 112\u001b[0m, in \u001b[0;36mAgent.ask\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minternal_thoughts \u001b[39m=\u001b[39m []\n\u001b[1;32m    111\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_history\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m: query})\n\u001b[0;32m--> 112\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_response()\n\u001b[1;32m    113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_history\u001b[39m.\u001b[39mappend(res\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mto_dict())\n\u001b[1;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "Cell \u001b[0;32mIn[8], line 59\u001b[0m, in \u001b[0;36mAgent._generate_response\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_chat_completion(\n\u001b[1;32m     60\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_history \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minternal_thoughts\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m     finish_reason \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mfinish_reason\n\u001b[1;32m     64\u001b[0m     \u001b[39mif\u001b[39;00m finish_reason \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minternal_thoughts) \u001b[39m>\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m     65\u001b[0m         \u001b[39m# create the final answer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m, in \u001b[0;36mAgent._create_chat_completion\u001b[0;34m(self, messages, use_functions)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_chat_completion\u001b[39m(\n\u001b[1;32m     41\u001b[0m     \u001b[39mself\u001b[39m, messages: \u001b[39mlist\u001b[39m, use_functions: \u001b[39mbool\u001b[39m\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     42\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m openai\u001b[39m.\u001b[39mChatCompletion:\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m use_functions \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunctions:\n\u001b[0;32m---> 44\u001b[0m         res \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     45\u001b[0m             model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name,\n\u001b[1;32m     46\u001b[0m             messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m     47\u001b[0m             functions\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunctions\n\u001b[1;32m     48\u001b[0m         )\n\u001b[1;32m     49\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m         res \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m     51\u001b[0m             model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name,\n\u001b[1;32m     52\u001b[0m             messages\u001b[39m=\u001b[39mmessages\n\u001b[1;32m     53\u001b[0m         )\n",
      "File \u001b[0;32m~/neuraldragon/frames_arc/BabyDragon/notebooks/venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/neuraldragon/frames_arc/BabyDragon/notebooks/venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/neuraldragon/frames_arc/BabyDragon/notebooks/venv/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/neuraldragon/frames_arc/BabyDragon/notebooks/venv/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/neuraldragon/frames_arc/BabyDragon/notebooks/venv/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: Invalid schema for function 'select_best_context': 'list' is not valid under any of the given schemas"
     ]
    }
   ],
   "source": [
    "agent_with_feedback = Agent(openai_api_key=openai.api_key, functions=[select_best_context])\n",
    "\n",
    "# Ask a question that requires a contextual answer\n",
    "response = agent_with_feedback.ask(\"What's the significance of this group theory?\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
