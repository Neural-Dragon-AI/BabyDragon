{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import libcst as cst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_rst_file(path, name):\n",
    "    \"\"\"\n",
    "    Create a .rst file with a title in the given directory.\n",
    "    The name parameter should not include the .rst extension.\n",
    "    \"\"\"\n",
    "    rst_path = os.path.join(path, f\"{name}.rst\")\n",
    "    with open(rst_path, 'w') as rst_file:\n",
    "        rst_file.write(f\"{name}\\n\")\n",
    "        rst_file.write(\"=\" * len(name) + \"\\n\\n\")\n",
    "        rst_file.write(\".. automodule:: \" + name + \"\\n\")\n",
    "        rst_file.write(\"   :members:\\n\")\n",
    "\n",
    "def create_sphinx_docs(src_dir, doc_dir):\n",
    "    \"\"\"\n",
    "    Generate .rst files in doc_dir for each .py file in src_dir and its subdirectories.\n",
    "    Does not overwrite existing .rst files.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        relative_path = os.path.relpath(root, src_dir)\n",
    "        rst_dir = os.path.join(doc_dir, relative_path)\n",
    "\n",
    "        if not os.path.exists(rst_dir):\n",
    "            os.makedirs(rst_dir)\n",
    "\n",
    "        for file in files:\n",
    "            name, ext = os.path.splitext(file)\n",
    "            if ext == \".py\" and name != \"__init__\":\n",
    "                rst_path = os.path.join(rst_dir, name + \".rst\")\n",
    "                if not os.path.exists(rst_path):\n",
    "                    create_rst_file(rst_dir, name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "create_sphinx_docs('/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def __init__(\\n    self,\\n    embedder: OpenAiEmbedder,\\n    values: List[Any],\\n    path: List[List[int]],\\n    max_workers: int = 1,\\n    task_id: str = \"task\",\\n    calls_per_minute: int = 1500,\\n    backup: bool = True,\\n):\\n    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup)\\n    self.embedder = embedder\\n    self.values = values\\n', '\\ndef _execute_sub_task(self, sub_path: List[int]) -> List[str]:\\n    # expected to work with a lig of a single element\\n    if len(sub_path) != 1:\\n        raise ValueError(\\n            \"Embedding task expected to work with a list of a single element\"\\n        )\\n    sub_results = {}\\n    for i in sub_path:\\n        embedded_value = self.embedder.embed(self.values[i])\\n        sub_results[i] = embedded_value\\n    return sub_results\\n', '\\ndef parallel_embeddings(embedder, values, max_workers, backup, name):\\n        # Prepare the paths for the EmbeddingTask\\n        print(\"Embedding {} values\".format(len(values)))\\n        paths = [[i] for i in range(len(values))]\\n\\n        # Initialize the EmbeddingTask and execute it\\n        embedding_task = EmbeddingTask(\\n            embedder,\\n            values,\\n            path=paths,\\n            max_workers=max_workers,\\n            task_id=name + \"_embedding_task\",\\n            backup=backup,\\n        )\\n        embeddings = embedding_task.work()\\n        embeddings = [x[1] for x in sorted(embeddings, key=lambda x: x[0])]\\n        return embeddings\\n']\n",
      "['def __init__(\\n    self,\\n    memory_kernel_dict: Dict,\\n    supplement_indexes: Dict,\\n    sim_threshold: float,\\n    chatbot: BaseChat,\\n    parent_kernel_label: str,\\n    child_kernel_label: str,\\n    system_prompt: str,\\n    clustering_method: str,\\n    task_id: str = \"TopicTreeTask\",\\n    max_workers: int = 1,\\n    calls_per_minute: int = 20,\\n):\\n    self.clustering_method = clustering_method\\n    self.supplement_indexes = supplement_indexes\\n    self.sim_threshold = sim_threshold\\n    self.parent_kernel_label = parent_kernel_label\\n    self.child_kernel_label = child_kernel_label\\n    self.memory_kernel_dict = memory_kernel_dict\\n    self._setup_memory_kernel_group()\\n    self.generate_task_paths()\\n    self.system_prompt = system_prompt\\n    self.chatbot = chatbot\\n    self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\\n    super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\\n', '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\ndef generate_task_paths(self):\\n    print(\"Generating task paths\")\\n\\n    self.memory_kernel_group.generate_path_groups()\\n', '\\ndef llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\\n    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\\n    return chatbot.reply(message)\\n', '\\ndef _execute_sub_task(self, sub_path) -> List[str]:\\n    if self.parallel:\\n        chatbot_instance = copy.deepcopy(self.chatbot)\\n    else:\\n        chatbot_instance = self.chatbot\\n\\n    sub_results = {}\\n    for i in sub_path:\\n        print(f\\'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}\\')\\n        try:\\n            current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\\n            supplement_values = []\\n            for key, index in self.supplement_indexes.items():\\n                results, scores, indeces = index.faiss_query(current_val, k=5)\\n                for result, score in zip(results, scores):\\n                    if score > self.sim_threshold:\\n                        supplement_values.append(result)\\n            topic_tree = self.create_topic_tree(supplement_values)\\n            #response = self.llm_response(chatbot_instance, current_val, id=i)\\n            sub_results[i] = topic_tree\\n        except IndexError:\\n            print(f\"Error: Invalid index {i} in sub_path\")\\n            sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\\n        except Exception as e:\\n            print(f\"Error in sub_task for index {i}: {e}\")\\n            sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\\n\\n    return sub_results\\n', '\\ndef execute_task(self) -> None:\\n    BaseTask.execute_task(self)\\n\\n    # Load the results from the JSON file\\n    # with open(f\"{self.task_id}_results.json\", \"r\") as f:\\n    #     task_results = json.load(f)\\n    self._load_results_from_file()\\n    task_results = self.results\\n    new_values = []\\n    #sort task_results by index and add to new_values 0- max values ascending\\n    for task_result in task_results:\\n        if isinstance(task_result, dict):\\n            for key, value in task_result.items():\\n                new_values.append((int(key), value))\\n        elif isinstance(task_result, str):\\n            print(f\"Error in task_result: {task_result}\")\\n\\n    new_values.sort(key=lambda x: x[0])\\n    values = [x[1] for x in new_values]\\n\\n    task_memory_index = MemoryIndex()\\n    task_memory_index.init_index(values=values)\\n    # Create a new MemoryKernel with the results\\n    new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\\n\\n    # Add the new MemoryKernel to the MultiKernel\\n    self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\\n    self.generate_task_paths()\\n\\n    #delete the results file\\n    # os.remove(f\"{self.task_id}_results.json\")\\n', '\\n\\ndef create_topic_tree(self, docs):\\n    return None\\n']\n",
      "['def __init__(\\n    self,\\n    memory_kernel_dict: Dict,\\n    chatbot: BaseChat,\\n    parent_kernel_label: str,\\n    child_kernel_label: str,\\n    system_prompt: str,\\n    clustering_method: str,\\n    path_group: Dict[str, List[List[int]]],\\n    task_id: str = \"MultiKernelTask\",\\n    max_workers: int = 1,\\n    calls_per_minute: int = 20,\\n):\\n    self.clustering_method = clustering_method\\n    self.parent_kernel_label = parent_kernel_label\\n    self.child_kernel_label = child_kernel_label\\n    self.memory_kernel_dict = memory_kernel_dict\\n    \\n    self._setup_memory_kernel_group()\\n    if path_group:\\n        self.memory_kernel_group.path_group = path_group\\n    else:\\n        self.generate_task_paths()\\n    self.system_prompt = system_prompt\\n    self.chatbot = chatbot\\n    self.paths = self.memory_kernel_group.path_group[self.parent_kernel_label]\\n    super().__init__(path = self.paths, max_workers=max_workers, task_id=task_id, calls_per_minute=calls_per_minute)\\n', '\\n\\ndef _setup_memory_kernel_group(self):\\n    if self.clustering_method == \"HDBSCAN\":\\n        print(\"Using HDBSCAN\")\\n        self.memory_kernel_group = HDBSCANMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    elif self.clustering_method == \"Spectral\":\\n        print(\"Using Spectral\")\\n        self.memory_kernel_group = SpectralClusteringMultiKernel(memory_kernel_dict=self.memory_kernel_dict)\\n    else:\\n        raise ValueError(f\"Unknown clustering method: {self.clustering_method}\")\\n', '\\ndef generate_task_paths(self):\\n    print(\"Generating task paths\")\\n\\n    self.memory_kernel_group.generate_path_groups()\\n', '\\ndef llm_response(self, chatbot: BaseChat, message: str, context=None, id=None):\\n    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\\n    return chatbot.reply(message)\\n', '\\ndef _execute_sub_task(self, sub_path) -> List[str]:\\n    if self.parallel:\\n        chatbot_instance = copy.deepcopy(self.chatbot)\\n    else:\\n        chatbot_instance = self.chatbot\\n\\n    sub_results = {}\\n    for i in sub_path:\\n        print(f\\'Current_node: {i}, size of values {len(self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values)}\\')\\n        try:\\n            current_val = self.memory_kernel_group.memory_kernel_dict[self.parent_kernel_label].values[i]\\n            response = self.llm_response(chatbot_instance, current_val, id=i)\\n            sub_results[i] = response\\n        except IndexError:\\n            print(f\"Error: Invalid index {i} in sub_path\")\\n            sub_results[i] = f\"Error: Invalid index {i} in sub_path\"\\n        except Exception as e:\\n            print(f\"Error in sub_task for index {i}: {e}\")\\n            sub_results[i] = f\"Error in sub_task for index {i}: {e}\"\\n\\n    return sub_results\\n', '\\ndef execute_task(self) -> None:\\n    BaseTask.execute_task(self)\\n\\n    # Load the results from the JSON file\\n    # with open(f\"{self.task_id}_results.json\", \"r\") as f:\\n    #     task_results = json.load(f)\\n    self._load_results_from_file()\\n    task_results = self.results\\n    new_values = []\\n    #sort task_results by index and add to new_values 0- max values ascending\\n    for task_result in task_results:\\n        if isinstance(task_result, dict):\\n            for key, value in task_result.items():\\n                new_values.append((int(key), value))\\n        elif isinstance(task_result, str):\\n            print(f\"Error in task_result: {task_result}\")\\n\\n    new_values.sort(key=lambda x: x[0])\\n    values = [x[1] for x in new_values]\\n\\n    task_memory_index = MemoryIndex()\\n    task_memory_index.init_index(values=values)\\n    # Create a new MemoryKernel with the results\\n    new_memory_kernel = MemoryKernel.from_task_results(task_memory_index)\\n\\n    # Add the new MemoryKernel to the MultiKernel\\n    self.memory_kernel_group.memory_kernel_dict[self.child_kernel_label] = new_memory_kernel\\n    self.generate_task_paths()\\n\\n    #delete the results file\\n    # os.remove(f\"{self.task_id}_results.json\")\\n']\n",
      "['def __init__(\\n    self,\\n    path: List[List[int]],\\n    max_workers: int = 1,\\n    task_id: str = \"task\",\\n    calls_per_minute: int = 20,\\n    backup: bool = True,\\n    save_path: str = None,\\n):\\n    self.task_id = task_id\\n    self.path = path\\n    self.results = []\\n    self.max_workers = max_workers\\n    self.parallel = True if max_workers > 1 else False\\n    self.rate_limiter = RateLimiter(calls_per_minute)\\n    self.failed_sub_tasks = []\\n    self.backup = backup\\n    print(\"setting up savepath\")\\n    self.save_path = save_path if save_path is not None else os.path.join(\"storage\", \"tasks\")\\n', '\\ndef _save_results_to_file(self) -> None:\\n    os.makedirs(self.save_path, exist_ok=True)\\n    with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"w\") as f:\\n        json.dump(self.results, f)\\n', '\\ndef _load_results_from_file(self) -> None:\\n    if os.path.exists(os.path.join(self.save_path, f\"{self.task_id}_results.json\")):\\n        try:\\n            with open(os.path.join(self.save_path, f\"{self.task_id}_results.json\"), \"r\") as f:\\n                self.results = json.load(f)\\n                print(f\"Loaded {len(self.results)} results from file.\")\\n        except Exception as e:\\n            print(f\"Error loading results from file: {e}\")\\n            print(\"Starting from scratch.\")\\n    else:\\n        print(\"No results file found, starting from scratch.\")\\n', '\\ndef _execute_sub_task(self, sub_path: List[int]) -> List[str]:\\n    sub_results = []\\n    for i in sub_path:\\n        response = \"Implement the response function in the subclass\"\\n        sub_results.append(response)\\n    return sub_results\\n', '\\ndef execute_task(self) -> None:\\n    if self.backup:\\n        self._load_results_from_file()\\n\\n    with RateLimitedThreadPoolExecutor(\\n        max_workers=self.max_workers,\\n        calls_per_minute=self.rate_limiter.calls_per_minute,\\n    ) as executor:\\n        futures = []\\n        print(f\"Executing task {self.task_id} using {self.max_workers} workers.\")\\n\\n        for i, sub_path in enumerate(self.path):\\n            if i < len(self.results):\\n                pass\\n            else:\\n                future = executor.submit(self._execute_sub_task, sub_path)\\n                futures.append((i, future))\\n\\n        for i, future in futures:\\n            try:\\n                execution_start_time = time.time()\\n                sub_task_result = future.result()\\n                execution_end_time = time.time()\\n                print(\\n                    f\"Sub-task {i} executed in {execution_end_time - execution_start_time:.2f} seconds.\"\\n                )\\n\\n                save_start_time = time.time()\\n                self.results.append(sub_task_result)\\n                # self.results.append(sub_task_result)\\n                if self.backup:\\n                    self._save_results_to_file()\\n                save_end_time = time.time()\\n                print(\\n                    f\"Sub-task {i} results saved in {save_end_time - save_start_time:.2f} seconds.\"\\n                )\\n            except Exception as e:\\n                print(f\"Error in sub-task {i}: {e}\")\\n                # default_result = f\"Error in sub-task {i}: {e}\"\\n                default_result =  {i:f\"Error in sub-task {i}: {e}\"} \\n                self.results.append(default_result)\\n                if self.backup:\\n                    self._save_results_to_file()\\n                self.failed_sub_tasks.append((self.path[i], str(e)))\\n\\n            except KeyboardInterrupt:\\n                print(\"Keyboard interrupt detected, stopping task execution.\")\\n                executor.shutdown(wait=False)\\n                break\\n\\n    print(\"Task execution completed.\")\\n', '\\ndef work(self) -> List[Any]:\\n    self.execute_task()\\n    if not self.backup:\\n        self._save_results_to_file()\\n    work = []\\n    for sub_result in self.results:\\n        for index_id, response in sub_result.items():\\n            work.append((index_id, response))\\n    # sort the content to write by index_id\\n    work.sort(key=lambda x: int(x[0]))\\n    return work\\n']\n",
      "['def __init__(\\n    self,\\n    index: MemoryIndex,\\n    path: List[List[int]],\\n    chatbot: Chat,\\n    read_func=None,\\n    max_workers: int = 1,\\n    task_id: str = \"LLMReadTask\",\\n    calls_per_minute: int = 20,\\n):\\n    \"\"\"\\n        Initialize a LLMReadTask instance.\\n\\n        :param index: List of strings representing the queries.\\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\\n        :param chatbot: Chatbot instance used for executing queries.\\n        :param max_workers: Maximum number of worker threads (default is 4).\\n        \"\"\"\\n    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute)\\n    self.index = index\\n    self.chatbot = chatbot\\n    self.read_func = read_func if read_func else self.llm_response\\n', '\\ndef llm_response(chatbot: Chat, message: str, string_out=False):\\n    if string_out:\\n        return chatbot.reply(message)\\n    return chatbot.query(message)\\n', '\\ndef _execute_sub_task(self, sub_path: List[int]) -> List[str]:\\n    \"\"\"\\n        Execute a sub-task using a separate copy of the chatbot instance. each sub-stasks uses a\\n        a clean memory instance.\\n\\n        :param sub_path: List of indices representing the sub-task\\'s sequence.\\n        :return: List of strings representing the responses for each query in the sub-task.\\n        \"\"\"\\n    if self.parallel:\\n        # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\\n        chatbot_instance = copy.deepcopy(self.chatbot)\\n    else:\\n        chatbot_instance = self.chatbot\\n    if isinstance(self.chatbot, BaseThread):\\n        chatbot_instance.reset_memory()\\n\\n    sub_results = []\\n    for i in sub_path:\\n        response = self.read_func(chatbot_instance, self.index.values[i])\\n        sub_results.append(response)\\n    return sub_results\\n', '\\ndef read(self):\\n    self.execute_task()\\n    return self.results\\n', 'def __init__(\\n    self,\\n    index: MemoryIndex,\\n    path: List[List[int]],\\n    chatbot: Chat,\\n    write_func=None,\\n    context=None,\\n    task_name=\"summary\",\\n    max_workers: int = 1,\\n    task_id: str = \"LLMWriteTask\",\\n    calls_per_minute: int = 20,\\n    backup: bool = True,\\n):\\n    \"\"\"\\n        Initialize a LLMWriteTask instance.\\n\\n        :param index: List of strings representing the queries.\\n        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\\n        :param chatbot: Chatbot instance used for executing queries.\\n        :param max_workers: Maximum number of worker threads (default is 4).\\n        \"\"\"\\n    BaseTask.__init__(self, path, max_workers, task_id, calls_per_minute, backup=backup)\\n    self.index = index\\n    self.chatbot = chatbot\\n    self.write_func = write_func if write_func else self.llm_response\\n    self.new_index_name = self.index.name + f\"_{task_name}\"\\n    self.context = context\\n', '\\ndef llm_response(self, chatbot: Chat, message: str, context=None, id=None):\\n    max_tokens = 8000 if chatbot.model == \"gpt-4\" else 4000\\n    # if len(self.index.tokenizer.encode(message))+chatbot.max_output_tokens> max_tokens:\\n    #     return \"the message is too long to be processed\"\\n    # moved the error catching to multi-threading but custom method could report the error here\\n    return chatbot.reply(message)\\n', '\\ndef _execute_sub_task(self, sub_path: List[int]) -> List[str]:\\n    \"\"\"\\n        Execute a sub-task using a separate copy of the chatbot instance.\\n\\n        :param sub_path: List of indices representing the sub-task\\'s sequence.\\n        :return: List of strings representing the responses for each query in the sub-task.\\n        \"\"\"\\n    if self.parallel:\\n        # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\\n        chatbot_instance = copy.deepcopy(self.chatbot)\\n    else:\\n        chatbot_instance = self.chatbot\\n    if isinstance(self.chatbot, BaseThread):\\n        chatbot_instance.reset_memory()\\n\\n    sub_results = {}\\n    for i in sub_path:\\n        current_val = self.index.values[i]\\n        response = self.write_func(\\n            chatbot_instance, current_val, self.context, id=i\\n        )\\n        sub_results[i] = response\\n    return sub_results\\n', '\\ndef write(self):\\n    content_to_write = self.work()\\n    self.new_index = MemoryIndex(name=self.new_index_name, values=[x[1] for x in content_to_write], max_workers=self.max_workers, backup=self.backup)\\n    self.new_index.save()\\n    return self.new_index\\n']\n",
      "['def __init__(\\n    self,\\n    mem_index: MemoryIndex,\\n    name: str = \"memory_kernel\",\\n    k: int = 2,\\n    save_path: str = None,\\n):\\n    \"\"\"\\n        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\\n\\n        Args:\\n            mem_index (MemoryIndex): A MemoryIndex instance.\\n            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\\n            k (int, optional): The number of hops for message passing. Defaults to 2.\\n            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\\n        \"\"\"\\n    super().__init__(\\n        index=mem_index.index,\\n        values=mem_index.values,\\n        embeddings=mem_index.embeddings,\\n        name=name,\\n        save_path=save_path,\\n    )\\n    self.k = k\\n    if len(self.values) > 0: \\n        self.create_k_hop_index(k=k)\\n    else:\\n        raise ValueError(\"The input MemoryIndex is empty. Please check the input MemoryIndex.\")\\n', '\\ndef cos_sim(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:\\n    \"\"\"\\n        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n        \"\"\"\\n    if not isinstance(a, np.ndarray):\\n        a = np.array(a)\\n\\n    if not isinstance(b, np.ndarray):\\n        b = np.array(b)\\n\\n    if len(a.shape) == 1:\\n        a = a[np.newaxis, :]\\n\\n    if len(b.shape) == 1:\\n        b = b[np.newaxis, :]\\n\\n    a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\\n    b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\\n    return np.dot(a_norm, b_norm.T)\\n', '\\ndef compute_kernel(\\n    self,\\n    embedding_set: np.ndarray,\\n    threshold: float = 0.65,\\n    use_softmax: bool = False,\\n) -> np.ndarray:\\n\\n    \"\"\"\\n        Compute the adjacency matrix of the graph.\\n\\n        Parameters:\\n        embedding_set (numpy array): The embedding matrix of the nodes.\\n        threshold (float): The threshold for the adjacency matrix.\\n        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\\n        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\\n\\n        Returns:\\n        adj_matrix (numpy array): The adjacency matrix of the graph.\\n        \"\"\"\\n\\n    A = self.cos_sim(embedding_set, embedding_set)\\n    if use_softmax:\\n        # softmax\\n        A = np.exp(A)\\n        A = A / np.sum(A, axis=1)[:, np.newaxis]\\n    adj_matrix = np.zeros_like(A)\\n    adj_matrix[A > threshold] = 1\\n    adj_matrix[A <= threshold] = 0\\n    adj_matrix = adj_matrix.astype(np.float32)\\n    return adj_matrix\\n', '\\ndef k_hop_message_passing(\\n    self, A: np.ndarray, node_features: np.ndarray, k: int\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \"\"\"\\n        Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n        Parameters:\\n        A (numpy array): The adjacency matrix of the graph.\\n        node_features (numpy array): The feature matrix of the nodes.\\n        k (int): The number of hops for message passing.\\n\\n        Returns:\\n        A_k (numpy array): The k-hop adjacency matrix.\\n        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n        \"\"\"\\n\\n    print(\"Compute the k-hop adjacency matrix\")\\n    A_k = np.linalg.matrix_power(A, k)\\n\\n    print(\"Aggregate the messages from the k-hop neighborhood:\")\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\\n\\n    return A_k, agg_features\\n', '\\ndef graph_sylvester_embedding(self, G: Tuple, m: int, ts: np.ndarray) -> np.ndarray:\\n    \"\"\"\\n        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\\n\\n        Args:\\n            G (Tuple): A tuple containing the graph\\'s vertices (V) and weights (W).\\n            m (int): The number of singular values to consider.\\n            ts (np.ndarray): The spectral scales.\\n\\n        Returns:\\n            np.ndarray: The node_embeddings matrix.\\n        \"\"\"\\n    V, W = G\\n    n = len(V)\\n    D_BE = np.diag(W.sum(axis=1))\\n    L_BE = np.identity(n) - np.dot(\\n        np.diag(1 / np.sqrt(D_BE.diagonal())),\\n        np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\\n    )\\n\\n    A = W\\n    B = L_BE\\n    C = np.identity(n)\\n    X = solve_sylvester(A, B, C)\\n\\n    U, S, _ = svd(X, full_matrices=False)\\n    U_m = U[:, :m]\\n    S_m = S[:m]\\n\\n    node_embeddings = np.zeros((n, m))\\n\\n    for i in range(n):\\n        for s in range(m):\\n            # Spectral kernel descriptor\\n            node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\\n\\n    return node_embeddings\\n', '\\ndef gen_gse_embeddings(\\n    self, A: np.ndarray, embeddings: np.ndarray, m: int = 7\\n) -> np.ndarray:\\n    \"\"\"\\n        Generate Graph Sylvester Embeddings.\\n\\n        Args:\\n            A (np.ndarray): The adjacency matrix of the graph.\\n            embeddings (np.ndarray): The original node embeddings.\\n            m (int, optional): The number of spectral scales. Defaults to 7.\\n\\n        Returns:\\n            np.ndarray: The generated Graph Sylvester Embeddings.\\n        \"\"\"\\n    V = list(range(len(embeddings)))\\n    W = A\\n\\n    G = (V, W)\\n    ts = np.linspace(0, 1, m)  # equally spaced scales\\n\\n    gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\\n    return gse_embeddings\\n', '\\ndef create_k_hop_index(self, k: int = 2):\\n    \"\"\"\\n        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\\n        aggregated features, and updating the memory index.\\n\\n        Args:\\n            k (int, optional): The number of hops for message passing. Defaults to 2.\\n        \"\"\"\\n    self.k = k\\n    print(\"Computing the adjacency matrix\")\\n    print(\"Embeddings shape: \", self.embeddings.shape)\\n    self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\\n    print(\"Computing the k-hop adjacency matrix and aggregated features\")\\n    self.A_k, self.node_embeddings = self.k_hop_message_passing(\\n        self.A, self.embeddings, k\\n    )\\n    print(\"Updating the memory index\")\\n    self.k_hop_index = MemoryIndex(name=self.name)\\n    self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\\n', '\\n@classmethod\\ndef from_task_results(cls, task_memory_index):\\n    new_memory_kernel = cls(mem_index=task_memory_index)\\n\\n    # Create a new index for the new MemoryKernel\\n    new_memory_kernel.create_k_hop_index()\\n\\n    return new_memory_kernel\\n']\n",
      "['\\n\\ndef calc_shgo_mode(scores: List[float]) -> float:\\n    def objective(x):\\n        return -estimate_pdf(scores)(x)\\n\\n    bounds = [(min(scores), max(scores))]\\n    result = scipy.optimize.shgo(objective, bounds)\\n    return result.x\\n', 'def objective(x):\\n    return -estimate_pdf(scores)(x)\\n', '\\n\\ndef estimate_pdf(scores: List[float]) -> callable:\\n    pdf = scipy.stats.gaussian_kde(scores)\\n    return pdf\\n', '\\n\\ndef sort_paths_by_mode_distance(\\n    paths, memory_kernel, distance_metric: str = \"cosine\"\\n) -> List[List[int]]:\\n    sorted_paths = []\\n    for i, path in enumerate(paths):\\n        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\\n        cluster_embeddings = np.array(cluster_embeddings)\\n        cluster_mean = np.mean(cluster_embeddings, axis=0)\\n        if distance_metric == \"cosine\" or distance_metric == \"guassian\":\\n            scores = [\\n                (i, cosine(cluster_mean, emb))\\n                for i, emb in zip(path, cluster_embeddings)\\n            ]\\n        elif distance_metric == \"euclidean\":\\n            scores = [\\n                (i, np.linalg.norm(cluster_mean - emb))\\n                for i, emb in zip(path, cluster_embeddings)\\n            ]\\n        score_values = [score for _, score in scores]  # Extract score values\\n        mu = calc_shgo_mode(score_values)\\n        sigma = np.std(score_values)\\n        if distance_metric == \"guassian\":\\n            scores = [\\n                (i, np.exp(-((x - mu) ** 2) / (2 * sigma**2))) for i, x in scores\\n            ]\\n        # Sort path by score\\n        sorted_path_and_scores = sorted(scores, key=lambda x: x[1], reverse=True)\\n        sorted_path = [x[0] for x in sorted_path_and_scores]\\n        sorted_paths.append(sorted_path)\\n    return sorted_paths\\n', '\\n\\ndef sort_paths_by_kernel_density(\\n    paths, memory_kernel, distance_metric: str = \"cosine\"\\n) -> List[List[int]]:\\n    sorted_paths = []\\n    for i, path in enumerate(paths):\\n        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\\n        cluster_embeddings = np.array(cluster_embeddings)\\n        cluster_mean = np.mean(cluster_embeddings, axis=0)\\n        if distance_metric == \"cosine\":\\n            scores = [\\n                (i, cosine(cluster_mean, emb))\\n                for i, emb in zip(path, cluster_embeddings)\\n            ]\\n        elif distance_metric == \"euclidean\":\\n            scores = [\\n                (i, np.linalg.norm(cluster_mean - emb))\\n                for i, emb in zip(path, cluster_embeddings)\\n            ]\\n        score_values = [score for _, score in scores]  # Extract score values\\n\\n        # Estimate PDF using Kernel Density Estimation\\n        kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(\\n            np.array(score_values).reshape(-1, 1)\\n        )\\n        kde_scores = [kde.score_samples([[x]])[0] for _, x in scores]\\n\\n        # Sort path by score\\n        sorted_path_and_scores = sorted(\\n            zip(path, kde_scores), key=lambda x: x[1], reverse=True\\n        )\\n        sorted_path = [x[0] for x in sorted_path_and_scores]\\n        sorted_paths.append(sorted_path)\\n    return sorted_paths\\n']\n",
      "['def create_paths(\\n    self, embeddings: np.ndarray, num_clusters: int\\n) -> List[List[int]]:\\n    raise NotImplementedError\\n', 'def create_paths(\\n    self, embeddings: np.ndarray, num_clusters: int\\n) -> List[List[int]]:\\n    clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\\n    cluster_assignments = clusterer.fit_predict(embeddings)\\n    paths = [[] for _ in range(num_clusters)]\\n    for i, cluster in enumerate(cluster_assignments):\\n        paths[cluster].append(i)\\n    paths = [path for path in paths if path]\\n    return paths\\n', 'def create_paths(\\n    self, A: np.ndarray, num_clusters: int\\n) -> List[List[int]]:\\n    n_samples = A.shape[0]\\n    n_neighbors = min(n_samples - 1, 10)  # Set n_neighbors to min(n_samples - 1, 10)\\n    spectral_clustering = SpectralClustering(\\n        n_clusters=num_clusters,\\n        affinity=\"precomputed\",\\n        n_neighbors=n_neighbors,\\n        random_state=42,\\n    )\\n    cluster_assignments = spectral_clustering.fit_predict(A)\\n    paths = [[] for _ in range(num_clusters)]\\n    for i, cluster in enumerate(cluster_assignments):\\n        paths[cluster].append(i)\\n    paths = [path for path in paths if path]\\n    return paths\\n']\n",
      "['def __init__(self, memory_kernel_group: MultiKernel):\\n    self.memory_kernel_group = memory_kernel_group\\n    self.memory_kernel_dict = memory_kernel_group.memory_kernel_dict\\n    self.memory_kernel_group.generate_path_groups()\\n', '\\ndef plot_embeddings_with_path(self, embeddings, title, paths):\\n    tsne = TSNE(n_components=2, random_state=42)\\n    reduced_embeddings = tsne.fit_transform(embeddings)\\n\\n    plt.figure(figsize=(10, 8))\\n    colors = cm.rainbow(np.linspace(0, 1, len(paths)))\\n    for i, path in enumerate(paths):\\n        path_embeddings = reduced_embeddings[path]\\n        plt.scatter(\\n            path_embeddings[:, 0],\\n            path_embeddings[:, 1],\\n            color=colors[i],\\n            label=f\"Cluster {i}\",\\n        )\\n        for j in range(len(path) - 1):\\n            plt.plot(\\n                [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\\n                [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\\n                color=colors[i],\\n            )\\n    plt.title(title)\\n    plt.legend()\\n    plt.show()\\n', '\\ndef visualize_paths(self):\\n    #loop through memory kernels and print path_group\\n    for key, kernel in self.memory_kernel_dict.items():\\n        print(f\"Kernel: {key}\")\\n        paths = self.memory_kernel_group.path_group[key]\\n        print(f\"Path Group: {paths}\")\\n        node_embeddings = kernel.node_embeddings\\n        self.plot_embeddings_with_path(\\n            node_embeddings, f\"Node Embeddings for {key}\", paths\\n        )\\n', 'def plot_singular_values(self):\\n    #loop through memory kernels and print path_group\\n    for key, kernel in self.memory_kernel_dict.items():\\n        print(f\"Kernel: {key}\")\\n        A_k = kernel.A_k\\n        U, S, V = np.linalg.svd(A_k)\\n        plt.plot(S)\\n        plt.show()\\n', 'def __init__(self, memory_kernel_group: MultiKernel):\\n    self.memory_kernel_group = memory_kernel_group\\n', '\\ndef get_cluster_labels(self, kernel_label: str) -> Tuple[np.ndarray, int]:\\n    paths = self.memory_kernel_group.path_group[kernel_label]\\n    num_clusters = len(paths)\\n    cluster_labels = np.empty(len(self.memory_kernel_group.memory_kernel_dict[kernel_label].node_embeddings), dtype=int)\\n\\n    for cluster_index, path in enumerate(paths):\\n        cluster_labels[path] = cluster_index\\n\\n    return cluster_labels, num_clusters\\n', '\\ndef compute_nmi(self, kernel_label1: str, kernel_label2: str) -> float:\\n    cluster_labels1, _ = self.get_cluster_labels(kernel_label1)\\n    cluster_labels2, _ = self.get_cluster_labels(kernel_label2)\\n    nmi = normalized_mutual_info_score(cluster_labels1, cluster_labels2)\\n    return nmi\\n', '\\ndef evaluate_stability(self) -> float:\\n    kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\\n    pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\\n    nmi_sum = 0\\n\\n    for kernel_label1, kernel_label2 in pairwise_combinations:\\n        nmi = self.compute_nmi(kernel_label1, kernel_label2)\\n        nmi_sum += nmi\\n\\n    stability_score = nmi_sum / len(pairwise_combinations)\\n    return stability_score\\n']\n",
      "['def __init__(\\n    self,\\n    memory_kernel_dict: Dict[str, MemoryKernel],\\n    name: str = \"memory_kernel_group\",\\n):\\n    \"\"\"\\n        Initialize the MultiKernel with a dictionary of MemoryKernel instances.\\n\\n        Args:\\n            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\\n            name (str, optional): The name of the MultiKernel. Defaults to \"memory_kernel_group\".\\n        \"\"\"\\n    self.memory_kernel_dict = memory_kernel_dict\\n    self.path_group = {}\\n    self.name = name\\n', 'def __init__(\\n    self,\\n    memory_kernel_dict: Dict[str, MemoryKernel],\\n    name: str = \"memory_kernel_group\",\\n):\\n    super().__init__(memory_kernel_dict, name)\\n    self.cluster_paths = HDBSCANPaths()\\n', '\\ndef generate_path_groups(self, num_clusters: int = None) -> None:\\n    path_group = {}\\n    for k, v in self.memory_kernel_dict.items():\\n        embeddings = v.node_embeddings\\n        if num_clusters is None:\\n            num_clusters = int(np.sqrt(len(embeddings)))\\n        paths = self.cluster_paths.create_paths(embeddings, num_clusters)\\n        path_group[k] = paths\\n    self.path_group = path_group\\n', 'def __init__(\\n    self,\\n    memory_kernel_dict: Dict[str, MemoryKernel],\\n    name: str = \"memory_kernel_group\",\\n):\\n    super().__init__(memory_kernel_dict, name)\\n    self.cluster_paths = SpectralClusteringPaths()\\n', '\\ndef generate_path_groups(self, num_clusters: int = None) -> None:\\n    path_group = {}\\n    for k, v in self.memory_kernel_dict.items():\\n        A_k = v.A_k\\n        if num_clusters is None:\\n            num_clusters = int(np.sqrt(len(A_k)))\\n        paths = self.cluster_paths.create_paths(A_k, num_clusters)\\n        path_group[k] = paths\\n    self.path_group = path_group\\n']\n",
      "['\\ndef __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\\n    BaseThread.__init__(self, name=name, max_memory=None)\\n    MemoryIndex.__init__(self, index=None, name=name)\\n    self.max_context = max_context\\n    self.use_mark = use_mark\\n    self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n', '\\ndef index_message(self, message: str, verbose: bool = False):\\n    \"\"\"index a message in the faiss index, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated\\n        \"\"\"\\n\\n    self.add_to_index(value=message, verbose=verbose)\\n', '\\ndef add_message(self, message_dict: dict, verbose: bool = False):\\n    \"\"\"add a message to the memory thread, the message is embedded and added to the index\\n        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\\n        \"\"\"\\n    # print(\"checking the dict\")\\n    message_dict = check_dict(message_dict)\\n    # print(\"trying to add the message\")\\n    BaseThread.add_message(self, message_dict)\\n    # print(message_dict)\\n    message = message_dict[\"content\"]\\n    self.index_message(message, verbose=verbose)\\n    return True\\n', '\\ndef token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\\n    \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\\n    if self.use_mark:\\n        query = mark_question(query)\\n    return MemoryIndex.token_bound_query(self, query, k, max_tokens)\\n', '\\ndef sorted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    reverse: bool = False,\\n    return_from_thread=True,\\n) -> Tuple[List[str], List[float], List[int]]:\\n    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\\n        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\\n        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\\n        \"\"\"\\n    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(query, k, max_tokens=max_tokens)\\n\\n    num_results = min(len(unsorted_messages), len(unsorted_scores), len(unsorted_indices))\\n    # unsorted_indices = [int(i) for i in unsorted_indices]  # convert numpy arrays to integers\\n    unsorted_indices = [int(i) for sublist in unsorted_indices for i in sublist]\\n\\n    # Sort the indices\\n    sorted_indices = sorted(range(num_results), key=lambda x: unsorted_indices[x])\\n    \\n    print(sorted_indices)\\n    print(type(sorted_indices))\\n\\n    if reverse:\\n        sorted_indices.reverse()\\n\\n    # Fetch the sorted messages, scores, and indices based on sorted_indices\\n    sorted_messages = [unsorted_messages[i] for i in sorted_indices]\\n    sorted_scores = [unsorted_scores[i] for i in sorted_indices]\\n    sorted_indices = [unsorted_indices[i] for i in sorted_indices]\\n\\n    if return_from_thread:\\n        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\\n\\n    return sorted_messages, sorted_scores, sorted_indices\\n', 'def weighted_query(\\n    self,\\n    query,\\n    k: int = 10,\\n    max_tokens: int = 4000,\\n    decay_factor: float = 0.1,\\n    temporal_weight: float = 0.5,\\n    order_by: str = \"chronological\",\\n    reverse: bool = False,\\n) -> list:\\n    \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\\n        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\\n        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\\n        The order_by parameter controls the order of the results. If it is set to \\'similarity\\', the results are sorted in similarity order. If it is set to \\'chronological\\', the results are sorted in chronological order with the most recent message first.\\n        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\\n        \"\"\"\\n    # Validate order_by parameter\\n    if order_by not in (\"similarity\", \"chronological\"):\\n        raise ValueError(\\n            \"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\"\\n        )\\n\\n    # Get similarity-based results\\n    sim_messages, sim_scores, sim_indices = self.sorted_query(\\n        query, k, max_tokens=max_tokens\\n    )\\n\\n    # Get token-bound history\\n    hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\\n\\n    # Combine messages and indices\\n    combined_messages = sim_messages + hist_messages\\n    combined_indices = sim_indices + hist_indices\\n\\n    # Create the local_index and populate it\\n    self.local_index = MemoryIndex(name=\"local_index\")\\n    for message in combined_messages:\\n        self.local_index.add_to_index(value=message, verbose=False)\\n\\n    # Perform a new query on the combined index\\n    (\\n        new_query_results,\\n        new_query_scores,\\n        new_query_indices,\\n    ) = self.local_index.token_bound_query(\\n        query, k=len(combined_messages), max_tokens=max_tokens\\n    )\\n\\n    # Compute temporal weights\\n    temporal_weights = [\\n        np.exp(-decay_factor * i) for i in range(len(combined_messages))\\n    ]\\n    temporal_weights = [\\n        w / sum(temporal_weights) for w in temporal_weights\\n    ]  # Normalize the temporal weights\\n\\n    # Combine similarity scores and temporal weights\\n    weighted_scores = []\\n    for i in range(len(new_query_scores)):\\n        sim_score = new_query_scores[i]\\n        temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\\n        weighted_score = (\\n            1 - temporal_weight\\n        ) * sim_score + temporal_weight * temp_weight\\n        weighted_scores.append(weighted_score)\\n\\n    # Sort the results based on the order_by parameter\\n    if order_by == \"similarity\":\\n        sorting_key = lambda k: weighted_scores[k]\\n    elif order_by == \"chronological\":  # order_by == \\'chronological\\'\\n        sorting_key = lambda k: new_query_indices[k]\\n    else:\\n        raise ValueError(\\n            \"Invalid value for order_by parameter. It should be either \\'similarity\\' or \\'chronological\\'.\"\\n        )\\n\\n    sorted_indices = [\\n        new_query_indices[i]\\n        for i in sorted(\\n            range(len(new_query_indices)), key=sorting_key, reverse=not reverse\\n        )\\n    ]\\n    sorted_results = [\\n        new_query_results[i]\\n        for i in sorted(\\n            range(len(new_query_results)), key=sorting_key, reverse=not reverse\\n        )\\n    ]\\n    sorted_scores = [\\n        weighted_scores[i]\\n        for i in sorted(\\n            range(len(weighted_scores)), key=sorting_key, reverse=not reverse\\n        )\\n    ]\\n\\n    # Return only the top k results without exceeding max_tokens\\n    final_results, final_scores, final_indices = [], [], []\\n    current_tokens = 0\\n    for i in range(min(k, len(sorted_results))):\\n        message_tokens = self.get_message_tokens(sorted_results[i])\\n        if current_tokens + message_tokens <= max_tokens:\\n            final_results.append(sorted_results[i])\\n            final_scores.append(sorted_scores[i])\\n            final_indices.append(sorted_indices[i])\\n            current_tokens += message_tokens\\n        else:\\n            break\\n\\n    return final_results, final_scores, final_indices\\n']\n",
      "['\\ndef __init__(\\n    self,\\n    name: str = \"memory\",\\n    max_memory: Optional[int] = None,\\n    tokenizer: Optional[Any] = None,\\n) -> None:\\n    \"\"\"\\n        Initialize the BaseThread instance.\\n\\n        :param name: The name of the memory thread. Defaults to \\'memory\\'.\\n        :param max_memory: The maximum number of tokens allowed in the memory thread.\\n                           Defaults to None, which means no limit.\\n        :param tokenizer: The tokenizer to be used for tokenizing messages.\\n                          Defaults to None, which means using the tiktoken encoding for the \\'gpt-3.5-turbo\\' model.\\n        \"\"\"\\n    self.name = name\\n    self.max_memory = max_memory\\n    self.memory_thread = []\\n    self.time_stamps = []\\n    self.message_tokens = []\\n    self.total_tokens = 0\\n    if tokenizer is None:\\n        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n', '\\ndef __getitem__(self, idx):\\n    return self.memory_thread[idx]\\n', '\\ndef __len__(self):\\n    return len(self.memory_thread)\\n', '\\ndef reset_memory(self) -> None:\\n    \"\"\"\\n        Reset the memory thread.\\n        \"\"\"\\n    self.memory_thread = []\\n    self.time_stamps = []\\n    self.message_tokens = []\\n    self.total_tokens = 0\\n', '\\ndef get_message_tokens(self, message_dict: dict) -> int:\\n    \"\"\"\\n        Calculate the number of tokens in a message, including the role token.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The total number of tokens in the message.\\n        \"\"\"\\n    message_dict = check_dict(message_dict)\\n    message = message_dict[\"content\"]\\n    return len(self.tokenizer.encode(message)) + 6  # +6 for the role token\\n', '\\ndef get_message_role(self, message_dict: dict) -> str:\\n    \"\"\"\\n        Get the role of the message from a message dictionary.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        :return: The role of the message.\\n        \"\"\"\\n    message_dict = check_dict(message_dict)\\n    return message_dict[\"role\"]\\n', '\\ndef add_message(self, message_dict: dict) -> None:\\n    \"\"\"\\n        Add a message to the memory thread.\\n\\n        :param message_dict: A dictionary containing the role and content of the message.\\n        \"\"\"\\n    message_tokens = self.get_message_tokens(message_dict)\\n\\n    if (\\n        self.max_memory is None\\n        or self.total_tokens + message_tokens <= self.max_memory\\n    ):\\n        # add the message_dict to the memory_thread\\n        # update the total number of tokens\\n        self.memory_thread.append(message_dict)\\n        self.total_tokens += message_tokens\\n        self.message_tokens.append(message_tokens)\\n        time_stamp = time.time()\\n        self.time_stamps.append(time_stamp)\\n    else:\\n        display(\\n            Markdown(\\n                \"The memory BaseThread is full, the last message was not added\"\\n            )\\n        )\\n', '\\ndef remove_message(\\n    self, message_dict: Union[dict, None] = None, idx: Union[int, None] = None\\n) -> None:\\n    \"\"\"\\n        Remove a message from the memory thread.\\n        \"\"\"\\n    if message_dict is None and idx is None:\\n        raise Exception(\"You need to provide either a message_dict or an idx\")\\n    elif message_dict is not None and idx is not None:\\n        raise Exception(\"You need to provide either a message_dict or an idx\")\\n\\n    if idx is None:\\n        message_dict = check_dict(message_dict)\\n        search_results = self.find_message(message_dict)\\n        if search_results is not None:\\n            idx = search_results[-1][\"idx\"]\\n            message = search_results[-1][\"message_dict\"]\\n            self.memory_thread.pop(idx)\\n            self.message_tokens.pop(idx)\\n            self.time_stamps.pop(idx)\\n            self.total_tokens -= self.get_message_tokens(message)\\n        else:\\n            raise Exception(\"The message was not found in the memory BaseThread\")\\n    else:\\n        if idx < len(self.memory_thread):\\n            message = self.memory_thread.pop(idx)\\n            self.total_tokens -= self.get_message_tokens(message)\\n        else:\\n            raise Exception(\"The index was out bound\")\\n', '\\ndef find_message(\\n    self, message: Union[dict, str], role: Union[str, None] = None\\n) -> Union[None, list]:\\n    \"\"\"\\n        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\\n        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\\n    # check if the message is a dictioanry or a string\\n    message = message if isinstance(message, str) else check_dict(message)\\n    search_results = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        target = (\\n            message_dict if isinstance(message, dict) else message_dict[\"content\"]\\n        )\\n        if target == message and (role is None or message_dict[\"role\"] == role):\\n            search_results.append({\"idx\": idx, \"message_dict\": message_dict})\\n    return search_results if len(search_results) > 0 else None\\n', '\\ndef find_role(self, role: str) -> Union[None, list]:\\n    \"\"\"\\n        Find all messages with a specific role in the memory thread.\\n        \"\"\"\\n    search_results = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if message_dict[\"role\"] == role:\\n            search_results.append({\"idx\": idx, \"message_dict\": message_dict})\\n    return search_results if len(search_results) > 0 else None\\n', '\\ndef last_message(self, role: Union[str, None] = None) -> Union[None, dict]:\\n    \"\"\"\\n        Get the last message in the memory thread with a specific role.\"\"\"\\n    if role is None:\\n        return self.memory_thread[-1]\\n    else:\\n        for message_dict in reversed(self.memory_thread):\\n            if message_dict[\"role\"] == role:\\n                return message_dict\\n        return None\\n', '\\ndef first_message(self, role: Union[str, None] = None) -> Union[None, dict]:\\n    \"\"\"\\n        Get the first message in the memory thread with a specific role.\"\"\"\\n    if role is None:\\n        return self.memory_thread[0]\\n    else:\\n        for message_dict in self.memory_thread:\\n            if message_dict[\"role\"] == role:\\n                return message_dict\\n        return None\\n', '\\ndef messages_before(\\n    self, message: dict, role: Union[str, None] = None\\n) -> Union[None, list]:\\n    \"\"\"\\n        Get all messages before a specific message in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    # print(\"ci siamo\")\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        # print(message, message_dict)\\n        if message_dict == message :\\n            for mess in self.memory_thread[:idx]:\\n                if role is None or mess[\"role\"] == role:\\n                    messages.append(mess)\\n            break\\n    return messages if len(messages) > 0 else None\\n', '\\ndef messages_after(\\n    self, message: dict, role: Union[str, None] = None\\n) -> Union[None, list]:\\n    \"\"\"\\n        Get all messages after a specific message in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if message_dict == message:\\n            for mess in self.memory_thread[:idx]:\\n                if role is None or mess[\"role\"] == role:\\n                    messages.append(mess)\\n            break\\n    return messages if len(messages) > 0 else None\\n', '\\ndef messages_between(\\n    self, start_message: dict, end_message: dict, role: Union[str, None] = None\\n) -> Union[None, list]:\\n    \"\"\"\\n        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if message_dict == start_message:\\n            start_idx = idx\\n            break\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if message_dict == end_message:\\n            end_idx = idx\\n            break\\n    for mess in self.memory_thread[start_idx + 1 : end_idx-1]:\\n                if role is None or mess[\"role\"] == role:\\n                    messages.append(mess)\\n    return messages if len(messages) > 0 else None\\n', '\\ndef messages_more_tokens(self, tokens: int, role: Union[str, None] = None):\\n    \"\"\"\\n        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if self.message_tokens[idx] > tokens and (\\n            role is None or message_dict[\"role\"] == role\\n        ):\\n            messages.append(message_dict)\\n    return messages if len(messages) > 0 else None\\n', '\\ndef messages_less_tokens(self, tokens: int, role: Union[str, None] = None):\\n    \"\"\"\\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if self.message_tokens[idx] < tokens and (\\n            role is None or message_dict[\"role\"] == role\\n        ):\\n            messages.append(message_dict)\\n    return messages if len(messages) > 0 else None\\n', '\\ndef messages_between_tokens(\\n    self, start_tokens: int, end_tokens: int, role: Union[str, None] = None\\n):\\n    \"\"\"\\n        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if (\\n            self.message_tokens[idx] > start_tokens\\n            and self.message_tokens[idx] < end_tokens\\n            and (role is None or message_dict[\"role\"] == role)\\n        ):\\n            messages.append(message_dict)\\n    return messages if len(messages) > 0 else None\\n', '\\ndef messages_before_time(self, time_stamp, role: Union[str, None] = None):\\n    \"\"\"\\n        Get all messages before a specific time in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if self.time_stamps[idx] < time_stamp and (\\n            role is None or message_dict[\"role\"] == role\\n        ):\\n            messages.append(message_dict)\\n    return messages if len(messages) > 0 else None\\n', '\\ndef messages_after_time(self, time_stamp, role: Union[str, None] = None):\\n    \"\"\"\\n        Get all messages after a specific time in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if self.time_stamps[idx] > time_stamp and (\\n            role is None or message_dict[\"role\"] == role\\n        ):\\n            messages.append(message_dict)\\n    return messages if len(messages) > 0 else None\\n', '\\ndef messages_between_time(\\n    self, start_time, end_time, role: Union[str, None] = None\\n):\\n    \"\"\"\\n        Get all messages between two specific times in the memory thread with a specific role.\"\"\"\\n    messages = []\\n    for idx, message_dict in enumerate(self.memory_thread):\\n        if (\\n            self.time_stamps[idx] > start_time\\n            and self.time_stamps[idx] < end_time\\n            and (role is None or message_dict[\"role\"] == role)\\n        ):\\n            messages.append(message_dict)\\n    return messages if len(messages) > 0 else None\\n', '\\ndef token_bound_history(\\n    self, max_tokens: int, max_history=None, role: Union[str, None] = None\\n):\\n    messages = []\\n    indices = []\\n    tokens = 0\\n    if max_history is None:\\n        max_history = len(self.memory_thread)\\n\\n    for idx, message_dict in enumerate(reversed(self.memory_thread)):\\n        if  tokens + self.message_tokens[idx] <= max_tokens:\\n            if role is not None and message_dict[\"role\"] != role:\\n                continue\\n            messages.append(message_dict)\\n            indices.append(len(self.memory_thread) - 1 - idx)\\n            tokens += self.message_tokens[idx]\\n        else:\\n            break\\n    return messages, indices if len(messages) > 0 else (None, None)\\n']\n",
      "['\\ndef __init__(\\n    self, name=\"fifo_memory\", max_memory=None, longterm_thread=None, redundant=True\\n):\\n\\n    BaseThread.__init__(self, name=name, max_memory=None)\\n    if redundant is True:\\n        self.redundant_thread = BaseThread(name=\"lucid_memory\", max_memory=None)\\n    else:\\n        self.redundant_thread = None\\n    if longterm_thread is None:\\n        self.longterm_thread = BaseThread(name=\"longterm_memory\", max_memory=None)\\n    else:\\n        self.longterm_thread = longterm_thread\\n    # create an alias for the memory_thread to make the code more readable\\n    self.fifo_thread = self.memory_thread\\n    self.max_memory = max_memory\\n', '\\ndef to_longterm(self, idx: int):\\n    \"\"\"move the message at the index idx to the longterm_memory\"\"\"\\n    # move the message at the index idx to the longterm_memory\\n    display(\\n        Markdown(\\n            \"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(\\n                idx\\n            )\\n        )\\n    )\\n    message = copy.deepcopy(self.memory_thread[idx])\\n    # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\\n    self.longterm_thread.add_message(message)\\n    self.remove_message(idx=idx)\\n', '\\ndef add_message(self, message_dict: dict):\\n    \"\"\"add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\\n    # message_dict = {\"role\": role, \"content\": content}\\n    # chek that the message_dict is a dictionary or a list of dictionaries\\n    message_dict = check_dict(message_dict)\\n    if self.redundant_thread is not None:\\n        self.redundant_thread.add_message(message_dict)\\n    message_tokens = self.get_message_tokens(message_dict)\\n\\n    if self.total_tokens + message_tokens > self.max_memory:\\n        while self.total_tokens + message_tokens > self.max_memory:\\n            if len(self.memory_thread) > 0:\\n                self.to_longterm(idx=0)\\n        super().add_message(message_dict)\\n\\n    else:\\n        # add the message_dict to the memory_thread\\n        # update the total number of tokens\\n        super().add_message(message_dict)\\n']\n",
      "[]\n",
      "['\\ndef prune_index(\\n    cls: \"MemoryIndex\",\\n    constraint: Optional[str] = None,\\n    regex_pattern: Optional[str] = None,\\n    length_constraint: Optional[Tuple[int,int]] = None,\\n    tokenizer: Optional[tiktoken.Encoding] = None,\\n) -> \"MemoryIndex\" :\\n    if constraint is not None:\\n        if constraint == \"regex\":\\n            if regex_pattern is None:\\n                raise ValueError(\"regex_pattern must be provided for regex constraint.\")\\n            pruned_values, pruned_embeddings = _prune_by_regex(cls, regex_pattern)\\n        elif constraint == \"length\":\\n            if length_constraint is None:\\n                raise ValueError(\"length_constraint must be provided for length constraint.\")\\n            pruned_values, pruned_embeddings = _prune_by_length(cls, length_constraint, tokenizer)\\n        else:\\n            raise ValueError(\"Invalid constraint type provided.\")\\n    else:\\n        raise ValueError(\"constraint must be provided for pruning the index.\")\\n\\n    pruned_memory_index = cls.__class__(\\n        values=pruned_values,\\n        embeddings=pruned_embeddings,\\n        name=cls.name + \"_pruned\",\\n    )\\n\\n    return pruned_memory_index\\n', '\\n\\ndef _prune_by_regex(cls: \"MemoryIndex\", regex_pattern: str) -> Tuple[List[str], List[np.ndarray]]:\\n    pruned_values = []\\n    pruned_embeddings = []\\n\\n    for value in cls.values:\\n        if re.search(regex_pattern, value):\\n            pruned_values.append(value)\\n            pruned_embeddings.append(cls.get_embedding_by_value(value))\\n\\n    return pruned_values, pruned_embeddings\\n', '\\n\\ndef _prune_by_length(cls: \"MemoryIndex\", length_constraint: Tuple[int,int], tokenizer) -> Tuple[List[str], List[np.ndarray]]:\\n    pruned_values = []\\n    pruned_embeddings = []\\n    if tokenizer is None:\\n        len_func = len\\n    else:\\n        def len_func(value):\\n            return len(tokenizer.encode(value))\\n    print(\"Pruning by length\")\\n    print(\"Length constraint: \", length_constraint)\\n    print(\"Number of values: \", len(cls.values))\\n    print(\"tokenizer: \", tokenizer)\\n    for value in cls.values:\\n        if len_func(value) <= length_constraint[1] and len_func(value) >= length_constraint[0]:\\n            print(f\"value {value} is in range {length_constraint}\")\\n            pruned_values.append(value)\\n            pruned_embeddings.append(cls.get_embedding_by_value(value))\\n\\n    return pruned_values, pruned_embeddings\\n', 'def len_func(value):\\n    return len(tokenizer.encode(value))\\n', '\\ndef save(cls):\\n    save_directory = os.path.join(cls.save_path, cls.name)\\n    os.makedirs(save_directory, exist_ok=True)\\n\\n    index_filename = os.path.join(save_directory, f\"{cls.name}_index.faiss\")\\n    faiss.write_index(cls.index, index_filename)\\n\\n    values_filename = os.path.join(save_directory, f\"{cls.name}_values.json\")\\n    with open(values_filename, \"w\") as f:\\n        json.dump(cls.values, f)\\n\\n    embeddings_filename = os.path.join(save_directory, f\"{cls.name}_embeddings.npz\")\\n    np.savez_compressed(embeddings_filename, cls.get_all_embeddings())\\n', '\\ndef load(cls):\\n    load_directory = os.path.join(cls.save_path, cls.name)\\n    if not os.path.exists(load_directory):\\n        cls.loaded = False\\n        print(\"I did not find the directory to load the index from.\", load_directory)\\n        return\\n\\n    print(f\"Loading index from {load_directory}\")\\n\\n    index_filename = os.path.join(load_directory, f\"{cls.name}_index.faiss\")\\n    cls.index = faiss.read_index(index_filename)\\n\\n    values_filename = os.path.join(load_directory, f\"{cls.name}_values.json\")\\n    with open(values_filename, \"r\") as f:\\n        cls.values = json.load(f)\\n    embeddings_filename = os.path.join(load_directory, f\"{cls.name}_embeddings.npz\")\\n    print(embeddings_filename)\\n    embeddings_data = np.load(embeddings_filename)\\n    cls.embeddings = embeddings_data[\"arr_0\"]\\n    cls.loaded = True\\n', '\\ndef __init__(\\n    self,\\n    index: Optional[faiss.Index] = None,\\n    values: Optional[List[str]] = None,\\n    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n    name: str = \"memory_index\",\\n    save_path: Optional[str] = None,\\n    load: bool = False,\\n    tokenizer: Optional[tiktoken.Encoding] = None,\\n    max_workers: int = 1,\\n    backup: bool = False,\\n    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]] = OpenAiEmbedder,\\n    is_batched: bool = False,\\n    markdown: str = \"text/markdown\",\\n):\\n\\n    self.name = name\\n    self.embedder = embedder()\\n    self.save_path = save_path if save_path is not None else \"storage\"\\n    os.makedirs(self.save_path, exist_ok=True)\\n    self.values = []\\n    self.embeddings = []\\n    self.max_workers = max_workers\\n    self.is_batched = is_batched\\n    self.markdown = markdown\\n\\n    if load is True:\\n        self.load()\\n    else:\\n        self.loaded = False\\n    if not self.loaded:\\n        if (\\n            self.max_workers > 1\\n            and values is not None\\n            and embeddings is None\\n            and index is None\\n        ):\\n            embeddings = parallel_embeddings(self.embedder,\\n                values, max_workers, backup=backup, name=name\\n            )\\n        self.init_index(index, values, embeddings, is_embed_batched=is_batched)\\n    if tokenizer is None:\\n        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n    else:\\n        self.tokenizer = tokenizer\\n    self.query_history = []\\n    if not self.loaded:\\n        self.save()\\n', '\\ndef init_index(\\n    self,\\n    index: Optional[faiss.Index] = None,\\n    values: Optional[List[str]] = None,\\n    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\\n    is_embed_batched: bool = False,\\n) -> None:\\n\\n    \"\"\"\\n        initializes the index, there are 4 cases:\\n        1. we create a new index from scratch\\n        2. we create a new index from a list of embeddings and values\\n        3. we create a new index from a faiss index and values list\\n        4. we load an index from a file\\n        \"\"\"\\n    if index is None and values is None and embeddings is None:\\n        print(\"Creating a new index\")\\n        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n        self.values = []\\n    elif (\\n        index is None\\n        and values is not None\\n        and embeddings is not None\\n        and len(values) == len(embeddings)\\n    ):\\n        print(\"Creating a new index from a list of embeddings and values\")\\n        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n        #add all the embeddings to the index\\n        if is_embed_batched:\\n            print(\"Adding batched embeddings to index\")\\n            print(type(embeddings))\\n            embeddings = np.array(embeddings)\\n            self.add_batch_to_index(values=values, embeddings=embeddings)\\n        else:\\n            for embedding, value in zip(embeddings, values):\\n                self.add_to_index(value, embedding)\\n\\n    elif (\\n        isinstance(index, faiss.Index)\\n        and index.d == self.embedder.get_embedding_size()\\n        and type(values) == list\\n        and len(values) == index.ntotal\\n    ):\\n        print(\"Creating a new index from a faiss index and values list\")\\n        self.index = index\\n        self.values = values\\n    elif index is None and values is not None and embeddings is None:\\n        print(\"Creating a new index from a list of values\")\\n        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\\n        if is_embed_batched:\\n            batch = []\\n            i = 0\\n            for value in values:\\n                batch.append(value)\\n                if len(batch) == 1000:\\n                    start = time.time()\\n                    self.add_batch_to_index(values=batch)\\n                    print(f\"Embedding batch {i} took \", time.time() - start, \" seconds\")\\n                    print(f\"Batch {i} of {len(values)//1000}\")\\n                    i +=1\\n                    batch = []\\n            if len(batch) > 0:\\n                self.add_batch_to_index(values=batch)\\n        else:\\n            i = 0\\n            for value in values:\\n                print(\"Embedding value \", i, \" of \", len(values))\\n                start = time.time()\\n                self.add_to_index(value)\\n                print(\"Embedding value \", i, \" took \", time.time() - start, \" seconds\")\\n                i += 1\\n    else:\\n        print(type(values))\\n        print(type(embeddings))\\n        print(type(index))\\n        raise ValueError(\\n            \"The index is not a valid faiss index or the embedding dimension is not correct\"\\n        )\\n    print(len(self.values), \" values in the index\")\\n    print(self.index.ntotal, \" embeddings in the index\")\\n', '\\ndef __getstate__(self):\\n    state = self.__dict__.copy()\\n    del state[\"index\"]\\n\\n    index_buffer = io.BytesIO()\\n    faiss.write_index(state[\"index\"], index_buffer)\\n    state[\"index_bytes\"] = index_buffer.getvalue()\\n\\n    return state\\n', '\\ndef __setstate__(self, state):\\n    index_buffer = io.BytesIO(state[\"index_bytes\"])\\n    state[\"index\"] = faiss.read_index(index_buffer)\\n\\n    del state[\"index_bytes\"]\\n\\n    self.__dict__.update(state)\\n', '\\n@classmethod\\ndef from_pandas(\\n    cls,\\n    data_frame: Union[pd.DataFrame, str],\\n    columns: Optional[Union[str, List[str]]] = None,\\n    name: str = \"memory_index\",\\n    save_path: Optional[str] = None,\\n    in_place: bool = True,\\n    embeddings_col: Optional[str] = None,\\n    markdown: str = \"text/markdown\",\\n) -> \"MemoryIndex\":\\n    \"\"\"\\n        Initialize a MemoryIndex object from a pandas DataFrame.\\n\\n        Args:\\n            data_frame: The DataFrame or path to a CSV file.\\n            columns: The columns of the DataFrame to use as values.\\n            name: The name of the index.\\n            save_path: The path to save the index.\\n            in_place: Whether to work on the DataFrame in place or create a copy.\\n            embeddings_col: The column name containing the embeddings.\\n\\n        Returns:\\n            A MemoryIndex object initialized with values and embeddings from the DataFrame.\\n        \"\"\"\\n\\n    if (\\n        isinstance(data_frame, str)\\n        and data_frame.endswith(\".csv\")\\n        and os.path.isfile(data_frame)\\n    ):\\n        print(\"Loading the CSV file\")\\n        try:\\n            data_frame = pd.read_csv(data_frame)\\n        except:\\n            raise ValueError(\"The CSV file is not valid\")\\n        name = data_frame.split(\"/\")[-1].split(\".\")[0]\\n    elif isinstance(data_frame, pd.core.frame.DataFrame) and columns is not None:\\n        print(\"Loading the DataFrame\")\\n        if not in_place:\\n            data_frame = copy.deepcopy(data_frame)\\n    else:\\n        raise ValueError(\\n            \"The data_frame is not a valid pandas dataframe or the columns are not valid or the path is not valid\"\\n        )\\n\\n    values, embeddings = extract_values_and_embeddings(\\n        data_frame, columns, embeddings_col\\n    )\\n    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path,markdown=markdown)\\n', '\\n@classmethod\\ndef from_hf_dataset(\\n    cls,\\n    dataset_url: str,\\n    value_column: str,\\n    embeddings_column: Optional[str] = None,\\n    name: str = \"memory_index\",\\n    save_path: Optional[str] = None,\\n    embedder: Optional[Union[OpenAiEmbedder,CohereEmbedder]]= CohereEmbedder,\\n    is_batched: bool = False,\\n    markdown: str = \"text/markdown\"\\n) -> \"MemoryIndex\":\\n    \"\"\"\\n        Initialize a MemoryIndex object from a Hugging Face dataset.\\n\\n        Args:\\n            dataset_url: The URL of the Hugging Face dataset.\\n            value_column: The column of the dataset to use as values.\\n            embeddings_column: The column of the dataset containing the embeddings.\\n            name: The name of the index.\\n            save_path: The path to save the index.\\n\\n        Returns:\\n            A MemoryIndex object initialized with values and embeddings from the Hugging Face dataset.\\n        \"\"\"\\n    dataset = load_dataset(dataset_url)[\\'train\\']\\n    if embeddings_column is not None:\\n        values, embeddings = extract_values_and_embeddings_hf(\\n            dataset, value_column, embeddings_column\\n        )\\n    elif embeddings_column is None:\\n        values = extract_values_hf(dataset, value_column)\\n        embeddings = None\\n    else:\\n        raise ValueError(\\n            \"The dataset is not a valid Hugging Face dataset or the columns are not valid\"\\n        )\\n    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path, embedder=embedder, is_batched=is_batched, markdown=markdown)\\n', '\\ndef add_to_index(\\n    self,\\n    value: str,\\n    embedding: Optional[Union[List[float], np.ndarray, str]] = None,\\n    verbose: bool = False,\\n    default_save: bool = False,\\n) -> None:\\n    \"\"\"\\n        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\\n        \"\"\"\\n    if value not in self.values:\\n        if embedding is None:\\n            embedding = self.embedder.embed(value)\\n            if verbose:\\n                display(\\n                    Markdown(\"The value {value} was embedded\".format(value=value))\\n                )\\n            if type(embedding) is list:\\n                embedding = np.array([embedding])\\n            self.index.add(embedding)\\n            self.values.append(value)\\n        elif embedding is not None:\\n            if type(embedding) is list:\\n                embedding = np.array([embedding])\\n            elif type(embedding) is str:\\n                try:\\n                    embedding = eval(embedding)\\n                    embedding = np.array([embedding]).astype(np.float32)\\n                except (SyntaxError, ValueError):\\n                    print(\"The string is not a valid list, probably an error:\", embedding)\\n                    return\\n            elif type(embedding) is not np.ndarray:\\n                raise ValueError(\"The embedding is not a valid type\")\\n\\n            if embedding.ndim == 1:\\n                embedding = embedding.reshape(1, -1)\\n            self.index.add(embedding)\\n            self.values.append(value)\\n            if default_save:\\n                self.save()  # we should check here the save time is not too long\\n    else:\\n        if verbose:\\n            display(\\n                Markdown(\\n                    \"The value {value} was already in the index\".format(value=value)\\n                )\\n            )\\n', 'def add_batch_to_index(\\n    self,\\n    values: List[str],\\n    embeddings: Optional[Union[List[float], np.ndarray, str]] = None,\\n    verbose: bool = False,\\n    default_save: bool = False,\\n) -> None:\\n    \"\"\"\\n        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\\n        \"\"\"\\n\\n    if embeddings is None:\\n        embeddings = self.embedder.batch_embed(values)\\n        if verbose:\\n            display(\\n                Markdown(\"The value batch was embedded\")\\n            )\\n        if type(embeddings) is list:\\n            embeddings = np.array(embeddings)\\n        self.index.add(embeddings)\\n        self.values.extend(values)\\n    elif embeddings is not None:\\n        if type(embeddings) is list:\\n            embeddings = np.array([embeddings])\\n        elif type(embeddings) is str:\\n            try:\\n                embeddings = eval(embeddings)\\n                embeddings = np.array([embeddings]).astype(np.float32)\\n            except (SyntaxError, ValueError):\\n                print(\"The string is not a valid list, probably an error:\", embeddings)\\n                return\\n        elif type(embeddings) is not np.ndarray:\\n            raise ValueError(\"The embedding is not a valid type\")\\n\\n        self.index.add(embeddings)\\n        self.values.extend(values)\\n        if default_save:\\n            self.save()  # we should check here the save time is not too long\\n', '\\n\\ndef remove_from_index(self, value: str) -> None:\\n    \"\"\"\\n        Remove a value from the index and the values list.\\n        Args:\\n            value: The value to remove from the index.\\n        \"\"\"\\n    index = self.get_index_by_value(value)\\n    if index is not None:\\n        self.values.pop(index)\\n\\n        id_selector = faiss.IDSelectorArray(np.array([index], dtype=np.int64))\\n        self.index.remove_ids(id_selector)\\n\\n        self.save()\\n    else:\\n        print(f\"The value \\'{value}\\' was not found in the index.\")\\n', '\\ndef get_embedding_by_index(self, index: int) -> np.ndarray:\\n    \"\"\"\\n        Get the embedding corresponding to a certain index value.\\n        \"\"\"\\n    if index < 0 or index >= len(self.values):\\n        raise ValueError(\"The index is out of range\")\\n\\n    embedding = self.index.reconstruct(index)\\n\\n    return embedding\\n', '\\ndef get_index_by_value(self, value: str) -> Optional[int]:\\n    \"\"\"\\n        Get the index corresponding to a value in self.values.\\n        \"\"\"\\n    if value in self.values:\\n        index = self.values.index(value)\\n        return index\\n    else:\\n        return None\\n', '\\ndef get_embedding_by_value(self, value: str) -> Optional[np.ndarray]:\\n    \"\"\"\\n        Get the embedding corresponding to a certain value in self.values.\\n        \"\"\"\\n    index = self.get_index_by_value(value)\\n    if index is not None:\\n        embedding = self.get_embedding_by_index(index)\\n        return embedding\\n    else:\\n        return None\\n', '\\ndef get_all_embeddings(self) -> np.ndarray:\\n    \"\"\"\\n        Get all the embeddings in the index.\\n        \"\"\"\\n    embeddings = []\\n    for i in range(len(self.values)):\\n        embeddings.append(self.get_embedding_by_index(i))\\n    self.embeddings = np.array(embeddings)\\n    return self.embeddings\\n', '\\ndef faiss_query(self, query: str, k: int = 10) -> Tuple[List[str], List[float]]:\\n    \"\"\"Query the faiss index for the top-k most similar values to the query\"\"\"\\n\\n    embedding = self.embedder.embed(query)\\n    if k > len(self.values):\\n        k = len(self.values)\\n    D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\\n    values = [self.values[i] for i in I[0]]\\n    scores = [d for d in D[0]]\\n    return values, scores, I\\n', '\\ndef token_bound_query(self, query, k=10, max_tokens=4000):\\n    \"\"\"Query the faiss index for the top-k most similar values to the query, but bound the number of tokens retrieved by the max_tokens parameter\"\"\"\\n    returned_tokens = 0\\n    top_k_hint = []\\n    scores = []\\n    tokens = []\\n    indices = []\\n\\n    if len(self.values) > 0:\\n        top_k, scores, indices = self.faiss_query(query, k=min(k, len(self.values)))\\n\\n        for hint in top_k:\\n            message_tokens = len(self.tokenizer.encode(hint))\\n            tokens.append(message_tokens)\\n            if returned_tokens + message_tokens <= max_tokens:\\n                top_k_hint += [hint]\\n                returned_tokens += message_tokens\\n\\n        self.query_history.append(\\n            {\\n                \"query\": query,\\n                \"hints\": top_k_hint,\\n                \"scores\": scores,\\n                \"indices\": indices,\\n                \"hints_tokens\": tokens,\\n                \"returned_tokens\": returned_tokens,\\n                \"max_tokens\": max_tokens,\\n                \"k\": k,\\n            }\\n        )\\n\\n    return top_k_hint, scores, indices\\n', '\\ndef save(self):\\n    save(self)\\n', '\\ndef load(self):\\n    load(self)\\n', '\\ndef prune(\\n    self,\\n    constraint: Optional[str] = None,\\n    regex_pattern: Optional[str] = None,\\n    length_constraint: Optional[int] = None,\\n    tokenizer: Optional[tiktoken.Encoding] = None,\\n) -> \"MemoryIndex\":\\n    if tokenizer is None:\\n        tokenizer = self.tokenizer\\n    return prune_index(self, constraint, regex_pattern, length_constraint,tokenizer)\\n']\n",
      "['def __init__(\\n    self,\\n    directory_path: str,\\n    name: str = \"python_index\",\\n    save_path: Optional[str] = None,\\n    load: bool = False,\\n    minify_code: bool = False,\\n    remove_docstrings: bool = False,\\n    tokenizer: Optional[tiktoken.Encoding] = None,\\n    max_workers: int = 1,\\n    backup: bool = False,\\n    filter: str = \"class_function\"\\n):\\n\\n    PythonParser.__init__(\\n        self,\\n        directory_path=directory_path,\\n        minify_code=minify_code,\\n        remove_docstrings=remove_docstrings,\\n    )\\n    #check if load folder exists\\n    if save_path is None:\\n        save_path = \"storage\"\\n    load_directory = os.path.join(save_path, name)\\n    loadcheck = not load or not os.path.exists(load_directory)\\n    if load and not os.path.exists(load_directory):\\n        print(\"No python-index found even if load=True, indexing from scratch\")\\n    if loadcheck:\\n        # Extract functions and classes source code\\n        function_source_codes, class_source_codes, _, _ = self.process_directory()\\n        print(\\n            \"Indexing {} functions and {} classes\".format(\\n                len(function_source_codes), len(class_source_codes)\\n            )\\n        )\\n        # Concatenate function and class source code and index them\\n        if filter == \"function\":\\n            codes = function_source_codes\\n        elif filter == \"class\":\\n            codes = class_source_codes\\n        elif filter == \"class_function\":\\n            codes = function_source_codes + class_source_codes\\n        load = False\\n        self.function_source_codes = function_source_codes\\n        self.class_source_codes = class_source_codes\\n\\n     # Initialize the MemoryIndex\\n    MemoryIndex.__init__(\\n        self,\\n        name=name,\\n        values=codes if loadcheck else None,\\n        save_path=save_path,\\n        load=load,\\n        tokenizer=tokenizer,\\n        max_workers=max_workers,\\n        backup=backup,\\n    )\\n    self.markdown = \"python\"\\n']\n",
      "['\\ndef __init__(\\n    self,\\n    df: pd.DataFrame,\\n    row_func: Optional[Callable[[pd.Series], str]] = None,\\n    name=\"pandas_index\",\\n    columns: Optional[List[str]] = None,\\n    load=False,\\n):\\n    \"\"\"\\n        Initialize a PandasIndex object.\\n\\n        Args:\\n            df: A pandas DataFrame to index.\\n            row_func: An optional function to process rows before adding them to the index.\\n            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\\n        \"\"\"\\n    if row_func is None:\\n        row_func = lambda row: str(row)\\n    self.row_func = row_func\\n\\n    self.df = df\\n    MemoryIndex.__init__(\\n        self, name=name, load=load\\n    )  # Initialize the parent MemoryIndex class\\n\\n    for _, row in df.iterrows():\\n        self.add_to_index(row_func(row))\\n\\n    self.columns: Dict[str, MemoryIndex] = {}\\n\\n    # Set up columns during initialization\\n    if columns is None:\\n        self.setup_columns()\\n    else:\\n        self.setup_columns(columns)\\n    self.save()\\n    for col in self.columns:\\n        self.columns[col].save()\\n    self.executed_tasks = []\\n', '\\ndef setup_columns(self, columns: Optional[List[str]] = None):\\n    \"\"\"\\n        Set up columns for indexing.\\n\\n        Args:\\n            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\\n        \"\"\"\\n    if columns is None:\\n        # Use string columns or columns with lists containing a single string by default\\n        columns = []\\n\\n    for col in columns:\\n        self.columns[col] = MemoryIndex.from_pandas(\\n            self.df, columns=col, name=f\"{self.name}_{col}\"\\n        )\\n', '\\ndef query_columns(self, query: str, columns: List[str]) -> List[Tuple[str, float]]:\\n    \"\"\"\\n        Query the indexed columns of the DataFrame.\\n\\n        Args:\\n            query: The search query as a string.\\n            columns: A list of column names to query.\\n\\n        Returns:\\n            A list of tuples containing the matched value and its similarity score.\\n        \"\"\"\\n    results = []\\n    for col in columns:\\n        if col in self.columns:\\n            results.extend(self.columns[col].faiss_query(query))\\n        else:\\n            raise KeyError(\\n                f\"Column \\'{col}\\' not found in PandaDb columns dictionary.\"\\n            )\\n    return results\\n', '\\ndef add_row(self, row: pd.Series) -> None:\\n    \"\"\"\\n        Add a row to the DataFrame and update the row and column indexes.\\n\\n        Args:\\n            row: A pandas Series representing the row to add.\\n        \"\"\"\\n    self.df = pd.concat([self.df, row.to_frame().T], ignore_index=True)\\n    self.add_to_index(self.row_func(row))\\n\\n    for col in self.columns:\\n        if col in row:\\n            self.columns[col].add_to_index(row[col])\\n', '\\ndef remove_row(self, index: int) -> None:\\n    \"\"\"\\n        Remove a row from the DataFrame and update the row and column indexes.\\n\\n        Args:\\n            index: The index of the row to remove.\\n        \"\"\"\\n    if 0 <= index < len(self.df):\\n        self.remove_from_index(self.values[index])\\n\\n        for col in self.columns:\\n            self.columns[col].remove_from_index(self.columns[col].values[index])\\n\\n        self.df.drop(index, inplace=True)\\n        self.df.reset_index(drop=True, inplace=True)\\n    else:\\n        raise IndexError(\\n            f\"Index {index} is out of bounds for DataFrame with length {len(self.df)}\"\\n        )\\n', '\\ndef rows_from_value(\\n    self, value: Union[str, int, float], column: Optional[str] = None\\n) -> pd.DataFrame:\\n    \"\"\"\\n        Return all rows of the DataFrame that have a particular value in the row index or a column index.\\n\\n        Args:\\n            value: The value to search for in the DataFrame.\\n            column: The name of the column to search in. If None, search in the row index.\\n\\n        Returns:\\n            A pandas DataFrame containing the rows with the specified value.\\n        \"\"\"\\n    if column is None:\\n        return self.df.loc[self.df.index == value]\\n    else:\\n        if column in self.df.columns:\\n            return self.df.loc[self.df[column] == value]\\n        else:\\n            raise KeyError(f\"Column \\'{column}\\' not found in the DataFrame.\")\\n', '\\ndef apply_llmtask(\\n    self,\\n    path: List[List[int]],\\n    chatbot: Chat,\\n    task_name=None,\\n    write_func=None,\\n    columns: Optional[List[str]] = None,\\n    task_id=None,\\n    max_workers=1,\\n    calls_per_minute: int = 20,\\n) -> pd.DataFrame:\\n    \"\"\"\\n        Apply a writing task to the specified columns or the main index, and create new modified indexes and a corresponding DataFrame with new values.\\n\\n        Args:\\n            write_task: An instance of a writing task (subclass of BaseTask).\\n            columns: A list of column names to apply the writing task to, or None (default) to apply the task to the main index.\\n\\n        Returns:\\n            A pandas DataFrame containing the modified values in the specified columns or a new column with the modified values of the main index.\\n        \"\"\"\\n    modified_df = self.df.copy()\\n    if task_name is None and task_id is None:\\n        task_name = \"llm_task\"\\n    elif task_name is None:\\n        task_name = task_id\\n\\n    if columns is None:\\n        # Apply the writing task to the main index\\n        write_index = self\\n        write_task = LLMWriter(\\n            write_index,\\n            path,\\n            chatbot,\\n            task_name=task_name,\\n            write_func=write_func,\\n            context=self.df,\\n            task_id=task_id,\\n            max_workers=max_workers,\\n            calls_per_minute=calls_per_minute,\\n        )\\n\\n        new_index = write_task.write()\\n\\n        # Create a mapping of old values to new values\\n        old_to_new_values = dict(zip(self.values, new_index.values))\\n\\n        # Update the row values in the modified DataFrame\\n        modified_df[\"new_column\"] = modified_df.apply(\\n            lambda row: old_to_new_values.get(\\n                self.row_func(row), self.row_func(row)\\n            ),\\n            axis=1,\\n        )\\n    else:\\n        # Iterate over the specified columns\\n        for col in columns:\\n            if col in self.columns:\\n                # Apply the writing task to the column\\n                write_index = self.columns[col]\\n                write_task = LLMWriter(\\n                    write_index,\\n                    path,\\n                    chatbot,\\n                    write_func=write_func,\\n                    context=self.df,\\n                    task_id=task_id,\\n                    max_workers=max_workers,\\n                    calls_per_minute=calls_per_minute,\\n                )\\n                new_index = write_task.write()\\n\\n                # Create a mapping of old values to new values\\n                old_to_new_values = dict(\\n                    zip(self.columns[col].values, new_index.values)\\n                )\\n\\n                # Update the column values in the modified DataFrame\\n                modified_df[col] = modified_df[col].apply(\\n                    lambda x: old_to_new_values.get(x, x)\\n                )\\n\\n                # Update the column\\'s MemoryIndex\\n                self.columns[col] = new_index\\n                self.columns[col].save()\\n            else:\\n                raise KeyError(\\n                    f\"Column \\'{col}\\' not found in PandasIndex columns dictionary.\"\\n                )\\n    # remove context from the write_task to avoid memory leak\\n    write_task.context = None\\n    self.executed_tasks.append({\"task\": write_task, \"output\": modified_df})\\n\\n    return modified_df\\n']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['\\ndef __init__(\\n    self,\\n    model: Optional[str] = None,\\n    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\\n    system_prompt: Optional[str] = None,\\n    user_prompt: Optional[str] = None,\\n    name: str = \"fifo_memory\",\\n    max_index_memory: int = 400,\\n    max_fifo_memory: int = 2048,\\n    max_output_tokens: int = 1000,\\n    longterm_thread: Optional[BaseThread] = None,\\n):\\n\\n    FifoThread.__init__(\\n        self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread\\n    )\\n    Chat.__init__(\\n        self,\\n        model=model,\\n        index_dict=index_dict,\\n        max_output_tokens=max_output_tokens,\\n        max_index_memory=max_index_memory,\\n        system_prompt=system_prompt,\\n        user_prompt=user_prompt,\\n        name=name,\\n    )\\n\\n    self.prompt_func = self.fifo_memory_prompt\\n', '\\ndef fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\\n    \"\"\"\\n        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n    marked_question = mark_question(self.user_prompt(message))\\n    prompt = (\\n        [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\\n    )\\n    return prompt, marked_question\\n', '\\ndef query(self, question: str, verbose: bool = True) -> str:\\n    \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n    # First call the base class\\'s query method\\n    answer = BaseChat.query(self, message=question, verbose=verbose)\\n    marked_question = mark_question(question)\\n    # Add the marked question and answer to the memory\\n    self.add_message(marked_question)\\n    self.add_message(answer)\\n\\n    return answer\\n', '\\ndef __init__(\\n    self,\\n    model: Optional[str] = None,\\n    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\\n    name: str = \"vector_memory\",\\n    max_index_memory: int = 400,\\n    max_vector_memory: int = 2048,\\n    max_output_tokens: int = 1000,\\n    system_prompt: str = None,\\n    user_prompt: str = None,\\n\\n):\\n    VectorThread.__init__(self, name=name, max_context=max_vector_memory)\\n    Chat.__init__(\\n        self,\\n        model=model,\\n        index_dict=index_dict,\\n        max_output_tokens=max_output_tokens,\\n        max_index_memory=max_index_memory,\\n        system_prompt=system_prompt,\\n        user_prompt=user_prompt,\\n        name=name,\\n    )\\n    self.max_vector_memory = self.max_context\\n    self.prompt_func = self.vector_memory_prompt\\n', '\\ndef vector_memory_prompt(\\n    self, message: str, k: int = 10\\n) -> Tuple[List[dict], dict]:\\n    \"\"\"\\n        Combine system prompt, k most similar messages to the question, and the user prompt.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include in the prompt.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n    sorted_messages, sorted_scores, sorted_indices = self.sorted_query(\\n        message, k=k, max_tokens=self.max_vector_memory, reverse=True\\n    )\\n    marked_question = mark_question(self.user_prompt(message))\\n    prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\\n    return prompt, marked_question\\n', '\\ndef weighted_memory_prompt(\\n    self,\\n    message: str,\\n    k: int = 10,\\n    decay_factor: float = 0.1,\\n    temporal_weight: float = 0.5,\\n) -> Tuple[List[dict], dict]:\\n    \"\"\"\\n        Combine system prompt, weighted k most similar messages to the question, and the user prompt.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include in the prompt.\\n        :param decay_factor: A float representing the decay factor for weighting.\\n        :param temporal_weight: A float representing the weight of the temporal aspect.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n    weighted_messages, weighted_scores, weighted_indices = self.weighted_query(\\n        message,\\n        k=k,\\n        max_tokens=self.max_vector_memory,\\n        decay_factor=decay_factor,\\n        temporal_weight=temporal_weight,\\n        order_by=\"chronological\",\\n        reverse=True,\\n    )\\n    marked_question = mark_question(self.user_prompt(message))\\n    prompt = (\\n        [mark_system(self.system_prompt)] + weighted_messages + [marked_question]\\n    )\\n    return prompt, marked_question\\n', '\\ndef query(self, question: str, verbose: bool = False) -> str:\\n    \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n    # First call the base class\\'s query method\\n    answer = BaseChat.query(self, message=question, verbose=verbose)\\n    marked_question = mark_question(question)\\n    # Add the marked question and answer to the memory\\n    self.add_message(marked_question)\\n    self.add_message(answer)\\n    return answer\\n', '\\ndef __init__(\\n    self,\\n    model: str = None,\\n    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\\n    system_prompt: str = None,\\n    user_prompt: str = None,\\n    name: str = \"fifo_vector_memory\",\\n    max_memory: int = 2048,\\n    max_index_memory: int = 400,\\n    max_output_tokens: int = 1000,\\n    longterm_thread: Optional[VectorThread] = None,\\n    longterm_frac: float = 0.5,\\n):\\n    self.total_max_memory = max_memory\\n\\n    self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\\n    FifoThread.__init__(\\n        self,\\n        name=name,\\n        max_memory=self.max_fifo_memory,\\n        longterm_thread=self.longterm_thread,\\n    )\\n    Chat.__init__(\\n        self,\\n        model=model,\\n        index_dict=index_dict,\\n        max_output_tokens=max_output_tokens,\\n        max_index_memory=max_index_memory,\\n        system_prompt=system_prompt,\\n        user_prompt=user_prompt,\\n        name=name,\\n    )\\n    self.prompt_func = self.fifovector_memory_prompt\\n    self.prompt_list = []\\n', '\\ndef setup_longterm_memory(\\n    self,\\n    longterm_thread: Optional[VectorThread],\\n    max_memory: int,\\n    longterm_frac: float,\\n):\\n    \"\"\"\\n        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\\n\\n        :param longterm_thread: An optional VectorThread for long-term memory.\\n        :param max_memory: The maximum amount of memory for the chatbot.\\n        :param longterm_frac: The fraction of memory dedicated to long-term memory.\\n        \"\"\"\\n    if longterm_thread is None:\\n        self.longterm_frac = longterm_frac\\n        self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\\n        self.max_vector_memory = max_memory - self.max_fifo_memory\\n        self.longterm_thread = VectorThread(\\n            name=\"longterm_memory\", max_context=self.max_vector_memory\\n        )\\n    else:\\n        self.longterm_thread = longterm_thread\\n        self.max_vector_memory = self.longterm_thread.max_context\\n        self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\\n        self.longterm_frac = self.max_vector_memory / self.total_max_memory\\n', '\\ndef fifovector_memory_prompt(\\n    self, message: str, k: int = 10\\n) -> Tuple[List[dict], dict]:\\n    \"\"\"\\n        Combine the system prompt, long-term memory (vector memory), short-term memory (FIFO memory), and the user prompt.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include from the long-term memory.\\n        :return: A tuple containing a list of strings as the prompt and the marked question.\\n        \"\"\"\\n    prompt = [mark_system(self.system_prompt)]\\n    if (\\n        len(self.longterm_thread.memory_thread) > 0\\n        and self.longterm_thread.total_tokens <= self.max_vector_memory\\n    ):\\n        prompt += self.longterm_thread.memory_thread\\n    elif (\\n        len(self.longterm_thread.memory_thread) > 0\\n        and self.longterm_thread.total_tokens > self.max_vector_memory\\n    ):\\n        (\\n            sorted_messages,\\n            sorted_scores,\\n            sorted_indices,\\n        ) = self.longterm_thread.sorted_query(\\n            message, k=k, max_tokens=self.max_vector_memory, reverse=True\\n        )\\n        prompt += sorted_messages\\n\\n    prompt += self.memory_thread\\n    marked_question = mark_question(self.user_prompt(message))\\n    prompt += [marked_question]\\n    return prompt, marked_question\\n', '\\ndef query(self, question: str, verbose: bool = False) -> str:\\n    \"\"\"\\n        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\\n        and added to the memory.\\n\\n        :param question: A string representing the user question.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n    answer = BaseChat.query(self, message=question, verbose=verbose)\\n    marked_question = mark_question(question)\\n    self.add_message(marked_question)\\n    self.add_message(answer)\\n    return answer\\n']\n",
      "['\\ndef __init__(\\n    self,\\n    model: str = None,\\n    max_output_tokens: int = 1000,\\n    system_prompt: str = None,\\n    user_prompt: str = None,\\n    index_dict: Optional[Dict[str, MemoryIndex]] = None,\\n    max_index_memory: int = 1000,\\n    name: str = \"Chat\",\\n) -> None:\\n    BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\\n    Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\\n    self.index_dict = index_dict\\n    self.setup_indices(max_index_memory)\\n    self.name = name\\n', '\\ndef add_user_defined_ids(self, id_dict: Dict[str, list]):\\n    self.user_defined_ids.append(id_dict)\\n    self.use_user_defined_ids = True\\n    self.setup_index_prompts()\\n', '\\n\\ndef setup_index_prompts(self):\\n    if self.current_index is not None or self.use_user_defined_ids:\\n        print(\"Index is available so using index prompts\")\\n        self.system_prompt = (\\n            INDEX_SYSTEM_PROMPT \\n            if self.user_defined_system_prompt is None \\n            else self.user_defined_system_prompt\\n        )\\n        self.user_prompt = (\\n            self.get_index_hints\\n            if self.user_defined_user_prompt is None\\n            else self.user_defined_user_prompt\\n        )\\n    else:\\n        if self.user_defined_system_prompt is None:\\n            print(\"No user defined system prompt defaulting to default prompts\")\\n            self.set_default_prompts()\\n        else:\\n            print(\"User defined system prompt and default user prompt\")\\n            self.system_prompt = self.user_defined_system_prompt\\n            self.user_prompt = self.default_user_prompt\\n\\n\\n            \\n', '\\ndef setup_indices(self, max_index_memory):\\n    \"\"\"setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index.\"\"\"\\n    if self.index_dict is not None:\\n        self.max_index_memory = max_index_memory\\n        # set the last index to be the current index\\n        self.current_index = list(self.index_dict.keys())[-1]\\n        self.setup_index_prompts()    \\n    else:\\n        self.current_index = None\\n', '\\ndef get_index_hints(\\n    self, question: str, k: int = 10, max_tokens: int = None\\n) -> str:\\n    \"\"\"\\n        Get hints from the current index for the given question.\\n\\n        :param question: A string representing the user question.\\n        :param k: The number of most similar messages to include from the index.\\n        :param max_tokens: The maximum number of tokens to be retrieved from the index.\\n        :return: A string representing the hint prompt with the question.\\n        \"\"\"\\n    if max_tokens is None:\\n        max_tokens = self.max_index_memory\\n    hints = []\\n    if self.use_user_defined_ids is True:\\n        user_defined_id = self.user_defined_ids[-1]\\n        for index, ids in user_defined_id.items():\\n            for i in ids:\\n                hints.append(self.index_dict[index].values[i])\\n        self.use_user_defined_ids = False\\n        self.setup_index_prompts()\\n    elif self.current_index is not None:\\n        index_instance = self.index_dict[self.current_index]\\n        if isinstance(index_instance, MemoryIndex):\\n            hints, _, _ = index_instance.token_bound_query(\\n                question, k=k, max_tokens=max_tokens\\n            )\\n        else:\\n            raise ValueError(\"The current index is not a valid index instance.\")\\n    if len(hints) == 0:\\n        return question\\n    else:\\n        hints_string = \"\\\\n\".join(hints)\\n        hint_prompt = INDEX_HINT_PROMPT\\n        question_intro = QUESTION_INTRO\\n        return hint_prompt.format(\\n            hints_string=hints_string\\n        ) + question_intro.format(question=question)\\n', '\\ndef set_current_index(self, index_name: Optional[str]) -> None:\\n    \"\"\"\\n        Set the current index to be used for hints.\\n\\n        :param index_name: A string representing the index name or None to clear the current index.\\n        :raise ValueError: If the provided index name is not available.\\n        \"\"\"\\n    if self.index_dict is None:\\n        raise ValueError(\"No index_dict are available.\")\\n    elif index_name in self.index_dict:\\n        self.current_index = index_name\\n    elif index_name is None:\\n        self.current_index = None\\n    else:\\n        raise ValueError(\"The provided index name is not available.\")\\n    self.setup_index_prompts()\\n']\n",
      "['\\ndef __init__(self, system_prompt: str = None, user_prompt: str = None):\\n    \"\"\"\\n        Initialize the Prompter with system and user prompts.\\n\\n        :param system_prompt: A string representing the system prompt.\\n        :param user_prompt: A string representing the user prompt.\\n        \"\"\"\\n    \\n    if system_prompt is None:\\n        self.system_prompt = DEFAULT_SYSTEM_PROMPT\\n        self.user_defined_system_prompt = None\\n    else:\\n        self.system_prompt = system_prompt\\n        self.user_defined_system_prompt = system_prompt\\n    if user_prompt is None:\\n        self.user_prompt = self.default_user_prompt\\n        self.user_defined_user_prompt = None\\n    else:\\n        self.user_prompt = user_prompt\\n        self.user_defined_user_prompt = user_prompt\\n\\n    self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\\n    self.user_defined_ids = []\\n    self.user_defined_values = []\\n    self.use_user_defined_ids = False\\n', '\\ndef set_default_prompts(self):\\n    self.system_prompt = DEFAULT_SYSTEM_PROMPT\\n    self.user_prompt = self.default_user_prompt\\n', '\\n\\ndef default_user_prompt(self, message: str) -> str:\\n    return DEFAULT_USER_PROMPT.format(question=message)\\n', '\\ndef one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\\n    \"\"\"\\n        Compose the prompt for the chat-gpt API.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\\n        \"\"\"\\n    marked_question = mark_question(self.user_prompt(message))\\n    prompt = [mark_system(self.system_prompt)] + [marked_question]\\n    return prompt, marked_question\\n', '\\ndef update_system_prompt(self, new_prompt: str) -> None:\\n    \"\"\"\\n        Update the system prompt.\\n\\n        :param new_prompt: A string representing the new system prompt.\\n        \"\"\"\\n    self.system_prompt = new_prompt\\n', '\\ndef update_user_prompt(self, new_prompt ) -> None:\\n    \"\"\"\\n        Update the user prompt.\\n\\n        :param new_prompt: A string representing the new user prompt.\\n        \"\"\"\\n    self.user_prompt = new_prompt\\n', '\\ndef __init__(self, model: str = None, max_output_tokens: int = 200):\\n    \"\"\"\\n        Initialize the BaseChat with a model and max_output_tokens.\\n\\n        :param model: A string representing the chat model to be used.\\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\\n        \"\"\"\\n    if model is None:\\n        self.model = \"gpt-3.5-turbo\"\\n    else:\\n        self.model = model\\n    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n    self.max_output_tokens = max_output_tokens\\n    self.failed_responses = []\\n    self.outputs = []\\n    self.inputs = []\\n    self.prompts = []\\n    self.prompt_func = self.identity_prompter\\n', '\\ndef __getstate__(self):\\n    state = self.__dict__.copy()\\n    # Remove the tokenizer attribute from the state\\n    del state[\"tokenizer\"]\\n    return state\\n', '\\ndef __setstate__(self, state):\\n    self.__dict__.update(state)\\n    # Reinitialize the tokenizer attribute after unpickling\\n    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\\n', '\\ndef identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\\n    \"\"\"\\n        A simple identity prompter that takes a message and returns the message marked as a question.\\n\\n        :param message: A string representing the user message.\\n        :return: A tuple containing the marked question and the original message.\\n        \"\"\"\\n    return [mark_question(message)], mark_question(message)\\n', '\\ndef chat_response(\\n    self, prompt: List[dict], max_tokens: int = None ) -> Tuple[Dict, bool]:\\n    if max_tokens is None:\\n        max_tokens = self.max_output_tokens\\n    if \"gpt\" in self.model:\\n        response, status = chatgpt_response(prompt=prompt,model=self.model, max_tokens = max_tokens)\\n    elif \"command\" in self.model:\\n        response, status = cohere_response(prompt=prompt,model=self.model, max_tokens = max_tokens)  \\n    if not status:\\n        self.failed_responses.append(response)\\n        return response, False\\n    return response, True\\n', '\\ndef reply(self, message: str, verbose: bool = True) -> str:\\n    \"\"\"\\n        Reply to a given message using the chatbot.\\n\\n        :param message: A string representing the user message.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n    return self.query(message, verbose)[\"content\"]\\n', '\\ndef query(self, message: str, verbose: bool = True) -> str:\\n    \"\"\"\\n        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\\n\\n        :param message: A string representing the user message.\\n        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\\n        :return: A string representing the chatbot\\'s response.\\n        \"\"\"\\n\\n    prompt, _ = self.prompt_func(message)\\n    response, success = self.chat_response(prompt)\\n    if verbose:\\n        display(Markdown(\"#### Question: \\\\n {question}\".format(question=message)))\\n    if success:\\n        answer = get_mark_from_response(response, self.model)\\n        self.outputs.append(answer)\\n        self.inputs.append(message)\\n        self.prompts.append(prompt)\\n        if verbose:\\n            display(\\n                Markdown(\\n                    \" #### Anwser: \\\\n {answer}\".format(\\n                        answer=get_str_from_response(response, self.model)\\n                    )\\n                )\\n            )\\n        return answer\\n    else:\\n        raise Exception(\"OpenAI API Error inside query function\")\\n', '\\ndef reset_logs(self):\\n    \"\"\"\\n        Reset the chatbot\\'s memory.\\n        \"\"\"\\n    self.outputs = []\\n    self.inputs = []\\n    self.prompts = []\\n', '\\ndef get_logs(self):\\n    \"\"\"\\n        Get the chatbot\\'s memory.\\n\\n        :return: A tuple containing the chatbot\\'s memory as three lists of strings.\\n        \"\"\"\\n    return self.inputs, self.outputs, self.prompts\\n', '\\ndef run_text(\\n    self, text: str, state: List[Tuple[str, str]]\\n) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\\n    \"\"\"\\n        Process the user\\'s text input and update the chat state.\\n\\n        :param text: A string representing the user input.\\n        :param state: A list of tuples representing the current chat state.\\n        :return: A tuple containing the updated chat state as two lists of tuples.\\n        \"\"\"\\n    print(\"===============Running run_text =============\")\\n    print(\"Inputs:\", text)\\n    try:\\n        print(\"======>Current memory:\\\\n %s\" % self.memory_thread)\\n    except:\\n        print(\"======>No memory\")\\n    print(\"failed here\")\\n    response = self.reply(text)\\n    state = state + [(text, response)]\\n    print(\"Outputs:\", state)\\n    return state, state\\n', '\\ndef gradio(self):\\n    \"\"\"\\n        Create and launch a Gradio interface for the chatbot.\\n        \"\"\"\\n    with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\\n        chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\\n        state = gr.State([])\\n        with gr.Row():\\n            with gr.Column(scale=1):\\n                txt = gr.Textbox(\\n                    show_label=False,\\n                    placeholder=\"Enter text and press enter, or upload an image\",\\n                ).style(container=False)\\n            with gr.Column(scale=0.15, min_width=0):\\n                clear = gr.Button(\"Clear\")\\n\\n        txt.submit(lambda text, state: self.run_text(text, state), [txt, state], [chatbot, state])\\n        txt.submit(lambda: \"\", None, txt)\\n        demo.launch(server_name=\"localhost\", server_port=7860)\\n']\n",
      "[]\n",
      "[\"\\n\\ndef list_subjects_and_perspective():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n        perspectives.add(perspective)\\n    return subjects, perspectives\\n\", \"\\ndef list_subjects():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    subjects = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        subjects.add(subject)\\n    return subjects\\n\", \"\\ndef list_perspectives():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    subject_and_perspective = list(prompts.keys())\\n    perspectives = set()\\n    for item in subject_and_perspective:\\n        subject, perspective = item.split('\\\\\\\\')\\n        perspectives.add(perspective)\\n    return perspectives\\n\", \"\\ndef get_perspective_prompt(subject, perspective):\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    key = subject + '\\\\\\\\' + perspective\\n    if key in prompts:\\n        return prompts[key]\\n    else:\\n        raise Exception('No prompt found for subject: ' + subject + ' and perspective: ' + perspective +' use list_subjects() and list_perspectives() to see available prompts')\\n\", \"\\ndef get_random_perspective_prompt():\\n    # read from the file\\n    prompts = json.load(open(filename))\\n    key = random.choice(list(prompts.keys()))\\n    subject = key.split('\\\\\\\\')[0]\\n    perspective = key.split('\\\\\\\\')[1]\\n    return subject, perspective, prompts[key]\\n\"]\n",
      "['\\ndef extract_values_and_embeddings(\\n        data_frame: pd.DataFrame,\\n        columns: Union[str, List[str]],\\n        embeddings_col: Optional[str],\\n    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n        Extract values and embeddings from a pandas DataFrame.\\n\\n        Args:\\n            data_frame: The DataFrame to extract values and embeddings from.\\n            columns: The columns of the DataFrame to use as values.\\n            embeddings_col: The column name containing the embeddings.\\n\\n        Returns:\\n            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n        \"\"\"\\n\\n    if isinstance(columns, list) and len(columns) > 1:\\n        data_frame[\"values_combined\"] = data_frame[columns].apply(\\n            lambda x: \" \".join(x), axis=1\\n        )\\n        columns = \"values_combined\"\\n    elif isinstance(columns, list) and len(columns) == 1:\\n        columns = columns[0]\\n    elif not isinstance(columns, str):\\n        raise ValueError(\"The columns are not valid\")\\n\\n    values = []\\n    embeddings = []\\n\\n    for _, row in data_frame.iterrows():\\n        value = row[columns]\\n        values.append(value)\\n\\n        if embeddings_col is not None:\\n            embedding = row[embeddings_col]\\n            embeddings.append(embedding)\\n\\n    return values, embeddings if embeddings_col is not None else None\\n']\n",
      "['\\ndef convert_mark_to_str_prompt(messages: List[dict], prompt: str = \"\") -> str:\\n    prompt = \"\"\\n    for message in messages:\\n        role = message[\"role\"].upper()\\n        content = message[\"content\"]\\n        prompt += f\" #{role}: {content}\"\\n\\n    return prompt\\n', '\\ndef mark_system(system_prompt):\\n    return {\"role\": \"system\", \"content\": system_prompt}\\n', '\\n\\ndef mark_answer(answer):\\n    return {\"role\": \"assistant\", \"content\": answer}\\n', '\\n\\ndef mark_question(question):\\n    return {\"role\": \"user\", \"content\": question}\\n', '\\n\\ndef check_dict(message_dict):\\n    if (\\n        type(message_dict) is list\\n        and len(message_dict) == 1\\n        and type(message_dict[0]) is dict\\n    ):\\n        message_dict = message_dict[0]\\n    elif type(message_dict) is not dict:\\n        raise Exception(\\n            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\\n            message_dict,\\n            type(message_dict),\\n        )\\n    return message_dict\\n', '\\n\\ndef get_mark_from_response(response, model = \"gpt\"):\\n    # return the answer from the response\\n    if \"gpt\" in model:\\n        role = response[\"choices\"][0][\"message\"][\"role\"]\\n        message = response[\"choices\"][0][\"message\"][\"content\"]\\n    elif \"command\" in model:\\n        role = \"assistant\"\\n        message = response[0].text \\n    else:\\n        raise Exception(\"Unknown model type\")\\n    return {\"role\": role, \"content\": message}\\n', '\\n\\ndef get_str_from_response(response, model = \"gpt\"):\\n    # return the answer from the response\\n    if \"gpt\" in model:\\n        return response[\"choices\"][0][\"message\"][\"content\"]\\n    elif \"command\" in model:\\n        text = response[0].text\\n        text_without_assistant = text.replace(\"#ASSISTANT\", \"\")\\n        return text_without_assistant\\n    else:\\n        raise Exception(\"Unknown model type\")\\n']\n",
      "['\\ndef concat_columns(example, index=None):\\n    column1=\\'title\\'\\n    column2=\\'text\\'\\n    example[\\'merged_column\\'] = example[column1] +\" - \" + example[column2]\\n    return example\\n', '\\ndef extract_values_and_embeddings_hf(\\n    dataset: datasets.Dataset,\\n    value_column: Union[str, List[str]],\\n    embeddings_column: Optional[str],\\n) -> Tuple[List[str], Optional[List[np.ndarray]]]:\\n    \"\"\"\\n    Extract values and embeddings from a Hugging Face dataset.\\n\\n    Args:\\n        dataset: The Hugging Face dataset to extract values and embeddings from.\\n        value_column: The column(s) of the dataset to use as values.\\n        embeddings_column: The column name containing the embeddings.\\n\\n    Returns:\\n        A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\\n    \"\"\"\\n    if isinstance(value_column, str):\\n        value_column = [value_column]\\n    print(\"Merging values: Start\")\\n    merged_docs = dataset.map(concat_columns, with_indices=True)\\n    print(\"Merging values: Done\")\\n    values = merged_docs[\\'merged_column\\']\\n    embeddings = dataset[embeddings_column]\\n    return values, embeddings if embeddings_column is not None else None\\n', '\\ndef extract_values_hf(dataset: datasets.Dataset, value_column: Union[str, List[str]]) -> List[str]:\\n    \"\"\"\\n    Extract values from a Hugging Face dataset.\\n\\n    Args:\\n        dataset: The Hugging Face dataset to extract values from.\\n        value_column: The column(s) of the dataset to use as values.\\n\\n    Returns:\\n        A list with the extracted values.\\n    \"\"\"\\n    if isinstance(value_column, str):\\n        value_column = [value_column]\\n    if len(value_column) == 1:\\n        return dataset[value_column[0]]\\n    elif len(value_column) > 1:\\n        merged_docs = dataset.map(concat_columns)\\n        return merged_docs\\n    else:\\n        raise ValueError(\"No value column specified.\")\\n']\n",
      "['def __init__(self, calls_per_minute: int, verbose: bool = False):\\n    self.calls_per_minute = calls_per_minute\\n    self.interval = 60 / calls_per_minute\\n    self.lock = Lock()\\n    self.last_call_time = None\\n    self.verbose = verbose\\n', '\\ndef __call__(self, func):\\n    @functools.wraps(func)\\n    def wrapper(*args, **kwargs):\\n        with self.lock:\\n            if self.last_call_time is not None:\\n                time_since_last_call = time.time() - self.last_call_time\\n                if time_since_last_call < self.interval:\\n                    time_to_wait = self.interval - time_since_last_call\\n                    if self.verbose:\\n                        print(\\n                            f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\\n                        )\\n                    time.sleep(time_to_wait)\\n                elif self.verbose:\\n                    print(\\n                        f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\\n                    )\\n            else:\\n                if self.verbose:\\n                    print(\"RateLimiter: This is the first call, no wait required.\")\\n            self.last_call_time = time.time()\\n        return func(*args, **kwargs)\\n\\n    return wrapper\\n', '@functools.wraps(func)\\ndef wrapper(*args, **kwargs):\\n    with self.lock:\\n        if self.last_call_time is not None:\\n            time_since_last_call = time.time() - self.last_call_time\\n            if time_since_last_call < self.interval:\\n                time_to_wait = self.interval - time_since_last_call\\n                if self.verbose:\\n                    print(\\n                        f\"RateLimiter: Waiting for {time_to_wait:.2f} seconds before next call.\"\\n                    )\\n                time.sleep(time_to_wait)\\n            elif self.verbose:\\n                print(\\n                    f\"RateLimiter: No wait required, time since last call: {time_since_last_call:.2f} seconds.\"\\n                )\\n        else:\\n            if self.verbose:\\n                print(\"RateLimiter: This is the first call, no wait required.\")\\n        self.last_call_time = time.time()\\n    return func(*args, **kwargs)\\n', 'def __init__(self, max_workers=None, *args, **kwargs):\\n    super().__init__(max_workers)\\n    self.rate_limiter = RateLimiter(kwargs.get(\"calls_per_minute\", 20), kwargs.get(\"verbose\", False))\\n', '\\ndef submit(self, fn, *args, **kwargs):\\n    rate_limited_fn = self.rate_limiter(fn)\\n    return super().submit(rate_limited_fn, *args, **kwargs)\\n']\n",
      "['def get_embedding_size(self):\\n    return ADA_EMBEDDING_SIZE\\n', '\\ndef embed(self, data, verbose=False):\\n\\n    if type(data) is dict and \"content\" in data:\\n        if verbose is True:\\n            print(\"Embedding without mark\", data[\"content\"])\\n        out = openai.Embedding.create(\\n            input=data[\"content\"], engine=\"text-embedding-ada-002\"\\n        )\\n    else:\\n        if len(tokenizer.encode(data)) > MAX_CONTEXT_LENGTH:\\n            raise ValueError(f\" The input is too long for OpenAI, num tokens is {len(tokenizer.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\\n        if verbose is True:\\n            print(\"Embedding without preprocessing the input\", data)\\n        out = openai.Embedding.create(\\n            input=str(data), engine=\"text-embedding-ada-002\"\\n        )\\n    return out.data[0].embedding\\n', '\\ndef batch_embed(self, data: List[str]):\\n    if type(data) is dict and \"content\" in data:\\n        raise ValueError(\"Batch embedding not supported for dictionaries\")\\n    elif type(data) is str:\\n        return self.embed(data)\\n    elif type(data) is list:\\n        out = openai.Embedding.create(\\n            input=data, engine=\"text-embedding-ada-002\"\\n        )\\n        embeddings = []\\n        for embedding in out.data:\\n            embeddings.append(embedding.embedding)\\n        return embeddings\\n', '\\n\\n\\ndef parse_and_embed_functions(input_str: str) -> List[np.ndarray]:\\n    # Parse the input string with libcst\\n    module = cst.parse_module(input_str)\\n\\n    # Find all the functions in the module and embed them separately\\n    embeddings = []\\n    for node in module.body:\\n\\n        if isinstance(node, cst.FunctionDef) or isinstance(node, cst.ClassDef):\\n            func_str = cst.Module(body=[node]).code\\n            print(\"Function string\", func_str)\\n            embedding = openai.Embedding.create(\\n                input=str(func_str)[:MAX_CONTEXT_LENGTH],\\n                engine=\"text-embedding-ada-002\",\\n            )\\n            if embedding is not None:\\n                embeddings.append(embedding.data[0].embedding)\\n\\n    avg_embedding = avg_embeddings(embeddings)\\n    print(avg_embedding.shape)\\n    return avg_embedding\\n', '\\n\\ndef avg_embeddings(embeddings: List[np.ndarray]) -> np.ndarray:\\n    print(\"Embeddings len\", len(embeddings))\\n    # convert embeddings to numpy array\\n    embeddings = np.array(embeddings)\\n    print(\"Embedding Matrix Shape\", embeddings.shape)\\n    return np.array([np.sum(embeddings.T, axis=1)]).astype(np.float32)\\n']\n",
      "['def get_embedding_size(self):\\n    return SBERT_EMBEDDING_SIZE\\n', '\\ndef embed(self,\\n    data,\\n    key=\"content\",\\n    model_name=\"all-MiniLM-L6-v2\",\\n    batch_size=128,\\n):\\n    \"\"\"\\n        Embed the sentences/text using the MiniLM language model (which uses mean pooling)\\n        \"\"\"\\n    print(\"Embedding data\")\\n    model = SentenceTransformer(model_name)\\n    print(\"Model loaded\")\\n    if isinstance(data, dict):\\n        sentences = data[key].tolist()\\n        unique_sentences = data[key].unique()\\n    elif isinstance(data, str):\\n        #breal the string into sentences based on . or ? or !\\n        sentences = re.split(\\'[.!?]\\', data)\\n        sentences = [s.strip() for s in sentences if s.strip()]  #\\n        #filter empty sentences\\n        sentences = list(filter(lambda x: len(x) > 0, sentences))\\n        unique_sentences = list(set(sentences))\\n    else:\\n        raise ValueError(f\"Data must be a dictionary with attribute {key} or a string, but got {type(data)} instead\")\\n    \\n    print(\"Unique sentences\", len(unique_sentences))\\n    self.unique_sentences = unique_sentences\\n    for sentence in unique_sentences:\\n        tokens = tokenizer.encode(sentence)\\n        if len(tokens) > MAX_CONTEXT_LENGTH:\\n            raise ValueError(f\" The input subsentence is too long for SBERT, num tokens is {len(tokens)}, instead of {MAX_CONTEXT_LENGTH}\")\\n\\n       \\n    embeddings = model.encode(\\n            unique_sentences, show_progress_bar=True, batch_size=batch_size\\n        )\\n\\n    print(\"Embeddings computed\")\\n\\n    mapping = {\\n        sentence: embedding\\n        for sentence, embedding in zip(unique_sentences, embeddings)\\n    }\\n    embeddings = np.array([mapping[sentence] for sentence in sentences])\\n\\n    return np.mean(embeddings, axis=0).tolist()\\n']\n",
      "['def get_embedding_size(self):\\n    return COHERE_EMBEDDING_SIZE\\n', '\\ndef embed(self, data, verbose=False):\\n    if type(data) is dict and \"content\" in data:\\n        if verbose is True:\\n            print(\"Embedding from dictionary\", data[\"content\"])\\n            response = co.embed(texts= data[\"content\"],model=\\'multilingual-22-12\\')\\n    else:\\n        if len(tokenizer.encode(data)) > MAX_CONTEXT_LENGTH:\\n            raise ValueError(f\" The input is too long for Cohere, num tokens is {len(tokenizer.encode(data))}, instead of {MAX_CONTEXT_LENGTH}\")\\n        if verbose is True:\\n            print(\"Embedding without preprocessing the input\", data)\\n        response = co.embed(texts=[str(data)],model=\\'multilingual-22-12\\')\\n    return response.embeddings[0]\\n']\n",
      "['\\ndef chatgpt_response(prompt: List[dict],model: str = \"gpt-3.5-turbo\", max_tokens: int = 1000\\n    ) -> Tuple[Dict, bool]:\\n    \"\"\"\\n        Call the OpenAI API with the given prompt and maximum number of output tokens.\\n\\n        :param prompt: A list of strings representing the prompt to send to the API.\\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\\n        :return: A tuple containing the API response as a dictionary and a boolean indicating success.\\n        \"\"\"             \\n    try:\\n        print(\"Trying to call OpenAI API...\")\\n        response = openai.ChatCompletion.create(\\n            model=model,\\n            messages=prompt,\\n            max_tokens=max_tokens,\\n        )\\n        return response, True\\n\\n    except openai.error.APIError as e:\\n        print(e)\\n        fail_response = {\\n            \"choices\": [\\n                {\\n                    \"message\": {\\n                        \"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"\\n                    }\\n                }\\n            ]\\n        }\\n        return fail_response, False\\n']\n",
      "['\\n\\n\\ndef cohere_response(prompt: List[dict],model: str = \"command\", max_tokens: int = 1000\\n    ) -> Tuple[Dict, bool]:\\n    \"\"\"\\n        Call the Cohere API with the given prompt and maximum number of output tokens.\\n        \\n\\n        :param prompt: A list of strings representing the prompt to send to the API.\\n        :param max_output_tokens: An integer representing the maximum number of output tokens.\\n        :param model: A string representing the model to use. either command or command-nightly\\n        :return: A tuple containing the API response cohere object and a boolean indicating success.\\n        \"\"\"             \\n    try:\\n        prompt= convert_mark_to_str_prompt(prompt)\\n        print(\"Trying to call Cohere API... using model:\", model)\\n        response = co.generate(\\n        model= model,\\n        prompt= prompt,\\n        max_tokens=max_tokens,\\n        #end_sequences=[\\'#SYSTEM:\\', \\'#USER:\\'],\\n        )\\n        return response, True\\n\\n    except:\\n          return None, False\\n    \\n', '\\ndef cohere_summarize(prompt: str, model: str = \"summarize-xlarge\", length: str = \"medium\", extractiveness: str = \"medium\", format: str = \"bullets\", additional_command: str = None) -> str:\\n    response = co.summarize( \\n    text=prompt,model=model, \\n    length=length,\\n    extractiveness=extractiveness,\\n    format=format,\\n    additional_command=additional_command\\n    )\\n\\n    summary = response.summary\\n    return summary\\n              \\n']\n",
      "['def __init__(self, directory_path: str):\\n    self.directory_path = directory_path\\n', '\\ndef get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\\n    \"\"\"Returns a list of all files in a directory\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    all_files = []\\n    for root, _, files in os.walk(directory_path):\\n        for file in files:\\n            all_files.append(os.path.join(root, file))\\n\\n    return all_files\\n', '\\ndef get_files_with_extension(\\n    self, extension: str, directory_path: Optional[str] = None\\n) -> List[str]:\\n    \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    all_files = self.get_all_files(directory_path)\\n    files_with_extension = [file for file in all_files if file.endswith(extension)]\\n\\n    return files_with_extension\\n', '\\ndef get_file_extension(self, file_path: str) -> str:\\n    \"\"\"Returns the extension of a file\"\"\"\\n    return Path(file_path).suffix\\n', '\\ndef get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\\n    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\\n    if directory_path is None:\\n        directory_path = self.directory_path\\n\\n    subdirectories = [\\n        os.path.join(directory_path, d)\\n        for d in os.listdir(directory_path)\\n        if os.path.isdir(os.path.join(directory_path, d))\\n    ]\\n\\n    return subdirectories\\n', '\\ndef create_directory(self, directory_path: str) -> None:\\n    \"\"\"Creates a directory if it does not exist\"\"\"\\n    if not os.path.exists(directory_path):\\n        os.makedirs(directory_path)\\n', '\\ndef delete_directory(self, directory_path: str) -> None:\\n    \"\"\"Deletes a directory if it exists\"\"\"\\n    if os.path.exists(directory_path):\\n        shutil.rmtree(directory_path)\\n', '\\ndef copy_file(self, source_path: str, destination_path: str) -> None:\\n    \"\"\"Copies a file from one location to another\"\"\"\\n    shutil.copy2(source_path, destination_path)\\n', '\\ndef move_file(self, source_path: str, destination_path: str) -> None:\\n    \"\"\"Moves a file from one location to another\"\"\"\\n    shutil.move(source_path, destination_path)\\n']\n",
      "['def __init__(\\n    self,\\n    base_directory: str,\\n    username=None,\\n    repo_name=None,\\n    code_parsers=None,\\n    minify_code: bool = False,\\n    remove_docstrings: bool = False,\\n):\\n    self.username = username\\n    self.repo_name = repo_name\\n    self.base_directory = base_directory\\n    self.github = Github()\\n    self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n    repo_path = self.clone_repo(self.repo.clone_url)\\n\\n    OsProcessor.__init__(self, repo_path)\\n    self.code_parsers = code_parsers or [\\n        PythonParser(\\n            repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\\n        )\\n    ]\\n', '\\ndef get_public_repos(self):\\n    \"\"\"Returns a list of all public repos for the user.\"\"\"\\n    user = self.github.get_user(self.username)\\n    return user.get_repos()\\n', '\\ndef clone_repo(self, repo_url: str):\\n    \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\\n    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n    target_directory = os.path.join(self.base_directory, repo_name)\\n\\n    if os.path.exists(target_directory):\\n        shutil.rmtree(target_directory)\\n\\n    subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n    return target_directory\\n', '\\ndef process_repo(self, repo_path=None):\\n    \"\"\"Processes the repo at the specified path.\\n        If no path is specified, the repo at self.directory_path is processed.\\n        Returns the list of parsed functions and classes.\"\"\"\\n    if repo_path is None:\\n        repo_path = self.directory_path\\n\\n    for code_parser in self.code_parsers:\\n        code_parser.directory_path = repo_path\\n        code_parser.process_directory(repo_path)\\n', '\\ndef process_repos(self):\\n    \"\"\"Processes all public repos for the user.\"\"\"\\n    for repo in self.get_public_repos():\\n        if not repo.private:\\n            print(f\"Processing repo: {repo.name}\")\\n            repo_path = self.clone_repo(repo.clone_url)\\n            self.process_repo(repo_path)\\n            shutil.rmtree(repo_path)\\n', '\\ndef get_repo(self, repo_name):\\n    \"\"\"Returns the repo with the specified name.\"\"\"\\n    user = self.github.get_user(self.username)\\n    return user.get_repo(repo_name)\\n', '\\ndef process_single_repo(self):\\n\\n    repo = self.get_repo(self.repo_name)\\n    print(f\"Processing repo: {self.repo_name}\")\\n    repo_path = self.clone_repo(repo.clone_url)\\n    self.process_repo(repo_path)\\n    shutil.rmtree(repo_path)\\n', '\\ndef get_issues(self, state=\"open\"):\\n    \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n    issues = []\\n    for issue in self.repo.get_issues(state=state):\\n        issues.append(issue)\\n    return issues\\n', '\\ndef parse_issues(self, state=\"open\"):\\n    \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n    parsed_issues = []\\n    issues = self.get_issues(state=state)\\n    for issue in issues:\\n        parsed_issue = {\\n            \"number\": issue.number,\\n            \"title\": issue.title,\\n            \"body\": issue.body,\\n            \"labels\": [label.name for label in issue.labels],\\n        }\\n        parsed_issues.append(parsed_issue)\\n    return parsed_issues\\n', '\\ndef get_commits(self):\\n    \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n    commits = []\\n    branch = self.repo.get_branch(\"main\")\\n    for commit in self.repo.get_commits(sha=branch.commit.sha):\\n        commits.append(commit)\\n    return commits\\n', '\\ndef parse_commits(self):\\n    \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n    parsed_commits = []\\n    commits = self.get_commits()\\n    for commit in commits:\\n        parsed_commit = {\\n            \"sha\": commit.sha,\\n            \"message\": commit.commit.message,\\n            \"author\": {\\n                \"name\": commit.commit.author.name,\\n                \"email\": commit.commit.author.email,\\n                \"date\": commit.commit.author.date,\\n            },\\n        }\\n        parsed_commits.append(parsed_commit)\\n    return parsed_commits\\n']\n",
      "['\\ndef visit_Call(self, node: cst.Call) -> None:\\n    function_name = None\\n    if isinstance(node.func, cst.Name):\\n        function_name = node.func.value\\n\\n    if function_name:\\n        pos = self.get_metadata(PositionProvider, node).start\\n        print(\\n            f\"Function \\'{function_name}\\' called at line {pos.line}, column {pos.column} with arguments:\"\\n        )\\n\\n        for arg in node.args:\\n            arg_start_pos = self.get_metadata(PositionProvider, arg).start\\n            arg_value = arg.value\\n            if isinstance(arg_value, cst.SimpleString):\\n                arg_value = arg_value.evaluated_value\\n            print(\\n                f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\\n            )\\n', 'def __init__(self):\\n    self.count = 0\\n    self.functions_with_operation_dict = {}\\n', '\\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n    self.current_function = node\\n    self.functions_with_operation_dict[node.name] = []\\n', '\\ndef leave_FunctionDef(self, node: cst.FunctionDef) -> None:\\n    self.current_function = None\\n', '\\ndef visit_BinaryOperation(self, node: cst.BinaryOperation) -> None:\\n    if isinstance(node.operator, cst.Multiply) or isinstance(\\n        node.operator, cst.BitAnd\\n    ):\\n        self.count += 1\\n        if self.current_function:\\n            self.functions_with_operation_dict[self.current_function.name].append(\\n                cst.Module([]).code_for_node(node)\\n            )\\n', '\\ndef visit_Call(self, node: cst.Call) -> None:\\n    if m.matches(node, m.Call(func=m.Attribute(attr=m.Name(\"dot\")))) or m.matches(\\n        node, m.Call(func=m.Name(\"dot\"), args=[m.Arg(), m.Arg()])\\n    ):\\n        self.count += 1\\n        if self.current_function:\\n            self.functions_with_operation_dict[self.current_function.name].append(\\n                cst.Module([]).code_for_node(node)\\n            )\\n', 'def __init__(self):\\n    self.function_source_codes = []\\n    self.function_nodes = []\\n    self.class_source_codes = []\\n    self.class_nodes = []\\n', '\\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n    function_source_code = cst.Module([]).code_for_node(node)\\n    # add in place summary and code mod\\n    self.function_nodes.append(node)\\n    self.function_source_codes.append(function_source_code)\\n', '\\ndef visit_ClassDef(self, node: cst.ClassDef) -> None:\\n    class_source_code = cst.Module([]).code_for_node(node)\\n    # add in place summary and code mod\\n    self.class_nodes.append(node)\\n    self.class_source_codes.append(class_source_code)\\n', 'def __init__(self):\\n    # stack for storing the canonical name of the current function\\n    self.stack: List[Tuple[str, ...]] = []\\n    # store the annotations\\n    self.annotations: Dict[\\n        Tuple[str, ...],  # key: tuple of canonical class/function name\\n        Tuple[cst.Parameters, Optional[cst.Annotation]],  # value: (params, returns)\\n    ] = {}\\n', '\\ndef visit_ClassDef(self, node: cst.ClassDef) -> Optional[bool]:\\n    self.stack.append(node.name.value)\\n', '\\ndef leave_ClassDef(self, node: cst.ClassDef) -> None:\\n    self.stack.pop()\\n', \"\\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> Optional[bool]:\\n    self.stack.append(node.name.value)\\n    self.annotations[tuple(self.stack)] = (node.params, node.returns)\\n    return False  # pyi files don't support inner functions, return False to stop the traversal.\\n\", '\\ndef leave_FunctionDef(self, node: cst.FunctionDef) -> None:\\n    self.stack.pop()\\n']\n",
      "['def __init__(self, code: str = None):\\n\\n    self.code = code\\n    self.output_code = None\\n', '\\ndef minify(self):\\n    if self.code:\\n        self.output_code = self.minify_code(self.code)\\n', '\\ndef get_minified_code(self):\\n    if not self.output_code:\\n        self.minify()\\n    return self.output_code\\n', '\\n@staticmethod\\ndef minify_code(code: str) -> str:\\n    return minify(code)\\n', '@staticmethod\\ndef extract_docstring(function_def: cst.FunctionDef) -> str:\\n    docstring = None\\n\\n    for stmt in function_def.body.body:\\n        if isinstance(stmt, cst.SimpleStatementLine):\\n            for expr in stmt.body:\\n                if isinstance(expr, cst.Expr) and isinstance(\\n                    expr.value, cst.SimpleString\\n                ):\\n                    docstring = expr.value.value.strip(\\'\"\\').strip(\"\\'\")\\n                    break\\n        if docstring is not None:\\n            break\\n\\n    if docstring is not None:\\n        return docstring.strip()\\n    else:\\n        function_name = function_def.name.value\\n        return f\"No docstring provided for function \\'{function_name}\\'. Please add a docstring to describe this function.\"\\n', 'def __init__(self):\\n    self.function_source_codes = []\\n    self.function_nodes = []\\n    self.class_source_codes = []\\n    self.class_nodes = []\\n', '\\ndef visit_FunctionDef(self, node: cst.FunctionDef) -> None:\\n    \"\"\"This method is called for every FunctionDef node in the tree.\\n        and it does the following:\\n        1. Gets the source code for the node\\n        2. Adds the node to the list of function nodes\\n        3. Adds the source code to the list of function source codes\\n        \"\"\"\\n    function_source_code = cst.Module([]).code_for_node(node)\\n    self.function_nodes.append(node)\\n    self.function_source_codes.append(function_source_code)\\n', '\\ndef visit_ClassDef(self, node: cst.ClassDef) -> None:\\n    \"\"\"This method is called for every ClassDef node in the tree.\\n        and it does the following:\\n        1. Gets the source code for the node\\n        2. Adds the node to the list of class nodes\\n        3. Adds the source code to the list of class source codes\\n        \"\"\"\\n    class_source_code = cst.Module([]).code_for_node(node)\\n    self.class_nodes.append(node)\\n    self.class_source_codes.append(class_source_code)\\n', 'def __init__(\\n    self,\\n    directory_path: str,\\n    visitor: Optional[FunctionAndClassVisitor] = None,\\n    minify_code: bool = False,\\n    remove_docstrings: bool = False,\\n):\\n    super().__init__(directory_path)\\n    self.visitor = visitor if visitor else FunctionAndClassVisitor()\\n    self.minify_code = minify_code\\n    self.remove_docstrings = remove_docstrings\\n', '\\ndef remove_docstring(self, tree: cst.Module) -> str:\\n    \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\\n\\n    # Remove docstrings using a transformer\\n    class DocstringRemover(cst.CSTTransformer):\\n        def leave_FunctionDef(\\n            self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\\n        ) -> cst.FunctionDef:\\n            docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n            if docstring.startswith(\"No docstring\"):\\n                return updated_node\\n\\n            return updated_node.with_changes(\\n                body=updated_node.body.with_changes(\\n                    body=[\\n                        stmt\\n                        for stmt in updated_node.body.body\\n                        if not (\\n                            isinstance(stmt, cst.SimpleStatementLine)\\n                            and any(\\n                                isinstance(expr, cst.Expr)\\n                                and isinstance(expr.value, cst.SimpleString)\\n                                for expr in stmt.body\\n                            )\\n                        )\\n                    ]\\n                )\\n            )\\n\\n    tree = tree.visit(DocstringRemover())\\n    return tree.code\\n', 'def leave_FunctionDef(\\n    self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\\n) -> cst.FunctionDef:\\n    docstring = PythonDocstringExtractor.extract_docstring(original_node)\\n    if docstring.startswith(\"No docstring\"):\\n        return updated_node\\n\\n    return updated_node.with_changes(\\n        body=updated_node.body.with_changes(\\n            body=[\\n                stmt\\n                for stmt in updated_node.body.body\\n                if not (\\n                    isinstance(stmt, cst.SimpleStatementLine)\\n                    and any(\\n                        isinstance(expr, cst.Expr)\\n                        and isinstance(expr.value, cst.SimpleString)\\n                        for expr in stmt.body\\n                    )\\n                )\\n            ]\\n        )\\n    )\\n', '\\ndef _process_file(self, file_path: str):\\n    \"\"\"This method is called for every file in the directory.\\n        It does the following:\\n        1. Reads the file\\n        2. Parses the file\\n        3. Visits the file with the visitor\\n        \"\"\"\\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\\n        source_code = file.read()\\n\\n    try:\\n        tree = cst.parse_module(source_code)\\n    except cst.ParserSyntaxError:\\n        print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n        return\\n\\n    tree.visit(self.visitor)\\n\\n    # Remove docstrings if specified\\n    if self.remove_docstrings:\\n        source_code = self.remove_docstring(source_code, tree)\\n\\n    # Minify the code if specified\\n    if self.minify_code:\\n        minifier = PythonMinifier(source_code)\\n        source_code = minifier.get_minified_code()\\n\\n    # Add the processed code to the corresponding list in the visitor\\n    self.visitor.function_source_codes.append(source_code)\\n', '\\ndef process_file(self, file_path: str):\\n    \"\"\"This method is called for every file in the directory.\\n        It does the following:\\n        1. Runs flake8 on the file\\n        if flake8 returns a non-zero exit code, it means the file has a syntax error\\n        2. Reads the file\\n        3. Parses the file\\n        4. Visits the file with the visitor\\n\\n        \"\"\"\\n    result = subprocess.run(\\n        [\"flake8\", \"--select=E999\", file_path], capture_output=True\\n    )\\n\\n    if result.returncode != 0:\\n        print(f\"Skipping file with syntax error: {file_path}\")\\n        print(result.stderr.decode(\"utf-8\"))\\n        return\\n\\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\\n        source_code = f.read()\\n\\n    try:\\n        tree = cst.parse_module(source_code)\\n        tree.visit(self.visitor)\\n    except cst.ParserSyntaxError as e:\\n        print(f\"Syntax error: {e}\")\\n        print(f\"Skipping file with syntax error: {file_path}\")\\n', '\\ndef process_directory(\\n    self,\\n) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\\n    \"\"\"This method is called for every directory.\\n        It does the following:\\n        1. Gets all the python files in the directory\\n        2. Processes each file\\n        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\\n        \"\"\"\\n    function_source_codes = []\\n    class_source_codes = []\\n\\n    python_files = self.get_files_with_extension(\".py\")\\n\\n    for file_path in python_files:\\n        self._process_file(file_path)\\n\\n    function_source_codes = self.visitor.function_source_codes\\n    function_nodes = self.visitor.function_nodes\\n    class_source_codes = self.visitor.class_source_codes\\n    class_nodes = self.visitor.class_nodes\\n\\n    return function_source_codes, class_source_codes, function_nodes, class_nodes\\n']\n",
      "['def __init__(self, repo_name):\\n    self.g = Github()\\n    self.repo = self.g.get_repo(repo_name)\\n', '\\ndef get_issues(self, state=\"open\"):\\n    \"\"\"\\n        Returns a list of all issues in the repo with the specified state.\\n        \"\"\"\\n    issues = []\\n    for issue in self.repo.get_issues(state=state):\\n        issues.append(issue)\\n    return issues\\n', '\\ndef parse_issues(self, state=\"open\"):\\n    \"\"\"\\n        Parses all issues in the repo with the specified state and returns a list of dicts.\\n        Each dict contains the issue number, title, body, and labels.\\n        \"\"\"\\n    parsed_issues = []\\n    issues = self.get_issues(state=state)\\n    for issue in issues:\\n        parsed_issue = {\\n            \"number\": issue.number,\\n            \"title\": issue.title,\\n            \"body\": issue.body,\\n            \"labels\": [label.name for label in issue.labels],\\n        }\\n        parsed_issues.append(parsed_issue)\\n    return parsed_issues\\n', 'def __init__(self, repo_name):\\n    self.g = Github()\\n    self.repo = self.g.get_repo(repo_name)\\n', '\\ndef get_commits(self):\\n    \"\"\"\\n        Returns a list of all commits in the main branch of the repository.\\n        \"\"\"\\n    commits = []\\n    branch = self.repo.get_branch(\"main\")\\n    for commit in self.repo.get_commits(sha=branch.commit.sha):\\n        commits.append(commit)\\n    return commits\\n', '\\ndef parse_commits(self):\\n    \"\"\"\\n        Parses all commits in the main branch of the repository and returns a list of dicts.\\n        Each dict contains the commit sha, commit message, and author information.\\n        \"\"\"\\n    parsed_commits = []\\n    commits = self.get_commits()\\n    for commit in commits:\\n        parsed_commit = {\\n            \"sha\": commit.sha,\\n            \"message\": commit.commit.message,\\n            \"author\": {\\n                \"name\": commit.commit.author.name,\\n                \"email\": commit.commit.author.email,\\n                \"date\": commit.commit.author.date,\\n            },\\n        }\\n        parsed_commits.append(parsed_commit)\\n    return parsed_commits\\n']\n",
      "['def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\\n    self.directory_path = directory_path\\n    self.visitor = visitor\\n', '\\ndef _process_file(self, file_path: str):\\n    with open(file_path, \"r\") as file:\\n        source_code = file.read()\\n\\n    try:\\n        tree = cst.parse_module(source_code)\\n    except cst.ParserSyntaxError:\\n        print(f\"Skipping file {file_path}: Failed to parse syntax\")\\n        return\\n\\n    tree.visit(self.visitor)\\n', '\\ndef process_file(self, file_path: str):\\n    # Run flake8 on the file\\n    result = subprocess.run(\\n        [\"flake8\", \"--select=E999\", file_path], capture_output=True\\n    )\\n\\n    if result.returncode != 0:\\n        print(f\"Skipping file with syntax error: {file_path}\")\\n        print(result.stderr.decode(\"utf-8\"))\\n        return\\n\\n    with open(file_path, \"r\") as f:\\n        source_code = f.read()\\n\\n    try:\\n        tree = cst.parse_module(source_code)\\n        tree.visit(self.visitor)\\n    except cst.ParserSyntaxError as e:\\n        print(f\"Syntax error: {e}\")\\n        print(f\"Skipping file with syntax error: {file_path}\")\\n', '\\ndef process_directory(self) -> List[str]:\\n    function_source_codes = []\\n    class_source_codes = []\\n\\n    for root, _, files in os.walk(self.directory_path):\\n        for file in files:\\n            if file.endswith(\".py\"):\\n                file_path = os.path.join(root, file)\\n                self._process_file(file_path)\\n\\n    function_source_codes = self.visitor.function_source_codes\\n    function_nodes = self.visitor.function_nodes\\n    class_source_codes = self.visitor.class_source_codes\\n    class_nodes = self.visitor.class_nodes\\n\\n    return function_source_codes, class_source_codes, function_nodes, class_nodes\\n', '\\ndef clone_repo(self, repo_url):\\n    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\\n    target_directory = os.path.join(self.directory_path, repo_name)\\n\\n    if os.path.exists(target_directory):\\n        shutil.rmtree(target_directory)\\n\\n    subprocess.run([\"git\", \"clone\", repo_url, target_directory])\\n\\n    return target_directory\\n', 'def __init__(\\n    self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\\n):\\n    self.username = username\\n    self.repo_name = repo_name\\n    self.github = Github()\\n    self.directory_processor = None\\n    self.function_source_codes = []\\n    self.class_source_codes = []\\n    self.visitor = visitor\\n', '\\ndef get_public_repos(self):\\n    user = self.github.get_user(self.username)\\n    return user.get_repos()\\n', '\\ndef process_repos(self, base_directory):\\n    self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\\n    for repo in self.get_public_repos():\\n        if not repo.private:\\n            print(f\"Processing repo: {repo.name}\")\\n            repo_path = self.directory_processor.clone_repo(repo.clone_url)\\n            (\\n                function_source_codes,\\n                class_source_codes,\\n            ) = self.directory_processor.process_directory()\\n            self.function_source_codes.extend(function_source_codes)\\n            self.class_source_codes.extend(class_source_codes)\\n            shutil.rmtree(repo_path)\\n\\n    return self.directory_processor\\n', 'def __init__(\\n    self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\\n):\\n    self.username = username\\n    self.repo_name = repo_name\\n    self.github = Github()\\n    self.directory_processor = None\\n    self.function_source_codes = []\\n    self.function_nodes = []\\n    self.class_source_codes = []\\n    self.class_nodes = []\\n    self.visitor = visitor\\n', '\\ndef get_repo(self, repo_name):\\n    user = self.github.get_user(self.username)\\n    return user.get_repo(repo_name)\\n', '\\ndef process_repo(self, base_directory):\\n    self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\\n    repo = self.get_repo(self.repo_name)\\n    print(f\"Processing repo: {self.repo_name}\")\\n    repo_path = self.directory_processor.clone_repo(repo.clone_url)\\n    (\\n        function_source_codes,\\n        class_source_codes,\\n        function_nodes,\\n        class_nodes,\\n    ) = self.directory_processor.process_directory()\\n    self.function_source_codes.extend(function_source_codes)\\n    self.function_nodes.extend(function_nodes)\\n    self.class_source_codes.extend(class_source_codes)\\n    self.class_nodes.extend(class_nodes)\\n    shutil.rmtree(repo_path)\\n    return self.directory_processor\\n', '\\ndef get_values(self):\\n    # concatenate the function and class source codes\\n    self.function_source_codes.extend(self.class_source_codes)\\n    self.function_nodes.extend(self.class_nodes)\\n    return self.function_source_codes, self.function_nodes\\n']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['def __init__(self):\\n    self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\\n', '\\ndef search(self, query, max_results=10):\\n    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\\n    record = Entrez.read(handle)\\n    handle.close()\\n    return record[\"IdList\"]\\n', '\\ndef fetch_abstract(self, pubmed_id):\\n    handle = Entrez.efetch(\\n        db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\"\\n    )\\n    abstract = handle.read()\\n    handle.close()\\n    return abstract\\n', '\\ndef fetch_pmc_full_text(self, pubmed_id):\\n    # Get the PMC ID for the PubMed ID\\n    handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\\n    record = Entrez.read(handle)\\n    handle.close()\\n    pmc_id = None\\n    for link in record[0][\"LinkSetDb\"]:\\n        if link[\"DbTo\"] == \"pmc\":\\n            pmc_id = link[\"Link\"][0][\"Id\"]\\n            break\\n\\n    if not pmc_id:\\n        return None\\n\\n    # Fetch the PMC article XML\\n    handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\\n    xml_content = handle.read()\\n    handle.close()\\n\\n    # Parse the XML and extract the full text\\n    soup = BeautifulSoup(xml_content, \"xml\")\\n    full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\\n\\n    return full_text\\n', 'def __init__(self):\\n    self.api = PubmedAPI()\\n', '\\ndef parse_papers(self, query, max_results=10):\\n    pubmed_ids = self.api.search(query, max_results)\\n    paper_list = []\\n    for pubmed_id in pubmed_ids:\\n        paper_dict = {}\\n        paper_dict[\"pubmed_id\"] = pubmed_id\\n        paper_dict[\"abstract\"] = self.api.fetch_abstract(pubmed_id)\\n        paper_dict[\"content\"] = self.api.fetch_pmc_full_text(pubmed_id)\\n        paper_list.append(paper_dict)\\n    return paper_list\\n']\n",
      "[]\n",
      "[]\n",
      "['def __init__(self):\\n    self.base_url = \"https://www.arxiv-vanity.com/\"\\n', '\\ndef _get_vanity_url(self, arxiv_id):\\n    return urljoin(self.base_url, \"papers/\" + arxiv_id)\\n', '\\ndef _fetch_html(self, url):\\n    response = requests.get(url)\\n    if response.status_code == 200:\\n        return response.text\\n    else:\\n        return None\\n', '\\ndef _extract_main_content(self, html):\\n    soup = BeautifulSoup(html, \"html.parser\")\\n    paragraphs = soup.find_all(\"div\", {\"class\": \"ltx_para\"})\\n    content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\\n    return content\\n', '\\ndef parse_paper(self, arxiv_id):\\n    vanity_url = self._get_vanity_url(arxiv_id)\\n    html = self._fetch_html(vanity_url)\\n    if html is not None:\\n        return self._extract_main_content(html)\\n    else:\\n        return None\\n', 'def __init__(self):\\n    self.base_url = \"http://export.arxiv.org/api/query?\"\\n    self.pdf_download_url = \"https://arxiv.org/pdf/\"\\n', '\\ndef search(self, query, max_results=10):\\n    url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\\n    response = requests.get(url)\\n    if response.status_code == 200:\\n        return response.text\\n    else:\\n        return None\\n', '\\ndef download_pdf(self, paper_key, save_directory=\"./\"):\\n    pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\\n    response = requests.get(pdf_url)\\n    if response.status_code == 200:\\n        with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\\n            f.write(response.content)\\n        print(f\"PDF for {paper_key} downloaded successfully.\")\\n    else:\\n        print(f\"Error downloading PDF for {paper_key}.\")\\n', 'def __init__(self):\\n    self.api = ArxivAPI()\\n    self.vanity_parser = ArxivVanityParser()\\n', '\\ndef _parse_arxiv_id(self, url):\\n    return url.split(\"/\")[-1]\\n', '\\ndef parse_papers(self, query, max_results=10):\\n    search_results = self.api.search(query, max_results)\\n    if search_results is not None:\\n        soup = BeautifulSoup(search_results, \"html.parser\")\\n        entries = soup.find_all(\"entry\")\\n        paper_list = []\\n        for entry in entries:\\n            paper_dict = {}\\n            arxiv_id = self._parse_arxiv_id(entry.id.string)\\n            paper_dict[\"arxiv_id\"] = arxiv_id\\n            paper_dict[\"title\"] = entry.title.string\\n            paper_dict[\"summary\"] = entry.summary.string\\n            paper_dict[\"content\"] = self.vanity_parser.parse_paper(str(arxiv_id))\\n            if paper_dict[\"content\"] == None:\\n                continue\\n            paper_list.append(paper_dict)\\n        return paper_list\\n    else:\\n        return None\\n']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[\"def __init__(self, chatbot: 'Chat'):\\n    self.chatbot = chatbot\\n\", '\\ndef analyze_subject_perspective(self, user_subject: str, user_perspective: str) -> dict:\\n    prompts = [\\n        f\"Generate ideas and concepts that explore the connection between {user_subject} and {user_perspective}, considering both traditional and unconventional approaches.\",\\n        f\"List key concepts or topics that would help analyze {user_subject} through the lens of {user_perspective}, including relevant principles, theories, or models.\",\\n        f\"Identify potential areas or research topics where {user_subject} and {user_perspective} intersect, highlighting intriguing or innovative perspectives.\"\\n    ]\\n\\n    output = {}\\n\\n    with RateLimitedThreadPoolExecutor(max_workers=3, calls_per_minute=20, verbose = False) as executor:\\n        future_to_prompt = {executor.submit(self._analyze_prompt, prompt): prompt for prompt in prompts}\\n        for future in as_completed(future_to_prompt):\\n            prompt = future_to_prompt[future]\\n            try:\\n                output[prompt] = future.result()\\n            except Exception as exc:\\n                counter = 0\\n                print(f\\'An exception occurred while analyzing prompt \"{prompt}\": {exc}\\')\\n                while counter < 3:\\n                    try:\\n                        output[prompt] = self._analyze_prompt(prompt)\\n                        break\\n                    except Exception as exc:\\n                        counter += 1\\n                        print(f\\'An exception occurred while analyzing prompt \"{prompt}\": {exc}\\')\\n                if counter == 3:\\n                    output[prompt] = []\\n\\n    return output\\n', '\\ndef _analyze_prompt(self, prompt: str) -> list:\\n    response = self.chatbot.reply(prompt, verbose=False)\\n    return self._format_response(response)\\n', \"\\ndef _format_response(self, response: str) -> list:\\n    formatted_response = response.strip().split('\\\\n')\\n    return formatted_response\\n\", 'def __init__(self, memory_index: MemoryIndex):\\n    self.memory_index = memory_index\\n', '\\ndef retrieve_ideas(self, queries: Dict, k: int = 30, max_tokens: int = 10000):\\n    \"\"\"\\n        Generate ideas based on the given list of queries.\\n\\n        Args:\\n            queries: The list of queries for generating ideas.\\n            k: The number of top search results to consider.\\n            max_tokens: The maximum number of tokens to return.\\n\\n        Returns:\\n            A list of ideas generated based on the queries.\\n        \"\"\"\\n    ideas = []\\n    for key, queries in queries.items():\\n        for query in queries:\\n            if query is None or len(query) < 10:\\n                continue\\n            top_k_hints, scores, indices = self.memory_index.token_bound_query(\\n                query, k=k, max_tokens=max_tokens\\n            )\\n            last_query = self.memory_index.query_history[-1]\\n            hints_tokens = last_query[\"hints_tokens\"]\\n            returned_tokens = last_query[\"returned_tokens\"]\\n\\n            ideas.append({\"key_task\": key, \"query\": query, \"hints\": top_k_hints, \"scores\": scores, \"hints_tokens\": hints_tokens, \"returned_tokens\": returned_tokens})\\n\\n    return ideas\\n', 'def __init__(self, ideas: list, max_tokens_per_cluster: int):\\n    self.ideas = ideas\\n    self.max_tokens_per_cluster = max_tokens_per_cluster\\n    self.idea_index = self.create_idea_index()\\n    self.cluster_labels = None\\n', '\\ndef create_idea_index(self):\\n    gathered_docs = []\\n    for idea in self.ideas:\\n        for hint in idea[\"hints\"]:\\n            gathered_docs.append(hint)\\n    self.gathered_docs = set(gathered_docs)\\n    idea_index = MemoryIndex(values=self.gathered_docs, is_batched=True, name=\"ideas\")\\n    return idea_index\\n', '\\ndef cluster_embeddings(self, n_neighbors: int = 10, min_cluster_size: int = 5):\\n    reducer = umap.UMAP(n_neighbors=n_neighbors)\\n    reduced_embeddings = reducer.fit_transform(self.idea_index.get_all_embeddings())\\n\\n    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\\n    labels = clusterer.fit_predict(reduced_embeddings)\\n\\n    token_count_per_cluster = self.count_tokens_per_cluster(labels)\\n    print(token_count_per_cluster)\\n    if max(token_count_per_cluster.values()) <= self.max_tokens_per_cluster:\\n        self.cluster_labels = labels\\n        print(\"Clusters created successfully.\")\\n    else:\\n        print(\"Clusters exceed the maximum token count.\")\\n', '\\ndef count_tokens_per_cluster(self, labels):\\n    token_count_per_cluster = {}\\n\\n    for label, doc in zip(labels, self.gathered_docs):\\n        if label not in token_count_per_cluster:\\n            token_count_per_cluster[label] = len(tokenizer.encode(doc))\\n        else:\\n            token_count_per_cluster[label] += len(tokenizer.encode(doc))\\n    return token_count_per_cluster\\n', '\\ndef create_minimum_spanning_paths(self):\\n    if self.cluster_labels is None:\\n        raise ValueError(\"You must run cluster_embeddings() before creating minimum spanning paths.\")\\n\\n    unique_labels = np.unique(self.cluster_labels)\\n    min_span_paths = []\\n\\n    for label in unique_labels:\\n\\n\\n        # Get the indices of the current cluster\\n        cluster_indices = np.where(self.cluster_labels == label)[0]\\n\\n        # Calculate the pairwise distances between embeddings in the cluster\\n        cluster_embeddings = self.idea_index.embeddings[cluster_indices]\\n        dist_matrix = squareform(pdist(cluster_embeddings))\\n\\n        # Create a graph from the distance matrix\\n        graph = nx.from_numpy_array(dist_matrix)\\n\\n        # Compute the minimum spanning tree of the graph\\n        min_span_tree = nx.minimum_spanning_tree(graph)\\n\\n        # Get the minimum spanning paths\\n        min_span_paths_cluster = []\\n        visited = set()\\n        for u, v in min_span_tree.edges():\\n            if u not in visited and v not in visited:\\n                orig_u = cluster_indices[u]\\n                orig_v = cluster_indices[v]\\n                min_span_paths_cluster.append(orig_u)\\n                visited.add(u)\\n                visited.add(v)\\n        # Add the last node to complete the path\\n        min_span_paths_cluster.append(orig_v)\\n\\n        min_span_paths.append(min_span_paths_cluster)\\n\\n    self.min_span_paths = min_span_paths\\n', '\\n\\ndef plot_embeddings_with_path(self):\\n    paths = self.min_span_paths\\n    embeddings = self.idea_index.embeddings\\n    title = \"Minimum Spanning Paths\"\\n    tsne = TSNE(n_components=2, random_state=42)\\n    reduced_embeddings = tsne.fit_transform(embeddings)\\n\\n    plt.figure(figsize=(10, 8))\\n    colors = cm.rainbow(np.linspace(0, 1, len(paths)))\\n    for i, path in enumerate(paths):\\n        path_embeddings = reduced_embeddings[path]\\n        plt.scatter(\\n            path_embeddings[:, 0],\\n            path_embeddings[:, 1],\\n            color=colors[i],\\n            label=f\"Cluster {i}\",\\n        )\\n        for j in range(len(path) - 1):\\n            plt.plot(\\n                [path_embeddings[j, 0], path_embeddings[j + 1, 0]],\\n                [path_embeddings[j, 1], path_embeddings[j + 1, 1]],\\n                color=colors[i],\\n            )\\n    plt.title(title)\\n    plt.legend()\\n    plt.show()\\n', 'def get_clustered_ideas(self):\\n    if self.cluster_labels is None:\\n        raise ValueError(\"You must run cluster_embeddings() before getting clustered ideas.\")\\n    \\n    clustered_ideas = {}\\n    for label, idea in zip(self.cluster_labels, self.idea_index.values):\\n        if label not in clustered_ideas:\\n            clustered_ideas[label] = [idea]\\n        else:\\n            clustered_ideas[label].append(idea)\\n    \\n    # Convert the dictionary to a list of lists (each list corresponds to a cluster)\\n    return list(clustered_ideas.values())\\n', 'def __init__(self, texts: list):\\n    self.texts = texts\\n', \"\\ndef summarize_texts(self) -> dict:\\n    output = {}\\n\\n    with RateLimitedThreadPoolExecutor(max_workers=12, calls_per_minute=200, verbose = False) as executor:\\n        future_to_text = {executor.submit(self._summarize_text, text): text for text in self.texts}\\n        for future in as_completed(future_to_text):\\n            text = future_to_text[future]\\n            try:\\n                output[text] = future.result()\\n            except Exception as exc:\\n                print(f'An exception occurred while summarizing text: {exc}')\\n\\n    return output\\n\", '\\ndef _summarize_text(self, text: str) -> str:\\n    summary = cohere_summarize(text, model=\"summarize-xlarge\", length=\"auto\", extractiveness=\"low\", format=\"auto\")\\n    return summary\\n', '\\n\\ndef generate_perspective_prompt(user_subject, user_perspective, seed_model = \"gpt-3.5-turbo\"):\\n    start = perf_counter()\\n    chat_instance = Chat(model=seed_model)\\n    analyzer = SubjectPerspectiveAnalyzer(chat_instance)\\n    output = analyzer.analyze_subject_perspective(user_subject, user_perspective)\\n    end = perf_counter()\\n    print(\"Time to analyze_perspective: \", end - start)\\n\\n    dataset_url = \"Cohere/wikipedia-22-12-simple-embeddings\"\\n    start = perf_counter()\\n    index = MemoryIndex(name=\"wiki_index\", load=True, is_batched=True,embedder=CohereEmbedder)\\n    if len(index.values)>0:\\n        loaded = True\\n    else:\\n        loaded = False\\n\\n    if not loaded:\\n        print(\"Index not found, creating new index\")\\n        index = MemoryIndex.from_hf_dataset(dataset_url, [\"title\", \"text\"],embeddings_column= \"emb\", name=\"wiki_index\", is_batched=True,embedder=CohereEmbedder)\\n    end = perf_counter()\\n    print(\"Time to index: \", end - start)\\n\\n    start = perf_counter()\\n    ideation = Ideation(memory_index=index)\\n    ideas = ideation.retrieve_ideas(output, k=40, max_tokens=10000)\\n    token_count = 0\\n    for idea in ideas:\\n        token_count += idea[\"returned_tokens\"]\\n    end = perf_counter()\\n    print(\"Time to retrieve ideas: \", end - start)\\n    print(\"Number of tokens: \", token_count)\\n\\n    start = perf_counter()\\n    max_tokens_per_cluster = 20000\\n    idea_cluster = IdeaCluster(ideas, max_tokens_per_cluster)\\n    idea_cluster.cluster_embeddings()\\n    time.sleep(0.5)\\n    ideas = idea_cluster.get_clustered_ideas()\\n    end = perf_counter()\\n    combined_idea = []\\n    ideas_tokens = []\\n    for idea in ideas:\\n        combined_idea.append(\"\\\\n\".join(idea))\\n        ideas_tokens.append(tokenizer.encode(combined_idea[-1]))\\n    print(\"Time to cluster ideas: \", end - start)\\n    print(\"Number of clusters: \", len(ideas))\\n    print(\"Number of tokens in each cluster: \", [len(tokens) for tokens in ideas_tokens])\\n    start = perf_counter()\\n    summarizer = Summarizer(combined_idea)\\n    summaries = summarizer.summarize_texts()\\n    end = perf_counter()\\n    print(\"Time to summarize: \", end - start)\\n    print(\"Number of summaries: \", len(summaries))\\n    print(\"Number of tokens in each summary: \", [len(tokenizer.encode(summary)) for summary in summaries.values()])\\n    start = perf_counter()\\n    system_prompt = f\"\"\"With the summarized information from Cohere about the essential ideas, concenpts, priciples, and intersection points between {user_subject} and {user_perspective}, construct an appealing and context-aware chatbot prompt that impels the chatbot to respond with insights and perspectives born from the synergy of these two domains. Use the tips below to help you create an effective chatbot prompt: 1. Begin with a concise introduction: Initiate the chatbot prompt by setting the context, encompassing the user\\'s specified subject and perspective. 2. Accentuate the intersection: Secure that the chatbot prompt underlines the connection between the user_subject and user_perspective, leading to more pertinent and perceptive responses. 3. Foster exploration: Ensure the chatbot prompt provokes the chatbot to delve into the main ideas, principles, and concepts from the summaries with a thoughtful and reflective approach. 4. Pose open-ended questions: Incorporate open-ended queries in the chatbot prompt, stimulating the chatbot to contemplate beyond the summaries and offer comprehensive responses. 5. Prioritize simplicity: Maintain the chatbot prompt\\'s clarity and brevity, assisting the chatbot in comprehending the context and reacting suitably. 6. Steer the conversation: Craft the chatbot prompt in a manner that subtly directs the chatbot\\'s answers, confirming they consistently focus on the user_subject and user_perspective. By leveraging these tips, develop a chatbot prompt that generates responses illustrative of the user\\'s specified subject and perspective, culminating in a customized and significant interaction. Remember to conclude the prompt with 7 content pillars that will help the chatbot use the perspective at the best of its capacity. \"\"\".format(user_subject=user_subject, user_perspective=user_perspective)\\n    for i, summary in enumerate(summaries.values()):\\n        system_prompt += summary + \"\\\\n\\\\n\"\\n    prompt_generator = Chat(name= \"prompt generator\",system_prompt=system_prompt, max_output_tokens= 2000, model = \"gpt-4\")\\n    perspective_prompt = prompt_generator.reply(\"\", verbose=False)\\n    additional_prompt =  \"\"\" \\\\n\\\\n Bear in mind that while engaging with the user, questions may arise that seem unrelated to the initial prompt. It\\'s crucial to maintain flexibility and creativity, interpreting and addressing these topics through the unique prism provided by the initial prompt.\"\"\"\\n    perspective_prompt += additional_prompt\\n    end = perf_counter()\\n    print(\"Time to generate prompt: \", end - start)\\n    return perspective_prompt\\n', 'def __init__(self, subjects, perspectives, max_workers=10, calls_per_minute=20, base_filename=\"prompt_\"):\\n    self.subjects = subjects\\n    self.perspectives = perspectives\\n    self.executor = RateLimitedThreadPoolExecutor(\\n        max_workers=max_workers, \\n        calls_per_minute=calls_per_minute\\n    )\\n    self.prompts = []\\n    self.base_filename = base_filename\\n    self.lock = threading.Lock()  # create a lock\\n', '\\ndef handle_future(self, future):\\n    try:\\n        result = future.result()\\n        self.prompts.append(result)\\n        complete_filename = self.base_filename + \"results.json\"\\n        with self.lock:  # acquire the lock before writing to the file\\n            self.save_prompts_to_json(complete_filename)\\n    except Exception as e:\\n        error_report = {\"error\": str(e), \"traceback\": traceback.format_exc()}\\n        self.prompts.append(error_report)\\n        complete_filename = self.base_filename + \"errors.json\"\\n        with self.lock:  # acquire the lock before writing to the file\\n            self.save_prompts_to_json(complete_filename)\\n', '\\ndef generate_prompts(self):\\n    for subject in self.subjects:\\n        for perspective in self.perspectives:\\n            future = self.executor.submit(\\n                generate_perspective_prompt, \\n                subject, \\n                perspective\\n            )\\n            future.add_done_callback(self.handle_future)\\n    self.executor.shutdown(wait=True)\\n    return self.prompts\\n', \"\\ndef save_prompts_to_json(self, filename):\\n    with open(filename, 'w') as f:\\n        json.dump(self.prompts, f)\\n\"]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/danielhug/neuraldragon/gitensor/BabyDragon\")\n",
    "from babydragon.processors.parsers.python_parser import PythonParser, FunctionAndClassVisitor\n",
    "def create_rst_file(path, name, py_file_path):\n",
    "    \"\"\"\n",
    "    Create a .rst file with a title in the given directory.\n",
    "    The name parameter should not include the .rst extension.\n",
    "    \"\"\"\n",
    "\n",
    "    rst_path = os.path.join(path, f\"{name}.rst\")\n",
    "    with open(rst_path, 'w') as rst_file:\n",
    "        rst_file.write(f\"{name}\\n\")\n",
    "        rst_file.write(\"=\" * len(name) + \"\\n\\n\")\n",
    "        \n",
    "        # Instantiate the visitor and the parser\n",
    "        visitor = FunctionAndClassVisitor()\n",
    "        parser = PythonParser(py_file_path, visitor)\n",
    "\n",
    "        # Process the file\n",
    "        parser.process_file(py_file_path)\n",
    "\n",
    "        # Write the function and class code blocks\n",
    "        \n",
    "        print(visitor.function_source_codes)\n",
    "        \n",
    "        if len(visitor.function_source_codes) > 0:\n",
    "            for class_code in visitor.class_source_codes:\n",
    "                rst_file.write(\".. code-block:: python\\n\\n\")\n",
    "                rst_file.write('\\t'+class_code.replace('\\n', '\\n\\t')+'\\n\\n')\n",
    "        else:\n",
    "            for func_code in visitor.function_source_codes:\n",
    "                rst_file.write(\".. code-block:: python\\n\\n\")\n",
    "                rst_file.write('\\t'+func_code.replace('\\n', '\\n\\t')+'\\n\\n')\n",
    "            \n",
    "        rst_file.write(\".. automodule:: \" + name + \"\\n\")\n",
    "        rst_file.write(\"   :members:\\n\")\n",
    "\n",
    "def create_sphinx_docs(src_dir, doc_dir):\n",
    "    \"\"\"\n",
    "    Generate .rst files in doc_dir for each .py file in src_dir and its subdirectories.\n",
    "    Does not overwrite existing .rst files.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        relative_path = os.path.relpath(root, src_dir)\n",
    "        rst_dir = os.path.join(doc_dir, relative_path)\n",
    "\n",
    "        if not os.path.exists(rst_dir):\n",
    "            os.makedirs(rst_dir)\n",
    "\n",
    "        for file in files:\n",
    "            name, ext = os.path.splitext(file)\n",
    "            if ext == \".py\" and name != \"__init__\":\n",
    "                rst_path = os.path.join(rst_dir, name + \".rst\")\n",
    "                full_file_path = os.path.join(root, file)\n",
    "                create_rst_file(rst_dir, name, full_file_path)\n",
    "create_sphinx_docs('/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/tasks/llm_task.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/tasks/llm_task.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/tasks/multi_kernel_task.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/tasks/multi_kernel_task.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/tasks/base_task.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/tasks/base_task.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/tasks/topic_tree_task.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/tasks/topic_tree_task.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/tasks/embedding_task.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/tasks/embedding_task.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/kernels/memory_kernel.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/kernels/memory_kernel.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/kernels/kernel_clustering.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/kernels/kernel_clustering.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/kernels/multi_kernel.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/kernels/multi_kernel.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/kernels/multi_kernel_visualization.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/kernels/multi_kernel_visualization.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/kernels/kernel_utils.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/kernels/kernel_utils.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/threads/base_thread.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/threads/base_thread.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/threads/fifo_thread.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/threads/fifo_thread.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/threads/vector_thread.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/threads/vector_thread.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/indexes/memory_index.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/indexes/memory_index.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/indexes/pandas_index.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/indexes/pandas_index.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/chat/base_chat.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/chat/base_chat.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/chat/memory_chat.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/chat/memory_chat.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/chat/chat.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/chat/chat.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/utils/pandas.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/utils/pandas.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/utils/hf_datasets.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/utils/hf_datasets.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/utils/chatml.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/utils/chatml.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/models/embedders/ada2.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/models/embedders/ada2.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/models/embedders/cohere.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/models/embedders/cohere.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/models/embedders/sbert.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/models/embedders/sbert.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/models/generators/cohere.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/models/generators/cohere.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/models/generators/chatgpt.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/models/generators/chatgpt.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/processors/github_processors.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/processors/github_processors.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/processors/os_processor.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/processors/os_processor.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/processors/parsers/git_metadata.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/processors/parsers/git_metadata.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/processors/parsers/visitors.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/processors/parsers/visitors.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/processors/parsers/python_parser.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/processors/parsers/python_parser.py', '/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/apps/auto_perspective/perspective.rst': '/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/apps/auto_perspective/perspective.py'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "doc_dir = \"/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs\"\n",
    "source_dir = \"/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon\"\n",
    "doc_to_source_mapping = {}\n",
    "\n",
    "for root, dirs, files in os.walk(doc_dir):\n",
    "    for file in files:\n",
    "        name, ext = os.path.splitext(file)\n",
    "        if ext == \".rst\":\n",
    "            rst_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(root, doc_dir)\n",
    "            source_path = os.path.join(source_dir, relative_path, name + \".py\")\n",
    "            if os.path.exists(source_path):\n",
    "                doc_to_source_mapping[rst_path] = source_path\n",
    "\n",
    "print(doc_to_source_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/tasks/llm_task.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/tasks/llm_task.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/tasks/multi_kernel_task.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/tasks/multi_kernel_task.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/tasks/base_task.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/tasks/base_task.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/tasks/topic_tree_task.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/tasks/topic_tree_task.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/tasks/embedding_task.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/tasks/embedding_task.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/kernels/memory_kernel.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/kernels/memory_kernel.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/kernels/kernel_clustering.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/kernels/kernel_clustering.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/kernels/multi_kernel.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/kernels/multi_kernel.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/kernels/multi_kernel_visualization.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/kernels/multi_kernel_visualization.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/kernels/kernel_utils.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/kernels/kernel_utils.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/threads/base_thread.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/threads/base_thread.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/threads/fifo_thread.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/threads/fifo_thread.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/threads/vector_thread.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/threads/vector_thread.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/indexes/memory_index.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/indexes/memory_index.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/memory/indexes/pandas_index.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/memory/indexes/pandas_index.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/chat/base_chat.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/chat/base_chat.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/chat/memory_chat.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/chat/memory_chat.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/chat/chat.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/chat/chat.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/utils/pandas.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/utils/pandas.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/utils/hf_datasets.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/utils/hf_datasets.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/utils/chatml.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/utils/chatml.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/models/embedders/ada2.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/models/embedders/ada2.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/models/embedders/cohere.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/models/embedders/cohere.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/models/embedders/sbert.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/models/embedders/sbert.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/models/generators/cohere.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/models/generators/cohere.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/models/generators/chatgpt.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/models/generators/chatgpt.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/processors/github_processors.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/processors/github_processors.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/processors/os_processor.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/processors/os_processor.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/processors/parsers/git_metadata.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/processors/parsers/git_metadata.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/processors/parsers/visitors.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/processors/parsers/visitors.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/processors/parsers/python_parser.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/processors/parsers/python_parser.py\n",
      "\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/docs/apps/auto_perspective/perspective.rst\n",
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon/apps/auto_perspective/perspective.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc, src in doc_to_source_mapping.items():\n",
    "    print(doc)\n",
    "    print(src)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMReader\n",
      "=========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.tasks.llm_task.LLMReader\n",
      "    :members:\n",
      "\n",
      "\n",
      "LLMWriter\n",
      "=========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.tasks.llm_task.LLMWriter\n",
      "    :members:\n",
      "\n",
      "\n",
      "MultiKernelTask\n",
      "===============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.tasks.multi_kernel_task.MultiKernelTask\n",
      "    :members:\n",
      "\n",
      "\n",
      "BaseTask\n",
      "========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.tasks.base_task.BaseTask\n",
      "    :members:\n",
      "\n",
      "\n",
      "TopicTreeTask\n",
      "=============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.tasks.topic_tree_task.TopicTreeTask\n",
      "    :members:\n",
      "\n",
      "\n",
      "EmbeddingTask\n",
      "=============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.tasks.embedding_task.EmbeddingTask\n",
      "    :members:\n",
      "\n",
      "\n",
      "MemoryKernel\n",
      "============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.kernels.memory_kernel.MemoryKernel\n",
      "    :members:\n",
      "\n",
      "\n",
      "ClusterPaths\n",
      "============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.kernels.kernel_clustering.ClusterPaths\n",
      "    :members:\n",
      "\n",
      "\n",
      "HDBSCANPaths\n",
      "============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.kernels.kernel_clustering.HDBSCANPaths\n",
      "    :members:\n",
      "\n",
      "\n",
      "SpectralClusteringPaths\n",
      "=======================\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.kernels.kernel_clustering.SpectralClusteringPaths\n",
      "    :members:\n",
      "\n",
      "\n",
      "MultiKernel\n",
      "===========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.kernels.multi_kernel.MultiKernel\n",
      "    :members:\n",
      "\n",
      "\n",
      "HDBSCANMultiKernel\n",
      "==================\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.kernels.multi_kernel.HDBSCANMultiKernel\n",
      "    :members:\n",
      "\n",
      "\n",
      "SpectralClusteringMultiKernel\n",
      "=============================\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.kernels.multi_kernel.SpectralClusteringMultiKernel\n",
      "    :members:\n",
      "\n",
      "\n",
      "MultiKernelVisualization\n",
      "========================\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.kernels.multi_kernel_visualization.MultiKernelVisualization\n",
      "    :members:\n",
      "\n",
      "\n",
      "MultiKernelStabilityAnalysis\n",
      "============================\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.kernels.multi_kernel_visualization.MultiKernelStabilityAnalysis\n",
      "    :members:\n",
      "\n",
      "\n",
      "BaseThread\n",
      "==========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.threads.base_thread.BaseThread\n",
      "    :members:\n",
      "\n",
      "\n",
      "FifoThread\n",
      "==========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.threads.fifo_thread.FifoThread\n",
      "    :members:\n",
      "\n",
      "\n",
      "VectorThread\n",
      "============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.threads.vector_thread.VectorThread\n",
      "    :members:\n",
      "\n",
      "\n",
      "MemoryIndex\n",
      "===========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.indexes.memory_index.MemoryIndex\n",
      "    :members:\n",
      "\n",
      "\n",
      "PandasIndex\n",
      "===========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.memory.indexes.pandas_index.PandasIndex\n",
      "    :members:\n",
      "\n",
      "\n",
      "Prompter\n",
      "========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.chat.base_chat.Prompter\n",
      "    :members:\n",
      "\n",
      "\n",
      "BaseChat\n",
      "========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.chat.base_chat.BaseChat\n",
      "    :members:\n",
      "\n",
      "\n",
      "FifoChat\n",
      "========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.chat.memory_chat.FifoChat\n",
      "    :members:\n",
      "\n",
      "\n",
      "VectorChat\n",
      "==========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.chat.memory_chat.VectorChat\n",
      "    :members:\n",
      "\n",
      "\n",
      "FifoVectorChat\n",
      "==============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.chat.memory_chat.FifoVectorChat\n",
      "    :members:\n",
      "\n",
      "\n",
      "Chat\n",
      "====\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.chat.chat.Chat\n",
      "    :members:\n",
      "\n",
      "\n",
      "OpenAiEmbedder\n",
      "==============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.models.embedders.ada2.OpenAiEmbedder\n",
      "    :members:\n",
      "\n",
      "\n",
      "CohereEmbedder\n",
      "==============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.models.embedders.cohere.CohereEmbedder\n",
      "    :members:\n",
      "\n",
      "\n",
      "SBERTEmbedder\n",
      "=============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.models.embedders.sbert.SBERTEmbedder\n",
      "    :members:\n",
      "\n",
      "\n",
      "GithubProcessor\n",
      "===============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.processors.github_processors.GithubProcessor\n",
      "    :members:\n",
      "\n",
      "\n",
      "OsProcessor\n",
      "===========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.processors.os_processor.OsProcessor\n",
      "    :members:\n",
      "\n",
      "\n",
      "IssueParser\n",
      "===========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.processors.parsers.git_metadata.IssueParser\n",
      "    :members:\n",
      "\n",
      "\n",
      "CommitParser\n",
      "============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.processors.parsers.git_metadata.CommitParser\n",
      "    :members:\n",
      "\n",
      "\n",
      "FunctionCallFinder\n",
      "==================\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.processors.parsers.visitors.FunctionCallFinder\n",
      "    :members:\n",
      "\n",
      "\n",
      "MultiplicationCounterVisitor\n",
      "============================\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.processors.parsers.visitors.MultiplicationCounterVisitor\n",
      "    :members:\n",
      "\n",
      "\n",
      "FunctionAndClassVisitor\n",
      "=======================\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.processors.parsers.visitors.FunctionAndClassVisitor\n",
      "    :members:\n",
      "\n",
      "\n",
      "TypingCollector\n",
      "===============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.processors.parsers.visitors.TypingCollector\n",
      "    :members:\n",
      "\n",
      "\n",
      "PythonMinifier\n",
      "==============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.processors.parsers.python_parser.PythonMinifier\n",
      "    :members:\n",
      "\n",
      "\n",
      "PythonDocstringExtractor\n",
      "========================\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.processors.parsers.python_parser.PythonDocstringExtractor\n",
      "    :members:\n",
      "\n",
      "\n",
      "FunctionAndClassVisitor\n",
      "=======================\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.processors.parsers.python_parser.FunctionAndClassVisitor\n",
      "    :members:\n",
      "\n",
      "\n",
      "PythonParser\n",
      "============\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.processors.parsers.python_parser.PythonParser\n",
      "    :members:\n",
      "\n",
      "\n",
      "SubjectPerspectiveAnalyzer\n",
      "==========================\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.apps.auto_perspective.perspective.SubjectPerspectiveAnalyzer\n",
      "    :members:\n",
      "\n",
      "\n",
      "Ideation\n",
      "========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.apps.auto_perspective.perspective.Ideation\n",
      "    :members:\n",
      "\n",
      "\n",
      "IdeaCluster\n",
      "===========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.apps.auto_perspective.perspective.IdeaCluster\n",
      "    :members:\n",
      "\n",
      "\n",
      "Summarizer\n",
      "==========\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.apps.auto_perspective.perspective.Summarizer\n",
      "    :members:\n",
      "\n",
      "\n",
      "PerspectivePromptGenerator\n",
      "==========================\n",
      "\n",
      "\n",
      ".. autoclass:: babydragon.apps.auto_perspective.perspective.PerspectivePromptGenerator\n",
      "    :members:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "autoclass_template = \".. autoclass:: {}\\n    :members:\\n\\n\"\n",
    "header_template = \"{}\\n{}\\n\\n\"\n",
    "\n",
    "for rst_path, source_path in doc_to_source_mapping.items():\n",
    "    with open(source_path, 'r') as source_file:\n",
    "        source_code = source_file.read()\n",
    "\n",
    "    module = cst.parse_module(source_code)\n",
    "    class_definitions = [node for node in module.body if isinstance(node, cst.ClassDef)]\n",
    "\n",
    "    if len(class_definitions) > 0:\n",
    "        with open(rst_path, 'w') as rst_file:\n",
    "            for class_definition in class_definitions:\n",
    "                class_name = class_definition.name.value\n",
    "                header_directive = header_template.format(class_name, \"=\" * len(class_name))\n",
    "                print(header_directive)\n",
    "                rst_file.write(header_directive)\n",
    "\n",
    "                relative_path = os.path.relpath(source_path, source_dir).replace(\"\\\\\", \"/\")\n",
    "                class_path = \"babydragon.\" + relative_path.replace(\"/\", \".\")[:-3] + \".\" + class_name\n",
    "                autoclass_directive = autoclass_template.format(class_path)\n",
    "                print(autoclass_directive)\n",
    "                rst_file.write(autoclass_directive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
