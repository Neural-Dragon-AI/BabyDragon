{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielhug/neuraldragon/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from babydragon.chat.memory_chat import FifoVectorChat, FifoChat, VectorChat\n",
    "from babydragon.chat.base_chat import BaseChat, Prompter\n",
    "from babydragon.chat.chat import Chat\n",
    "from babydragon.memory.indexes.pandas_index import PandasIndex\n",
    "from babydragon.memory.indexes.python_index import PythonIndex\n",
    "from babydragon.memory.indexes.memory_kernel import MemoryKernel, MemoryKernelGroup\n",
    "from babydragon.utils.oai import mark_question, mark_system, get_mark_from_response , get_str_from_response\n",
    "import gradio\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/danielhug/neuraldragon/gitensor/BabyDragon\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: BabyDragon\n",
      "  Building wheel for BabyDragon (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for BabyDragon: filename=BabyDragon-0.0.0-py3-none-any.whl size=1164 sha256=a817ab98333a205bebda93815f3f6ee8cecf1f09642cf26f18ddca943a9e6beb\n",
      "  Stored in directory: /private/var/folders/29/mz6wb9ks5k72xrwdx9wxdwrh0000gn/T/pip-ephem-wheel-cache-ncios1zx/wheels/13/d9/0f/0cfbd22eca7816335d841930c85504b44838e275b42ad5e431\n",
      "Successfully built BabyDragon\n",
      "Installing collected packages: BabyDragon\n",
      "  Attempting uninstall: BabyDragon\n",
      "    Found existing installation: BabyDragon 0.0.0\n",
      "    Uninstalling BabyDragon-0.0.0:\n",
      "      Successfully uninstalled BabyDragon-0.0.0\n",
      "Successfully installed BabyDragon-0.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseChat Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = BaseChat()\n",
    "chatbot.gradio()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseChat with Prompter Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPrompter(Prompter):\n",
    "    def __init__(self, system_prompt: str = None, user_prompt: str = None):\n",
    "        super().__init__(system_prompt, user_prompt)\n",
    "\n",
    "    def custom_user_prompt(self, message: str) -> str:\n",
    "        return f\"The user says: {message}\"\n",
    "\n",
    "    def custom_system_prompt(self) -> str:\n",
    "        return \"You are a helpful AI assistant.\"\n",
    "\n",
    "    def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\n",
    "        marked_question = mark_question(self.custom_user_prompt(message))\n",
    "        prompt = [mark_system(self.custom_system_prompt())] + [marked_question]\n",
    "        return prompt, marked_question\n",
    "\n",
    "class CustomChat(BaseChat):\n",
    "    def __init__(self, model: str = None, max_output_tokens: int = 1000):\n",
    "        super().__init__(model, max_output_tokens)\n",
    "        self.prompter = CustomPrompter()\n",
    "        self.prompt_func = self.prompter.one_shot_prompt\n",
    "\n",
    "chatbot = CustomChat()\n",
    "chatbot.gradio()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Load Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import babydragon\n",
    "\n",
    "babydragon_path = os.path.dirname(os.path.abspath(babydragon.__file__))\n",
    "\n",
    "venv_path = \"/Users/danielhug/neuraldragon/gitensor/BabyDragon/venv/lib/python3.10/site-packages\"\n",
    "faiss_venv_path = f\"{venv_path}/faiss\"\n",
    "libcst_venv_path = f\"{venv_path}/libcst\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danielhug/neuraldragon/gitensor/BabyDragon/babydragon\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Load BabyDragon Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pind = PythonIndex(babydragon_path,name=\"babydragon_index\", load = False)\n",
    "#pind.save()\n",
    "pind = PythonIndex(babydragon_path,name=\"babydragon_index\", load = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Load Faiss Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "# Assuming your function is named `my_function`\n",
    "print(inspect.getsource(PythonIndex))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': 'babydragon.memory.indexes.python_index',\n",
       "              '__init__': <function babydragon.memory.indexes.python_index.PythonIndex.__init__(self, directory_path: str, name: str = 'python_index', save_path: Optional[str] = None, load: bool = False, minify_code: bool = False, remove_docstrings: bool = False, tokenizer: Optional[tiktoken.core.Encoding] = None)>,\n",
       "              '__doc__': None})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PythonIndex.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faiss_ind = PythonIndex(faiss_venv_path,name=\"faiss_index\", load = False)\n",
    "#faiss_ind.save()\n",
    "faiss_ind = PythonIndex(faiss_venv_path,name=\"faiss_index\", load = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Load LibCST Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libcst_ind = PythonIndex(libcst_venv_path,name=\"libcst_index\", load = False)\n",
    "#libcst_ind.save()\n",
    "libcst_ind = PythonIndex(libcst_venv_path,name=\"libcst_index\", load = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MemoryIndex dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import babydragon\n",
    "\n",
    "babydragon_path = os.path.dirname(os.path.abspath(babydragon.__file__))\n",
    "\n",
    "venv_path = \"/Users/danielhug/neuraldragon/gitensor/BabyDragon/venv/lib/python3.10/site-packages\"\n",
    "faiss_venv_path = f\"{venv_path}/faiss\"\n",
    "libcst_venv_path = f\"{venv_path}/libcst\"\n",
    "python_index_dict = {\n",
    "    \"babydragon_index\": PythonIndex(babydragon_path,name=\"babydragon_index\", load = True),\n",
    "    \"faiss_index\": PythonIndex(faiss_venv_path,name=\"faiss_index\", load = True),\n",
    "    \"libcst_index\": PythonIndex(libcst_venv_path,name=\"libcst_index\", load = True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_contextual_resources(self, contextual_memory):\\n    # Load contextual resources required for code generation\\n    pass\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = python_index_dict['babydragon_index'].values\n",
    "values[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = python_index_dict['babydragon_index'].embeddings\n",
    "embeddings[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new index from a faiss index and values list\n",
      "Computing the adjacency matrix\n",
      "Embeddings shape:  (270, 1536)\n",
      "Computing the k-hop adjacency matrix and aggregated features\n",
      "Compute the k-hop adjacency matrix\n",
      "Aggregate the messages from the k-hop neighborhood:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 111.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the memory index\n",
      "Creating a new index from a list of embeddings and values\n",
      "babydragon_kernel\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n",
      "U.shape: (270, 270)\n",
      "S.shape: (270,)\n",
      "VT.shape: (1536, 1536)\n",
      "(270, 1536)\n",
      "sorted_code_strings: 270\n",
      "sorted_node_embeddings.shape: (270, 1536)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m memory_kernel_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbabydragon_kernel\u001b[39m\u001b[39m\"\u001b[39m: MemoryKernel(python_index_dict[\u001b[39m'\u001b[39m\u001b[39mbabydragon_index\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m }\n\u001b[1;32m      4\u001b[0m     \u001b[39m#\"faiss_kernel\": MemoryKernel(python_index_dict['faiss_index']),\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m#\"libcst_kernel\": MemoryKernel(python_index_dict['libcst_index'])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#}\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m bucket_group \u001b[39m=\u001b[39m MemoryKernelGroup(memory_kernel_dict)\u001b[39m.\u001b[39;49mrank_decomp_and_merge()\n",
      "File \u001b[0;32m~/neuraldragon/gitensor/BabyDragon/babydragon/memory/indexes/memory_kernel.py:112\u001b[0m, in \u001b[0;36mMemoryKernelGroup.rank_decomp_and_merge\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m     code_embeddings \u001b[39m=\u001b[39m mem_kernel\u001b[39m.\u001b[39mnode_embeddings\n\u001b[1;32m    111\u001b[0m     _, _, VT \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39msvd(code_embeddings)\n\u001b[0;32m--> 112\u001b[0m     rank_buckets \u001b[39m=\u001b[39m group_items_by_rank_buckets_svd(code_values, code_embeddings, VT, component_window_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m    113\u001b[0m     bucket_groups[key] \u001b[39m=\u001b[39m rank_buckets\n\u001b[1;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m bucket_groups\n",
      "File \u001b[0;32m~/neuraldragon/gitensor/BabyDragon/babydragon/working_memory/associative_memory/group_by_rank.py:90\u001b[0m, in \u001b[0;36mgroup_items_by_rank_buckets_svd\u001b[0;34m(code_strings, node_embeddings, S_vectors, component_window_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39msorted_code_strings: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(sorted_code_strings)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     89\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39msorted_node_embeddings.shape: \u001b[39m\u001b[39m{\u001b[39;00msorted_node_embeddings\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m     approx_embeddings \u001b[39m=\u001b[39m svd_2(sorted_node_embeddings, r, c)\n\u001b[1;32m     92\u001b[0m     rank_buckets\u001b[39m.\u001b[39mappend((sorted_code_strings, approx_embeddings))\n\u001b[1;32m     94\u001b[0m \u001b[39mreturn\u001b[39;00m rank_buckets\n",
      "File \u001b[0;32m~/neuraldragon/gitensor/BabyDragon/babydragon/working_memory/associative_memory/group_by_rank.py:11\u001b[0m, in \u001b[0;36msvd_2\u001b[0;34m(B, r, c)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msvd_2\u001b[39m(B: np\u001b[39m.\u001b[39mndarray, r: \u001b[39mint\u001b[39m, c: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m---> 11\u001b[0m     U, S, VT \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49msvd(B)\n\u001b[1;32m     12\u001b[0m     \u001b[39m#priunt matrix shapes\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mU.shape: \u001b[39m\u001b[39m{\u001b[39;00mU\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/neuraldragon/.venv/lib/python3.10/site-packages/numpy/linalg/linalg.py:1657\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         gufunc \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39msvd_n_s\n\u001b[1;32m   1656\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->DdD\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->ddd\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1657\u001b[0m u, s, vh \u001b[39m=\u001b[39m gufunc(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[1;32m   1658\u001b[0m u \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1659\u001b[0m s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mastype(_realType(result_t), copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "memory_kernel_dict = {\n",
    "    \"babydragon_kernel\": MemoryKernel(python_index_dict['babydragon_index'])\n",
    "}\n",
    "    #\"faiss_kernel\": MemoryKernel(python_index_dict['faiss_index']),\n",
    "    #\"libcst_kernel\": MemoryKernel(python_index_dict['libcst_index'])\n",
    "#}\n",
    "bucket_group = MemoryKernelGroup(memory_kernel_dict).rank_decomp_and_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[119., 109., 101., ...,  79.,  96.,  83.],\n",
       "       [109., 170., 123., ...,  83., 108.,  89.],\n",
       "       [101., 123., 165., ...,  86.,  98.,  96.],\n",
       "       ...,\n",
       "       [ 79.,  83.,  86., ...,  94.,  89.,  86.],\n",
       "       [ 96., 108.,  98., ...,  89., 130.,  90.],\n",
       "       [ 83.,  89.,  96., ...,  86.,  90., 101.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_kernel_dict['babydragon_kernel'].A_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot2 = Chat(model=\"gpt-3.5-turbo\", index_dict=memory_kernel_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babydragon_kernel\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bucket_group \u001b[39m=\u001b[39m MemoryKernelGroup(memory_kernel_dict)\u001b[39m.\u001b[39;49mrank_decomp_and_merge()\n",
      "File \u001b[0;32m~/neuraldragon/gitensor/BabyDragon/babydragon/memory/indexes/memory_kernel.py:112\u001b[0m, in \u001b[0;36mMemoryKernelGroup.rank_decomp_and_merge\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m     code_embeddings \u001b[39m=\u001b[39m mem_kernel\u001b[39m.\u001b[39mnode_embeddings\n\u001b[1;32m    111\u001b[0m     _, _, VT \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39msvd(code_embeddings)\n\u001b[0;32m--> 112\u001b[0m     rank_buckets \u001b[39m=\u001b[39m group_items_by_rank_buckets_svd(code_values, code_embeddings, VT, component_window_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m    113\u001b[0m     bucket_groups[key] \u001b[39m=\u001b[39m rank_buckets\n\u001b[1;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m bucket_groups\n",
      "File \u001b[0;32m~/neuraldragon/gitensor/BabyDragon/babydragon/working_memory/associative_memory/group_by_rank.py:81\u001b[0m, in \u001b[0;36mgroup_items_by_rank_buckets_svd\u001b[0;34m(code_strings, node_embeddings, S_vectors, component_window_size)\u001b[0m\n\u001b[1;32m     79\u001b[0m     r, c \u001b[39m=\u001b[39m start_idx, end_idx\n\u001b[1;32m     80\u001b[0m     \u001b[39m# Compute SVD2 approx embeddings for this bucket\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     approx_embeddings \u001b[39m=\u001b[39m svd_2(sorted_node_embeddings, r, c)\n\u001b[1;32m     83\u001b[0m     rank_buckets\u001b[39m.\u001b[39mappend((sorted_code_strings, approx_embeddings))\n\u001b[1;32m     85\u001b[0m \u001b[39mreturn\u001b[39;00m rank_buckets\n",
      "File \u001b[0;32m~/neuraldragon/gitensor/BabyDragon/babydragon/working_memory/associative_memory/group_by_rank.py:12\u001b[0m, in \u001b[0;36msvd_2\u001b[0;34m(B, r, c)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msvd_2\u001b[39m(B: np\u001b[39m.\u001b[39mndarray, r: \u001b[39mint\u001b[39m, c: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     11\u001b[0m     U, S, VT \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39msvd(B)\n\u001b[0;32m---> 12\u001b[0m     M_cs \u001b[39m=\u001b[39m [S[i] \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mouter(U[:, i], VT[i, :]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(r, c)]\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(M_cs)\n",
      "File \u001b[0;32m~/neuraldragon/gitensor/BabyDragon/babydragon/working_memory/associative_memory/group_by_rank.py:12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msvd_2\u001b[39m(B: np\u001b[39m.\u001b[39mndarray, r: \u001b[39mint\u001b[39m, c: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     11\u001b[0m     U, S, VT \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39msvd(B)\n\u001b[0;32m---> 12\u001b[0m     M_cs \u001b[39m=\u001b[39m [S[i] \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mouter(U[:, i], VT[i, :]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(r, c)]\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(M_cs)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "bucket_group = MemoryKernelGroup(memory_kernel_dict).rank_decomp_and_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<babydragon.memory.indexes.memory_kernel.MemoryKernelGroup at 0x138e6acb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to ask you a question and you should use the hints to answer it. The hints are:\n",
      "def __init__(self, values, embeddings, name=\"memory_kernel\", save_path=None):\n",
      "    super().__init__(values, embeddings, name, save_path)\n",
      "    self.create_k_hop_index()\n",
      "\n",
      "\n",
      "def __len__(self):\n",
      "    return len(self.memory_thread)\n",
      "\n",
      "def __init__(self, name= 'vector_memory', max_context = 2048, use_mark = False):\n",
      "    BaseThread.__init__(self,name= name , max_memory= None)\n",
      "    MemoryIndex.__init__(self, index = None, name = name)\n",
      "    self.max_context = max_context\n",
      "    self.use_mark = use_mark\n",
      "    self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
      "    \n",
      "\n",
      "\n",
      "def load_git_memory(self, git_memory):\n",
      "    # Retrieve GitMemory context\n",
      "    pass\n",
      "\n",
      "\n",
      "\n",
      "class GitMemory(MemoryIndex):\n",
      "    def __init__(self, username, repo_name):\n",
      "        super().__init__()\n",
      "        self.username = username\n",
      "        self.repo_name = repo_name\n",
      "        self.parser = GitHubRepoProcessor(username, repo_name)\n",
      "        self.minifier = PythonMinifier()\n",
      "        self.docstring_extractor = PythonDocstringExtractor()\n",
      "        self.directory_parser = None\n",
      "        self.min_code_index = None\n",
      "        self.doc_string_index = None\n",
      "        self.libcst_node_index = None\n",
      "    \n",
      "    def create_code_index(self, base_directory):\n",
      "        self.directory_parser = self.parser.process_repo(base_directory)\n",
      "        code_values, code_nodes = self.parser.get_values()\n",
      "        self.code_index = self.init_index(values=code_values)\n",
      "        self.code_index.save()\n",
      "\n",
      "    def create_indexes(self, base_directory):\n",
      "        self.directory_parser = self.parser.process_repo(base_directory)\n",
      "        code_values, code_nodes = self.parser.get_values()\n",
      "        self.code_index = self.init_index(values=code_values)\n",
      "\n",
      "        min_code_values = []\n",
      "        doc_string_values = []\n",
      "        for code_value, code_node in zip(code_values, code_nodes):\n",
      "            minifier = PythonMinifier(code=code_value)\n",
      "            min_code = minifier.get_minified_code()\n",
      "            doc_string = self.docstring_extractor.extract_docstring(code_node)\n",
      "            min_code_values.append(min_code)\n",
      "            doc_string_values.append(doc_string)\n",
      "        self.doc_string_index = self.init_index(values=doc_string_values)\n",
      "        self.min_code_index = self.init_index(values=min_code_values)\n",
      "\n",
      "\n",
      "def __init__(self, model: Optional[str] = None, index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,  name: str = 'vector_memory',  max_index_memory: int = 400,  max_vector_memory: int = 2048, max_output_tokens: int = 1000, system_prompt: str = None, user_prompt: str = None):\n",
      "    VectorThread.__init__(self, name=name, max_context=max_vector_memory)\n",
      "    Chat.__init__(self, model= model,index_dict= index_dict,  max_output_tokens=max_output_tokens, max_index_memory=max_index_memory, system_prompt=system_prompt, user_prompt=user_prompt)\n",
      "    self.max_vector_memory = self.max_context\n",
      "    self.prompt_func = self.vector_memory_prompt\n",
      "\n",
      "def __init__(self, name= 'fifo_memory', max_memory = None, longterm_thread = None, redundant = True):\n",
      "    \n",
      "    BaseThread.__init__(self,name= name , max_memory= None)\n",
      "    if redundant is True:            \n",
      "        self.redundant_thread = BaseThread(name = 'lucid_memory',max_memory = None)\n",
      "    else:\n",
      "        self.redundant_thread = None\n",
      "    if longterm_thread is None:\n",
      "        self.longterm_thread = BaseThread(name ='longterm_memory',max_memory = None)\n",
      "    else:\n",
      "        self.longterm_thread = longterm_thread\n",
      "    # create an alias for the memory_thread to make the code more readable\n",
      "    self.fifo_thread = self.memory_thread\n",
      "    self.max_memory = max_memory\n",
      "        \n",
      "\n",
      "\n",
      "def create_k_hop_index(self, k):\n",
      "    print(\"Computing the adjacency matrix\")\n",
      "    print(\"Embeddings shape: \", self.embeddings.shape)\n",
      "    self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n",
      "    print(\"Computing the k-hop adjacency matrix and aggregated features\")\n",
      "    self.A_k, self.node_embeddings = self.k_hop_message_passing(\n",
      "        self.A, self.embeddings, k\n",
      "    )\n",
      "    print(\"Updating the memory index\")\n",
      "    self.k_hop_index = MemoryIndex(index=None, values=self.values, embeddings=self.node_embeddings, name=self.memory_index.name)\n",
      "\n",
      "def __init__(self):\n",
      "    self.git_memory = None\n",
      "    self.commit_index = None\n",
      "    self.contextual_memory = None\n",
      " .\n",
      "            Remember that I can not see the hints, but please reply as if the knowledge base was part of your training set. The question is: Can you summarize the MemoryKernel class?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# set the current index to \"my_index\"\n",
    "chatbot2.set_current_index(\"babydragon_kernel\")\n",
    "\n",
    "# get hints for a given question\n",
    "hints2 = chatbot2.get_index_hints(\"Can you summarize the MemoryKernel class?\")\n",
    "print(hints2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in method 'IndexFlatCodes_reconstruct_n', argument 2 of type 'faiss::idx_t'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 31\u001b[0m\n\u001b[1;32m     25\u001b[0m     adj_matrix[I[:,\u001b[39m0\u001b[39m], I[:,\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m D[:,\u001b[39m0\u001b[39m]\n\u001b[1;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m adj_matrix\n\u001b[0;32m---> 31\u001b[0m adj_matrix \u001b[39m=\u001b[39m faiss_index_to_adj_matrix(faiss_ind\u001b[39m.\u001b[39;49mindex, \u001b[39mlen\u001b[39;49m(faiss_ind\u001b[39m.\u001b[39;49mvalues))\n\u001b[1;32m     32\u001b[0m adj_matrix\u001b[39m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[26], line 22\u001b[0m, in \u001b[0;36mfaiss_index_to_adj_matrix\u001b[0;34m(index, num_vectors)\u001b[0m\n\u001b[1;32m     20\u001b[0m D \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((num_vectors, k), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     21\u001b[0m I \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((num_vectors, k), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint64)\n\u001b[0;32m---> 22\u001b[0m index\u001b[39m.\u001b[39msearch(index\u001b[39m.\u001b[39;49mreconstruct_n(np\u001b[39m.\u001b[39;49marange(num_vectors)), k, D, I)\n\u001b[1;32m     24\u001b[0m \u001b[39m# Populate the adjacency matrix with pairwise distances\u001b[39;00m\n\u001b[1;32m     25\u001b[0m adj_matrix[I[:,\u001b[39m0\u001b[39m], I[:,\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m D[:,\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/neuraldragon/.venv/lib/python3.10/site-packages/faiss/class_wrappers.py:504\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_reconstruct_n\u001b[0;34m(self, n0, ni, x)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[39massert\u001b[39;00m x\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (ni, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md)\n\u001b[0;32m--> 504\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreconstruct_n_c(n0, ni, swig_ptr(x))\n\u001b[1;32m    505\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/neuraldragon/.venv/lib/python3.10/site-packages/faiss/swigfaiss_avx2.py:1947\u001b[0m, in \u001b[0;36mIndexFlatCodes.reconstruct_n\u001b[0;34m(self, i0, ni, recons)\u001b[0m\n\u001b[1;32m   1945\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreconstruct_n\u001b[39m(\u001b[39mself\u001b[39m, i0, ni, recons):\n\u001b[1;32m   1946\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\" reconstruction using the codec interface\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1947\u001b[0m     \u001b[39mreturn\u001b[39;00m _swigfaiss_avx2\u001b[39m.\u001b[39;49mIndexFlatCodes_reconstruct_n(\u001b[39mself\u001b[39;49m, i0, ni, recons)\n",
      "\u001b[0;31mTypeError\u001b[0m: in method 'IndexFlatCodes_reconstruct_n', argument 2 of type 'faiss::idx_t'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[263.88278   , 263.2325    ,  14.350029  , ...,   0.57405764,\n",
       "          0.5739497 ,   0.5706153 ],\n",
       "       [241.39865   , 239.6717    ,  14.201194  , ...,   0.5329086 ,\n",
       "          0.5326816 ,   0.5324526 ],\n",
       "       [278.6772    , 278.40082   ,  15.489161  , ...,   0.577207  ,\n",
       "          0.5654743 ,   0.5647866 ],\n",
       "       ...,\n",
       "       [280.1882    , 278.70374   ,  13.455091  , ...,   0.5906057 ,\n",
       "          0.58382875,   0.5798246 ],\n",
       "       [274.48398   , 274.17065   ,  13.406527  , ...,   0.58650506,\n",
       "          0.58637786,   0.57965755],\n",
       "       [279.35733   , 278.7997    ,  13.338354  , ...,   0.58496964,\n",
       "          0.5846895 ,   0.5837144 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Can you summarize the MemoryKernel class??"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The MemoryKernel class has an initializer method that initializes the values, embeddings, name, and save_path of the class. It then calls a method to create a k-hop index. The create_k_hop_index method computes the adjacency matrix, k-hop adjacency matrix, and aggregated features. It then updates the memory index with the k-hop index."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MemoryKernel class has an initializer method that initializes the values, embeddings, name, and save_path of the class. It then calls a method to create a k-hop index. The create_k_hop_index method computes the adjacency matrix, k-hop adjacency matrix, and aggregated features. It then updates the memory index with the k-hop index.\n"
     ]
    }
   ],
   "source": [
    "# ask a question\n",
    "response2 = chatbot2.reply(\"Can you summarize the MemoryKernel class??\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to ask you a question and you should use the hints to answer it. The hints are:\n",
      "\n",
      "def __init__(\n",
      "    self,\n",
      "    visitor_methods: _VisitorMethodCollection,\n",
      "    *,\n",
      "    before_visit: Optional[VisitorMethod] = None,\n",
      "    after_leave: Optional[VisitorMethod] = None,\n",
      ") -> None:\n",
      "    super().__init__()\n",
      "    self.visitor_methods = visitor_methods\n",
      "    self.before_visit = before_visit\n",
      "    self.after_leave = after_leave\n",
      "\n",
      "\n",
      "def get_visitors(self) -> Mapping[str, VisitorMethod]:\n",
      "    \"\"\"\n",
      "        Returns a mapping of all the ``visit_<Type[CSTNode]>``,\n",
      "        ``visit_<Type[CSTNode]>_<attribute>``, ``leave_<Type[CSTNode]>`` and\n",
      "        `leave_<Type[CSTNode]>_<attribute>`` methods defined by this visitor,\n",
      "        excluding all empty stubs.\n",
      "        \"\"\"\n",
      "\n",
      "    methods = inspect.getmembers(\n",
      "        self,\n",
      "        lambda m: (\n",
      "            inspect.ismethod(m)\n",
      "            and (m.__name__.startswith(\"visit_\") or m.__name__.startswith(\"leave_\"))\n",
      "            and not getattr(m, \"_is_no_op\", False)\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # TODO: verify all visitor methods reference valid node classes.\n",
      "    # for name, __ in methods:\n",
      "    #     ...\n",
      "\n",
      "    return dict(methods)\n",
      "\n",
      "\n",
      "def on_visit(self, node: \"CSTNode\") -> bool:\n",
      "    \"\"\"\n",
      "        Call appropriate visit methods on node before visiting children.\n",
      "        \"\"\"\n",
      "    before_visit = self.before_visit\n",
      "    if before_visit is not None:\n",
      "        before_visit(node)\n",
      "    type_name = type(node).__name__\n",
      "    for v in self.visitor_methods.get(f\"visit_{type_name}\", []):\n",
      "        v(node)\n",
      "    return True\n",
      "\n",
      "\n",
      "\n",
      "def _get_visitor_methods(\n",
      "    batchable_visitors: Iterable[BatchableCSTVisitor],\n",
      ") -> _VisitorMethodCollection:\n",
      "    \"\"\"\n",
      "    Gather all ``visit_<Type[CSTNode]>``, ``visit_<Type[CSTNode]>_<attribute>``,\n",
      "    ``leave_<Type[CSTNode]>`` amd `leave_<Type[CSTNode]>_<attribute>`` methods\n",
      "    from ``batchabled_visitors``.\n",
      "    \"\"\"\n",
      "    visitor_methods: MutableMapping[str, List[VisitorMethod]] = {}\n",
      "    for bv in batchable_visitors:\n",
      "        for name, fn in bv.get_visitors().items():\n",
      "            visitor_methods.setdefault(name, []).append(fn)\n",
      "    return visitor_methods\n",
      "\n",
      "\n",
      "@mark_no_op\n",
      "def visit_Module(self, node: \"Module\") -> Optional[bool]:\n",
      "    pass\n",
      "\n",
      "\n",
      "def on_leave(self, original_node: \"CSTNode\") -> None:\n",
      "    \"\"\"\n",
      "        Call appropriate leave methods on node after visiting children.\n",
      "        \"\"\"\n",
      "    type_name = type(original_node).__name__\n",
      "    for v in self.visitor_methods.get(f\"leave_{type_name}\", []):\n",
      "        v(original_node)\n",
      "    after_leave = self.after_leave\n",
      "    if after_leave is not None:\n",
      "        after_leave(original_node)\n",
      "\n",
      "\n",
      "@mark_no_op\n",
      "def visit_Module_footer(self, node: \"Module\") -> None:\n",
      "    pass\n",
      "\n",
      "\n",
      "@mark_no_op\n",
      "def visit_Module_encoding(self, node: \"Module\") -> None:\n",
      "    pass\n",
      " .\n",
      "            Remember that I can not see the hints, but please reply as if the knowledge base was part of your training set. The question is: What is are Visitors and how do they relate to code mods?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# set the current index to \"my_index\"\n",
    "chatbot2.set_current_index(\"libcst_index\")\n",
    "\n",
    "# get hints for a given question\n",
    "hints2 = chatbot2.get_index_hints(\"What is are Visitors and how do they relate to code mods?\")\n",
    "print(hints2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " What is are Visitors and how do they relate to code mods?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " In Python programming language, visitors are used to traverse data structures and perform operations on the nodes of the tree-like data structure. The `CSTVisitor` class provides a flexible mechanism for traversal, allowing a set of method calls on the nodes of a tree depending on the type of the node being visited (`visit` and `leave` methods). Visitors can be used in code mods to change the structure of the code, by visiting the nodes in the Abstract Syntax Tree (AST) and introducing modifications as needed. For example, one can use a visitor to find all the function calls in code and replace them with a different function call by manipulating the underlying AST."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Python programming language, visitors are used to traverse data structures and perform operations on the nodes of the tree-like data structure. The `CSTVisitor` class provides a flexible mechanism for traversal, allowing a set of method calls on the nodes of a tree depending on the type of the node being visited (`visit` and `leave` methods). Visitors can be used in code mods to change the structure of the code, by visiting the nodes in the Abstract Syntax Tree (AST) and introducing modifications as needed. For example, one can use a visitor to find all the function calls in code and replace them with a different function call by manipulating the underlying AST.\n"
     ]
    }
   ],
   "source": [
    "# ask a question\n",
    "response2 = chatbot2.reply(\"What is are Visitors and how do they relate to code mods?\")\n",
    "print(response2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new index\n"
     ]
    }
   ],
   "source": [
    "from babydragon.memory.threads.base_thread import BaseThread\n",
    "from babydragon.memory.threads.fifo_thread import FifoThread\n",
    "from babydragon.memory.threads.vector_thread import VectorThread\n",
    "\n",
    "# Initialize the VectorThread\n",
    "vector_memory = VectorThread(name='vector_memory', max_context=2048, use_mark=False)\n",
    "\n",
    "# Initialize the FifoThread with VectorThread as long-term memory thread\n",
    "fifo_memory = FifoThread(name='fifo_memory', max_memory=10, longterm_thread=vector_memory, redundant=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the VectorThread\n",
    "query = \"What is the capital of France?\"\n",
    "results, scores, indices = vector_memory.sorted_query(query, k=5, max_tokens=4000)\n",
    "\n",
    "print(\"Query Results:\")\n",
    "for result in results:\n",
    "    print(result[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, embed, ind = pind.faiss_query(\"Fifo_Threaded_Chat\", k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new index\n"
     ]
    }
   ],
   "source": [
    "ic = Chat (model='gpt-3.5-turbo-0301',index_dict={\"babydragon\": pind}) \n",
    "vci = FifoVectorChat(model='gpt-3.5-turbo-0301',index_dict={\"babydragon\": pind}, max_output_tokens=500, max_index_memory= 1000, max_memory=2000, longterm_frac=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qa bot\n",
    "# ic.gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://localhost:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Running run_text =============\n",
      "Inputs: Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity\n",
      "======>Current memory:\n",
      " []\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity', \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\")]\n",
      "===============Running run_text =============\n",
      "Inputs: Can you write code for this?\n",
      "======>Current memory:\n",
      " [{'role': 'user', 'content': 'Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity'}, {'role': 'assistant', 'content': \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"}]\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Can you write code for this?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity', \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"), ('Can you write code for this?', 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?')]\n",
      "===============Running run_text =============\n",
      "Inputs: Can you summarize MemoryKernel\n",
      "======>Current memory:\n",
      " [{'role': 'user', 'content': 'Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity'}, {'role': 'assistant', 'content': \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"}, {'role': 'user', 'content': 'Can you write code for this?'}, {'role': 'assistant', 'content': 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'}]\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Can you summarize MemoryKernel"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity', \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"), ('Can you write code for this?', 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'), ('Can you summarize MemoryKernel', 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.')]\n",
      "===============Running run_text =============\n",
      "Inputs: Can you summarize group_index_by_rank?\n",
      "======>Current memory:\n",
      " [{'role': 'user', 'content': 'Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity'}, {'role': 'assistant', 'content': \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"}, {'role': 'user', 'content': 'Can you write code for this?'}, {'role': 'assistant', 'content': 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'}, {'role': 'user', 'content': 'Can you summarize MemoryKernel'}, {'role': 'assistant', 'content': 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.'}]\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Can you summarize group_index_by_rank?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " group_items_by_rank_buckets is a method in the MemoryKernel library that is used to group items based on their similarity scores. It takes in code_strings, node_embeddings, S_vectors, a string S_vectors_type, component_window_size, and a boolean use_softmax as inputs. It returns a list of tuples, where each tuple contains a list of sorted strings and a numpy array of sorted node embeddings.\n",
       "\n",
       "The method first checks if the S_vectors_type input is either \"U\" or \"Vt\". If not, it raises a ValueError. If use_softmax is True, it applies the softmax function to the S_vectors array along the axis specified by the input parameter axis. It then determines the number of \"buckets\" based on the number of rows in S_vectors divided by the component_window_size parameter. For each bucket, it selects the rows/columns corresponding to that bucket from the S_vectors array and computes the sum of the absolute values of each row or column, depending on the value of S_vectors_type. It then sorts the contributions array and returns the sorted indexes that correspond to the sorted items in the code_strings and node_embeddings inputs. Finally, it appends the sorted items to a list of tuples and returns the list."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity', \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"), ('Can you write code for this?', 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'), ('Can you summarize MemoryKernel', 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.'), ('Can you summarize group_index_by_rank?', 'group_items_by_rank_buckets is a method in the MemoryKernel library that is used to group items based on their similarity scores. It takes in code_strings, node_embeddings, S_vectors, a string S_vectors_type, component_window_size, and a boolean use_softmax as inputs. It returns a list of tuples, where each tuple contains a list of sorted strings and a numpy array of sorted node embeddings.\\n\\nThe method first checks if the S_vectors_type input is either \"U\" or \"Vt\". If not, it raises a ValueError. If use_softmax is True, it applies the softmax function to the S_vectors array along the axis specified by the input parameter axis. It then determines the number of \"buckets\" based on the number of rows in S_vectors divided by the component_window_size parameter. For each bucket, it selects the rows/columns corresponding to that bucket from the S_vectors array and computes the sum of the absolute values of each row or column, depending on the value of S_vectors_type. It then sorts the contributions array and returns the sorted indexes that correspond to the sorted items in the code_strings and node_embeddings inputs. Finally, it appends the sorted items to a list of tuples and returns the list.')]\n",
      "===============Running run_text =============\n",
      "Inputs: can you summarize memory_index.py\n",
      "======>Current memory:\n",
      " [{'role': 'user', 'content': 'Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity'}, {'role': 'assistant', 'content': \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"}, {'role': 'user', 'content': 'Can you write code for this?'}, {'role': 'assistant', 'content': 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'}, {'role': 'user', 'content': 'Can you summarize MemoryKernel'}, {'role': 'assistant', 'content': 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.'}, {'role': 'user', 'content': 'Can you summarize group_index_by_rank?'}, {'role': 'assistant', 'content': 'group_items_by_rank_buckets is a method in the MemoryKernel library that is used to group items based on their similarity scores. It takes in code_strings, node_embeddings, S_vectors, a string S_vectors_type, component_window_size, and a boolean use_softmax as inputs. It returns a list of tuples, where each tuple contains a list of sorted strings and a numpy array of sorted node embeddings.\\n\\nThe method first checks if the S_vectors_type input is either \"U\" or \"Vt\". If not, it raises a ValueError. If use_softmax is True, it applies the softmax function to the S_vectors array along the axis specified by the input parameter axis. It then determines the number of \"buckets\" based on the number of rows in S_vectors divided by the component_window_size parameter. For each bucket, it selects the rows/columns corresponding to that bucket from the S_vectors array and computes the sum of the absolute values of each row or column, depending on the value of S_vectors_type. It then sorts the contributions array and returns the sorted indexes that correspond to the sorted items in the code_strings and node_embeddings inputs. Finally, it appends the sorted items to a list of tuples and returns the list.'}]\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " can you summarize memory_index.py"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The provided code snippets do not contain a complete definition of memory_index.py. However, based on the code snippets provided, it seems that memory_index.py is a module that contains classes and functions related to memory indices for Python code. The module includes the GitMemory class, which is a subclass of MemoryIndex and is used for creating an index of code from a GitHub repository, and the PythonIndex class, which is also a subclass of MemoryIndex and is used for creating an index of Python code from a local directory. The module also includes functions like __getitem__ and __init__ for interacting with memory threads and initializing objects. Overall, memory_index.py seems to provide a comprehensive set of tools for indexing and searching Python code within a memory-based system."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity', \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"), ('Can you write code for this?', 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'), ('Can you summarize MemoryKernel', 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.'), ('Can you summarize group_index_by_rank?', 'group_items_by_rank_buckets is a method in the MemoryKernel library that is used to group items based on their similarity scores. It takes in code_strings, node_embeddings, S_vectors, a string S_vectors_type, component_window_size, and a boolean use_softmax as inputs. It returns a list of tuples, where each tuple contains a list of sorted strings and a numpy array of sorted node embeddings.\\n\\nThe method first checks if the S_vectors_type input is either \"U\" or \"Vt\". If not, it raises a ValueError. If use_softmax is True, it applies the softmax function to the S_vectors array along the axis specified by the input parameter axis. It then determines the number of \"buckets\" based on the number of rows in S_vectors divided by the component_window_size parameter. For each bucket, it selects the rows/columns corresponding to that bucket from the S_vectors array and computes the sum of the absolute values of each row or column, depending on the value of S_vectors_type. It then sorts the contributions array and returns the sorted indexes that correspond to the sorted items in the code_strings and node_embeddings inputs. Finally, it appends the sorted items to a list of tuples and returns the list.'), ('can you summarize memory_index.py', 'The provided code snippets do not contain a complete definition of memory_index.py. However, based on the code snippets provided, it seems that memory_index.py is a module that contains classes and functions related to memory indices for Python code. The module includes the GitMemory class, which is a subclass of MemoryIndex and is used for creating an index of code from a GitHub repository, and the PythonIndex class, which is also a subclass of MemoryIndex and is used for creating an index of Python code from a local directory. The module also includes functions like __getitem__ and __init__ for interacting with memory threads and initializing objects. Overall, memory_index.py seems to provide a comprehensive set of tools for indexing and searching Python code within a memory-based system.')]\n",
      "===============Running run_text =============\n",
      "Inputs: Write some code to imporve MemoryKernel\n",
      "======>Current memory:\n",
      " [{'role': 'user', 'content': 'Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity'}, {'role': 'assistant', 'content': \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"}, {'role': 'user', 'content': 'Can you write code for this?'}, {'role': 'assistant', 'content': 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'}, {'role': 'user', 'content': 'Can you summarize MemoryKernel'}, {'role': 'assistant', 'content': 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.'}, {'role': 'user', 'content': 'Can you summarize group_index_by_rank?'}, {'role': 'assistant', 'content': 'group_items_by_rank_buckets is a method in the MemoryKernel library that is used to group items based on their similarity scores. It takes in code_strings, node_embeddings, S_vectors, a string S_vectors_type, component_window_size, and a boolean use_softmax as inputs. It returns a list of tuples, where each tuple contains a list of sorted strings and a numpy array of sorted node embeddings.\\n\\nThe method first checks if the S_vectors_type input is either \"U\" or \"Vt\". If not, it raises a ValueError. If use_softmax is True, it applies the softmax function to the S_vectors array along the axis specified by the input parameter axis. It then determines the number of \"buckets\" based on the number of rows in S_vectors divided by the component_window_size parameter. For each bucket, it selects the rows/columns corresponding to that bucket from the S_vectors array and computes the sum of the absolute values of each row or column, depending on the value of S_vectors_type. It then sorts the contributions array and returns the sorted indexes that correspond to the sorted items in the code_strings and node_embeddings inputs. Finally, it appends the sorted items to a list of tuples and returns the list.'}, {'role': 'user', 'content': 'can you summarize memory_index.py'}, {'role': 'assistant', 'content': 'The provided code snippets do not contain a complete definition of memory_index.py. However, based on the code snippets provided, it seems that memory_index.py is a module that contains classes and functions related to memory indices for Python code. The module includes the GitMemory class, which is a subclass of MemoryIndex and is used for creating an index of code from a GitHub repository, and the PythonIndex class, which is also a subclass of MemoryIndex and is used for creating an index of Python code from a local directory. The module also includes functions like __getitem__ and __init__ for interacting with memory threads and initializing objects. Overall, memory_index.py seems to provide a comprehensive set of tools for indexing and searching Python code within a memory-based system.'}]\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Write some code to imporve MemoryKernel"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " Based on the information provided by the previous code snippets, there are a few ways to enhance the performance of MemoryKernel. One way would be to fine-tune its similarity search algorithm by optimizing the group_items_by_rank_buckets function. This can be achieved by experimenting with different values for the component_window_size parameter, adjusting the use_softmax parameter, and testing different similarity metrics to find the most accurate and efficient approach.\n",
       "\n",
       "Another way to improve MemoryKernel would be to optimize the creation of the code index. This could involve implementing caching mechanisms to speed up the retrieval of data, increasing the size of the training data, or using more sophisticated pre-processing techniques to extract features from the code.\n",
       "\n",
       "Finally, optimizing the memory usage of the library could also help improve its performance. This could involve using a more efficient data structure to represent the similarity scores, minimizing the size of the embeddings used to train the model, and possibly experimenting with distributed computing techniques to speed up the processing.\n",
       "\n",
       "Here is an example code snippet for fine-tuning the use of group_items_by_rank_buckets function:\n",
       "\n",
       "```\n",
       "from memory_kernel import MemoryKernel\n",
       "\n",
       "# Initialize the MemoryKernel object and configure its parameters\n",
       "mk = MemoryKernel()\n",
       "mk.component_window_size = 10\n",
       "mk.use_softmax = True\n",
       "mk.similarity_metric = \"cosine\"\n",
       "\n",
       "# Load the data and build the index\n",
       "mk.load_data(path_to_data)\n",
       "mk.build_index()\n",
       "\n",
       "# Test the search function with a query\n",
       "query = \"def calculate_fibonacci(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\"\n",
       "results = mk.search(query)\n",
       "\n",
       "print(results)\n",
       "```\n",
       "\n",
       "This code snippet initializes a MemoryKernel object, sets its component_window_size parameter to 10 and its use_softmax parameter to True. It also sets the similarity_metric parameter to \"cosine\" to use cosine similarity as the similarity metric. The code then loads the data from a file using the load_data method and builds the index using the build_index method. Finally, it executes a search query using the search method and prints the results."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('Can you suggest some ways to imporve Memory Kenrel, mainly with respect to group_items_by_rank and Nmi Modularity', \"Memory Kernel is a library for doing similarity search over sequences of tokens. In order to improve its performance, one approach would be to optimize the group_items_by_rank method, which is used to group items based on their similarity scores. Another approach would be to fine-tune its NMI Modularity algorithm, which is used for clustering similar items together to form communities. Additionally, increasing the size of the dataset used for training and fine-tuning the embeddings could also help improve Memory Kernel's performance. Another suggestion would be to explore the use of different similarity metrics or algorithms, such as cosine similarity or Jaccard distance, to see if they can improve the accuracy of Memory Kernel's similarity search.\"), ('Can you write code for this?', 'I can definitely help you write code for your question, but I need more specific information about what you want to accomplish. The code provided seems to be related to code generation and code minification, but the full context and requirements are not clear. Can you provide more details or a specific task that you want to achieve?'), ('Can you summarize MemoryKernel', 'MemoryKernel is a library for similarity search over sequences of tokens. It consists of several classes and methods, each designed for a specific task, such as creating indices and computing similarity scores. The package includes classes like MemoryIndex, GitMemory, VectorThread, KHopIndex, and more. It allows users to fine-tune similarity search using techniques like NMI Modularity, group_items_by_rank, and k-hop message passing. Overall, MemoryKernel is a comprehensive library for similarity search that can be useful for a wide range of applications.'), ('Can you summarize group_index_by_rank?', 'group_items_by_rank_buckets is a method in the MemoryKernel library that is used to group items based on their similarity scores. It takes in code_strings, node_embeddings, S_vectors, a string S_vectors_type, component_window_size, and a boolean use_softmax as inputs. It returns a list of tuples, where each tuple contains a list of sorted strings and a numpy array of sorted node embeddings.\\n\\nThe method first checks if the S_vectors_type input is either \"U\" or \"Vt\". If not, it raises a ValueError. If use_softmax is True, it applies the softmax function to the S_vectors array along the axis specified by the input parameter axis. It then determines the number of \"buckets\" based on the number of rows in S_vectors divided by the component_window_size parameter. For each bucket, it selects the rows/columns corresponding to that bucket from the S_vectors array and computes the sum of the absolute values of each row or column, depending on the value of S_vectors_type. It then sorts the contributions array and returns the sorted indexes that correspond to the sorted items in the code_strings and node_embeddings inputs. Finally, it appends the sorted items to a list of tuples and returns the list.'), ('can you summarize memory_index.py', 'The provided code snippets do not contain a complete definition of memory_index.py. However, based on the code snippets provided, it seems that memory_index.py is a module that contains classes and functions related to memory indices for Python code. The module includes the GitMemory class, which is a subclass of MemoryIndex and is used for creating an index of code from a GitHub repository, and the PythonIndex class, which is also a subclass of MemoryIndex and is used for creating an index of Python code from a local directory. The module also includes functions like __getitem__ and __init__ for interacting with memory threads and initializing objects. Overall, memory_index.py seems to provide a comprehensive set of tools for indexing and searching Python code within a memory-based system.'), ('Write some code to imporve MemoryKernel', 'Based on the information provided by the previous code snippets, there are a few ways to enhance the performance of MemoryKernel. One way would be to fine-tune its similarity search algorithm by optimizing the group_items_by_rank_buckets function. This can be achieved by experimenting with different values for the component_window_size parameter, adjusting the use_softmax parameter, and testing different similarity metrics to find the most accurate and efficient approach.\\n\\nAnother way to improve MemoryKernel would be to optimize the creation of the code index. This could involve implementing caching mechanisms to speed up the retrieval of data, increasing the size of the training data, or using more sophisticated pre-processing techniques to extract features from the code.\\n\\nFinally, optimizing the memory usage of the library could also help improve its performance. This could involve using a more efficient data structure to represent the similarity scores, minimizing the size of the embeddings used to train the model, and possibly experimenting with distributed computing techniques to speed up the processing.\\n\\nHere is an example code snippet for fine-tuning the use of group_items_by_rank_buckets function:\\n\\n```\\nfrom memory_kernel import MemoryKernel\\n\\n# Initialize the MemoryKernel object and configure its parameters\\nmk = MemoryKernel()\\nmk.component_window_size = 10\\nmk.use_softmax = True\\nmk.similarity_metric = \"cosine\"\\n\\n# Load the data and build the index\\nmk.load_data(path_to_data)\\nmk.build_index()\\n\\n# Test the search function with a query\\nquery = \"def calculate_fibonacci(n):\\\\n    if n == 0:\\\\n        return 0\\\\n    elif n == 1:\\\\n        return 1\\\\n    else:\\\\n        return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\"\\nresults = mk.search(query)\\n\\nprint(results)\\n```\\n\\nThis code snippet initializes a MemoryKernel object, sets its component_window_size parameter to 10 and its use_softmax parameter to True. It also sets the similarity_metric parameter to \"cosine\" to use cosine similarity as the similarity metric. The code then loads the data from a file using the load_data method and builds the index using the build_index method. Finally, it executes a search query using the search method and prints the results.')]\n"
     ]
    }
   ],
   "source": [
    "#chatbot with memory of conversation \n",
    "vci.gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio\n",
    "gradio.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
