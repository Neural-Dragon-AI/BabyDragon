{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Callable, List, Optional\n",
    "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
    "from babydragon.memory.indexes.python_index import PythonIndex\n",
    "from babydragon.chat.memory_chat import FifoChat\n",
    "import babydragon\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "babydragon_path = os.path.dirname(os.path.abspath(babydragon.__file__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new index\n",
      "Indexing 309 functions and 48 classes\n"
     ]
    }
   ],
   "source": [
    "babyindex= PythonIndex(babydragon_path, name=\"babyd_index\", minify_code=False, load = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = FifoChat(index_dict = {\"babyindex\":babyindex}, name=\"babyd_chatbot\", max_fifo_memory=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['\\n\\nclass PythonIndex(MemoryIndex, PythonParser):\\n    def __init__(\\n        self,\\n        directory_path: str,\\n        name: str = \"python_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        tokenizer: Optional[tiktoken.Encoding] = None,\\n    ):\\n        # Initialize the MemoryIndex\\n        MemoryIndex.__init__(\\n            self,\\n            name=name,\\n            save_path=save_path,\\n            load=load,\\n            tokenizer=tokenizer,\\n        )\\n        # Initialize the PythonParser\\n        PythonParser.__init__(\\n            self,\\n            directory_path=directory_path,\\n            minify_code=minify_code,\\n            remove_docstrings=remove_docstrings,\\n        )\\n\\n        if not load:\\n            # Extract functions and classes source code\\n            function_source_codes, class_source_codes, _, _ = self.process_directory()\\n            print(\\n                \"Indexing {} functions and {} classes\".format(\\n                    len(function_source_codes), len(class_source_codes)\\n                )\\n            )\\n            # Concatenate function and class source code and index them\\n            codes = function_source_codes + class_source_codes\\n            for code in codes:\\n                self.add_to_index(code)\\n\\n            self.save()\\n',\n",
       "  'from typing import Optional\\n\\nimport tiktoken\\n\\nfrom babydragon.memory.indexes.memory_index import MemoryIndex\\nfrom babydragon.processors.parsers.python_parser import PythonParser\\n\\n\\nclass PythonIndex(MemoryIndex, PythonParser):\\n    def __init__(\\n        self,\\n        directory_path: str,\\n        name: str = \"python_index\",\\n        save_path: Optional[str] = None,\\n        load: bool = False,\\n        minify_code: bool = False,\\n        remove_docstrings: bool = False,\\n        tokenizer: Optional[tiktoken.Encoding] = None,\\n    ):\\n        # Initialize the MemoryIndex\\n        MemoryIndex.__init__(\\n            self,\\n            name=name,\\n            save_path=save_path,\\n            load=load,\\n            tokenizer=tokenizer,\\n        )\\n        # Initialize the PythonParser\\n        PythonParser.__init__(\\n            self,\\n            directory_path=directory_path,\\n            minify_code=minify_code,\\n            remove_docstrings=remove_docstrings,\\n        )\\n\\n        if not load:\\n            # Extract functions and classes source code\\n            function_source_codes, class_source_codes, _, _ = self.process_directory()\\n            print(\\n                \"Indexing {} functions and {} classes\".format(\\n                    len(function_source_codes), len(class_source_codes)\\n                )\\n            )\\n            # Concatenate function and class source code and index them\\n            codes = function_source_codes + class_source_codes\\n            for code in codes:\\n                self.add_to_index(code)\\n\\n            self.save()\\n',\n",
       "  'def __init__(\\n    self,\\n    directory_path: str,\\n    name: str = \"python_index\",\\n    save_path: Optional[str] = None,\\n    load: bool = False,\\n    minify_code: bool = False,\\n    remove_docstrings: bool = False,\\n    tokenizer: Optional[tiktoken.Encoding] = None,\\n):\\n    # Initialize the MemoryIndex\\n    MemoryIndex.__init__(\\n        self,\\n        name=name,\\n        save_path=save_path,\\n        load=load,\\n        tokenizer=tokenizer,\\n    )\\n    # Initialize the PythonParser\\n    PythonParser.__init__(\\n        self,\\n        directory_path=directory_path,\\n        minify_code=minify_code,\\n        remove_docstrings=remove_docstrings,\\n    )\\n\\n    if not load:\\n        # Extract functions and classes source code\\n        function_source_codes, class_source_codes, _, _ = self.process_directory()\\n        print(\\n            \"Indexing {} functions and {} classes\".format(\\n                len(function_source_codes), len(class_source_codes)\\n            )\\n        )\\n        # Concatenate function and class source code and index them\\n        codes = function_source_codes + class_source_codes\\n        for code in codes:\\n            self.add_to_index(code)\\n\\n        self.save()\\n',\n",
       "  'from typing import List, Optional, Union\\n\\nimport libcst as cst\\nfrom python_minifier import minify\\n\\nfrom babydragon.memory.indexes.memory_index import MemoryIndex\\nfrom babydragon.working_memory.parsers.git_processor import GitHubRepoProcessor\\n\\n\\nclass PythonMinifier:\\n    def __init__(self, code: str = None):\\n\\n        self.code = code\\n        self.output_code = None\\n\\n    def minify(self):\\n        if self.code:\\n            self.output_code = self.minify_code(self.code)\\n\\n    def get_minified_code(self):\\n        if not self.output_code:\\n            self.minify()\\n        return self.output_code\\n\\n    @staticmethod\\n    def minify_code(code: str) -> str:\\n        return minify(code)\\n\\n\\nclass PythonDocstringExtractor:\\n    @staticmethod\\n    def extract_docstring(function_def: cst.FunctionDef) -> str:\\n        docstring = None\\n\\n        for stmt in function_def.body.body:\\n            if isinstance(stmt, cst.SimpleStatementLine):\\n                for expr in stmt.body:\\n                    if isinstance(expr, cst.Expr) and isinstance(\\n                        expr.value, cst.SimpleString\\n                    ):\\n                        docstring = expr.value.value.strip(\\'\"\\').strip(\"\\'\")\\n                        break\\n            if docstring is not None:\\n                break\\n\\n        if docstring is not None:\\n            return docstring.strip()\\n        else:\\n            function_name = function_def.name.value\\n            return f\"No docstring provided for function \\'{function_name}\\'. Please add a docstring to describe this function.\"\\n\\n\\nclass GitMemory(MemoryIndex):\\n    def __init__(self, username, repo_name):\\n        super().__init__()\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.parser = GitHubRepoProcessor(username, repo_name)\\n        self.minifier = PythonMinifier()\\n        self.docstring_extractor = PythonDocstringExtractor()\\n        self.directory_parser = None\\n        self.min_code_index = None\\n        self.doc_string_index = None\\n        self.libcst_node_index = None\\n\\n    def create_code_index(self, base_directory):\\n        self.directory_parser = self.parser.process_repo(base_directory)\\n        code_values, code_nodes = self.parser.get_values()\\n        self.code_index = self.init_index(values=code_values)\\n        self.code_index.save()\\n\\n    def create_indexes(self, base_directory):\\n        self.directory_parser = self.parser.process_repo(base_directory)\\n        code_values, code_nodes = self.parser.get_values()\\n        self.code_index = self.init_index(values=code_values)\\n\\n        min_code_values = []\\n        doc_string_values = []\\n        for code_value, code_node in zip(code_values, code_nodes):\\n            minifier = PythonMinifier(code=code_value)\\n            min_code = minifier.get_minified_code()\\n            doc_string = self.docstring_extractor.extract_docstring(code_node)\\n            min_code_values.append(min_code)\\n            doc_string_values.append(doc_string)\\n        self.doc_string_index = self.init_index(values=doc_string_values)\\n        self.min_code_index = self.init_index(values=min_code_values)\\n',\n",
       "  '\\ndef create_indexes(self, base_directory):\\n    self.directory_parser = self.parser.process_repo(base_directory)\\n    code_values, code_nodes = self.parser.get_values()\\n    self.code_index = self.init_index(values=code_values)\\n\\n    min_code_values = []\\n    doc_string_values = []\\n    for code_value, code_node in zip(code_values, code_nodes):\\n        minifier = PythonMinifier(code=code_value)\\n        min_code = minifier.get_minified_code()\\n        doc_string = self.docstring_extractor.extract_docstring(code_node)\\n        min_code_values.append(min_code)\\n        doc_string_values.append(doc_string)\\n    self.doc_string_index = self.init_index(values=doc_string_values)\\n    self.min_code_index = self.init_index(values=min_code_values)\\n',\n",
       "  '# This is the __init__.py file for the package.\\n',\n",
       "  '\\n\\nclass GitMemory(MemoryIndex):\\n    def __init__(self, username, repo_name):\\n        super().__init__()\\n        self.username = username\\n        self.repo_name = repo_name\\n        self.parser = GitHubRepoProcessor(username, repo_name)\\n        self.minifier = PythonMinifier()\\n        self.docstring_extractor = PythonDocstringExtractor()\\n        self.directory_parser = None\\n        self.min_code_index = None\\n        self.doc_string_index = None\\n        self.libcst_node_index = None\\n\\n    def create_code_index(self, base_directory):\\n        self.directory_parser = self.parser.process_repo(base_directory)\\n        code_values, code_nodes = self.parser.get_values()\\n        self.code_index = self.init_index(values=code_values)\\n        self.code_index.save()\\n\\n    def create_indexes(self, base_directory):\\n        self.directory_parser = self.parser.process_repo(base_directory)\\n        code_values, code_nodes = self.parser.get_values()\\n        self.code_index = self.init_index(values=code_values)\\n\\n        min_code_values = []\\n        doc_string_values = []\\n        for code_value, code_node in zip(code_values, code_nodes):\\n            minifier = PythonMinifier(code=code_value)\\n            min_code = minifier.get_minified_code()\\n            doc_string = self.docstring_extractor.extract_docstring(code_node)\\n            min_code_values.append(min_code)\\n            doc_string_values.append(doc_string)\\n        self.doc_string_index = self.init_index(values=doc_string_values)\\n        self.min_code_index = self.init_index(values=min_code_values)\\n',\n",
       "  '\\n\\nclass PythonMinifier:\\n    def __init__(self, code: str = None):\\n\\n        self.code = code\\n        self.output_code = None\\n\\n    def minify(self):\\n        if self.code:\\n            self.output_code = self.minify_code(self.code)\\n\\n    def get_minified_code(self):\\n        if not self.output_code:\\n            self.minify()\\n        return self.output_code\\n\\n    @staticmethod\\n    def minify_code(code: str) -> str:\\n        return minify(code)\\n',\n",
       "  'def __init__(\\n    self,\\n    base_directory: str,\\n    username=None,\\n    repo_name=None,\\n    code_parsers=None,\\n    minify_code: bool = False,\\n    remove_docstrings: bool = False,\\n):\\n    self.username = username\\n    self.repo_name = repo_name\\n    self.base_directory = base_directory\\n    self.github = Github()\\n    self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\\n    repo_path = self.clone_repo(self.repo.clone_url)\\n\\n    OsProcessor.__init__(self, repo_path)\\n    self.code_parsers = code_parsers or [\\n        PythonParser(\\n            repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\\n        )\\n    ]\\n',\n",
       "  '\\ndef __init__(\\n    self,\\n    model: str = None,\\n    max_output_tokens: int = 1000,\\n    system_prompt: str = None,\\n    user_prompt: str = None,\\n    index_dict: Optional[Dict[str,MemoryIndex]] = None,\\n    max_index_memory: int = 1000,\\n) -> None:\\n    BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\\n    Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\\n    self.index_dict = index_dict\\n    self.setup_indices(max_index_memory)\\n'],\n",
       " [0.81691384,\n",
       "  0.81105536,\n",
       "  0.78589565,\n",
       "  0.76883537,\n",
       "  0.767285,\n",
       "  0.7568508,\n",
       "  0.75186425,\n",
       "  0.7464661,\n",
       "  0.7445079,\n",
       "  0.74320173],\n",
       " array([[280,  91,  90, 268, 267,   0, 314, 299, 150,  18]], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "babyindex.faiss_query(\"PythonIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Can you explain me memorythread?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " Sure! MemoryThread is a class that is used for storing and retrieving memories in a conversational AI. It is defined in the code snippets you gave me. MemoryThread objects have various class methods like __init__(), __len__(), __getitem__(), reset_memory() etc. which allow for easy manipulation of the memory_thread.\n",
       "\n",
       "Essentially, memory_thread is a list that stores the conversation history and context. The methods of the MemoryThread class provide convenient ways to add to, retrieve from, and reset this list. The memory_thread can be used to keep track of the current state of the conversation, and to provide context for future interactions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Sure! MemoryThread is a class that is used for storing and retrieving memories in a conversational AI. It is defined in the code snippets you gave me. MemoryThread objects have various class methods like __init__(), __len__(), __getitem__(), reset_memory() etc. which allow for easy manipulation of the memory_thread.\\n\\nEssentially, memory_thread is a list that stores the conversation history and context. The methods of the MemoryThread class provide convenient ways to add to, retrieve from, and reset this list. The memory_thread can be used to keep track of the current state of the conversation, and to provide context for future interactions.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.reply(\"Can you explain me memorythread?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://localhost:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Running run_text =============\n",
      "Inputs: What is the best way to stor memory using an hibrid of vector search and temporal priority?\n",
      "======>Current memory:\n",
      " [{'role': 'user', 'content': 'Can you explain me memorythread?'}, {'role': 'assistant', 'content': 'Sure! MemoryThread is a class that is used for storing and retrieving memories in a conversational AI. It is defined in the code snippets you gave me. MemoryThread objects have various class methods like __init__(), __len__(), __getitem__(), reset_memory() etc. which allow for easy manipulation of the memory_thread.\\n\\nEssentially, memory_thread is a list that stores the conversation history and context. The methods of the MemoryThread class provide convenient ways to add to, retrieve from, and reset this list. The memory_thread can be used to keep track of the current state of the conversation, and to provide context for future interactions.'}]\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " What is the best way to stor memory using an hibrid of vector search and temporal priority?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " The best way to store memory using a hybrid of vector search and temporal priority is to use an implementation platform that combines the benefits of each method. One example of such a platform is the VectorMemory class provided by the code snippets you gave me. \n",
       "\n",
       "The VectorMemory class uses both vector search and temporal priority to organize and retrieve memories. It stores messages in a memory_thread, which is prioritized based on recency. The most recent messages are at the end of the list, while older messages are at the beginning. This allows for easy retrieval of the most recent messages.\n",
       "\n",
       "Additionally, the VectorMemory class also uses a local_index, which is a vector search index that provides quick access to messages based on their content. The local_index is built using faiss, a library for efficient similarity search and clustering of dense vectors.\n",
       "\n",
       "Using both temporal priority and vector search allows for quick and efficient storage and retrieval of memories, while also prioritizing recent messages for easier access."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('What is the best way to stor memory using an hibrid of vector search and temporal priority?', 'The best way to store memory using a hybrid of vector search and temporal priority is to use an implementation platform that combines the benefits of each method. One example of such a platform is the VectorMemory class provided by the code snippets you gave me. \\n\\nThe VectorMemory class uses both vector search and temporal priority to organize and retrieve memories. It stores messages in a memory_thread, which is prioritized based on recency. The most recent messages are at the end of the list, while older messages are at the beginning. This allows for easy retrieval of the most recent messages.\\n\\nAdditionally, the VectorMemory class also uses a local_index, which is a vector search index that provides quick access to messages based on their content. The local_index is built using faiss, a library for efficient similarity search and clustering of dense vectors.\\n\\nUsing both temporal priority and vector search allows for quick and efficient storage and retrieval of memories, while also prioritizing recent messages for easier access.')]\n",
      "===============Running run_text =============\n",
      "Inputs: can you show me an example of how to use it?\n",
      "======>Current memory:\n",
      " [{'role': 'user', 'content': 'Can you explain me memorythread?'}, {'role': 'assistant', 'content': 'Sure! MemoryThread is a class that is used for storing and retrieving memories in a conversational AI. It is defined in the code snippets you gave me. MemoryThread objects have various class methods like __init__(), __len__(), __getitem__(), reset_memory() etc. which allow for easy manipulation of the memory_thread.\\n\\nEssentially, memory_thread is a list that stores the conversation history and context. The methods of the MemoryThread class provide convenient ways to add to, retrieve from, and reset this list. The memory_thread can be used to keep track of the current state of the conversation, and to provide context for future interactions.'}, {'role': 'user', 'content': 'What is the best way to stor memory using an hibrid of vector search and temporal priority?'}, {'role': 'assistant', 'content': 'The best way to store memory using a hybrid of vector search and temporal priority is to use an implementation platform that combines the benefits of each method. One example of such a platform is the VectorMemory class provided by the code snippets you gave me. \\n\\nThe VectorMemory class uses both vector search and temporal priority to organize and retrieve memories. It stores messages in a memory_thread, which is prioritized based on recency. The most recent messages are at the end of the list, while older messages are at the beginning. This allows for easy retrieval of the most recent messages.\\n\\nAdditionally, the VectorMemory class also uses a local_index, which is a vector search index that provides quick access to messages based on their content. The local_index is built using faiss, a library for efficient similarity search and clustering of dense vectors.\\n\\nUsing both temporal priority and vector search allows for quick and efficient storage and retrieval of memories, while also prioritizing recent messages for easier access.'}]\n",
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " can you show me an example of how to use it?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " Sure! To show an example of how to use the index described in the hints, you can follow these steps:\n",
       "\n",
       "1. Create an instance of the index using the provided index_description. You can do this by calling the constructor of the index class with the description as a parameter.\n",
       "\n",
       "2. Add data to the index using the add_data() method. This method takes a list of vectors and an optional list of metadata.\n",
       "\n",
       "3. Build the index using the build() method. This will create the index and prepare it for querying.\n",
       "\n",
       "4. Query the index using the search() method. This method takes a query vector as input, along with a number of nearest neighbors to return. The method will return a list of vectors along with their distances from the query vector.\n",
       "\n",
       "Here is an example code snippet that shows how to use these steps:\n",
       "\n",
       "```\n",
       "# create an instance of the index\n",
       "index = Index(index_description)\n",
       "\n",
       "# add data to the index\n",
       "data = [[0.2, 0.3, 0.4], [0.1, 0.2, 0.3], [0.5, 0.2, 0.3]]\n",
       "metadata = [\"example1\", \"example2\", \"example3\"]\n",
       "index.add_data(data, metadata)\n",
       "\n",
       "# build the index\n",
       "index.build()\n",
       "\n",
       "# query the index\n",
       "query = [0.1, 0.1, 0.1]\n",
       "results = index.search(query, k=2)\n",
       "\n",
       "print(results)\n",
       "```\n",
       "\n",
       "This code will create an index with the provided description, add some example data to it, build the index, and then query it using a sample query vector. The search() method will return a list of vectors along with their distances from the query vector."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: [('What is the best way to stor memory using an hibrid of vector search and temporal priority?', 'The best way to store memory using a hybrid of vector search and temporal priority is to use an implementation platform that combines the benefits of each method. One example of such a platform is the VectorMemory class provided by the code snippets you gave me.<br>\\n<br>\\nThe VectorMemory class uses both vector search and temporal priority to organize and retrieve memories. It stores messages in a memory_thread, which is prioritized based on recency. The most recent messages are at the end of the list, while older messages are at the beginning. This allows for easy retrieval of the most recent messages.<br>\\n<br>\\nAdditionally, the VectorMemory class also uses a local_index, which is a vector search index that provides quick access to messages based on their content. The local_index is built using faiss, a library for efficient similarity search and clustering of dense vectors.<br>\\n<br>\\nUsing both temporal priority and vector search allows for quick and efficient storage and retrieval of memories, while also prioritizing recent messages for easier access.'), ('can you show me an example of how to use it?', 'Sure! To show an example of how to use the index described in the hints, you can follow these steps:\\n\\n1. Create an instance of the index using the provided index_description. You can do this by calling the constructor of the index class with the description as a parameter.\\n\\n2. Add data to the index using the add_data() method. This method takes a list of vectors and an optional list of metadata.\\n\\n3. Build the index using the build() method. This will create the index and prepare it for querying.\\n\\n4. Query the index using the search() method. This method takes a query vector as input, along with a number of nearest neighbors to return. The method will return a list of vectors along with their distances from the query vector.\\n\\nHere is an example code snippet that shows how to use these steps:\\n\\n```\\n# create an instance of the index\\nindex = Index(index_description)\\n\\n# add data to the index\\ndata = [[0.2, 0.3, 0.4], [0.1, 0.2, 0.3], [0.5, 0.2, 0.3]]\\nmetadata = [\"example1\", \"example2\", \"example3\"]\\nindex.add_data(data, metadata)\\n\\n# build the index\\nindex.build()\\n\\n# query the index\\nquery = [0.1, 0.1, 0.1]\\nresults = index.search(query, k=2)\\n\\nprint(results)\\n```\\n\\nThis code will create an index with the provided description, add some example data to it, build the index, and then query it using a sample query vector. The search() method will return a list of vectors along with their distances from the query vector.')]\n"
     ]
    }
   ],
   "source": [
    "chatbot.gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
