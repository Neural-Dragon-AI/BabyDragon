{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babydragon.models.generators.PolarsGenerator import PolarsGenerator\n",
    "from babydragon.utils.frame_generators import  cluster_summaries, load_generated_content\n",
    "\n",
    "from babydragon.memory.frames.code_frame import CodeFrame\n",
    "import polars as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "babydragon.utils.main_logger - INFO - Found 950 values in the directory /Users/danielhug/neuraldragon/frames_arc/BabyDragon/babydragon\n"
     ]
    }
   ],
   "source": [
    "code_repo = \"/Users/danielhug/neuraldragon/frames_arc/BabyDragon/babydragon\"\n",
    "mfp = CodeFrame.from_python(directory_path=code_repo, value_column=\"code\", embeddable_columns=[\"code\"], context_columns=[\"libcst_tree\", \"filename\"], name=\"babydragon_frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = mfp.df.groupby('filename').agg(pl.col('code').alias('code'))\n",
    "# convert content column to str\n",
    "grouped_df = grouped_df.with_columns(\n",
    "    pl.col(\"code\").apply(lambda x: ' '.join(x), return_dtype=pl.Utf8).alias(\"concatenated_content\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_column(df, name):\n",
    "    df = df.select(\"concatenated_content\").with_columns(pl.lit(\"gpt-3.5-turbo-16k\").alias(\"model\"))\n",
    "    def create_content(value):\n",
    "        system_prompt = \"You are tasked as a Code Summarizer. Please provide a succinct summary of the code below as if you we preparing to use it to create a readme.\"\n",
    "        return ([{\"role\": \"system\", \"content\":system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"{value}\"}])\n",
    "\n",
    "    input_df = df.with_columns(df[\"concatenated_content\"].apply(create_content, return_dtype=pl.List).alias('messages')).drop(\"concatenated_content\")\n",
    "    generator = PolarsGenerator( input_df = input_df, name = name)\n",
    "    generator.execute()\n",
    "    out_path = f\"./batch_generator/{name}_output.ndjson\"\n",
    "    output = load_generated_content(out_path)\n",
    "    summary_column = output.select(\"output\")\n",
    "    return summary_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"babydragon_readme_{str(np.random.randint(0,100000))}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "summary_column = generate_summary_column(grouped_df, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_column = summary_column.select(\"output\")\n",
    "grouped_df = grouped_df.with_columns(emb_column)\n",
    "grouped_df = grouped_df.rename({\"output\": \"code_summary\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This code provides a function called `chatgpt_response` that sends a prompt to the OpenAI API for generating a chatbot response using the GPT-3.5-turbo model. The function takes in a list of prompts, the model to use, the maximum number of tokens in the response, and a flag indicating whether to stream the response. If the API call is successful, the function returns the response and a True flag. If there is an error, the function handles the exception by returning a custom error response and a False flag.',\n",
       " 'This code provides functions for working with prompts stored in a file. \\n\\nThe `list_subjects_and_perspective()` function reads prompts from a file and returns a tuple containing sets of subjects and perspectives available in the prompts. \\n\\nThe `list_subjects()` function reads prompts from a file and returns a set of subjects available in the prompts. \\n\\nThe `list_perspectives()` function reads prompts from a file and returns a set of perspectives available in the prompts. \\n\\nThe `get_perspective_prompt(subject, perspective)` function reads prompts from a file and returns a specific prompt for a given subject and perspective. \\n\\nThe `get_random_perspective_prompt()` function reads prompts from a file and returns a random prompt along with its subject and perspective.',\n",
       " 'The code consists of several classes and methods that are used for parsing and processing Python code. \\n\\nThe `PythonMinifier` class is used to minify Python code by removing unnecessary whitespace and comments. It has a `minify` method that takes a code string and returns the minified version of the code.\\n\\nThe `PythonDocstringExtractor` class is used to extract the docstring from a function definition in Python code. It has a static method `extract_docstring` that takes a `FunctionDef` node from the `ast` module and returns the docstring as a string.\\n\\nThe `FunctionAndClassVisitor` class is a visitor pattern implementation that visits the AST nodes of a Python code file and collects information about function and class definitions. It has methods `visit_FunctionDef` and `visit_ClassDef` that are called for every `FunctionDef` and `ClassDef` node in the tree. These methods add the node and its corresponding source code to lists and increment counters.\\n\\nThe `PythonParser` class is used to parse and process Python code files in a given directory. It has methods `process_file` and `process_directory` that handle the processing of individual files and the entire directory, respectively. The processing involves parsing the code, visiting the AST nodes with the `FunctionAndClassVisitor`, and optionally minifying the code and removing docstrings.\\n\\nOverall, these classes and methods provide functionality for parsing, minifying, and extracting information from Python code files.',\n",
       " 'The code provided contains two functions: `cohere_response` and `cohere_summarize`.\\n\\nThe `cohere_response` function is used to call the Cohere API with a given prompt and maximum number of output tokens. The function takes in a list of dictionaries representing the prompt, and optionally, a model to use (default is \"command\") and the maximum number of output tokens (default is 1000). It returns a tuple containing the API response cohere object and a boolean indicating success.\\n\\nThe `cohere_summarize` function is used to summarize a given prompt using the Cohere API. The function takes in a string representing the prompt, and optionally, a model to use (default is \"summarize-xlarge\"), the length of the summary (default is \"medium\"), the extractiveness of the summary (default is \"medium\"), the format of the summary (default is \"bullets\"), and additional command (optional). It returns a string representing the summary.',\n",
       " 'This code defines two classes: `NaturalLanguageSingle` and `NaturalLanguageList`.\\n\\nThe `NaturalLanguageSingle` class represents a single natural language text. It has two fields: `text`, which is the actual text, and `max_tokens`, which is the maximum allowed length of the text in tokens. The `text` field is required and should be less than or equal to `max_tokens` in length when tokenized. The default value of `max_tokens` is 8000. The class also has a validator method that checks if the text is longer than the specified `max_tokens` value.\\n\\nThe `NaturalLanguageList` class represents a list of `NaturalLanguageSingle` objects. It has a single field named `texts`, which is a list of `NaturalLanguageSingle` objects. Each object in the list should pass the validation requirements of the `NaturalLanguageSingle` class.\\n\\nOverall, these classes provide a way to represent and validate natural language texts, both as single texts and as a list of texts.',\n",
       " 'The code defines multiple classes that extend the `cst.CSTVisitor` class to collect different types of information from Python code. Here is a summary of each class:\\n\\n- `DocstringCollector`: collects docstrings from modules, functions, and classes.\\n- `FunctionCallCollector`: collects function calls from the code.\\n- `ArgumentTypeCollector`: collects argument types for functions and methods.\\n- `ImportCollector`: collects import statements from the code.\\n- `IfStatementCollector`: collects if statements from the code.\\n- `BaseCompoundStatementCollector`: collects base compound statements from the code.\\n- `ForLoopCollector`: collects for loops from the code.\\n- `WhileLoopCollector`: collects while loops from the code.\\n- `TryExceptCollector`: collects try-except statements from the code.\\n- `WithCollector`: collects with statements from the code.\\n- `VariableDeclarationCollector`: collects variable declarations from the code.\\n- `ListComprehensionCollector`: collects list comprehensions from the code.\\n- `DictComprehensionCollector`: collects dictionary comprehensions from the code.\\n- `SetComprehensionCollector`: collects set comprehensions from the code.\\n- `GeneratorExpressionCollector`: collects generator expressions from the code.\\n- `YieldCollector`: collects yield statements from the code.\\n- `ReturnCollector`: collects return statements from the code.\\n- `RaiseCollector`: collects raise statements from the code.\\n- `AssertCollector`: collects assert statements from the code.\\n- `BreakCollector`: collects break statements from the code.\\n- `ContinueCollector`: collects continue statements from the code.\\n- `PassCollector`: collects pass statements from the code.\\n- `WithStatementCollector`: collects with statements from the code.\\n- `TryStatementCollector`: collects try statements from the code.\\n- `ExceptClauseCollector`: collects except clauses from the code.\\n- `LambdaFunctionCollector`: collects lambda functions from the code.\\n- `GlobalStatementCollector`: collects global statements from the code.\\n- `NonlocalStatementCollector`: collects nonlocal statements from the code.\\n\\nEach class has an `__init__` method that initializes the `module` attribute by parsing the code using `cst.parse_module`. They also have a `collect` method that visits the `module` using `self.module.visit(self)` and returns the collected information.\\n\\nTo use these collectors, pass the code to the respective class and call the `collect` method to get the collected information.',\n",
       " 'The code provides a class called `BaseThread` that is used as a base class for conversation memory threads. It keeps track of conversation history and the total number of tokens. The class has methods to add and remove messages, retrieve messages based on various filters, save and load memory threads, and perform other operations related to conversation history. It also includes methods to load conversation data from GPT-3 URLs or backups. The class uses the `polars` library for data manipulation and storage.',\n",
       " 'The code provides a class called `MemoryFrame` which is a subclass of `BaseFrame`. It represents a memory-based dataframe and contains various methods for manipulating and analyzing the data. Some of the main methods include tokenizing columns, embedding columns, generating new columns based on existing data, applying validators to columns, and searching for matches in columns. There are also methods for saving and loading the dataframe to/from disk, as well as a method for creating a `MemoryFrame` from a Hugging Face dataset. Additionally, there is a nested class called `MemoryFramePydantic` which is a Pydantic model representing the configuration of a `MemoryFrame` instance.',\n",
       " 'The code provided defines two models: `StatusTrackerModel` and `OpenaiRequestModel`. \\n\\n`StatusTrackerModel` represents a status tracker and has attributes for name, number of rate limit errors, number of overloaded errors, number of tasks started, number of API errors, and number of other errors.\\n\\n`OpenaiRequestModel` represents a request to the OpenAI API and has attributes including model, input, messages, function, function call, temperature, top-p, n, stream, stop, max tokens, presence penalty, frequency penalty, logit bias, and user.',\n",
       " 'The code provided includes three functions for extracting values and embeddings from a Hugging Face dataset. \\n\\nThe first function, `concat_columns`, takes a dataset and merges two specified columns (title and text) into a new column called \"merged_column\". It returns the modified dataset.\\n\\nThe second function, `extract_values_and_embeddings_hf`, takes a dataset, a value column(s), and an embeddings column. It maps the `concat_columns` function to the dataset with indices, merges the specified columns, and returns a tuple with two lists: one containing the extracted values and one containing the extracted embeddings (if any).\\n\\nThe third function, `extract_values_hf`, takes a dataset and a value column(s). It checks the type of the value column and proceeds accordingly. If there is only one value column, it returns the values. If there are multiple value columns, it maps the `concat_columns` function to the dataset and returns the merged dataset. If no value column is specified, it raises a ValueError.',\n",
       " 'The code provided includes the following classes:\\n\\n1. **DirectoryProcessor**: This class is responsible for processing a directory of files. It can process individual files, run flake8 on them to check for syntax errors, and visit the abstract syntax tree (AST) of the parsed code.\\n\\n2. **GitHubUserProcessor**: This class is used to process public repositories of a specific GitHub user. It utilizes the `DirectoryProcessor` class to clone the repositories, process the files, and retrieve the function and class source codes.\\n\\n3. **GitHubRepoProcessor**: Similar to the `GitHubUserProcessor`, this class is used to process a specific repository belonging to a GitHub user. It also utilizes the `DirectoryProcessor` to clone the repository, process the files, and retrieve the function and class source codes.\\n\\nThe `DirectoryProcessor` class contains methods to process files, process a directory of files, and clone a repository. The `GitHubUserProcessor` and `GitHubRepoProcessor` classes provide methods to fetch public repositories of a user or process a specific repository, respectively, and retrieve the function and class source codes.\\n\\nOverall, this code can be used as a tool to process code files and repositories, and retrieve specific information such as function and class source codes.',\n",
       " 'This code defines a set of classes used for validating and storing real value data. \\n\\nThe `RealData` class represents a single real value and has two fields: `range` (an optional range for the value) and `value` (the actual value). It also includes a validator method to make sure the value falls within the specified range.\\n\\nThe `RealDataList` class represents a list of `RealData` objects. It has an additional field `values` to store the list of real values. It also includes a validator method to check that all values in the list fall within the specified range.\\n\\nThe `MultiDimensionalReal` class is similar to `RealDataList` but is used for multi-dimensional data. It has an additional field `values` which is a list of `RealData` objects representing values for each dimension. The class includes a validator method to ensure all values in each dimension fall within the specified range.\\n\\nSimilarly, the `MultiDimensionalRealList` class is used for storing a list of `MultiDimensionalReal` objects. It includes a validator method to check that all values in each dimension of each object in the list fall within the specified range.\\n\\nOverall, these classes provide a way to define and validate real value data and lists of such data, both for single and multi-dimensional cases.',\n",
       " 'The code contains two functions: `traverse_and_collect_rtd` and `extract_values_python`.\\n\\nThe `traverse_and_collect_rtd` function takes a directory path as input and recursively traverses the directory to collect content from .rst files. The content is stored in a list of tuples, where the filename is the 0th element and the file content is the 1st element. The function returns the list of tuples.\\n\\nThe `extract_values_python` function takes multiple parameters: `directory_path`, `minify_code`, `remove_docstrings`, and `resolution`. It uses the `PythonParser` class to process the directory specified by `directory_path`. The `minify_code` and `remove_docstrings` parameters are optional and default to False.\\n\\nDepending on the `resolution` parameter value, the function extracts different values from the parsed directory. If `resolution` is \"function\", it retrieves the function source codes and nodes. If `resolution` is \"class\", it retrieves the class source codes and nodes. If `resolution` is \"both\", it retrieves the full source codes, nodes, and filenames from the parsed directory.\\n\\nThe function then populates the `values` and `context` lists based on the extracted values. For each extracted source code and node, the source code is added to the `values` list, and a dictionary with the \"libcst tree\" key and the string representation of the node is added to the `context` list. If the resolution is \"both\", the filename is also added to the dictionary.\\n\\nFinally, the function returns the `values` and `context` lists as a tuple.',\n",
       " \"This code provides a set of classes for collecting different types of operators in Python code. \\n\\nThe classes are organized based on the type of operators they collect:\\n- UnaryOperatorCollector: Collects unary operators (+, -, ~, !).\\n- BooleanOperatorCollector: Collects boolean operators (and, or).\\n- BinaryOperatorCollector: Collects binary operators (+, -, *, /, //, %, **, <<, >>, &, |, ^).\\n- ComparisonOperatorCollector: Collects comparison operators (==, >, >=, <, <=, !=, in, is, not in, is not).\\n- AugmentedAssignmentOperatorCollector: Collects augmented assignment operators (+=, -=, *=, /=, //=, %=, **=, <<=, >>=, &=, |=, ^=).\\n- MiscellaneousOperatorCollector: Collects miscellaneous operators (=, :, ,, ., *, ;).\\n- BitInvertOperatorCollector: Collects the unary bitwise operator (~).\\n- MinusOperatorCollector: Collects the unary minus operator (-).\\n- NotOperatorCollector: Collects the unary boolean not operator (!).\\n- PlusOperatorCollector: Collects the unary plus operator (+).\\n- AndOperatorCollector: Collects the boolean and operator (and).\\n- OrOperatorCollector: Collects the boolean or operator (or).\\n- AddOperatorCollector: Collects the binary addition operator (+).\\n- BitAndOperatorCollector: Collects the binary bitwise and operator (&).\\n- BitOrOperatorCollector: Collects the binary bitwise or operator (|).\\n- BitXorOperatorCollector: Collects the binary bitwise xor operator (^).\\n- DivideOperatorCollector: Collects the binary division operator (/).\\n- FloorDivideOperatorCollector: Collects the binary floor division operator (//).\\n- LeftShiftOperatorCollector: Collects the binary left shift operator (<<).\\n- MatrixMultiplyOperatorCollector: Collects the binary matrix multiplication operator (@).\\n- ModuloOperatorCollector: Collects the binary modulo operator (%).\\n- MultiplyOperatorCollector: Collects the binary multiplication operator (*).\\n- PowerOperatorCollector: Collects the binary power operator (**).\\n- RightShiftOperatorCollector: Collects the binary right shift operator (>>).\\n- SubtractOperatorCollector: Collects the binary subtraction operator (-).\\n- EqualOperatorCollector: Collects the comparison equal operator (==).\\n- GreaterThanOperatorCollector: Collects the comparison greater than operator (>).\\n- AddAssignOperatorCollector: Collects the augmented assignment add operator (+=).\\n\\nEach collector takes a Python code string as input and collects the corresponding operators by visiting the code's Abstract Syntax Tree (AST). The `collect` method returns a list of the collected operators.\\n\\nNote that there are some duplicated classes and methods in the code, which should be cleaned up before use.\",\n",
       " 'This code defines several models for working with discrete data. \\n\\nThe `DiscreteDataInt` and `DiscreteDataStr` models represent a single discrete data value, where `DiscreteDataInt` represents an integer value and `DiscreteDataStr` represents a string value. Both models have an `alphabet` field that defines the set of allowed discrete variables, and a `value` field that stores the actual data value. \\n\\nThe `DiscreteDataList` model represents a list of discrete data values. It has an `alphabet` field that defines the set of allowed discrete variables for the entire list, and a `value` field that stores a list of `DiscreteDataInt` or `DiscreteDataStr` objects. \\n\\nThe `MultiDimensionalDiscrete` model represents a multidimensional discrete data value. It has a `value` field that stores a list of `DiscreteDataInt` or `DiscreteDataStr` objects, and a `type_dictionary` field that contains the type of each dimension in the `value` list. \\n\\nThe `MultiDimensionalDiscreteList` model represents a list of multidimensional discrete data values. It has a `values` field that stores a list of `MultiDimensionalDiscrete` objects, and a `joint_alphabet` field that defines the set of allowed discrete variable combinations. \\n\\nEach model has field validators that ensure the integrity of the data. For example, the `check_alphabet` validator checks that all elements in the `alphabet` field are of the correct type, and the `check_value` validator verifies that the `value` field is in the defined `alphabet` if it is specified. \\n\\nThese models can be used to validate and work with discrete data in a structured and consistent manner.',\n",
       " 'The code provides several functions that can be used for converting and manipulating messages in a conversational prompt. \\n\\n- `convert_mark_to_str_prompt()` takes a list of messages in the form of dictionaries, converts them into a formatted string prompt, and returns it.\\n\\n- `mark_system()` creates a dictionary for a system message, with a specified content.\\n\\n- `mark_answer()` creates a dictionary for an assistant\\'s answer, with a specified content.\\n\\n- `mark_question()` creates a dictionary for a user\\'s question, with a specified content.\\n\\n- `check_dict()` checks if a message is in the correct format and converts it to a dictionary if necessary.\\n\\n- `get_mark_from_response()` extracts the role and content from a response in a specific model format (e.g., \"gpt\") and returns it as a dictionary.\\n\\n- `get_str_from_response()` extracts the content from a response in a specific model format (e.g., \"gpt\") and returns it as a string.\\n\\n- `apply_sigmoid()` applies a sigmoid non-linearity function to the elements of a matrix.\\n\\n- `apply_threshold()` applies a threshold to the elements of a matrix, setting values above the threshold to 1 and values below or equal to the threshold to 0.',\n",
       " 'The code consists of several classes and functions that work together to generate prompts for a chatbot based on user-specified subjects and perspectives. \\n\\nThe `SubjectPerspectiveAnalyzer` class takes a chatbot as input and uses it to analyze the subject and perspective provided by the user. It generates prompts and returns a dictionary of the analyzed results.\\n\\nThe `Ideation` class retrieves ideas based on a list of queries. It uses a memory index to search for relevant concepts and generate ideas.\\n\\nThe `IdeaCluster` class clusters the generated ideas based on embeddings and creates minimum spanning paths. It also provides methods for plotting the embeddings with paths and getting clustered ideas.\\n\\nThe `Summarizer` class summarizes texts using the Cohere API.\\n\\nThe `PerspectivePromptGenerator` class uses the above classes to generate prompts for different subjects and perspectives. It uses a thread pool executor to execute the generation process concurrently.\\n\\nThe `generate_perspective_prompt` function is the main function that combines the functionalities of the above classes and generates a perspective prompt for a given subject and perspective.\\n\\nOverall, the code allows for the generation of customized chatbot prompts based on user-specified subjects and perspectives, making it easier to create engaging and context-aware conversations.',\n",
       " 'The code provided is a Python class called `GithubProcessor` that extends a class called `OsProcessor`. It is used to process a GitHub repository by cloning it, parsing the code, extracting information about issues and commits, and performing various other operations.\\n\\nThe `GithubProcessor` class has the following attributes:\\n- `base_directory`: The base directory where the cloned repositories will be stored.\\n- `username`: The GitHub username.\\n- `repo_name`: The name of the GitHub repository.\\n- `code_parsers`: A list of code parsers to use for extracting information from the code. If not specified, a default Python parser is used.\\n- `minify_code`: A boolean flag indicating whether the code should be minified.\\n- `remove_docstrings`: A boolean flag indicating whether docstrings should be removed from the code.\\n\\nThe `GithubProcessor` class has the following methods:\\n- `get_public_repos()`: Returns a list of all public repositories for the user.\\n- `clone_repo(repo_url)`: Clones the repository at the specified URL and returns the path to the cloned repository.\\n- `process_repo(repo_path=None)`: Processes the repository at the specified path. If no path is specified, the repository at `self.directory_path` is processed. Returns the list of parsed functions and classes.\\n- `process_repos()`: Processes all public repositories for the user.\\n- `get_repo(repo_name)`: Returns the repository with the specified name.\\n- `process_single_repo()`: Processes a single repository specified by `self.repo_name`.\\n- `get_issues(state=\"open\")`: Returns a list of all issues in the repository with the specified state.\\n- `parse_issues(state=\"open\")`: Parses all issues in the repository with the specified state and returns a list of dictionaries containing the issue number, title, body, and labels.\\n- `get_commits()`: Returns a list of all commits in the main branch of the repository.\\n- `parse_commits()`: Parses all commits in the main branch of the repository and returns a list of dictionaries containing the commit SHA, commit message, and author information.',\n",
       " 'This code provides three classes: ArxivVanityParser, ArxivAPI, and ArxivParser. \\n\\nThe ArxivVanityParser class is used to parse the main content of an ArXiv paper from the ArXiv Vanity website. It has methods to fetch the HTML of a paper given its ArXiv ID, extract the main content from the HTML, and parse the paper using these methods.\\n\\nThe ArxivAPI class is used to interact with the ArXiv API. It has methods to search for papers given a query and download the PDF of a paper given its key.\\n\\nThe ArxivParser class combines the functionality of the ArxivVanityParser and ArxivAPI classes. It has a method to parse multiple ArXiv papers given a query, using the ArXiv API to search for papers and the ArXiv Vanity Parser to parse the content of each paper.\\n\\nOverall, these classes provide the tools needed to search for ArXiv papers, fetch their content, and parse them for further analysis or processing.',\n",
       " 'This code defines a base class called `BaseFrame` which serves as a foundation for creating custom data frames. It includes various abstract methods and attributes that can be overridden or implemented by derived classes. The class has methods to create stratas for columns, perform stratified sampling and cross-validation, as well as other data manipulation operations. The class also initializes some default attributes such as the tokenizer, meta columns, context columns, embeddable columns, embedding columns, and save path.',\n",
       " 'The code above defines a class called MemoryChat that extends a BaseChat class. It has several methods that perform different tasks related to chat conversations and memory management. \\n\\nThe constructor method initializes the MemoryChat object and sets its attributes. \\n\\nThe count_tokens method calculates the number of tokens in a given text. \\n\\nThe identity_prompter method adds a user message to the chat thread and returns the message as a list of dictionaries. \\n\\nThe add_message_to_thread method adds a message to the chat thread. \\n\\nThe chat_response method generates a response to a given message and adds the response to the chat thread. \\n\\nThe get_conversation_history method returns the conversation history stored in the chat thread. \\n\\nThe get_last_user_message method returns the last user message in the chat thread. \\n\\nThe get_last_assistant_message method returns the last assistant message in the chat thread.',\n",
       " 'The `CodeFrame` class is a versatile class for managing and manipulating code data. It is a subclass of the `BaseFrame` class and provides additional functionality specific to code frames. \\n\\nSome key features of the `CodeFrame` class include:\\n\\n- Tokenizing code columns\\n- Applying validators to code columns\\n- Embedding code columns using a specified model\\n- Generating code for a column using a language model\\n- Searching for code based on a query using SQL or dot product similarity\\n- Saving and loading code frames\\n\\nAdditionally, the `CodeFrame` class provides methods for manipulating code, such as counting node types and operators, and replacing code in files.\\n\\nThe class can be initialized using code data in various formats - Python scripts or documentation files - and additional parameters can be specified for customizing the behavior of the class.\\n\\nPlease refer to the code comments and docstrings for more detailed information on each method and its usage.',\n",
       " 'The code provided defines two classes, `IssueParser` and `CommitParser`, which are used to retrieve and parse issues and commits from a GitHub repository using the PyGithub library.\\n\\nThe `IssueParser` class has an initializer that takes a repository name and initializes the PyGithub library and the repository object. It provides two methods: `get_issues` which returns a list of all issues in the repository with the specified state, and `parse_issues` which parses the issues and returns a list of dictionaries containing the issue number, title, body, and labels.\\n\\nThe `CommitParser` class is similar to the `IssueParser` class. It also has an initializer that takes a repository name and initializes the PyGithub library and the repository object. It provides two methods: `get_commits` which returns a list of all commits in the main branch of the repository, and `parse_commits` which parses the commits and returns a list of dictionaries containing the commit sha, commit message, and author information.',\n",
       " 'This code includes various classes and methods for counting specific elements in Python code using the Concrete Syntax Tree (CST) library. \\n\\nThe main classes include:\\n- FunctionCallCounter: Counts the number of function calls in the code.\\n- ArgumentTypeCounter: Counts the number of argument types in function and class definitions.\\n- ImportCounter: Counts the number of import statements in the code.\\n- IfStatementCounter: Counts the number of if statements in the code.\\n- BaseCompoundStatementCounter: Counts the number of compound statements (such as if-else and try-except) in the code.\\n- ForLoopCounter: Counts the number of for loops in the code.\\n- WhileLoopCounter: Counts the number of while loops in the code.\\n- TryExceptCounter: Counts the number of try-except statements in the code.\\n- WithStatementCounter: Counts the number of with statements in the code.\\n- LambdaFunctionCounter: Counts the number of lambda functions in the code.\\n- GlobalStatementCounter: Counts the number of global statements in the code.\\n- NonlocalStatementCounter: Counts the number of nonlocal statements in the code.\\n- ListComprehensionCounter: Counts the number of list comprehensions in the code.\\n- DictComprehensionCounter: Counts the number of dictionary comprehensions in the code.\\n- SetComprehensionCounter: Counts the number of set comprehensions in the code.\\n- GeneratorExpressionCounter: Counts the number of generator expressions in the code.\\n- YieldCounter: Counts the number of yield statements in the code.\\n- AwaitCounter: Counts the number of await statements in the code.\\n- ReturnCounter: Counts the number of return statements in the code.\\n- BreakCounter: Counts the number of break statements in the code.\\n- ContinueCounter: Counts the number of continue statements in the code.\\n- RaiseCounter: Counts the number of raise statements in the code.\\n- AssertCounter: Counts the number of assert statements in the code.\\n- PassCounter: Counts the number of pass statements in the code.\\n\\nFor each counter class, there is an associated `__init__()` method to initialize the counter, a `visit_<target>` method to visit the specific target element, and a `collect()` method to trigger the visiting process and return the final count.\\n\\nTo use this code, you need to create an instance of the desired counter class and pass the Python code as a string to the `collect()` method. The method will return the count of the specific element in the code.',\n",
       " 'This code defines a base class called `BDType` that extends the `BaseModel` class. The `BDType` class has several fields such as `source`, `timestamp`, `id`, `data_name`, and `elements_name`, each with their own default values and descriptions. The `timestamp` field is a datetime field that defaults to the current time if not provided. The `id` field is a UUID field that generates a unique identifier if not provided. The code also includes two field validators, `set_timestamp` and `set_id`, that set default values for the `timestamp` and `id` fields respectively. These field validators are applied to the `timestamp` and `id` fields of the `BDType` class.',\n",
       " 'This code contains several custom visitors that can be used with the Python `libcst` library to analyze and extract information from abstract syntax trees (ASTs) of Python code.\\n\\n1. `FunctionCallFinder` is a visitor that finds function calls and their arguments in the AST. It prints the function name, line number, column number, and arguments for each function call.\\n\\n2. `MultiplicationCounterVisitor` is a visitor that counts the number of binary multiplication or bitwise AND operations in the AST. It also keeps track of the functions where these operations occur.\\n\\n3. `FunctionAndClassVisitor` is a visitor that collects the source code and AST nodes of all function and class definitions in the AST.\\n\\n4. `TypingCollector` is a visitor that collects function and class annotations from the AST. It stores the annotations in a dictionary using the canonical name of the function or class as the key.\\n\\nEach visitor has its own methods to handle different AST nodes (`visit_*` and `leave_*` methods) and instance variables to store the extracted information.\\n\\nTo use these visitors, you need to create an instance of the visitor class and then visit the AST using `visit()` method of the `CSTVisitor` base class provided by `libcst`.',\n",
       " \"The `PolarsGenerator` class is a code snippet that represents a batch generator for OpenAI's GPT-3 models. It initializes with various parameters such as the input dataframe, name, tokenizer, save path, process object number, and logging level. It creates queues for requests and retries, sets API authentication, and defines methods for enqueuing objects and processing objects. The `main` method runs the batch processing loop using asyncio, and the `execute` method executes the main loop. The class also includes an `OpenaiRequest` class that represents a request to the OpenAI API, including the request type, request body, and other attributes.\",\n",
       " 'This code defines a `ChatFrame` class that inherits from a `BaseThread` class. The `ChatFrame` class provides several methods for processing and analyzing chat data.\\n\\nThe `ChatFrame` class has an initializer method that takes several parameters including `name`, `context_columns`, `embeddable_columns`, `embedding_columns`, `markdown`, `max_memory`, `tokenizer`, and `save_path`.\\n\\nThe `search_column_with_dot_product` method performs a dot product query on a specified column in the data and returns the top `k` rows based on the dot product similarity.\\n\\nThe `tokenize_column` method tokenizes a specified column in the data using a tokenizer and returns the tokenized data.\\n\\nThe `prepare_column_for_embeddings` method prepares a specified column in the data for embedding by applying a text embedding model.\\n\\nThe `embed_column` method applies a specified text embedding model to a specified column in the data and saves the output.\\n\\nThe `convert_column_to_messages` method converts a specified column in the data to a format suitable for generating text using a language model.\\n\\nThe `generate_column` method generates text content for a specified column in the data using a language model and saves the output.\\n\\nThe `pipeline` method performs a sequence of processing steps on the data including grouping by conversation ID, concatenating content, tokenizing, summarizing, embedding summaries, clustering, and generating topic labels. It returns the resulting dataframes after each step.',\n",
       " 'This code provides two classes: `PassInserter` and `CodeReplacerVisitor`. \\n\\n`PassInserter` is responsible for inserting a `pass` statement as the body of every function and class in a given Python code. It parses the code into a CST (Concrete Syntax Tree), applies the transformer to insert the `pass` statement, and then converts the transformed CST back into code.\\n\\n`CodeReplacerVisitor` is responsible for replacing a specific portion of code with another in a given Python file. It takes in the filename, the original code, and the replacing code as parameters. It loads the content of the file, replaces the original code with the replacing code, and saves the modified content back to the file. It also updates the CST to reflect the modified content.',\n",
       " 'The code defines a class called `OsProcessor` that provides various methods for manipulating files and directories. \\n\\nThe methods include:\\n- `get_all_files`: Returns a list of all files in a directory\\n- `get_files_with_extension`: Returns a list of all files in a directory with a given extension\\n- `get_file_extension`: Returns the extension of a file\\n- `get_subdirectories`: Returns a list of all subdirectories in a directory\\n- `create_directory`: Creates a directory if it does not exist\\n- `delete_directory`: Deletes a directory if it exists\\n- `copy_file`: Copies a file from one location to another\\n- `move_file`: Moves a file from one location to another\\n\\nThe class requires the `directory_path` parameter to be passed in the constructor, which is used as the default directory for the methods. Some methods also accept an optional `directory_path` parameter to override the default directory.',\n",
       " 'This code defines several classes that count the occurrences of different types of operators in Python code. The operators are divided into different classes based on their category, such as unary operators, boolean operators, binary operators, comparison operators, augmented assignment operators, and miscellaneous operators. Each class is a subclass of `cst.CSTVisitor` and overrides the `visit_<Operator>` method to count the occurrences of that operator. The `collect` method is then used to traverse the CST (Concrete Syntax Tree) of the code and collect the counts. The code uses the `cst.parse_module` function from the `pycst` library to parse the input code into the CST.',\n",
       " 'The code provides a class called `PubmedAPI` which has methods for searching and fetching information from the PubMed database. The `search` method accepts a query and returns a list of PubMed IDs. The `fetch_abstract` method retrieves the abstract for a given PubMed ID. The `fetch_pmc_full_text` method fetches the full text of an article from the PubMed Central (PMC) database using the PubMed ID. \\n\\nThere is also a class called `PubmedParser` which utilizes the `PubmedAPI` class. The `parse_papers` method in `PubmedParser` takes a query and maximum number of results as inputs. It uses the `search` method from `PubmedAPI` to get a list of PubMed IDs matching the query, and then fetches the abstract and full text for each PubMed ID using the `fetch_abstract` and `fetch_pmc_full_text` methods. The result is a list of dictionaries containing the PubMed ID, abstract, and full text for each paper. \\n\\nOverall, this code provides a way to search and retrieve information from the PubMed database using its API.',\n",
       " \"The code provides classes and methods for building a chatbot using OpenAI's GPT-3.5 model. \\n\\nThe `PromptConfiguration` class defines a configuration for the prompt, including the system and user prompts. It includes validators to ensure that the values are not None.\\n\\nThe `MessageHandler` class handles the content of the message, with a validator to ensure that the value is not None.\\n\\nThe `Prompter` class manages the prompts and provides methods for setting default prompts, composing prompts with or without conversation history, and updating the prompts.\\n\\nThe `ChatModelConfiguration` class defines a configuration for the chat model, including the model type and maximum number of output tokens.\\n\\nThe `BaseChat` class is the base class for implementing a chatbot using the GPT-3.5 model. It provides methods for initializing the chat, replying to a message, querying the chatbot, handling failures, resetting the chatbot's memory, and retrieving the chatbot's memory. It also includes a method for processing user input and updating the chat state.\\n\\nOverall, this code provides a framework for building a chatbot with customizable prompts and the ability to query the GPT-3.5 model for responses.\",\n",
       " 'The code provided consists of various functions for generating summary columns, embedding summary columns, generating topic label columns, and clustering summaries. \\n\\nThe `generate_summary_column` function takes a dataframe (`df`) and a name as input. It selects the \"concatenated_content\" column from the dataframe and creates a new column named \"model\" with the value \"gpt-3.5-turbo-16k\". Then, it applies the `create_content` function to create a new column named \"messages\" based on the \"concatenated_content\" column. The function returns the \"output\" column from the generated content.\\n\\nThe `embed_summary_column` function takes a dataframe (`df`) and a name as input. It selects the \"summary\" column from the dataframe and creates a new column named \"model\" with the value \"text-embedding-ada-002\". Then, it creates a new column named \"input\" based on the \"summary\" column. The function returns the \"output\" column from the generated content.\\n\\nThe `generate_topic_label_column` function takes a dataframe (`df`) and a name as input. It selects the \"concatenated_summary\" column from the dataframe and creates a new column named \"model\" with the value \"gpt-4\". Then, it applies the `create_content` function to create a new column named \"messages\" based on the \"concatenated_summary\" column. The function returns the \"output\" column from the generated content.\\n\\nThe `cluster_summaries` function takes an input of embeddings. It performs dimensionality reduction using UMAP and clustering using HDBSCAN. It assigns labels to the clusters and returns a new series with the cluster labels.\\n\\nThe `load_generated_content` function takes the output path of the generated content as input. It loads the content from the output file, processes it, and returns a dataframe sorted by the \"id\" column.\\n\\nThese functions can be used to generate summary columns, embed summary columns, generate topic label columns, and cluster summaries based on the input data.',\n",
       " 'This code defines an enumeration called `EmbeddableType` that represents different types of data that can be embedded. The code also includes a function called `infer_embeddable_type` that takes a column as input and infers its embeddable type based on its data type. The function returns a tuple containing the embeddable type and a corresponding callable. The code also includes a function called `numeric_embedder` that implements the embedding strategy for columns of numeric type.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df['code_summary'].to_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
