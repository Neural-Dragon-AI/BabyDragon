{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Callable, List, Optional\n",
    "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
    "from babydragon.memory.indexes.python_index import PythonIndex\n",
    "from babydragon.chat.memory_chat import FifoChat\n",
    "import babydragon\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"your key here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "babydragon_path = os.path.dirname(os.path.abspath(babydragon.__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new index\n",
      "Indexing 298 functions and 46 classes\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The value # This is the __init__.py file for the package.\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def __init__(self, system_prompt: str = None, user_prompt: str = None):\n",
       "    \"\"\"\n",
       "        Initialize the Prompter with system and user prompts.\n",
       "\n",
       "        :param system_prompt: A string representing the system prompt.\n",
       "        :param user_prompt: A string representing the user prompt.\n",
       "        \"\"\"\n",
       "    if system_prompt is None:\n",
       "        self.system_prompt = DEFAULT_SYSTEM_PROMPT\n",
       "        self.user_defined_system_prompt = False\n",
       "    else:\n",
       "        self.system_prompt = system_prompt\n",
       "        self.user_defined_system_prompt = True\n",
       "    if user_prompt is None:\n",
       "        self.user_prompt = self.default_user_prompt\n",
       "        self.user_defined_user_prompt = False\n",
       "    else:\n",
       "        self.user_prompt = user_prompt\n",
       "        self.user_defined_user_prompt = True\n",
       "\n",
       "    self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def default_user_prompt(self, message: str) -> str:\n",
       "    return DEFAULT_USER_PROMPT.format(question=message)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\n",
       "    \"\"\"\n",
       "        Compose the prompt for the chat-gpt API.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\n",
       "        \"\"\"\n",
       "    marked_question = mark_question(self.user_prompt(message))\n",
       "    prompt = [mark_system(self.system_prompt)] + [marked_question]\n",
       "    return prompt, marked_question\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def update_system_prompt(self, new_prompt: str) -> None:\n",
       "    \"\"\"\n",
       "        Update the system prompt.\n",
       "\n",
       "        :param new_prompt: A string representing the new system prompt.\n",
       "        \"\"\"\n",
       "    self.system_prompt = new_prompt\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def update_user_prompt(self, new_prompt: str) -> None:\n",
       "    \"\"\"\n",
       "        Update the user prompt.\n",
       "\n",
       "        :param new_prompt: A string representing the new user prompt.\n",
       "        \"\"\"\n",
       "    self.user_prompt = new_prompt\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def __init__(self, model: str = None, max_output_tokens: int = 200):\n",
       "    \"\"\"\n",
       "        Initialize the BaseChat with a model and max_output_tokens.\n",
       "\n",
       "        :param model: A string representing the chat model to be used.\n",
       "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
       "        \"\"\"\n",
       "    if model is None:\n",
       "        self.model = \"gpt-3.5-turbo\"\n",
       "    else:\n",
       "        self.model = model\n",
       "    self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
       "    self.max_output_tokens = max_output_tokens\n",
       "    self.failed_responses = []\n",
       "    self.outputs = []\n",
       "    self.inputs = []\n",
       "    self.prompts = []\n",
       "    self.prompt_func = self.identity_prompter\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\n",
       "    \"\"\"\n",
       "        A simple identity prompter that takes a message and returns the message marked as a question.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing the marked question and the original message.\n",
       "        \"\"\"\n",
       "    return [mark_question(message)], mark_question(message)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def chat_response(\n",
       "    self, prompt: List[dict], max_tokens: int = None\n",
       ") -> Tuple[Dict, bool]:\n",
       "    \"\"\"\n",
       "        Call the OpenAI API with the given prompt and maximum number of output tokens.\n",
       "\n",
       "        :param prompt: A list of strings representing the prompt to send to the API.\n",
       "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
       "        :return: A tuple containing the API response as a dictionary and a boolean indicating success.\n",
       "        \"\"\"\n",
       "    if max_tokens is None:\n",
       "        max_tokens = self.max_output_tokens\n",
       "    try:\n",
       "        print(\"Trying to call OpenAI API...\")\n",
       "        response = openai.ChatCompletion.create(\n",
       "            model=self.model,\n",
       "            messages=prompt,\n",
       "            max_tokens=max_tokens,\n",
       "        )\n",
       "        return response, True\n",
       "\n",
       "    except openai.error.APIError as e:\n",
       "        print(e)\n",
       "        fail_response = {\n",
       "            \"choices\": [\n",
       "                {\n",
       "                    \"message\": {\n",
       "                        \"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"\n",
       "                    }\n",
       "                }\n",
       "            ]\n",
       "        }\n",
       "        self.failed_responses.append(fail_response)\n",
       "        return fail_response, False\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def reply(self, message: str, verbose: bool = True) -> str:\n",
       "    \"\"\"\n",
       "        Reply to a given message using the chatbot.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "    return self.query(message, verbose)[\"content\"]\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def query(self, message: str, verbose: bool = True) -> str:\n",
       "    \"\"\"\n",
       "        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "\n",
       "    prompt, _ = self.prompt_func(message)\n",
       "    response, success = self.chat_response(prompt)\n",
       "    if verbose:\n",
       "        display(Markdown(\"#### Question: \\n {question}\".format(question=message)))\n",
       "    if success:\n",
       "        answer = get_mark_from_response(response)\n",
       "        self.outputs.append(answer)\n",
       "        self.inputs.append(message)\n",
       "        self.prompts.append(prompt)\n",
       "        if verbose:\n",
       "            display(\n",
       "                Markdown(\n",
       "                    \" #### Anwser: \\n {answer}\".format(\n",
       "                        answer=get_str_from_response(response)\n",
       "                    )\n",
       "                )\n",
       "            )\n",
       "        return answer\n",
       "    else:\n",
       "        raise Exception(\"OpenAI API Error inside query function\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def reset_logs(self):\n",
       "    \"\"\"\n",
       "        Reset the chatbot's memory.\n",
       "        \"\"\"\n",
       "    self.outputs = []\n",
       "    self.inputs = []\n",
       "    self.prompts = []\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_logs(self):\n",
       "    \"\"\"\n",
       "        Get the chatbot's memory.\n",
       "\n",
       "        :return: A tuple containing the chatbot's memory as three lists of strings.\n",
       "        \"\"\"\n",
       "    return self.inputs, self.outputs, self.prompts\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def run_text(\n",
       "    self, text: str, state: List[Tuple[str, str]]\n",
       ") -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\n",
       "    \"\"\"\n",
       "        Process the user's text input and update the chat state.\n",
       "\n",
       "        :param text: A string representing the user input.\n",
       "        :param state: A list of tuples representing the current chat state.\n",
       "        :return: A tuple containing the updated chat state as two lists of tuples.\n",
       "        \"\"\"\n",
       "    print(\"===============Running run_text =============\")\n",
       "    print(\"Inputs:\", text)\n",
       "    try:\n",
       "        print(\"======>Current memory:\\n %s\" % self.memory_thread)\n",
       "    except:\n",
       "        print(\"======>No memory\")\n",
       "    response = self.reply(text)\n",
       "    state = state + [(text, response)]\n",
       "    print(\"Outputs:\", state)\n",
       "    return state, state\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def gradio(self):\n",
       "    \"\"\"\n",
       "        Create and launch a Gradio interface for the chatbot.\n",
       "        \"\"\"\n",
       "    with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n",
       "        chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\n",
       "        state = gr.State([])\n",
       "        with gr.Row():\n",
       "            with gr.Column(scale=1):\n",
       "                txt = gr.Textbox(\n",
       "                    show_label=False,\n",
       "                    placeholder=\"Enter text and press enter, or upload an image\",\n",
       "                ).style(container=False)\n",
       "            with gr.Column(scale=0.15, min_width=0):\n",
       "                clear = gr.Button(\"Clear️\")\n",
       "\n",
       "        txt.submit(self.run_text, [txt, state], [chatbot, state])\n",
       "        txt.submit(lambda: \"\", None, txt)\n",
       "        demo.launch(server_name=\"localhost\", server_port=7860)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value from typing import Callable, Dict, List, Tuple\n",
       "\n",
       "import gradio as gr\n",
       "import openai\n",
       "import tiktoken\n",
       "from IPython.display import Markdown, display\n",
       "\n",
       "from babydragon.chat.prompts.default_prompts import (DEFAULT_SYSTEM_PROMPT,\n",
       "                                                     DEFAULT_USER_PROMPT)\n",
       "from babydragon.utils.oai import (get_mark_from_response,\n",
       "                                  get_str_from_response, mark_question,\n",
       "                                  mark_system)\n",
       "\n",
       "\n",
       "class Prompter:\n",
       "    \"\"\"\n",
       "    This class handles the system and user prompts and the prompt_func. By subclassing and overriding the\n",
       "    prompt_func, you can change the way the prompts are composed.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, system_prompt: str = None, user_prompt: str = None):\n",
       "        \"\"\"\n",
       "        Initialize the Prompter with system and user prompts.\n",
       "\n",
       "        :param system_prompt: A string representing the system prompt.\n",
       "        :param user_prompt: A string representing the user prompt.\n",
       "        \"\"\"\n",
       "        if system_prompt is None:\n",
       "            self.system_prompt = DEFAULT_SYSTEM_PROMPT\n",
       "            self.user_defined_system_prompt = False\n",
       "        else:\n",
       "            self.system_prompt = system_prompt\n",
       "            self.user_defined_system_prompt = True\n",
       "        if user_prompt is None:\n",
       "            self.user_prompt = self.default_user_prompt\n",
       "            self.user_defined_user_prompt = False\n",
       "        else:\n",
       "            self.user_prompt = user_prompt\n",
       "            self.user_defined_user_prompt = True\n",
       "\n",
       "        self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\n",
       "\n",
       "    def default_user_prompt(self, message: str) -> str:\n",
       "        return DEFAULT_USER_PROMPT.format(question=message)\n",
       "\n",
       "    def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\n",
       "        \"\"\"\n",
       "        Compose the prompt for the chat-gpt API.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\n",
       "        \"\"\"\n",
       "        marked_question = mark_question(self.user_prompt(message))\n",
       "        prompt = [mark_system(self.system_prompt)] + [marked_question]\n",
       "        return prompt, marked_question\n",
       "\n",
       "    def update_system_prompt(self, new_prompt: str) -> None:\n",
       "        \"\"\"\n",
       "        Update the system prompt.\n",
       "\n",
       "        :param new_prompt: A string representing the new system prompt.\n",
       "        \"\"\"\n",
       "        self.system_prompt = new_prompt\n",
       "\n",
       "    def update_user_prompt(self, new_prompt: str) -> None:\n",
       "        \"\"\"\n",
       "        Update the user prompt.\n",
       "\n",
       "        :param new_prompt: A string representing the new user prompt.\n",
       "        \"\"\"\n",
       "        self.user_prompt = new_prompt\n",
       "\n",
       "\n",
       "class BaseChat:\n",
       "    \"\"\"\n",
       "    This is the base class for chatbots, defining the basic functions that a chatbot should have, mainly the calls to\n",
       "    chat-gpt API, and a basic Gradio interface. It has a prompt_func that acts as a placeholder for a call to chat-gpt\n",
       "    API without any additional messages. It can be overridden by subclasses to add additional messages to the prompt.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, model: str = None, max_output_tokens: int = 200):\n",
       "        \"\"\"\n",
       "        Initialize the BaseChat with a model and max_output_tokens.\n",
       "\n",
       "        :param model: A string representing the chat model to be used.\n",
       "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
       "        \"\"\"\n",
       "        if model is None:\n",
       "            self.model = \"gpt-3.5-turbo\"\n",
       "        else:\n",
       "            self.model = model\n",
       "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
       "        self.max_output_tokens = max_output_tokens\n",
       "        self.failed_responses = []\n",
       "        self.outputs = []\n",
       "        self.inputs = []\n",
       "        self.prompts = []\n",
       "        self.prompt_func = self.identity_prompter\n",
       "\n",
       "    def identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\n",
       "        \"\"\"\n",
       "        A simple identity prompter that takes a message and returns the message marked as a question.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing the marked question and the original message.\n",
       "        \"\"\"\n",
       "        return [mark_question(message)], mark_question(message)\n",
       "\n",
       "    def chat_response(\n",
       "        self, prompt: List[dict], max_tokens: int = None\n",
       "    ) -> Tuple[Dict, bool]:\n",
       "        \"\"\"\n",
       "        Call the OpenAI API with the given prompt and maximum number of output tokens.\n",
       "\n",
       "        :param prompt: A list of strings representing the prompt to send to the API.\n",
       "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
       "        :return: A tuple containing the API response as a dictionary and a boolean indicating success.\n",
       "        \"\"\"\n",
       "        if max_tokens is None:\n",
       "            max_tokens = self.max_output_tokens\n",
       "        try:\n",
       "            print(\"Trying to call OpenAI API...\")\n",
       "            response = openai.ChatCompletion.create(\n",
       "                model=self.model,\n",
       "                messages=prompt,\n",
       "                max_tokens=max_tokens,\n",
       "            )\n",
       "            return response, True\n",
       "\n",
       "        except openai.error.APIError as e:\n",
       "            print(e)\n",
       "            fail_response = {\n",
       "                \"choices\": [\n",
       "                    {\n",
       "                        \"message\": {\n",
       "                            \"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"\n",
       "                        }\n",
       "                    }\n",
       "                ]\n",
       "            }\n",
       "            self.failed_responses.append(fail_response)\n",
       "            return fail_response, False\n",
       "\n",
       "    def reply(self, message: str, verbose: bool = True) -> str:\n",
       "        \"\"\"\n",
       "        Reply to a given message using the chatbot.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "        return self.query(message, verbose)[\"content\"]\n",
       "\n",
       "    def query(self, message: str, verbose: bool = True) -> str:\n",
       "        \"\"\"\n",
       "        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "\n",
       "        prompt, _ = self.prompt_func(message)\n",
       "        response, success = self.chat_response(prompt)\n",
       "        if verbose:\n",
       "            display(Markdown(\"#### Question: \\n {question}\".format(question=message)))\n",
       "        if success:\n",
       "            answer = get_mark_from_response(response)\n",
       "            self.outputs.append(answer)\n",
       "            self.inputs.append(message)\n",
       "            self.prompts.append(prompt)\n",
       "            if verbose:\n",
       "                display(\n",
       "                    Markdown(\n",
       "                        \" #### Anwser: \\n {answer}\".format(\n",
       "                            answer=get_str_from_response(response)\n",
       "                        )\n",
       "                    )\n",
       "                )\n",
       "            return answer\n",
       "        else:\n",
       "            raise Exception(\"OpenAI API Error inside query function\")\n",
       "\n",
       "    def reset_logs(self):\n",
       "        \"\"\"\n",
       "        Reset the chatbot's memory.\n",
       "        \"\"\"\n",
       "        self.outputs = []\n",
       "        self.inputs = []\n",
       "        self.prompts = []\n",
       "\n",
       "    def get_logs(self):\n",
       "        \"\"\"\n",
       "        Get the chatbot's memory.\n",
       "\n",
       "        :return: A tuple containing the chatbot's memory as three lists of strings.\n",
       "        \"\"\"\n",
       "        return self.inputs, self.outputs, self.prompts\n",
       "\n",
       "    def run_text(\n",
       "        self, text: str, state: List[Tuple[str, str]]\n",
       "    ) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\n",
       "        \"\"\"\n",
       "        Process the user's text input and update the chat state.\n",
       "\n",
       "        :param text: A string representing the user input.\n",
       "        :param state: A list of tuples representing the current chat state.\n",
       "        :return: A tuple containing the updated chat state as two lists of tuples.\n",
       "        \"\"\"\n",
       "        print(\"===============Running run_text =============\")\n",
       "        print(\"Inputs:\", text)\n",
       "        try:\n",
       "            print(\"======>Current memory:\\n %s\" % self.memory_thread)\n",
       "        except:\n",
       "            print(\"======>No memory\")\n",
       "        response = self.reply(text)\n",
       "        state = state + [(text, response)]\n",
       "        print(\"Outputs:\", state)\n",
       "        return state, state\n",
       "\n",
       "    def gradio(self):\n",
       "        \"\"\"\n",
       "        Create and launch a Gradio interface for the chatbot.\n",
       "        \"\"\"\n",
       "        with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n",
       "            chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\n",
       "            state = gr.State([])\n",
       "            with gr.Row():\n",
       "                with gr.Column(scale=1):\n",
       "                    txt = gr.Textbox(\n",
       "                        show_label=False,\n",
       "                        placeholder=\"Enter text and press enter, or upload an image\",\n",
       "                    ).style(container=False)\n",
       "                with gr.Column(scale=0.15, min_width=0):\n",
       "                    clear = gr.Button(\"Clear️\")\n",
       "\n",
       "            txt.submit(self.run_text, [txt, state], [chatbot, state])\n",
       "            txt.submit(lambda: \"\", None, txt)\n",
       "            demo.launch(server_name=\"localhost\", server_port=7860)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def __init__(\n",
       "    self,\n",
       "    model: str = None,\n",
       "    max_output_tokens: int = 1000,\n",
       "    system_prompt: str = None,\n",
       "    user_prompt: str = None,\n",
       "    index_dict: Optional[Dict[str,MemoryIndex]] = None,\n",
       "    max_index_memory: int = 1000,\n",
       ") -> None:\n",
       "    BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\n",
       "    Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\n",
       "    self.index_dict = index_dict\n",
       "    self.setup_indices(max_index_memory)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def setup_indices(self, max_index_memory):\n",
       "    \"\"\"setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index.\"\"\"\n",
       "    if self.index_dict is not None:\n",
       "        self.current_index = list(self.index_dict.keys())[0]\n",
       "        self.system_prompt = (\n",
       "            INDEX_SYSTEM_PROMPT\n",
       "            if self.user_defined_system_prompt is False\n",
       "            else self.system_prompt\n",
       "        )\n",
       "        self.user_prompt = (\n",
       "            self.get_index_hints\n",
       "            if self.user_defined_user_prompt is False\n",
       "            else self.user_prompt\n",
       "        )\n",
       "    self.max_index_memory = max_index_memory\n",
       "    #set the last index to be the current index\n",
       "    self.current_index  = list(self.index_dict.keys())[-1]\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_index_hints(\n",
       "    self, question: str, k: int = 10, max_tokens: int = None\n",
       ") -> str:\n",
       "    \"\"\"\n",
       "        Get hints from the current index for the given question.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include from the index.\n",
       "        :param max_tokens: The maximum number of tokens to be retrieved from the index.\n",
       "        :return: A string representing the hint prompt with the question.\n",
       "        \"\"\"\n",
       "    if max_tokens is None:\n",
       "        max_tokens = self.max_index_memory\n",
       "    hints = []\n",
       "    if self.current_index is not None:\n",
       "        index_instance = self.index_dict[self.current_index]\n",
       "        if isinstance(index_instance, MemoryIndex):\n",
       "            hints, _, _ = index_instance.token_bound_query(\n",
       "                question, k=k, max_tokens=max_tokens\n",
       "            )\n",
       "        else:\n",
       "            raise ValueError(\"The current index is not a valid index instance.\")\n",
       "        hints_string = \"\\n\".join(hints)\n",
       "        hint_prompt = INDEX_HINT_PROMPT\n",
       "        question_intro = QUESTION_INTRO\n",
       "        return hint_prompt.format(\n",
       "            hints_string=hints_string\n",
       "        ) + question_intro.format(question=question)\n",
       "    else:\n",
       "        return question\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def set_current_index(self, index_name: Optional[str]) -> None:\n",
       "    \"\"\"\n",
       "        Set the current index to be used for hints.\n",
       "\n",
       "        :param index_name: A string representing the index name or None to clear the current index.\n",
       "        :raise ValueError: If the provided index name is not available.\n",
       "        \"\"\"\n",
       "    if self.index_dict is None:\n",
       "        raise ValueError(\"No index_dict are available.\")\n",
       "    elif index_name in self.index_dict:\n",
       "        self.current_index = index_name\n",
       "    elif index_name is None:\n",
       "        self.current_index = None\n",
       "    else:\n",
       "        raise ValueError(\"The provided index name is not available.\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value from typing import Dict, Optional, Union\n",
       "\n",
       "from babydragon.chat.base_chat import BaseChat, Prompter\n",
       "from babydragon.chat.prompts.default_prompts import (INDEX_HINT_PROMPT,\n",
       "                                                     INDEX_SYSTEM_PROMPT,\n",
       "                                                     QUESTION_INTRO)\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "\n",
       "\n",
       "class Chat(BaseChat, Prompter):\n",
       "    \"\"\"\n",
       "    This class combines the BaseChat and Prompter classes to create a oneshot chatbot with a system and user prompt,\n",
       "    and the ability to handle multiple index_dict.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        model: str = None,\n",
       "        max_output_tokens: int = 1000,\n",
       "        system_prompt: str = None,\n",
       "        user_prompt: str = None,\n",
       "        index_dict: Optional[Dict[str,MemoryIndex]] = None,\n",
       "        max_index_memory: int = 1000,\n",
       "    ) -> None:\n",
       "        BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\n",
       "        Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\n",
       "        self.index_dict = index_dict\n",
       "        self.setup_indices(max_index_memory)\n",
       "\n",
       "    def setup_indices(self, max_index_memory):\n",
       "        \"\"\"setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index.\"\"\"\n",
       "        if self.index_dict is not None:\n",
       "            self.current_index = list(self.index_dict.keys())[0]\n",
       "            self.system_prompt = (\n",
       "                INDEX_SYSTEM_PROMPT\n",
       "                if self.user_defined_system_prompt is False\n",
       "                else self.system_prompt\n",
       "            )\n",
       "            self.user_prompt = (\n",
       "                self.get_index_hints\n",
       "                if self.user_defined_user_prompt is False\n",
       "                else self.user_prompt\n",
       "            )\n",
       "        self.max_index_memory = max_index_memory\n",
       "        #set the last index to be the current index\n",
       "        self.current_index  = list(self.index_dict.keys())[-1]\n",
       "\n",
       "    def get_index_hints(\n",
       "        self, question: str, k: int = 10, max_tokens: int = None\n",
       "    ) -> str:\n",
       "        \"\"\"\n",
       "        Get hints from the current index for the given question.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include from the index.\n",
       "        :param max_tokens: The maximum number of tokens to be retrieved from the index.\n",
       "        :return: A string representing the hint prompt with the question.\n",
       "        \"\"\"\n",
       "        if max_tokens is None:\n",
       "            max_tokens = self.max_index_memory\n",
       "        hints = []\n",
       "        if self.current_index is not None:\n",
       "            index_instance = self.index_dict[self.current_index]\n",
       "            if isinstance(index_instance, MemoryIndex):\n",
       "                hints, _, _ = index_instance.token_bound_query(\n",
       "                    question, k=k, max_tokens=max_tokens\n",
       "                )\n",
       "            else:\n",
       "                raise ValueError(\"The current index is not a valid index instance.\")\n",
       "            hints_string = \"\\n\".join(hints)\n",
       "            hint_prompt = INDEX_HINT_PROMPT\n",
       "            question_intro = QUESTION_INTRO\n",
       "            return hint_prompt.format(\n",
       "                hints_string=hints_string\n",
       "            ) + question_intro.format(question=question)\n",
       "        else:\n",
       "            return question\n",
       "\n",
       "    def set_current_index(self, index_name: Optional[str]) -> None:\n",
       "        \"\"\"\n",
       "        Set the current index to be used for hints.\n",
       "\n",
       "        :param index_name: A string representing the index name or None to clear the current index.\n",
       "        :raise ValueError: If the provided index name is not available.\n",
       "        \"\"\"\n",
       "        if self.index_dict is None:\n",
       "            raise ValueError(\"No index_dict are available.\")\n",
       "        elif index_name in self.index_dict:\n",
       "            self.current_index = index_name\n",
       "        elif index_name is None:\n",
       "            self.current_index = None\n",
       "        else:\n",
       "            raise ValueError(\"The provided index name is not available.\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def __init__(\n",
       "    self,\n",
       "    model: Optional[str] = None,\n",
       "    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
       "    system_prompt: Optional[str] = None,\n",
       "    user_prompt: Optional[str] = None,\n",
       "    name: str = \"fifo_memory\",\n",
       "    max_index_memory: int = 400,\n",
       "    max_fifo_memory: int = 2048,\n",
       "    max_output_tokens: int = 1000,\n",
       "    longterm_thread: Optional[BaseThread] = None,\n",
       "):\n",
       "\n",
       "    FifoThread.__init__(\n",
       "        self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread\n",
       "    )\n",
       "    Chat.__init__(\n",
       "        self,\n",
       "        model=model,\n",
       "        index_dict=index_dict,\n",
       "        max_output_tokens=max_output_tokens,\n",
       "        max_index_memory=max_index_memory,\n",
       "        system_prompt=system_prompt,\n",
       "        user_prompt=user_prompt,\n",
       "    )\n",
       "\n",
       "    self.prompt_func = self.fifo_memory_prompt\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\n",
       "    \"\"\"\n",
       "        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "    marked_question = mark_question(self.user_prompt(message))\n",
       "    prompt = (\n",
       "        [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\n",
       "    )\n",
       "    return prompt, marked_question\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def query(self, question: str, verbose: bool = True) -> str:\n",
       "    \"\"\"\n",
       "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
       "        and added to the memory.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "    # First call the base class's query method\n",
       "    answer = BaseChat.query(self, message=question, verbose=verbose)\n",
       "    marked_question = mark_question(question)\n",
       "    # Add the marked question and answer to the memory\n",
       "    self.add_message(marked_question)\n",
       "    self.add_message(answer)\n",
       "\n",
       "    return answer\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def __init__(\n",
       "    self,\n",
       "    model: Optional[str] = None,\n",
       "    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
       "    name: str = \"vector_memory\",\n",
       "    max_index_memory: int = 400,\n",
       "    max_vector_memory: int = 2048,\n",
       "    max_output_tokens: int = 1000,\n",
       "    system_prompt: str = None,\n",
       "    user_prompt: str = None,\n",
       "):\n",
       "    VectorThread.__init__(self, name=name, max_context=max_vector_memory)\n",
       "    Chat.__init__(\n",
       "        self,\n",
       "        model=model,\n",
       "        index_dict=index_dict,\n",
       "        max_output_tokens=max_output_tokens,\n",
       "        max_index_memory=max_index_memory,\n",
       "        system_prompt=system_prompt,\n",
       "        user_prompt=user_prompt,\n",
       "    )\n",
       "    self.max_vector_memory = self.max_context\n",
       "    self.prompt_func = self.vector_memory_prompt\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def vector_memory_prompt(\n",
       "    self, message: str, k: int = 10\n",
       ") -> Tuple[List[dict], dict]:\n",
       "    \"\"\"\n",
       "        Combine system prompt, k most similar messages to the question, and the user prompt.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include in the prompt.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "    sorted_messages, sorted_scores, sorted_indices = self.sorted_query(\n",
       "        message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
       "    )\n",
       "    marked_question = mark_question(self.user_prompt(message))\n",
       "    prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\n",
       "    return prompt, marked_question\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def weighted_memory_prompt(\n",
       "    self,\n",
       "    message: str,\n",
       "    k: int = 10,\n",
       "    decay_factor: float = 0.1,\n",
       "    temporal_weight: float = 0.5,\n",
       ") -> Tuple[List[dict], dict]:\n",
       "    \"\"\"\n",
       "        Combine system prompt, weighted k most similar messages to the question, and the user prompt.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include in the prompt.\n",
       "        :param decay_factor: A float representing the decay factor for weighting.\n",
       "        :param temporal_weight: A float representing the weight of the temporal aspect.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "    weighted_messages, weighted_scores, weighted_indices = self.weighted_query(\n",
       "        message,\n",
       "        k=k,\n",
       "        max_tokens=self.max_vector_memory,\n",
       "        decay_factor=decay_factor,\n",
       "        temporal_weight=temporal_weight,\n",
       "        order_by=\"chronological\",\n",
       "        reverse=True,\n",
       "    )\n",
       "    marked_question = mark_question(self.user_prompt(message))\n",
       "    prompt = (\n",
       "        [mark_system(self.system_prompt)] + weighted_messages + [marked_question]\n",
       "    )\n",
       "    return prompt, marked_question\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def query(self, question: str, verbose: bool = False) -> str:\n",
       "    \"\"\"\n",
       "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
       "        and added to the memory.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "    # First call the base class's query method\n",
       "    answer = BaseChat.query(self, message=question, verbose=verbose)\n",
       "    marked_question = mark_question(question)\n",
       "    # Add the marked question and answer to the memory\n",
       "    self.add_message(marked_question)\n",
       "    self.add_message(answer)\n",
       "    return answer\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def __init__(\n",
       "    self,\n",
       "    model: str = None,\n",
       "    index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
       "    system_prompt: str = None,\n",
       "    user_prompt: str = None,\n",
       "    name: str = \"fifo_vector_memory\",\n",
       "    max_memory: int = 2048,\n",
       "    max_index_memory: int = 400,\n",
       "    max_output_tokens: int = 1000,\n",
       "    longterm_thread: Optional[VectorThread] = None,\n",
       "    longterm_frac: float = 0.5,\n",
       "):\n",
       "    self.total_max_memory = max_memory\n",
       "\n",
       "    self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n",
       "    FifoThread.__init__(\n",
       "        self,\n",
       "        name=name,\n",
       "        max_memory=self.max_fifo_memory,\n",
       "        longterm_thread=self.longterm_thread,\n",
       "    )\n",
       "    Chat.__init__(\n",
       "        self,\n",
       "        model=model,\n",
       "        index_dict=index_dict,\n",
       "        max_output_tokens=max_output_tokens,\n",
       "        max_index_memory=max_index_memory,\n",
       "        system_prompt=system_prompt,\n",
       "        user_prompt=user_prompt,\n",
       "    )\n",
       "    self.prompt_func = self.fifovector_memory_prompt\n",
       "    self.prompt_list = []\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def setup_longterm_memory(\n",
       "    self,\n",
       "    longterm_thread: Optional[VectorThread],\n",
       "    max_memory: int,\n",
       "    longterm_frac: float,\n",
       "):\n",
       "    \"\"\"\n",
       "        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n",
       "\n",
       "        :param longterm_thread: An optional VectorThread for long-term memory.\n",
       "        :param max_memory: The maximum amount of memory for the chatbot.\n",
       "        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n",
       "        \"\"\"\n",
       "    if longterm_thread is None:\n",
       "        self.longterm_frac = longterm_frac\n",
       "        self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n",
       "        self.max_vector_memory = max_memory - self.max_fifo_memory\n",
       "        self.longterm_thread = VectorThread(\n",
       "            name=\"longterm_memory\", max_context=self.max_vector_memory\n",
       "        )\n",
       "    else:\n",
       "        self.longterm_thread = longterm_thread\n",
       "        self.max_vector_memory = self.longterm_thread.max_context\n",
       "        self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n",
       "        self.longterm_frac = self.max_vector_memory / self.total_max_memory\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def fifovector_memory_prompt(\n",
       "    self, message: str, k: int = 10\n",
       ") -> Tuple[List[dict], dict]:\n",
       "    \"\"\"\n",
       "        Combine the system prompt, long-term memory (vector memory), short-term memory (FIFO memory), and the user prompt.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include from the long-term memory.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "    prompt = [mark_system(self.system_prompt)]\n",
       "    if (\n",
       "        len(self.longterm_thread.memory_thread) > 0\n",
       "        and self.longterm_thread.total_tokens <= self.max_vector_memory\n",
       "    ):\n",
       "        prompt += self.longterm_thread.memory_thread\n",
       "    elif (\n",
       "        len(self.longterm_thread.memory_thread) > 0\n",
       "        and self.longterm_thread.total_tokens > self.max_vector_memory\n",
       "    ):\n",
       "        (\n",
       "            sorted_messages,\n",
       "            sorted_scores,\n",
       "            sorted_indices,\n",
       "        ) = self.longterm_thread.sorted_query(\n",
       "            message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
       "        )\n",
       "        prompt += sorted_messages\n",
       "\n",
       "    prompt += self.memory_thread\n",
       "    marked_question = mark_question(self.user_prompt(message))\n",
       "    prompt += [marked_question]\n",
       "    return prompt, marked_question\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def query(self, question: str, verbose: bool = False) -> str:\n",
       "    \"\"\"\n",
       "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
       "        and added to the memory.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "    answer = BaseChat.query(self, message=question, verbose=verbose)\n",
       "    marked_question = mark_question(question)\n",
       "    self.add_message(marked_question)\n",
       "    self.add_message(answer)\n",
       "    return answer\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value from typing import Dict, List, Optional, Tuple, Union\n",
       "\n",
       "from babydragon.chat.chat import BaseChat, Chat, Prompter\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "from babydragon.memory.indexes.pandas_index import PandasIndex\n",
       "from babydragon.memory.threads.base_thread import BaseThread\n",
       "from babydragon.memory.threads.fifo_thread import FifoThread\n",
       "from babydragon.memory.threads.vector_thread import VectorThread\n",
       "from babydragon.utils.oai import mark_answer, mark_question, mark_system\n",
       "\n",
       "\n",
       "class FifoChat(FifoThread, Chat):\n",
       "    \"\"\"\n",
       "    A chatbot class that combines FIFO Memory Thread, BaseChat, and Prompter. The oldest messages are removed first\n",
       "    when reaching the max_memory limit. The memory is defined in terms of tokens, and outs are passed to the\n",
       "    longterm_memory. The lucid_memory is a redundant memory that stores all the messages.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        model: Optional[str] = None,\n",
       "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
       "        system_prompt: Optional[str] = None,\n",
       "        user_prompt: Optional[str] = None,\n",
       "        name: str = \"fifo_memory\",\n",
       "        max_index_memory: int = 400,\n",
       "        max_fifo_memory: int = 2048,\n",
       "        max_output_tokens: int = 1000,\n",
       "        longterm_thread: Optional[BaseThread] = None,\n",
       "    ):\n",
       "\n",
       "        FifoThread.__init__(\n",
       "            self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread\n",
       "        )\n",
       "        Chat.__init__(\n",
       "            self,\n",
       "            model=model,\n",
       "            index_dict=index_dict,\n",
       "            max_output_tokens=max_output_tokens,\n",
       "            max_index_memory=max_index_memory,\n",
       "            system_prompt=system_prompt,\n",
       "            user_prompt=user_prompt,\n",
       "        )\n",
       "\n",
       "        self.prompt_func = self.fifo_memory_prompt\n",
       "\n",
       "    def fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\n",
       "        \"\"\"\n",
       "        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "        marked_question = mark_question(self.user_prompt(message))\n",
       "        prompt = (\n",
       "            [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\n",
       "        )\n",
       "        return prompt, marked_question\n",
       "\n",
       "    def query(self, question: str, verbose: bool = True) -> str:\n",
       "        \"\"\"\n",
       "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
       "        and added to the memory.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "        # First call the base class's query method\n",
       "        answer = BaseChat.query(self, message=question, verbose=verbose)\n",
       "        marked_question = mark_question(question)\n",
       "        # Add the marked question and answer to the memory\n",
       "        self.add_message(marked_question)\n",
       "        self.add_message(answer)\n",
       "\n",
       "        return answer\n",
       "\n",
       "\n",
       "class VectorChat(VectorThread, Chat):\n",
       "    \"\"\"\n",
       "    A chatbot class that combines Vector Memory Thread, BaseChat, and Prompter. Memory prompt is constructed by\n",
       "    filling the memory with the k most similar messages to the question until the max prompt memory tokens are reached.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        model: Optional[str] = None,\n",
       "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
       "        name: str = \"vector_memory\",\n",
       "        max_index_memory: int = 400,\n",
       "        max_vector_memory: int = 2048,\n",
       "        max_output_tokens: int = 1000,\n",
       "        system_prompt: str = None,\n",
       "        user_prompt: str = None,\n",
       "    ):\n",
       "        VectorThread.__init__(self, name=name, max_context=max_vector_memory)\n",
       "        Chat.__init__(\n",
       "            self,\n",
       "            model=model,\n",
       "            index_dict=index_dict,\n",
       "            max_output_tokens=max_output_tokens,\n",
       "            max_index_memory=max_index_memory,\n",
       "            system_prompt=system_prompt,\n",
       "            user_prompt=user_prompt,\n",
       "        )\n",
       "        self.max_vector_memory = self.max_context\n",
       "        self.prompt_func = self.vector_memory_prompt\n",
       "\n",
       "    def vector_memory_prompt(\n",
       "        self, message: str, k: int = 10\n",
       "    ) -> Tuple[List[dict], dict]:\n",
       "        \"\"\"\n",
       "        Combine system prompt, k most similar messages to the question, and the user prompt.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include in the prompt.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "        sorted_messages, sorted_scores, sorted_indices = self.sorted_query(\n",
       "            message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
       "        )\n",
       "        marked_question = mark_question(self.user_prompt(message))\n",
       "        prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\n",
       "        return prompt, marked_question\n",
       "\n",
       "    def weighted_memory_prompt(\n",
       "        self,\n",
       "        message: str,\n",
       "        k: int = 10,\n",
       "        decay_factor: float = 0.1,\n",
       "        temporal_weight: float = 0.5,\n",
       "    ) -> Tuple[List[dict], dict]:\n",
       "        \"\"\"\n",
       "        Combine system prompt, weighted k most similar messages to the question, and the user prompt.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include in the prompt.\n",
       "        :param decay_factor: A float representing the decay factor for weighting.\n",
       "        :param temporal_weight: A float representing the weight of the temporal aspect.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "        weighted_messages, weighted_scores, weighted_indices = self.weighted_query(\n",
       "            message,\n",
       "            k=k,\n",
       "            max_tokens=self.max_vector_memory,\n",
       "            decay_factor=decay_factor,\n",
       "            temporal_weight=temporal_weight,\n",
       "            order_by=\"chronological\",\n",
       "            reverse=True,\n",
       "        )\n",
       "        marked_question = mark_question(self.user_prompt(message))\n",
       "        prompt = (\n",
       "            [mark_system(self.system_prompt)] + weighted_messages + [marked_question]\n",
       "        )\n",
       "        return prompt, marked_question\n",
       "\n",
       "    def query(self, question: str, verbose: bool = False) -> str:\n",
       "        \"\"\"\n",
       "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
       "        and added to the memory.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "        # First call the base class's query method\n",
       "        answer = BaseChat.query(self, message=question, verbose=verbose)\n",
       "        marked_question = mark_question(question)\n",
       "        # Add the marked question and answer to the memory\n",
       "        self.add_message(marked_question)\n",
       "        self.add_message(answer)\n",
       "        return answer\n",
       "\n",
       "\n",
       "class FifoVectorChat(FifoThread, Chat):\n",
       "    \"\"\"\n",
       "    A chatbot class that combines FIFO Memory Thread, Vector Memory Thread, BaseChat, and Prompter.\n",
       "    The memory prompt is constructed by including both FIFO memory and Vector memory.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        model: str = None,\n",
       "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
       "        system_prompt: str = None,\n",
       "        user_prompt: str = None,\n",
       "        name: str = \"fifo_vector_memory\",\n",
       "        max_memory: int = 2048,\n",
       "        max_index_memory: int = 400,\n",
       "        max_output_tokens: int = 1000,\n",
       "        longterm_thread: Optional[VectorThread] = None,\n",
       "        longterm_frac: float = 0.5,\n",
       "    ):\n",
       "        self.total_max_memory = max_memory\n",
       "\n",
       "        self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n",
       "        FifoThread.__init__(\n",
       "            self,\n",
       "            name=name,\n",
       "            max_memory=self.max_fifo_memory,\n",
       "            longterm_thread=self.longterm_thread,\n",
       "        )\n",
       "        Chat.__init__(\n",
       "            self,\n",
       "            model=model,\n",
       "            index_dict=index_dict,\n",
       "            max_output_tokens=max_output_tokens,\n",
       "            max_index_memory=max_index_memory,\n",
       "            system_prompt=system_prompt,\n",
       "            user_prompt=user_prompt,\n",
       "        )\n",
       "        self.prompt_func = self.fifovector_memory_prompt\n",
       "        self.prompt_list = []\n",
       "\n",
       "    def setup_longterm_memory(\n",
       "        self,\n",
       "        longterm_thread: Optional[VectorThread],\n",
       "        max_memory: int,\n",
       "        longterm_frac: float,\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n",
       "\n",
       "        :param longterm_thread: An optional VectorThread for long-term memory.\n",
       "        :param max_memory: The maximum amount of memory for the chatbot.\n",
       "        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n",
       "        \"\"\"\n",
       "        if longterm_thread is None:\n",
       "            self.longterm_frac = longterm_frac\n",
       "            self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n",
       "            self.max_vector_memory = max_memory - self.max_fifo_memory\n",
       "            self.longterm_thread = VectorThread(\n",
       "                name=\"longterm_memory\", max_context=self.max_vector_memory\n",
       "            )\n",
       "        else:\n",
       "            self.longterm_thread = longterm_thread\n",
       "            self.max_vector_memory = self.longterm_thread.max_context\n",
       "            self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n",
       "            self.longterm_frac = self.max_vector_memory / self.total_max_memory\n",
       "\n",
       "    def fifovector_memory_prompt(\n",
       "        self, message: str, k: int = 10\n",
       "    ) -> Tuple[List[dict], dict]:\n",
       "        \"\"\"\n",
       "        Combine the system prompt, long-term memory (vector memory), short-term memory (FIFO memory), and the user prompt.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include from the long-term memory.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "        prompt = [mark_system(self.system_prompt)]\n",
       "        if (\n",
       "            len(self.longterm_thread.memory_thread) > 0\n",
       "            and self.longterm_thread.total_tokens <= self.max_vector_memory\n",
       "        ):\n",
       "            prompt += self.longterm_thread.memory_thread\n",
       "        elif (\n",
       "            len(self.longterm_thread.memory_thread) > 0\n",
       "            and self.longterm_thread.total_tokens > self.max_vector_memory\n",
       "        ):\n",
       "            (\n",
       "                sorted_messages,\n",
       "                sorted_scores,\n",
       "                sorted_indices,\n",
       "            ) = self.longterm_thread.sorted_query(\n",
       "                message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
       "            )\n",
       "            prompt += sorted_messages\n",
       "\n",
       "        prompt += self.memory_thread\n",
       "        marked_question = mark_question(self.user_prompt(message))\n",
       "        prompt += [marked_question]\n",
       "        return prompt, marked_question\n",
       "\n",
       "    def query(self, question: str, verbose: bool = False) -> str:\n",
       "        \"\"\"\n",
       "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
       "        and added to the memory.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "        answer = BaseChat.query(self, message=question, verbose=verbose)\n",
       "        marked_question = mark_question(question)\n",
       "        self.add_message(marked_question)\n",
       "        self.add_message(answer)\n",
       "        return answer\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value # This is the __init__.py file for the package.\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value DEFAULT_SYSTEM_PROMPT = \"You are a useful Assistant you role is to answer questions in an exhaustive way! Please be helpful to the user he loves you!\"\n",
       "\n",
       "DEFAULT_USER_PROMPT = \"{question}\"\n",
       "\n",
       "index_description = \"This index is help\"\n",
       "\n",
       "INDEX_SYSTEM_PROMPT = \"\"\"You are a Chatbot assistant that can use a external knowledge base to answer questions.\n",
       "The user will always add hints from the external knowledge base.\n",
       "You express your thoughts using princpled reasoning and always pay attention to the\n",
       "hints.  Your knowledge base description is:\"\"\"\n",
       "# system_prompt = system_prompt.format(index_descrpiton = index_description)\n",
       "\n",
       "INDEX_HINT_PROMPT = \"\"\"I am going to ask you a question and you should use the hints to answer it. The hints are:\\n{hints_string} .\n",
       "            Remember that I can not see the hints, but please reply as if the knowledge base was part of your training set. \"\"\"\n",
       "\n",
       "\n",
       "QUESTION_INTRO = \"The question is: {question}\"\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value # This is the __init__.py file for the package.\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def __init__(\n",
       "    self,\n",
       "    index: Optional[faiss.Index] = None,\n",
       "    values: Optional[List[str]] = None,\n",
       "    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
       "    name: str = \"memory_index\",\n",
       "    save_path: Optional[str] = None,\n",
       "    load: bool = False,\n",
       "    tokenizer: Optional[tiktoken.Encoding] = None,\n",
       "):\n",
       "\n",
       "    self.name = name\n",
       "    self.embedder = OpenAiEmbedder()\n",
       "    if save_path is None:\n",
       "        save_path = \"storage\"\n",
       "\n",
       "    self.save_path = save_path\n",
       "\n",
       "    # Create the 'storage' folder if it does not exist\n",
       "    os.makedirs(self.save_path, exist_ok=True)\n",
       "    self.values = []\n",
       "    if load is True:\n",
       "        self.load()\n",
       "    else:\n",
       "        self.init_index(index, values, embeddings)\n",
       "    if tokenizer is None:\n",
       "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
       "    else:\n",
       "        self.tokenizer = tokenizer\n",
       "    self.query_history = []\n",
       "    self.save()\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def init_index(\n",
       "    self,\n",
       "    index: Optional[faiss.Index] = None,\n",
       "    values: Optional[List[str]] = None,\n",
       "    embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
       ") -> None:\n",
       "\n",
       "    \"\"\"\n",
       "        initializes the index, there are 4 cases:\n",
       "        1. we create a new index from scratch\n",
       "        2. we create a new index from a list of embeddings and values\n",
       "        3. we create a new index from a faiss index and values list\n",
       "        4. we load an index from a file\n",
       "        \"\"\"\n",
       "    # fist case is when we create a new index from scratch\n",
       "    if index is None and values is None and embeddings is None:\n",
       "        print(\"Creating a new index\")\n",
       "        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
       "        self.values = []\n",
       "    # second case is where we create the index from a list of embeddings\n",
       "    elif (\n",
       "        index is None\n",
       "        and values is not None\n",
       "        and embeddings is not None\n",
       "        and len(values) == len(embeddings)\n",
       "    ):\n",
       "        print(\"Creating a new index from a list of embeddings and values\")\n",
       "        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
       "        for embedding, value in zip(embeddings, values):\n",
       "            self.add_to_index(value, embedding)\n",
       "        # third case is where we create the index from a faiss index and values list\n",
       "    elif (\n",
       "        isinstance(index, faiss.Index)\n",
       "        and index.d == self.embedder.get_embedding_size()\n",
       "        and type(values) == list\n",
       "        and len(values) == index.ntotal\n",
       "    ):\n",
       "        print(\"Creating a new index from a faiss index and values list\")\n",
       "        self.index = index\n",
       "        self.values = values\n",
       "    # fourth case is where we create an index from a list of values, the values are embedded and the index is created\n",
       "    elif index is None and values is not None and embeddings is None:\n",
       "        print(\"Creating a new index from a list of values\")\n",
       "        self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
       "        i = 0\n",
       "        for value in values:\n",
       "            # print the value id to see the progress\n",
       "            print(\"Embedding value \", i, \" of \", len(values))\n",
       "            # start tracking the time using time\n",
       "            start = time.time()\n",
       "            self.add_to_index(value)\n",
       "            # print the time it took to embed the value\n",
       "            print(\"Embedding value \", i, \" took \", time.time() - start, \" seconds\")\n",
       "            i += 1\n",
       "    else:\n",
       "        raise ValueError(\n",
       "            \"The index is not a valid faiss index or the embedding dimension is not correct\"\n",
       "        )\n",
       "    \n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value @classmethod\n",
       "def from_pandas(\n",
       "    cls,\n",
       "    data_frame: Union[pd.DataFrame, str],\n",
       "    columns: Optional[Union[str, List[str]]] = None,\n",
       "    name: str = 'memory_index',\n",
       "    save_path: Optional[str] = None,\n",
       "    in_place: bool = True,\n",
       "    embeddings_col: Optional[str] = None,\n",
       ") -> 'MemoryIndex':\n",
       "    \"\"\"\n",
       "        Initialize a MemoryIndex object from a pandas DataFrame.\n",
       "\n",
       "        Args:\n",
       "            data_frame: The DataFrame or path to a CSV file.\n",
       "            columns: The columns of the DataFrame to use as values.\n",
       "            name: The name of the index.\n",
       "            save_path: The path to save the index.\n",
       "            in_place: Whether to work on the DataFrame in place or create a copy.\n",
       "            embeddings_col: The column name containing the embeddings.\n",
       "\n",
       "        Returns:\n",
       "            A MemoryIndex object initialized with values and embeddings from the DataFrame.\n",
       "        \"\"\"\n",
       "\n",
       "    if isinstance(data_frame, str) and data_frame.endswith(\".csv\") and os.path.isfile(data_frame):\n",
       "        print(\"Loading the CSV file\")\n",
       "        try:\n",
       "            data_frame = pd.read_csv(data_frame)\n",
       "        except:\n",
       "            raise ValueError(\"The CSV file is not valid\")\n",
       "        name = data_frame.split(\"/\")[-1].split(\".\")[0]\n",
       "    elif isinstance(data_frame, pd.core.frame.DataFrame) and columns is not None:\n",
       "        print(\"Loading the DataFrame\")\n",
       "        if not in_place:\n",
       "            data_frame = copy.deepcopy(data_frame)\n",
       "    else:\n",
       "        raise ValueError(\"The data_frame is not a valid pandas dataframe or the columns are not valid or the path is not valid\")\n",
       "\n",
       "    values, embeddings = cls.extract_values_and_embeddings(data_frame, columns, embeddings_col)\n",
       "    return cls(values=values, embeddings=embeddings, name=name, save_path=save_path)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "@staticmethod\n",
       "def extract_values_and_embeddings(\n",
       "    data_frame: pd.DataFrame,\n",
       "    columns: Union[str, List[str]],\n",
       "    embeddings_col: Optional[str],\n",
       ") -> Tuple[List[str], Optional[List[np.ndarray]]]:\n",
       "    \"\"\"\n",
       "        Extract values and embeddings from a pandas DataFrame.\n",
       "\n",
       "        Args:\n",
       "            data_frame: The DataFrame to extract values and embeddings from.\n",
       "            columns: The columns of the DataFrame to use as values.\n",
       "            embeddings_col: The column name containing the embeddings.\n",
       "\n",
       "        Returns:\n",
       "            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n",
       "        \"\"\"\n",
       "\n",
       "    if isinstance(columns, list) and len(columns) > 1:\n",
       "        data_frame[\"values_combined\"] = data_frame[columns].apply(lambda x: ' '.join(x), axis=1)\n",
       "        columns = \"values_combined\"\n",
       "    elif isinstance(columns, list) and len(columns) == 1:\n",
       "        columns = columns[0]\n",
       "    elif not isinstance(columns, str):\n",
       "        raise ValueError(\"The columns are not valid\")\n",
       "\n",
       "    values = []\n",
       "    embeddings = []\n",
       "\n",
       "    for _, row in data_frame.iterrows():\n",
       "        value = row[columns]\n",
       "        values.append(value)\n",
       "\n",
       "        if embeddings_col is not None:\n",
       "            embedding = row[embeddings_col]\n",
       "            embeddings.append(embedding)\n",
       "\n",
       "    return values, embeddings if embeddings_col is not None else None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def add_to_index(\n",
       "    self,\n",
       "    value: str,\n",
       "    embedding: Optional[Union[List[float], np.ndarray, str]] = None,\n",
       "    verbose: bool = True,\n",
       ") -> None:\n",
       "    \"\"\"\n",
       "        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n",
       "        \"\"\"\n",
       "    if value not in self.values:\n",
       "        if embedding is None:\n",
       "            embedding = self.embedder.embed(value)\n",
       "            if verbose:\n",
       "                display(\n",
       "                    Markdown(\"The value {value} was embedded\".format(value=value))\n",
       "                )\n",
       "        if embedding is not None:\n",
       "            if type(embedding) is list:\n",
       "                embedding = np.array([embedding])\n",
       "            elif type(embedding) is str:\n",
       "                embedding = eval(embedding)\n",
       "                embedding = np.array([embedding]).astype(np.float32)\n",
       "            elif type(embedding) is not np.ndarray:\n",
       "                raise ValueError(\"The embedding is not a valid type\")\n",
       "\n",
       "            # Ensure that the embedding is a 2D numpy array\n",
       "            if embedding.ndim == 1:\n",
       "                embedding = embedding.reshape(1, -1)\n",
       "            # print(\"embedding is \", embedding)\n",
       "            # print(\"embedding type is \", type(embedding))\n",
       "            # print(\"embedding shape is \", embedding.shape)\n",
       "            self.index.add(embedding)\n",
       "            self.values.append(value)\n",
       "            self.save()\n",
       "    else:\n",
       "        if verbose:\n",
       "            display(\n",
       "                Markdown(\n",
       "                    \"The value {value} was already in the index\".format(value=value)\n",
       "                )\n",
       "            )\n",
       "            \n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def remove_from_index(self, value: str) -> None:\n",
       "        \"\"\"\n",
       "            Remove a value from the index and the values list.\n",
       "            Args:\n",
       "                value: The value to remove from the index.\n",
       "            \"\"\"\n",
       "        index = self.get_index_by_value(value)\n",
       "        if index is not None:\n",
       "            # Remove the value from the values list\n",
       "            self.values.pop(index)\n",
       "\n",
       "            # Remove the corresponding embedding from the index\n",
       "            id_selector = faiss.IDSelectorArray(np.array([index], dtype=np.int64))\n",
       "            self.index.remove_ids(id_selector)\n",
       "\n",
       "            # Save the changes\n",
       "            self.save()\n",
       "        else:\n",
       "            print(f\"The value '{value}' was not found in the index.\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_embedding_by_index(self, index: int) -> np.ndarray:\n",
       "    \"\"\"\n",
       "        Get the embedding corresponding to a certain index value.\n",
       "        \"\"\"\n",
       "    if index < 0 or index >= len(self.values):\n",
       "        raise ValueError(\"The index is out of range\")\n",
       "\n",
       "    # Fetch the embedding from the Faiss index\n",
       "    embedding = self.index.reconstruct(index)\n",
       "\n",
       "    return embedding\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_index_by_value(self, value: str) -> Optional[int]:\n",
       "    \"\"\"\n",
       "        Get the index corresponding to a value in self.values.\n",
       "        \"\"\"\n",
       "    if value in self.values:\n",
       "        index = self.values.index(value)\n",
       "        return index\n",
       "    else:\n",
       "        return None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_embedding_by_value(self, value: str) -> Optional[np.ndarray]:\n",
       "    \"\"\"\n",
       "        Get the embedding corresponding to a certain value in self.values.\n",
       "        \"\"\"\n",
       "    index = self.get_index_by_value(value)\n",
       "    if index is not None:\n",
       "        embedding = self.get_embedding_by_index(index)\n",
       "        return embedding\n",
       "    else:\n",
       "        return None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_all_embeddings(self) -> np.ndarray:\n",
       "    \"\"\"\n",
       "        Get all the embeddings in the index.\n",
       "        \"\"\"\n",
       "    embeddings = []\n",
       "    for i in range(len(self.values)):\n",
       "        embeddings.append(self.get_embedding_by_index(i))\n",
       "    self.embeddings = np.array(embeddings)\n",
       "    return self.embeddings\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def faiss_query(self, query: str, k: int = 10) -> Tuple[List[str], List[float]]:\n",
       "    \"\"\"Query the faiss index for the top-k most similar values to the query\"\"\"\n",
       "\n",
       "    # Embed the data\n",
       "    embedding = self.embedder.embed(query)\n",
       "    if k > len(self.values):\n",
       "        k = len(self.values)\n",
       "    # Query the Faiss index for the top-K most similar values\n",
       "    D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\n",
       "    # Get the values corresponding to the indices\n",
       "    values = [self.values[i] for i in I[0]]\n",
       "    scores = [d for d in D[0]]\n",
       "    return values, scores, I\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def token_bound_query(self, query, k=10, max_tokens=4000):\n",
       "    \"\"\"Query the faiss index for the top-k most similar values to the query, but bound the number of tokens retrieved by the max_tokens parameter\"\"\"\n",
       "    returned_tokens = 0\n",
       "    top_k_hint = []\n",
       "    scores = []\n",
       "    tokens = []\n",
       "    indices = []\n",
       "\n",
       "    if len(self.values) > 0:\n",
       "        top_k, scores, indices = self.faiss_query(query, k=min(k, len(self.values)))\n",
       "\n",
       "        for hint in top_k:\n",
       "            # mark the message and gets the length in tokens\n",
       "            message_tokens = len(self.tokenizer.encode(hint))\n",
       "            tokens.append(message_tokens)\n",
       "            if returned_tokens + message_tokens <= max_tokens:\n",
       "                top_k_hint += [hint]\n",
       "                returned_tokens += message_tokens\n",
       "\n",
       "        self.query_history.append(\n",
       "            {\n",
       "                \"query\": query,\n",
       "                \"hints\": top_k_hint,\n",
       "                \"scores\": scores,\n",
       "                \"indices\": indices,\n",
       "                \"hints_tokens\": tokens,\n",
       "                \"returned_tokens\": returned_tokens,\n",
       "                \"max_tokens\": max_tokens,\n",
       "                \"k\": k,\n",
       "            }\n",
       "        )\n",
       "\n",
       "    return top_k_hint, scores, indices\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def save(self):\n",
       "    \"\"\"Save the index to disk using faiss and json and numpy\"\"\"\n",
       "    # Create the directory to save the index, values, and embeddings\n",
       "    save_directory = os.path.join(self.save_path, self.name)\n",
       "    os.makedirs(save_directory, exist_ok=True)\n",
       "\n",
       "    # Save the FAISS index\n",
       "    index_filename = os.path.join(save_directory, f\"{self.name}_index.faiss\")\n",
       "    faiss.write_index(self.index, index_filename)\n",
       "\n",
       "    # Save the index values\n",
       "    values_filename = os.path.join(save_directory, f\"{self.name}_values.json\")\n",
       "    with open(values_filename, \"w\") as f:\n",
       "        json.dump(self.values, f)\n",
       "\n",
       "    # Save the numpy array of the embeddings\n",
       "    embeddings_filename = os.path.join(\n",
       "        save_directory, f\"{self.name}_embeddings.npz\"\n",
       "    )\n",
       "    # print(f\"embs: {self.get_all_embeddings().shape}\")\n",
       "    np.savez_compressed(embeddings_filename, self.get_all_embeddings())\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def load(self):\n",
       "    \"\"\"Load the index, values, and embeddings from disk\"\"\"\n",
       "    # Set the directory to load the index, values, and embeddings from\n",
       "    load_directory = os.path.join(self.save_path, self.name)\n",
       "\n",
       "    # Load the FAISS index\n",
       "    index_filename = os.path.join(load_directory, f\"{self.name}_index.faiss\")\n",
       "    self.index = faiss.read_index(index_filename)\n",
       "\n",
       "    # Load the index values\n",
       "    values_filename = os.path.join(load_directory, f\"{self.name}_values.json\")\n",
       "    with open(values_filename, \"r\") as f:\n",
       "        self.values = json.load(f)\n",
       "\n",
       "    # Load the numpy array of the embeddings\n",
       "    embeddings_filename = os.path.join(\n",
       "        load_directory, f\"{self.name}_embeddings.npz\"\n",
       "    )\n",
       "    embeddings_data = np.load(embeddings_filename)\n",
       "    self.embeddings = embeddings_data[\"arr_0\"]\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def prune_index(\n",
       "    self,\n",
       "    constraint: Optional[str] = None,\n",
       "    regex_pattern: Optional[str] = None,\n",
       "    length_constraint: Optional[int] = None,\n",
       ") -> \"MemoryIndex\":\n",
       "    \"\"\"Prune the index based on the constraint provided. Currently, only regex and length constraints are supported.\"\"\"\n",
       "\n",
       "    if constraint is not None:\n",
       "        if constraint == \"regex\":\n",
       "            if regex_pattern is None:\n",
       "                raise ValueError(\n",
       "                    \"regex_pattern must be provided for regex constraint.\"\n",
       "                )\n",
       "            pruned_values, pruned_embeddings = self._prune_by_regex(regex_pattern)\n",
       "        elif constraint == \"length\":\n",
       "            if length_constraint is None:\n",
       "                raise ValueError(\n",
       "                    \"length_constraint must be provided for length constraint.\"\n",
       "                )\n",
       "            pruned_values, pruned_embeddings = self._prune_by_length(\n",
       "                length_constraint\n",
       "            )\n",
       "        else:\n",
       "            raise ValueError(\"Invalid constraint type provided.\")\n",
       "    else:\n",
       "        raise ValueError(\"constraint must be provided for pruning the index.\")\n",
       "\n",
       "    # Create a new index with pruned values and embeddings\n",
       "    pruned_memory_index = MemoryIndex(\n",
       "        values=pruned_values,\n",
       "        embeddings=pruned_embeddings,\n",
       "        name=self.name + \"_pruned\",\n",
       "    )\n",
       "\n",
       "    return pruned_memory_index\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def _prune_by_regex(self, regex_pattern: str) -> Tuple[List[str], List[np.ndarray]]:\n",
       "    \"\"\"Prune the index by the regex pattern provided.\"\"\"\n",
       "    pruned_values = []\n",
       "    pruned_embeddings = []\n",
       "\n",
       "    for value in self.values:\n",
       "        if re.search(regex_pattern, value):\n",
       "            pruned_values.append(value)\n",
       "            pruned_embeddings.append(self.get_embedding_by_value(value))\n",
       "\n",
       "    return pruned_values, pruned_embeddings\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def _prune_by_length(\n",
       "    self, length_constraint: int\n",
       ") -> Tuple[List[str], List[np.ndarray]]:\n",
       "    \"\"\"Prune the index by the length constraint provided.\"\"\"\n",
       "    pruned_values = []\n",
       "    pruned_embeddings = []\n",
       "\n",
       "    for value in self.values:\n",
       "        if len(value) >= length_constraint:\n",
       "            pruned_values.append(value)\n",
       "            pruned_embeddings.append(self.get_embedding_by_value(value))\n",
       "\n",
       "    return pruned_values, pruned_embeddings\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import copy\n",
       "import json\n",
       "import os\n",
       "import pickle\n",
       "import re\n",
       "import time\n",
       "from typing import Dict, List, Optional, Tuple, Union\n",
       "\n",
       "import faiss\n",
       "import numpy as np\n",
       "import tiktoken\n",
       "from IPython.display import Markdown, display\n",
       "\n",
       "from babydragon.models.embedders.ada2 import OpenAiEmbedder\n",
       "import pandas as pd\n",
       "\n",
       "class MemoryIndex:\n",
       "    \"\"\"\n",
       "    this class is a wrapper for a faiss index, it contains information about the format of the index the faiss index itself\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        index: Optional[faiss.Index] = None,\n",
       "        values: Optional[List[str]] = None,\n",
       "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
       "        name: str = \"memory_index\",\n",
       "        save_path: Optional[str] = None,\n",
       "        load: bool = False,\n",
       "        tokenizer: Optional[tiktoken.Encoding] = None,\n",
       "    ):\n",
       "\n",
       "        self.name = name\n",
       "        self.embedder = OpenAiEmbedder()\n",
       "        if save_path is None:\n",
       "            save_path = \"storage\"\n",
       "\n",
       "        self.save_path = save_path\n",
       "\n",
       "        # Create the 'storage' folder if it does not exist\n",
       "        os.makedirs(self.save_path, exist_ok=True)\n",
       "        self.values = []\n",
       "        if load is True:\n",
       "            self.load()\n",
       "        else:\n",
       "            self.init_index(index, values, embeddings)\n",
       "        if tokenizer is None:\n",
       "            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
       "        else:\n",
       "            self.tokenizer = tokenizer\n",
       "        self.query_history = []\n",
       "        self.save()\n",
       "\n",
       "    def init_index(\n",
       "        self,\n",
       "        index: Optional[faiss.Index] = None,\n",
       "        values: Optional[List[str]] = None,\n",
       "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
       "    ) -> None:\n",
       "\n",
       "        \"\"\"\n",
       "        initializes the index, there are 4 cases:\n",
       "        1. we create a new index from scratch\n",
       "        2. we create a new index from a list of embeddings and values\n",
       "        3. we create a new index from a faiss index and values list\n",
       "        4. we load an index from a file\n",
       "        \"\"\"\n",
       "        # fist case is when we create a new index from scratch\n",
       "        if index is None and values is None and embeddings is None:\n",
       "            print(\"Creating a new index\")\n",
       "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
       "            self.values = []\n",
       "        # second case is where we create the index from a list of embeddings\n",
       "        elif (\n",
       "            index is None\n",
       "            and values is not None\n",
       "            and embeddings is not None\n",
       "            and len(values) == len(embeddings)\n",
       "        ):\n",
       "            print(\"Creating a new index from a list of embeddings and values\")\n",
       "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
       "            for embedding, value in zip(embeddings, values):\n",
       "                self.add_to_index(value, embedding)\n",
       "        # third case is where we create the index from a faiss index and values list\n",
       "        elif (\n",
       "            isinstance(index, faiss.Index)\n",
       "            and index.d == self.embedder.get_embedding_size()\n",
       "            and type(values) == list\n",
       "            and len(values) == index.ntotal\n",
       "        ):\n",
       "            print(\"Creating a new index from a faiss index and values list\")\n",
       "            self.index = index\n",
       "            self.values = values\n",
       "        # fourth case is where we create an index from a list of values, the values are embedded and the index is created\n",
       "        elif index is None and values is not None and embeddings is None:\n",
       "            print(\"Creating a new index from a list of values\")\n",
       "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
       "            i = 0\n",
       "            for value in values:\n",
       "                # print the value id to see the progress\n",
       "                print(\"Embedding value \", i, \" of \", len(values))\n",
       "                # start tracking the time using time\n",
       "                start = time.time()\n",
       "                self.add_to_index(value)\n",
       "                # print the time it took to embed the value\n",
       "                print(\"Embedding value \", i, \" took \", time.time() - start, \" seconds\")\n",
       "                i += 1\n",
       "        else:\n",
       "            raise ValueError(\n",
       "                \"The index is not a valid faiss index or the embedding dimension is not correct\"\n",
       "            )\n",
       "        \n",
       "    @classmethod\n",
       "    def from_pandas(\n",
       "        cls,\n",
       "        data_frame: Union[pd.DataFrame, str],\n",
       "        columns: Optional[Union[str, List[str]]] = None,\n",
       "        name: str = 'memory_index',\n",
       "        save_path: Optional[str] = None,\n",
       "        in_place: bool = True,\n",
       "        embeddings_col: Optional[str] = None,\n",
       "    ) -> 'MemoryIndex':\n",
       "        \"\"\"\n",
       "        Initialize a MemoryIndex object from a pandas DataFrame.\n",
       "\n",
       "        Args:\n",
       "            data_frame: The DataFrame or path to a CSV file.\n",
       "            columns: The columns of the DataFrame to use as values.\n",
       "            name: The name of the index.\n",
       "            save_path: The path to save the index.\n",
       "            in_place: Whether to work on the DataFrame in place or create a copy.\n",
       "            embeddings_col: The column name containing the embeddings.\n",
       "\n",
       "        Returns:\n",
       "            A MemoryIndex object initialized with values and embeddings from the DataFrame.\n",
       "        \"\"\"\n",
       "\n",
       "        if isinstance(data_frame, str) and data_frame.endswith(\".csv\") and os.path.isfile(data_frame):\n",
       "            print(\"Loading the CSV file\")\n",
       "            try:\n",
       "                data_frame = pd.read_csv(data_frame)\n",
       "            except:\n",
       "                raise ValueError(\"The CSV file is not valid\")\n",
       "            name = data_frame.split(\"/\")[-1].split(\".\")[0]\n",
       "        elif isinstance(data_frame, pd.core.frame.DataFrame) and columns is not None:\n",
       "            print(\"Loading the DataFrame\")\n",
       "            if not in_place:\n",
       "                data_frame = copy.deepcopy(data_frame)\n",
       "        else:\n",
       "            raise ValueError(\"The data_frame is not a valid pandas dataframe or the columns are not valid or the path is not valid\")\n",
       "\n",
       "        values, embeddings = cls.extract_values_and_embeddings(data_frame, columns, embeddings_col)\n",
       "        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path)\n",
       "\n",
       "    @staticmethod\n",
       "    def extract_values_and_embeddings(\n",
       "        data_frame: pd.DataFrame,\n",
       "        columns: Union[str, List[str]],\n",
       "        embeddings_col: Optional[str],\n",
       "    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\n",
       "        \"\"\"\n",
       "        Extract values and embeddings from a pandas DataFrame.\n",
       "\n",
       "        Args:\n",
       "            data_frame: The DataFrame to extract values and embeddings from.\n",
       "            columns: The columns of the DataFrame to use as values.\n",
       "            embeddings_col: The column name containing the embeddings.\n",
       "\n",
       "        Returns:\n",
       "            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n",
       "        \"\"\"\n",
       "\n",
       "        if isinstance(columns, list) and len(columns) > 1:\n",
       "            data_frame[\"values_combined\"] = data_frame[columns].apply(lambda x: ' '.join(x), axis=1)\n",
       "            columns = \"values_combined\"\n",
       "        elif isinstance(columns, list) and len(columns) == 1:\n",
       "            columns = columns[0]\n",
       "        elif not isinstance(columns, str):\n",
       "            raise ValueError(\"The columns are not valid\")\n",
       "\n",
       "        values = []\n",
       "        embeddings = []\n",
       "\n",
       "        for _, row in data_frame.iterrows():\n",
       "            value = row[columns]\n",
       "            values.append(value)\n",
       "\n",
       "            if embeddings_col is not None:\n",
       "                embedding = row[embeddings_col]\n",
       "                embeddings.append(embedding)\n",
       "\n",
       "        return values, embeddings if embeddings_col is not None else None\n",
       "\n",
       "    def add_to_index(\n",
       "        self,\n",
       "        value: str,\n",
       "        embedding: Optional[Union[List[float], np.ndarray, str]] = None,\n",
       "        verbose: bool = True,\n",
       "    ) -> None:\n",
       "        \"\"\"\n",
       "        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n",
       "        \"\"\"\n",
       "        if value not in self.values:\n",
       "            if embedding is None:\n",
       "                embedding = self.embedder.embed(value)\n",
       "                if verbose:\n",
       "                    display(\n",
       "                        Markdown(\"The value {value} was embedded\".format(value=value))\n",
       "                    )\n",
       "            if embedding is not None:\n",
       "                if type(embedding) is list:\n",
       "                    embedding = np.array([embedding])\n",
       "                elif type(embedding) is str:\n",
       "                    embedding = eval(embedding)\n",
       "                    embedding = np.array([embedding]).astype(np.float32)\n",
       "                elif type(embedding) is not np.ndarray:\n",
       "                    raise ValueError(\"The embedding is not a valid type\")\n",
       "\n",
       "                # Ensure that the embedding is a 2D numpy array\n",
       "                if embedding.ndim == 1:\n",
       "                    embedding = embedding.reshape(1, -1)\n",
       "                # print(\"embedding is \", embedding)\n",
       "                # print(\"embedding type is \", type(embedding))\n",
       "                # print(\"embedding shape is \", embedding.shape)\n",
       "                self.index.add(embedding)\n",
       "                self.values.append(value)\n",
       "                self.save()\n",
       "        else:\n",
       "            if verbose:\n",
       "                display(\n",
       "                    Markdown(\n",
       "                        \"The value {value} was already in the index\".format(value=value)\n",
       "                    )\n",
       "                )\n",
       "                \n",
       "    def remove_from_index(self, value: str) -> None:\n",
       "            \"\"\"\n",
       "            Remove a value from the index and the values list.\n",
       "            Args:\n",
       "                value: The value to remove from the index.\n",
       "            \"\"\"\n",
       "            index = self.get_index_by_value(value)\n",
       "            if index is not None:\n",
       "                # Remove the value from the values list\n",
       "                self.values.pop(index)\n",
       "\n",
       "                # Remove the corresponding embedding from the index\n",
       "                id_selector = faiss.IDSelectorArray(np.array([index], dtype=np.int64))\n",
       "                self.index.remove_ids(id_selector)\n",
       "\n",
       "                # Save the changes\n",
       "                self.save()\n",
       "            else:\n",
       "                print(f\"The value '{value}' was not found in the index.\")\n",
       "\n",
       "    def get_embedding_by_index(self, index: int) -> np.ndarray:\n",
       "        \"\"\"\n",
       "        Get the embedding corresponding to a certain index value.\n",
       "        \"\"\"\n",
       "        if index < 0 or index >= len(self.values):\n",
       "            raise ValueError(\"The index is out of range\")\n",
       "\n",
       "        # Fetch the embedding from the Faiss index\n",
       "        embedding = self.index.reconstruct(index)\n",
       "\n",
       "        return embedding\n",
       "\n",
       "    def get_index_by_value(self, value: str) -> Optional[int]:\n",
       "        \"\"\"\n",
       "        Get the index corresponding to a value in self.values.\n",
       "        \"\"\"\n",
       "        if value in self.values:\n",
       "            index = self.values.index(value)\n",
       "            return index\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "    def get_embedding_by_value(self, value: str) -> Optional[np.ndarray]:\n",
       "        \"\"\"\n",
       "        Get the embedding corresponding to a certain value in self.values.\n",
       "        \"\"\"\n",
       "        index = self.get_index_by_value(value)\n",
       "        if index is not None:\n",
       "            embedding = self.get_embedding_by_index(index)\n",
       "            return embedding\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "    def get_all_embeddings(self) -> np.ndarray:\n",
       "        \"\"\"\n",
       "        Get all the embeddings in the index.\n",
       "        \"\"\"\n",
       "        embeddings = []\n",
       "        for i in range(len(self.values)):\n",
       "            embeddings.append(self.get_embedding_by_index(i))\n",
       "        self.embeddings = np.array(embeddings)\n",
       "        return self.embeddings\n",
       "\n",
       "    def faiss_query(self, query: str, k: int = 10) -> Tuple[List[str], List[float]]:\n",
       "        \"\"\"Query the faiss index for the top-k most similar values to the query\"\"\"\n",
       "\n",
       "        # Embed the data\n",
       "        embedding = self.embedder.embed(query)\n",
       "        if k > len(self.values):\n",
       "            k = len(self.values)\n",
       "        # Query the Faiss index for the top-K most similar values\n",
       "        D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\n",
       "        # Get the values corresponding to the indices\n",
       "        values = [self.values[i] for i in I[0]]\n",
       "        scores = [d for d in D[0]]\n",
       "        return values, scores, I\n",
       "\n",
       "    def token_bound_query(self, query, k=10, max_tokens=4000):\n",
       "        \"\"\"Query the faiss index for the top-k most similar values to the query, but bound the number of tokens retrieved by the max_tokens parameter\"\"\"\n",
       "        returned_tokens = 0\n",
       "        top_k_hint = []\n",
       "        scores = []\n",
       "        tokens = []\n",
       "        indices = []\n",
       "\n",
       "        if len(self.values) > 0:\n",
       "            top_k, scores, indices = self.faiss_query(query, k=min(k, len(self.values)))\n",
       "\n",
       "            for hint in top_k:\n",
       "                # mark the message and gets the length in tokens\n",
       "                message_tokens = len(self.tokenizer.encode(hint))\n",
       "                tokens.append(message_tokens)\n",
       "                if returned_tokens + message_tokens <= max_tokens:\n",
       "                    top_k_hint += [hint]\n",
       "                    returned_tokens += message_tokens\n",
       "\n",
       "            self.query_history.append(\n",
       "                {\n",
       "                    \"query\": query,\n",
       "                    \"hints\": top_k_hint,\n",
       "                    \"scores\": scores,\n",
       "                    \"indices\": indices,\n",
       "                    \"hints_tokens\": tokens,\n",
       "                    \"returned_tokens\": returned_tokens,\n",
       "                    \"max_tokens\": max_tokens,\n",
       "                    \"k\": k,\n",
       "                }\n",
       "            )\n",
       "\n",
       "        return top_k_hint, scores, indices\n",
       "\n",
       "    def save(self):\n",
       "        \"\"\"Save the index to disk using faiss and json and numpy\"\"\"\n",
       "        # Create the directory to save the index, values, and embeddings\n",
       "        save_directory = os.path.join(self.save_path, self.name)\n",
       "        os.makedirs(save_directory, exist_ok=True)\n",
       "\n",
       "        # Save the FAISS index\n",
       "        index_filename = os.path.join(save_directory, f\"{self.name}_index.faiss\")\n",
       "        faiss.write_index(self.index, index_filename)\n",
       "\n",
       "        # Save the index values\n",
       "        values_filename = os.path.join(save_directory, f\"{self.name}_values.json\")\n",
       "        with open(values_filename, \"w\") as f:\n",
       "            json.dump(self.values, f)\n",
       "\n",
       "        # Save the numpy array of the embeddings\n",
       "        embeddings_filename = os.path.join(\n",
       "            save_directory, f\"{self.name}_embeddings.npz\"\n",
       "        )\n",
       "        # print(f\"embs: {self.get_all_embeddings().shape}\")\n",
       "        np.savez_compressed(embeddings_filename, self.get_all_embeddings())\n",
       "\n",
       "    def load(self):\n",
       "        \"\"\"Load the index, values, and embeddings from disk\"\"\"\n",
       "        # Set the directory to load the index, values, and embeddings from\n",
       "        load_directory = os.path.join(self.save_path, self.name)\n",
       "\n",
       "        # Load the FAISS index\n",
       "        index_filename = os.path.join(load_directory, f\"{self.name}_index.faiss\")\n",
       "        self.index = faiss.read_index(index_filename)\n",
       "\n",
       "        # Load the index values\n",
       "        values_filename = os.path.join(load_directory, f\"{self.name}_values.json\")\n",
       "        with open(values_filename, \"r\") as f:\n",
       "            self.values = json.load(f)\n",
       "\n",
       "        # Load the numpy array of the embeddings\n",
       "        embeddings_filename = os.path.join(\n",
       "            load_directory, f\"{self.name}_embeddings.npz\"\n",
       "        )\n",
       "        embeddings_data = np.load(embeddings_filename)\n",
       "        self.embeddings = embeddings_data[\"arr_0\"]\n",
       "\n",
       "    def prune_index(\n",
       "        self,\n",
       "        constraint: Optional[str] = None,\n",
       "        regex_pattern: Optional[str] = None,\n",
       "        length_constraint: Optional[int] = None,\n",
       "    ) -> \"MemoryIndex\":\n",
       "        \"\"\"Prune the index based on the constraint provided. Currently, only regex and length constraints are supported.\"\"\"\n",
       "\n",
       "        if constraint is not None:\n",
       "            if constraint == \"regex\":\n",
       "                if regex_pattern is None:\n",
       "                    raise ValueError(\n",
       "                        \"regex_pattern must be provided for regex constraint.\"\n",
       "                    )\n",
       "                pruned_values, pruned_embeddings = self._prune_by_regex(regex_pattern)\n",
       "            elif constraint == \"length\":\n",
       "                if length_constraint is None:\n",
       "                    raise ValueError(\n",
       "                        \"length_constraint must be provided for length constraint.\"\n",
       "                    )\n",
       "                pruned_values, pruned_embeddings = self._prune_by_length(\n",
       "                    length_constraint\n",
       "                )\n",
       "            else:\n",
       "                raise ValueError(\"Invalid constraint type provided.\")\n",
       "        else:\n",
       "            raise ValueError(\"constraint must be provided for pruning the index.\")\n",
       "\n",
       "        # Create a new index with pruned values and embeddings\n",
       "        pruned_memory_index = MemoryIndex(\n",
       "            values=pruned_values,\n",
       "            embeddings=pruned_embeddings,\n",
       "            name=self.name + \"_pruned\",\n",
       "        )\n",
       "\n",
       "        return pruned_memory_index\n",
       "\n",
       "    def _prune_by_regex(self, regex_pattern: str) -> Tuple[List[str], List[np.ndarray]]:\n",
       "        \"\"\"Prune the index by the regex pattern provided.\"\"\"\n",
       "        pruned_values = []\n",
       "        pruned_embeddings = []\n",
       "\n",
       "        for value in self.values:\n",
       "            if re.search(regex_pattern, value):\n",
       "                pruned_values.append(value)\n",
       "                pruned_embeddings.append(self.get_embedding_by_value(value))\n",
       "\n",
       "        return pruned_values, pruned_embeddings\n",
       "\n",
       "    def _prune_by_length(\n",
       "        self, length_constraint: int\n",
       "    ) -> Tuple[List[str], List[np.ndarray]]:\n",
       "        \"\"\"Prune the index by the length constraint provided.\"\"\"\n",
       "        pruned_values = []\n",
       "        pruned_embeddings = []\n",
       "\n",
       "        for value in self.values:\n",
       "            if len(value) >= length_constraint:\n",
       "                pruned_values.append(value)\n",
       "                pruned_embeddings.append(self.get_embedding_by_value(value))\n",
       "\n",
       "        return pruned_values, pruned_embeddings\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self, mem_index, name=\"memory_kernel\", k=2, save_path=None):\n",
       "    \"\"\"\n",
       "        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\n",
       "\n",
       "        Args:\n",
       "            mem_index (MemoryIndex): A MemoryIndex instance.\n",
       "            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\n",
       "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
       "            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\n",
       "        \"\"\"\n",
       "    super().__init__(\n",
       "        index=mem_index.index,\n",
       "        values=mem_index.values,\n",
       "        embeddings=mem_index.embeddings,\n",
       "        name=name,\n",
       "        save_path=save_path,\n",
       "    )\n",
       "    self.k = k\n",
       "    self.create_k_hop_index(k=k)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def cos_sim(self, a, b):\n",
       "    \"\"\"\n",
       "        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
       "        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
       "        \"\"\"\n",
       "    if not isinstance(a, np.ndarray):\n",
       "        a = np.array(a)\n",
       "\n",
       "    if not isinstance(b, np.ndarray):\n",
       "        b = np.array(b)\n",
       "\n",
       "    if len(a.shape) == 1:\n",
       "        a = a[np.newaxis, :]\n",
       "\n",
       "    if len(b.shape) == 1:\n",
       "        b = b[np.newaxis, :]\n",
       "\n",
       "    a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\n",
       "    b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\n",
       "    return np.dot(a_norm, b_norm.T)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def compute_kernel(\n",
       "    self, embedding_set, threshold=0.65, use_softmax=False, cos_sim_batch=True\n",
       "):\n",
       "    \"\"\"\n",
       "        Compute the adjacency matrix of the graph.\n",
       "\n",
       "        Parameters:\n",
       "        embedding_set (numpy array): The embedding matrix of the nodes.\n",
       "        threshold (float): The threshold for the adjacency matrix.\n",
       "        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\n",
       "        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\n",
       "\n",
       "        Returns:\n",
       "        adj_matrix (numpy array): The adjacency matrix of the graph.\n",
       "        \"\"\"\n",
       "\n",
       "    A = self.cos_sim(embedding_set, embedding_set)\n",
       "    if use_softmax:\n",
       "        # softmax\n",
       "        A = np.exp(A)\n",
       "        A = A / np.sum(A, axis=1)[:, np.newaxis]\n",
       "    adj_matrix = np.zeros_like(A)\n",
       "    adj_matrix[A > threshold] = 1\n",
       "    adj_matrix[A <= threshold] = 0\n",
       "    adj_matrix = adj_matrix.astype(np.float32)\n",
       "    return adj_matrix\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def k_hop_message_passing(self, A, node_features, k):\n",
       "    \"\"\"\n",
       "        Compute the k-hop adjacency matrix and aggregated features using message passing.\n",
       "\n",
       "        Parameters:\n",
       "        A (numpy array): The adjacency matrix of the graph.\n",
       "        node_features (numpy array): The feature matrix of the nodes.\n",
       "        k (int): The number of hops for message passing.\n",
       "\n",
       "        Returns:\n",
       "        A_k (numpy array): The k-hop adjacency matrix.\n",
       "        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n",
       "        \"\"\"\n",
       "\n",
       "    print(\"Compute the k-hop adjacency matrix\")\n",
       "    A_k = np.linalg.matrix_power(A, k)\n",
       "\n",
       "    print(\"Aggregate the messages from the k-hop neighborhood:\")\n",
       "    agg_features = node_features.copy()\n",
       "\n",
       "    for i in tqdm(range(k)):\n",
       "        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n",
       "\n",
       "    return A_k, agg_features\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def graph_sylvester_embedding(self, G, m: int, ts: np.ndarray) -> np.ndarray:\n",
       "    \"\"\"\n",
       "        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\n",
       "\n",
       "        Args:\n",
       "            G (Tuple): A tuple containing the graph's vertices (V) and weights (W).\n",
       "            m (int): The number of singular values to consider.\n",
       "            ts (np.ndarray): The spectral scales.\n",
       "\n",
       "        Returns:\n",
       "            np.ndarray: The node_embeddings matrix.\n",
       "        \"\"\"\n",
       "    V, W = G\n",
       "    n = len(V)\n",
       "    D_BE = np.diag(W.sum(axis=1))\n",
       "    L_BE = np.identity(n) - np.dot(\n",
       "        np.diag(1 / np.sqrt(D_BE.diagonal())),\n",
       "        np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n",
       "    )\n",
       "\n",
       "    A = W\n",
       "    B = L_BE\n",
       "    C = np.identity(n)\n",
       "    X = solve_sylvester(A, B, C)\n",
       "\n",
       "    U, S, Vh = svd(X, full_matrices=False)\n",
       "    U_m = U[:, :m]\n",
       "    S_m = S[:m]\n",
       "\n",
       "    node_embeddings = np.zeros((n, m))\n",
       "\n",
       "    for i in range(n):\n",
       "        for s in range(m):\n",
       "            # Spectral kernel descriptor\n",
       "            node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n",
       "\n",
       "    return node_embeddings\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def gen_gse_embeddings(self, A, embeddings, m: int = 7):\n",
       "    \"\"\"\n",
       "        Generate Graph Sylvester Embeddings.\n",
       "\n",
       "        Args:\n",
       "            A (np.ndarray): The adjacency matrix of the graph.\n",
       "            embeddings (np.ndarray): The original node embeddings.\n",
       "            m (int, optional): The number of spectral scales. Defaults to 7.\n",
       "\n",
       "        Returns:\n",
       "            np.ndarray: The generated Graph Sylvester Embeddings.\n",
       "        \"\"\"\n",
       "    V = list(range(len(embeddings)))\n",
       "    W = A\n",
       "\n",
       "    G = (V, W)\n",
       "    ts = np.linspace(0, 1, m)  # equally spaced scales\n",
       "\n",
       "    gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\n",
       "    return gse_embeddings\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def create_k_hop_index(self, k=2):\n",
       "    \"\"\"\n",
       "        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\n",
       "        aggregated features, and updating the memory index.\n",
       "\n",
       "        Args:\n",
       "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
       "        \"\"\"\n",
       "    self.k = k\n",
       "    print(\"Computing the adjacency matrix\")\n",
       "    print(\"Embeddings shape: \", self.embeddings.shape)\n",
       "    self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n",
       "    print(\"Computing the k-hop adjacency matrix and aggregated features\")\n",
       "    self.A_k, self.node_embeddings = self.k_hop_message_passing(\n",
       "        self.A, self.embeddings, k\n",
       "    )\n",
       "    print(\"Updating the memory index\")\n",
       "    self.k_hop_index = MemoryIndex(name=self.name)\n",
       "    self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self, memory_kernel_dict: Dict[str, MemoryKernel], name=\"memory_kernel_group\"):\n",
       "    \"\"\"\n",
       "        Initialize the MemoryKernelGroup with a dictionary of MemoryKernel instances.\n",
       "\n",
       "        Args:\n",
       "            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\n",
       "            name (str, optional): The name of the MemoryKernelGroup. Defaults to \"memory_kernel_group\".\n",
       "        \"\"\"\n",
       "    self.memory_kernel_dict = memory_kernel_dict\n",
       "    self.name = name\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def create_paths_hdbscan(\n",
       "    self, embeddings: np.ndarray, num_clusters: int\n",
       ") -> List[List[int]]:\n",
       "    \"\"\"\n",
       "        Create paths using the HDBSCAN clustering algorithm.\n",
       "\n",
       "        Args:\n",
       "            embeddings (np.ndarray): The embeddings to be clustered.\n",
       "            num_clusters (int): The minimum number of clusters.\n",
       "\n",
       "        Returns:\n",
       "            List[List[int]]: A list of lists containing the clustered paths.\n",
       "        \"\"\"\n",
       "    clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n",
       "    cluster_assignments = clusterer.fit_predict(embeddings)\n",
       "\n",
       "    paths = [[] for _ in range(num_clusters)]\n",
       "    for i, cluster in enumerate(cluster_assignments):\n",
       "        paths[cluster].append(i)\n",
       "    paths = [path for path in paths if path]\n",
       "    return paths\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def create_paths_spectral_clustering(\n",
       "    self, embeddings: np.ndarray, num_clusters: int\n",
       ") -> List[List[int]]:\n",
       "    \"\"\"\n",
       "        Create paths using the spectral clustering algorithm.\n",
       "\n",
       "        Args:\n",
       "            embeddings (np.ndarray): The embeddings to be clustered.\n",
       "            num_clusters (int): The number of clusters.\n",
       "\n",
       "        Returns:\n",
       "            List[List[int]]: A list of lists containing the clustered paths.\n",
       "        \"\"\"\n",
       "    spectral_clustering = SpectralClustering(\n",
       "        n_clusters=num_clusters, affinity=\"nearest_neighbors\", random_state=42\n",
       "    )\n",
       "    cluster_assignments = spectral_clustering.fit_predict(embeddings)\n",
       "\n",
       "    paths = [[] for _ in range(num_clusters)]\n",
       "    for i, cluster in enumerate(cluster_assignments):\n",
       "        paths[cluster].append(i)\n",
       "    paths = [path for path in paths if path]\n",
       "    return paths\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def calc_shgo_mode(self, scores: List[float]) -> float:\n",
       "    \"\"\"\n",
       "        Calculate the mode of the given scores using the SHGO optimization algorithm.\n",
       "\n",
       "        Args:\n",
       "            scores (List[float]): The scores for which the mode is to be calculated.\n",
       "\n",
       "        Returns:\n",
       "            float: The mode of the given scores.\n",
       "        \"\"\"\n",
       "    def objective(x):\n",
       "        return -self.estimate_pdf(scores)(x)\n",
       "\n",
       "    bounds = [(min(scores), max(scores))]\n",
       "    result = scipy.optimize.shgo(objective, bounds)\n",
       "    return result.x\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def objective(x):\n",
       "    return -self.estimate_pdf(scores)(x)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def estimate_pdf(self, scores: List[float]) -> callable:\n",
       "    \"\"\"\n",
       "        Estimate the probability density function of the given scores.\n",
       "\n",
       "        Args:\n",
       "            scores (List[float]): The scores for which the PDF is to be estimated.\n",
       "\n",
       "        Returns:\n",
       "            callable: A callable object representing the PDF.\n",
       "        \"\"\"\n",
       "    pdf = scipy.stats.gaussian_kde(scores)\n",
       "    return pdf\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def print_path(self, kernel_label: str, path: List[int]) -> None:\n",
       "    \"\"\"\n",
       "        Print the path for the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "            kernel_label (str): The label of the kernel.\n",
       "            path (List[int]): The path to be printed.\n",
       "        \"\"\"\n",
       "    for i in path:\n",
       "        print(self.memory_kernel_dict[kernel_label].values[i])\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def sort_paths_by_mode_distance(\n",
       "    self, kernel_label: str, distance_metric: str = \"cosine\"\n",
       ")-> None:\n",
       "    \"\"\"\n",
       "        Sort paths by the mode distance of the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "            kernel_label (str): The label of the kernel.\n",
       "            distance_metric (str, optional): The distance metric to be used. Defaults to \"cosine\".\n",
       "        \"\"\"\n",
       "    paths = self.path_group[kernel_label]\n",
       "    memory_kernel = self.memory_kernel_dict[kernel_label]\n",
       "    sorted_paths = []\n",
       "    for i, path in enumerate(paths):\n",
       "        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
       "        cluster_embeddings = np.array(cluster_embeddings)\n",
       "        cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
       "        if distance_metric == \"cosine\" or distance_metric == \"guassian\":\n",
       "            scores = [\n",
       "                (i, cosine(cluster_mean, emb))\n",
       "                for i, emb in zip(path, cluster_embeddings)\n",
       "            ]\n",
       "        elif distance_metric == \"euclidean\":\n",
       "            scores = [\n",
       "                (i, np.linalg.norm(cluster_mean - emb))\n",
       "                for i, emb in zip(path, cluster_embeddings)\n",
       "            ]\n",
       "        score_values = [score for _, score in scores]  # Extract score values\n",
       "        mu = self.calc_shgo_mode(score_values)\n",
       "        sigma = np.std(score_values)\n",
       "        if distance_metric == \"guassian\":\n",
       "            scores = [\n",
       "                (i, np.exp(-((x - mu) ** 2) / (2 * sigma**2))) for i, x in scores\n",
       "            ]\n",
       "        # Sort path by score\n",
       "        sorted_path_and_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
       "        sorted_path = [x[0] for x in sorted_path_and_scores]\n",
       "        sorted_paths.append(sorted_path)\n",
       "    self.path_group[kernel_label] = sorted_paths\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def sort_paths_by_kernel_density(\n",
       "    self, kernel_label: str, distance_metric: str = \"cosine\"\n",
       ") -> None:\n",
       "    \"\"\"\n",
       "        Sort paths by the mode distance of the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "            kernel_label (str): The label of the kernel.\n",
       "            distance_metric (str, optional): The distance metric to be used. Defaults to \"cosine\".\n",
       "        \"\"\"\n",
       "    paths = self.path_group[kernel_label]\n",
       "    memory_kernel = self.memory_kernel_dict[kernel_label]\n",
       "    sorted_paths = []\n",
       "    for i, path in enumerate(paths):\n",
       "        cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
       "        cluster_embeddings = np.array(cluster_embeddings)\n",
       "        cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
       "        if distance_metric == \"cosine\":\n",
       "            scores = [\n",
       "                (i, cosine(cluster_mean, emb))\n",
       "                for i, emb in zip(path, cluster_embeddings)\n",
       "            ]\n",
       "        elif distance_metric == \"euclidean\":\n",
       "            scores = [\n",
       "                (i, np.linalg.norm(cluster_mean - emb))\n",
       "                for i, emb in zip(path, cluster_embeddings)\n",
       "            ]\n",
       "        score_values = [score for _, score in scores]  # Extract score values\n",
       "\n",
       "        # Estimate PDF using Kernel Density Estimation\n",
       "        kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(\n",
       "            np.array(score_values).reshape(-1, 1)\n",
       "        )\n",
       "        kde_scores = [kde.score_samples([[x]])[0] for _, x in scores]\n",
       "\n",
       "        # Sort path by score\n",
       "        sorted_path_and_scores = sorted(\n",
       "            zip(path, kde_scores), key=lambda x: x[1], reverse=True\n",
       "        )\n",
       "        sorted_path = [x[0] for x in sorted_path_and_scores]\n",
       "        sorted_paths.append(sorted_path)\n",
       "    self.path_group[kernel_label] = sorted_paths\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def gen_index_aligned_kernel(\n",
       "    self, chatbot: Chat, parent_kernel_label: str, child_kernel_label: str\n",
       ") -> None:\n",
       "    \"\"\"\n",
       "        Generate an index-aligned kernel using LLMWriter for the given parent and child kernel labels.\n",
       "\n",
       "        Args:\n",
       "            chatbot (Chat): The Chat instance.\n",
       "            parent_kernel_label (str): The label of the parent kernel.\n",
       "            child_kernel_label (str): The label of the child kernel.\n",
       "        \"\"\"\n",
       "    llm_writer = LLMWriter(\n",
       "        index=self.memory_kernel_dict[parent_kernel_label],\n",
       "        path=self.path_group[parent_kernel_label],\n",
       "        chatbot=chatbot,\n",
       "        write_func=None,\n",
       "        max_workers=1,\n",
       "    )\n",
       "    new_index = llm_writer.write()\n",
       "    new_memory_kernel = MemoryKernel(mem_index=new_index, name=child_kernel_label)\n",
       "    new_memory_kernel.create_k_hop_index(k=2)\n",
       "    self.memory_kernel_dict[child_kernel_label] = new_memory_kernel\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def generate_path_groups(self, method: str = \"hdbscan\") -> None:\n",
       "    \"\"\"\n",
       "        Generate path groups for all memory kernels in the memory_kernel_dict using the specified clustering method.\n",
       "\n",
       "        Args:\n",
       "            method (str, optional): The clustering method to be used. Defaults to \"hdbscan\".\n",
       "        \"\"\"\n",
       "    path_group = {}\n",
       "    for k, v in self.memory_kernel_dict.items():\n",
       "        embeddings = v.node_embeddings\n",
       "        num_clusters = int(np.sqrt(len(embeddings)))\n",
       "        if method == \"hdbscan\":\n",
       "            paths = self.create_paths_hdbscan(embeddings, num_clusters)\n",
       "        elif method == \"spectral_clustering\":\n",
       "            paths = self.create_paths_spectral_clustering(embeddings, num_clusters)\n",
       "        path_group[k] = paths\n",
       "\n",
       "    self.path_group = path_group\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def batch_sort_kernel_group(self, kernel_label: str):\n",
       "    \"\"\"\n",
       "        Batch sort the kernel group by the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "        \"\"\"\n",
       "    if all(\n",
       "        [\n",
       "            v.node_embeddings.shape\n",
       "            == self.memory_kernel_dict[kernel_label].node_embeddings.shape\n",
       "            for k, v in self.memory_kernel_dict.items()\n",
       "        ]\n",
       "    ):\n",
       "        self.memory_kernel_sort(self.path_group[kernel_label])\n",
       "    else:\n",
       "        return ValueError(\"Not all kernels are of the same dimensions.\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def memory_kernel_sort(self, paths: List[List[int]]):\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def is_kernel_group_isomorphic(self):\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self, memory_kernel_group: MemoryKernelGroup):\n",
       "    \"\"\"\n",
       "        Initialize the MemoryKernelGroupStabilityAnalysis with a MemoryKernelGroup instance.\n",
       "\n",
       "        Args:\n",
       "            memory_kernel_group (MemoryKernelGroup): A MemoryKernelGroup instance.\n",
       "        \"\"\"\n",
       "    self.memory_kernel_group = memory_kernel_group\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_cluster_labels(self, kernel_label: str) -> Tuple[np.ndarray, int]:\n",
       "    \"\"\"\n",
       "        Get the cluster labels for the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "            kernel_label (str): The label of the kernel.\n",
       "\n",
       "        Returns:\n",
       "            Tuple[np.ndarray, int]: A tuple containing an array of cluster labels and the number of clusters.\n",
       "        \"\"\"\n",
       "    paths = self.memory_kernel_group.path_group[kernel_label]\n",
       "    num_clusters = len(paths)\n",
       "    cluster_labels = np.empty(\n",
       "        len(\n",
       "            self.memory_kernel_group.memory_kernel_dict[\n",
       "                kernel_label\n",
       "            ].node_embeddings\n",
       "        ),\n",
       "        dtype=int,\n",
       "    )\n",
       "\n",
       "    for cluster_index, path in enumerate(paths):\n",
       "        cluster_labels[path] = cluster_index\n",
       "\n",
       "    return cluster_labels, num_clusters\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def compute_nmi(self, kernel_label1: str, kernel_label2: str) -> float:\n",
       "    \"\"\"\n",
       "        Compute the normalized mutual information (NMI) between two kernel by labels.\n",
       "\n",
       "        Args:\n",
       "            kernel_label1 (str): The first kernel label.\n",
       "            kernel_label2 (str): The second kernel label.\n",
       "\n",
       "        Returns:\n",
       "            float: The NMI value between the two kernel labels.\n",
       "        \"\"\"\n",
       "    cluster_labels1, _ = self.get_cluster_labels(kernel_label1)\n",
       "    cluster_labels2, _ = self.get_cluster_labels(kernel_label2)\n",
       "    nmi = normalized_mutual_info_score(cluster_labels1, cluster_labels2)\n",
       "    return nmi\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def evaluate_stability(self) -> float:\n",
       "    \"\"\"\n",
       "        Evaluate the stability of the MemoryKernelGroup by calculating the average NMI between all pairs of kernels.\n",
       "\n",
       "        Returns:\n",
       "            float: The stability score of the MemoryKernelGroup.\n",
       "        \"\"\"\n",
       "    kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\n",
       "    pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\n",
       "    nmi_sum = 0\n",
       "\n",
       "    for kernel_label1, kernel_label2 in pairwise_combinations:\n",
       "        nmi = self.compute_nmi(kernel_label1, kernel_label2)\n",
       "        nmi_sum += nmi\n",
       "\n",
       "    stability_score = nmi_sum / len(pairwise_combinations)\n",
       "    return stability_score\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import itertools\n",
       "from typing import List, Tuple, Dict\n",
       "\n",
       "import hdbscan\n",
       "import numpy as np\n",
       "import scipy\n",
       "import umap.umap_ as umap\n",
       "from numpy.linalg import svd\n",
       "from scipy.linalg import solve_sylvester\n",
       "from scipy.spatial.distance import cosine\n",
       "from sklearn.cluster import SpectralClustering\n",
       "from sklearn.metrics import normalized_mutual_info_score\n",
       "from sklearn.neighbors import KernelDensity\n",
       "from tqdm import tqdm\n",
       "\n",
       "from babydragon.chat.chat import Chat\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "from babydragon.tasks.llm_task import LLMWriter\n",
       "\n",
       "\n",
       "class MemoryKernel(MemoryIndex):\n",
       "    def __init__(self, mem_index, name=\"memory_kernel\", k=2, save_path=None):\n",
       "        \"\"\"\n",
       "        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\n",
       "\n",
       "        Args:\n",
       "            mem_index (MemoryIndex): A MemoryIndex instance.\n",
       "            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\n",
       "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
       "            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\n",
       "        \"\"\"\n",
       "        super().__init__(\n",
       "            index=mem_index.index,\n",
       "            values=mem_index.values,\n",
       "            embeddings=mem_index.embeddings,\n",
       "            name=name,\n",
       "            save_path=save_path,\n",
       "        )\n",
       "        self.k = k\n",
       "        self.create_k_hop_index(k=k)\n",
       "\n",
       "    def cos_sim(self, a, b):\n",
       "        \"\"\"\n",
       "        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
       "        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
       "        \"\"\"\n",
       "        if not isinstance(a, np.ndarray):\n",
       "            a = np.array(a)\n",
       "\n",
       "        if not isinstance(b, np.ndarray):\n",
       "            b = np.array(b)\n",
       "\n",
       "        if len(a.shape) == 1:\n",
       "            a = a[np.newaxis, :]\n",
       "\n",
       "        if len(b.shape) == 1:\n",
       "            b = b[np.newaxis, :]\n",
       "\n",
       "        a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\n",
       "        b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\n",
       "        return np.dot(a_norm, b_norm.T)\n",
       "\n",
       "    def compute_kernel(\n",
       "        self, embedding_set, threshold=0.65, use_softmax=False, cos_sim_batch=True\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Compute the adjacency matrix of the graph.\n",
       "\n",
       "        Parameters:\n",
       "        embedding_set (numpy array): The embedding matrix of the nodes.\n",
       "        threshold (float): The threshold for the adjacency matrix.\n",
       "        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\n",
       "        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\n",
       "\n",
       "        Returns:\n",
       "        adj_matrix (numpy array): The adjacency matrix of the graph.\n",
       "        \"\"\"\n",
       "\n",
       "        A = self.cos_sim(embedding_set, embedding_set)\n",
       "        if use_softmax:\n",
       "            # softmax\n",
       "            A = np.exp(A)\n",
       "            A = A / np.sum(A, axis=1)[:, np.newaxis]\n",
       "        adj_matrix = np.zeros_like(A)\n",
       "        adj_matrix[A > threshold] = 1\n",
       "        adj_matrix[A <= threshold] = 0\n",
       "        adj_matrix = adj_matrix.astype(np.float32)\n",
       "        return adj_matrix\n",
       "\n",
       "    def k_hop_message_passing(self, A, node_features, k):\n",
       "        \"\"\"\n",
       "        Compute the k-hop adjacency matrix and aggregated features using message passing.\n",
       "\n",
       "        Parameters:\n",
       "        A (numpy array): The adjacency matrix of the graph.\n",
       "        node_features (numpy array): The feature matrix of the nodes.\n",
       "        k (int): The number of hops for message passing.\n",
       "\n",
       "        Returns:\n",
       "        A_k (numpy array): The k-hop adjacency matrix.\n",
       "        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n",
       "        \"\"\"\n",
       "\n",
       "        print(\"Compute the k-hop adjacency matrix\")\n",
       "        A_k = np.linalg.matrix_power(A, k)\n",
       "\n",
       "        print(\"Aggregate the messages from the k-hop neighborhood:\")\n",
       "        agg_features = node_features.copy()\n",
       "\n",
       "        for i in tqdm(range(k)):\n",
       "            agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n",
       "\n",
       "        return A_k, agg_features\n",
       "\n",
       "    def graph_sylvester_embedding(self, G, m: int, ts: np.ndarray) -> np.ndarray:\n",
       "        \"\"\"\n",
       "        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\n",
       "\n",
       "        Args:\n",
       "            G (Tuple): A tuple containing the graph's vertices (V) and weights (W).\n",
       "            m (int): The number of singular values to consider.\n",
       "            ts (np.ndarray): The spectral scales.\n",
       "\n",
       "        Returns:\n",
       "            np.ndarray: The node_embeddings matrix.\n",
       "        \"\"\"\n",
       "        V, W = G\n",
       "        n = len(V)\n",
       "        D_BE = np.diag(W.sum(axis=1))\n",
       "        L_BE = np.identity(n) - np.dot(\n",
       "            np.diag(1 / np.sqrt(D_BE.diagonal())),\n",
       "            np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n",
       "        )\n",
       "\n",
       "        A = W\n",
       "        B = L_BE\n",
       "        C = np.identity(n)\n",
       "        X = solve_sylvester(A, B, C)\n",
       "\n",
       "        U, S, Vh = svd(X, full_matrices=False)\n",
       "        U_m = U[:, :m]\n",
       "        S_m = S[:m]\n",
       "\n",
       "        node_embeddings = np.zeros((n, m))\n",
       "\n",
       "        for i in range(n):\n",
       "            for s in range(m):\n",
       "                # Spectral kernel descriptor\n",
       "                node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n",
       "\n",
       "        return node_embeddings\n",
       "\n",
       "    def gen_gse_embeddings(self, A, embeddings, m: int = 7):\n",
       "        \"\"\"\n",
       "        Generate Graph Sylvester Embeddings.\n",
       "\n",
       "        Args:\n",
       "            A (np.ndarray): The adjacency matrix of the graph.\n",
       "            embeddings (np.ndarray): The original node embeddings.\n",
       "            m (int, optional): The number of spectral scales. Defaults to 7.\n",
       "\n",
       "        Returns:\n",
       "            np.ndarray: The generated Graph Sylvester Embeddings.\n",
       "        \"\"\"\n",
       "        V = list(range(len(embeddings)))\n",
       "        W = A\n",
       "\n",
       "        G = (V, W)\n",
       "        ts = np.linspace(0, 1, m)  # equally spaced scales\n",
       "\n",
       "        gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\n",
       "        return gse_embeddings\n",
       "\n",
       "    def create_k_hop_index(self, k=2):\n",
       "        \"\"\"\n",
       "        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\n",
       "        aggregated features, and updating the memory index.\n",
       "\n",
       "        Args:\n",
       "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
       "        \"\"\"\n",
       "        self.k = k\n",
       "        print(\"Computing the adjacency matrix\")\n",
       "        print(\"Embeddings shape: \", self.embeddings.shape)\n",
       "        self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n",
       "        print(\"Computing the k-hop adjacency matrix and aggregated features\")\n",
       "        self.A_k, self.node_embeddings = self.k_hop_message_passing(\n",
       "            self.A, self.embeddings, k\n",
       "        )\n",
       "        print(\"Updating the memory index\")\n",
       "        self.k_hop_index = MemoryIndex(name=self.name)\n",
       "        self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\n",
       "\n",
       "\n",
       "class MemoryKernelGroup(MemoryKernel):\n",
       "    def __init__(self, memory_kernel_dict: Dict[str, MemoryKernel], name=\"memory_kernel_group\"):\n",
       "        \"\"\"\n",
       "        Initialize the MemoryKernelGroup with a dictionary of MemoryKernel instances.\n",
       "\n",
       "        Args:\n",
       "            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\n",
       "            name (str, optional): The name of the MemoryKernelGroup. Defaults to \"memory_kernel_group\".\n",
       "        \"\"\"\n",
       "        self.memory_kernel_dict = memory_kernel_dict\n",
       "        self.name = name\n",
       "\n",
       "    def create_paths_hdbscan(\n",
       "        self, embeddings: np.ndarray, num_clusters: int\n",
       "    ) -> List[List[int]]:\n",
       "        \"\"\"\n",
       "        Create paths using the HDBSCAN clustering algorithm.\n",
       "\n",
       "        Args:\n",
       "            embeddings (np.ndarray): The embeddings to be clustered.\n",
       "            num_clusters (int): The minimum number of clusters.\n",
       "\n",
       "        Returns:\n",
       "            List[List[int]]: A list of lists containing the clustered paths.\n",
       "        \"\"\"\n",
       "        clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n",
       "        cluster_assignments = clusterer.fit_predict(embeddings)\n",
       "\n",
       "        paths = [[] for _ in range(num_clusters)]\n",
       "        for i, cluster in enumerate(cluster_assignments):\n",
       "            paths[cluster].append(i)\n",
       "        paths = [path for path in paths if path]\n",
       "        return paths\n",
       "\n",
       "    def create_paths_spectral_clustering(\n",
       "        self, embeddings: np.ndarray, num_clusters: int\n",
       "    ) -> List[List[int]]:\n",
       "        \"\"\"\n",
       "        Create paths using the spectral clustering algorithm.\n",
       "\n",
       "        Args:\n",
       "            embeddings (np.ndarray): The embeddings to be clustered.\n",
       "            num_clusters (int): The number of clusters.\n",
       "\n",
       "        Returns:\n",
       "            List[List[int]]: A list of lists containing the clustered paths.\n",
       "        \"\"\"\n",
       "        spectral_clustering = SpectralClustering(\n",
       "            n_clusters=num_clusters, affinity=\"nearest_neighbors\", random_state=42\n",
       "        )\n",
       "        cluster_assignments = spectral_clustering.fit_predict(embeddings)\n",
       "\n",
       "        paths = [[] for _ in range(num_clusters)]\n",
       "        for i, cluster in enumerate(cluster_assignments):\n",
       "            paths[cluster].append(i)\n",
       "        paths = [path for path in paths if path]\n",
       "        return paths\n",
       "\n",
       "    def calc_shgo_mode(self, scores: List[float]) -> float:\n",
       "        \"\"\"\n",
       "        Calculate the mode of the given scores using the SHGO optimization algorithm.\n",
       "\n",
       "        Args:\n",
       "            scores (List[float]): The scores for which the mode is to be calculated.\n",
       "\n",
       "        Returns:\n",
       "            float: The mode of the given scores.\n",
       "        \"\"\"\n",
       "        def objective(x):\n",
       "            return -self.estimate_pdf(scores)(x)\n",
       "\n",
       "        bounds = [(min(scores), max(scores))]\n",
       "        result = scipy.optimize.shgo(objective, bounds)\n",
       "        return result.x\n",
       "\n",
       "    def estimate_pdf(self, scores: List[float]) -> callable:\n",
       "        \"\"\"\n",
       "        Estimate the probability density function of the given scores.\n",
       "\n",
       "        Args:\n",
       "            scores (List[float]): The scores for which the PDF is to be estimated.\n",
       "\n",
       "        Returns:\n",
       "            callable: A callable object representing the PDF.\n",
       "        \"\"\"\n",
       "        pdf = scipy.stats.gaussian_kde(scores)\n",
       "        return pdf\n",
       "\n",
       "    def print_path(self, kernel_label: str, path: List[int]) -> None:\n",
       "        \"\"\"\n",
       "        Print the path for the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "            kernel_label (str): The label of the kernel.\n",
       "            path (List[int]): The path to be printed.\n",
       "        \"\"\"\n",
       "        for i in path:\n",
       "            print(self.memory_kernel_dict[kernel_label].values[i])\n",
       "\n",
       "    def sort_paths_by_mode_distance(\n",
       "        self, kernel_label: str, distance_metric: str = \"cosine\"\n",
       "    )-> None:\n",
       "        \"\"\"\n",
       "        Sort paths by the mode distance of the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "            kernel_label (str): The label of the kernel.\n",
       "            distance_metric (str, optional): The distance metric to be used. Defaults to \"cosine\".\n",
       "        \"\"\"\n",
       "        paths = self.path_group[kernel_label]\n",
       "        memory_kernel = self.memory_kernel_dict[kernel_label]\n",
       "        sorted_paths = []\n",
       "        for i, path in enumerate(paths):\n",
       "            cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
       "            cluster_embeddings = np.array(cluster_embeddings)\n",
       "            cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
       "            if distance_metric == \"cosine\" or distance_metric == \"guassian\":\n",
       "                scores = [\n",
       "                    (i, cosine(cluster_mean, emb))\n",
       "                    for i, emb in zip(path, cluster_embeddings)\n",
       "                ]\n",
       "            elif distance_metric == \"euclidean\":\n",
       "                scores = [\n",
       "                    (i, np.linalg.norm(cluster_mean - emb))\n",
       "                    for i, emb in zip(path, cluster_embeddings)\n",
       "                ]\n",
       "            score_values = [score for _, score in scores]  # Extract score values\n",
       "            mu = self.calc_shgo_mode(score_values)\n",
       "            sigma = np.std(score_values)\n",
       "            if distance_metric == \"guassian\":\n",
       "                scores = [\n",
       "                    (i, np.exp(-((x - mu) ** 2) / (2 * sigma**2))) for i, x in scores\n",
       "                ]\n",
       "            # Sort path by score\n",
       "            sorted_path_and_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
       "            sorted_path = [x[0] for x in sorted_path_and_scores]\n",
       "            sorted_paths.append(sorted_path)\n",
       "        self.path_group[kernel_label] = sorted_paths\n",
       "\n",
       "    def sort_paths_by_kernel_density(\n",
       "        self, kernel_label: str, distance_metric: str = \"cosine\"\n",
       "    ) -> None:\n",
       "        \"\"\"\n",
       "        Sort paths by the mode distance of the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "            kernel_label (str): The label of the kernel.\n",
       "            distance_metric (str, optional): The distance metric to be used. Defaults to \"cosine\".\n",
       "        \"\"\"\n",
       "        paths = self.path_group[kernel_label]\n",
       "        memory_kernel = self.memory_kernel_dict[kernel_label]\n",
       "        sorted_paths = []\n",
       "        for i, path in enumerate(paths):\n",
       "            cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
       "            cluster_embeddings = np.array(cluster_embeddings)\n",
       "            cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
       "            if distance_metric == \"cosine\":\n",
       "                scores = [\n",
       "                    (i, cosine(cluster_mean, emb))\n",
       "                    for i, emb in zip(path, cluster_embeddings)\n",
       "                ]\n",
       "            elif distance_metric == \"euclidean\":\n",
       "                scores = [\n",
       "                    (i, np.linalg.norm(cluster_mean - emb))\n",
       "                    for i, emb in zip(path, cluster_embeddings)\n",
       "                ]\n",
       "            score_values = [score for _, score in scores]  # Extract score values\n",
       "\n",
       "            # Estimate PDF using Kernel Density Estimation\n",
       "            kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(\n",
       "                np.array(score_values).reshape(-1, 1)\n",
       "            )\n",
       "            kde_scores = [kde.score_samples([[x]])[0] for _, x in scores]\n",
       "\n",
       "            # Sort path by score\n",
       "            sorted_path_and_scores = sorted(\n",
       "                zip(path, kde_scores), key=lambda x: x[1], reverse=True\n",
       "            )\n",
       "            sorted_path = [x[0] for x in sorted_path_and_scores]\n",
       "            sorted_paths.append(sorted_path)\n",
       "        self.path_group[kernel_label] = sorted_paths\n",
       "\n",
       "    def gen_index_aligned_kernel(\n",
       "        self, chatbot: Chat, parent_kernel_label: str, child_kernel_label: str\n",
       "    ) -> None:\n",
       "        \"\"\"\n",
       "        Generate an index-aligned kernel using LLMWriter for the given parent and child kernel labels.\n",
       "\n",
       "        Args:\n",
       "            chatbot (Chat): The Chat instance.\n",
       "            parent_kernel_label (str): The label of the parent kernel.\n",
       "            child_kernel_label (str): The label of the child kernel.\n",
       "        \"\"\"\n",
       "        llm_writer = LLMWriter(\n",
       "            index=self.memory_kernel_dict[parent_kernel_label],\n",
       "            path=self.path_group[parent_kernel_label],\n",
       "            chatbot=chatbot,\n",
       "            write_func=None,\n",
       "            max_workers=1,\n",
       "        )\n",
       "        new_index = llm_writer.write()\n",
       "        new_memory_kernel = MemoryKernel(mem_index=new_index, name=child_kernel_label)\n",
       "        new_memory_kernel.create_k_hop_index(k=2)\n",
       "        self.memory_kernel_dict[child_kernel_label] = new_memory_kernel\n",
       "\n",
       "    def generate_path_groups(self, method: str = \"hdbscan\") -> None:\n",
       "        \"\"\"\n",
       "        Generate path groups for all memory kernels in the memory_kernel_dict using the specified clustering method.\n",
       "\n",
       "        Args:\n",
       "            method (str, optional): The clustering method to be used. Defaults to \"hdbscan\".\n",
       "        \"\"\"\n",
       "        path_group = {}\n",
       "        for k, v in self.memory_kernel_dict.items():\n",
       "            embeddings = v.node_embeddings\n",
       "            num_clusters = int(np.sqrt(len(embeddings)))\n",
       "            if method == \"hdbscan\":\n",
       "                paths = self.create_paths_hdbscan(embeddings, num_clusters)\n",
       "            elif method == \"spectral_clustering\":\n",
       "                paths = self.create_paths_spectral_clustering(embeddings, num_clusters)\n",
       "            path_group[k] = paths\n",
       "\n",
       "        self.path_group = path_group\n",
       "\n",
       "    def batch_sort_kernel_group(self, kernel_label: str):\n",
       "        \"\"\"\n",
       "        Batch sort the kernel group by the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "        \"\"\"\n",
       "        if all(\n",
       "            [\n",
       "                v.node_embeddings.shape\n",
       "                == self.memory_kernel_dict[kernel_label].node_embeddings.shape\n",
       "                for k, v in self.memory_kernel_dict.items()\n",
       "            ]\n",
       "        ):\n",
       "            self.memory_kernel_sort(self.path_group[kernel_label])\n",
       "        else:\n",
       "            return ValueError(\"Not all kernels are of the same dimensions.\")\n",
       "\n",
       "    def memory_kernel_sort(self, paths: List[List[int]]):\n",
       "        pass\n",
       "\n",
       "    def is_kernel_group_isomorphic(self):\n",
       "        pass\n",
       "\n",
       "\n",
       "class MemoryKernelGroupStabilityAnalysis:\n",
       "    def __init__(self, memory_kernel_group: MemoryKernelGroup):\n",
       "        \"\"\"\n",
       "        Initialize the MemoryKernelGroupStabilityAnalysis with a MemoryKernelGroup instance.\n",
       "\n",
       "        Args:\n",
       "            memory_kernel_group (MemoryKernelGroup): A MemoryKernelGroup instance.\n",
       "        \"\"\"\n",
       "        self.memory_kernel_group = memory_kernel_group\n",
       "\n",
       "    def get_cluster_labels(self, kernel_label: str) -> Tuple[np.ndarray, int]:\n",
       "        \"\"\"\n",
       "        Get the cluster labels for the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "            kernel_label (str): The label of the kernel.\n",
       "\n",
       "        Returns:\n",
       "            Tuple[np.ndarray, int]: A tuple containing an array of cluster labels and the number of clusters.\n",
       "        \"\"\"\n",
       "        paths = self.memory_kernel_group.path_group[kernel_label]\n",
       "        num_clusters = len(paths)\n",
       "        cluster_labels = np.empty(\n",
       "            len(\n",
       "                self.memory_kernel_group.memory_kernel_dict[\n",
       "                    kernel_label\n",
       "                ].node_embeddings\n",
       "            ),\n",
       "            dtype=int,\n",
       "        )\n",
       "\n",
       "        for cluster_index, path in enumerate(paths):\n",
       "            cluster_labels[path] = cluster_index\n",
       "\n",
       "        return cluster_labels, num_clusters\n",
       "\n",
       "    def compute_nmi(self, kernel_label1: str, kernel_label2: str) -> float:\n",
       "        \"\"\"\n",
       "        Compute the normalized mutual information (NMI) between two kernel by labels.\n",
       "\n",
       "        Args:\n",
       "            kernel_label1 (str): The first kernel label.\n",
       "            kernel_label2 (str): The second kernel label.\n",
       "\n",
       "        Returns:\n",
       "            float: The NMI value between the two kernel labels.\n",
       "        \"\"\"\n",
       "        cluster_labels1, _ = self.get_cluster_labels(kernel_label1)\n",
       "        cluster_labels2, _ = self.get_cluster_labels(kernel_label2)\n",
       "        nmi = normalized_mutual_info_score(cluster_labels1, cluster_labels2)\n",
       "        return nmi\n",
       "\n",
       "    def evaluate_stability(self) -> float:\n",
       "        \"\"\"\n",
       "        Evaluate the stability of the MemoryKernelGroup by calculating the average NMI between all pairs of kernels.\n",
       "\n",
       "        Returns:\n",
       "            float: The stability score of the MemoryKernelGroup.\n",
       "        \"\"\"\n",
       "        kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\n",
       "        pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\n",
       "        nmi_sum = 0\n",
       "\n",
       "        for kernel_label1, kernel_label2 in pairwise_combinations:\n",
       "            nmi = self.compute_nmi(kernel_label1, kernel_label2)\n",
       "            nmi_sum += nmi\n",
       "\n",
       "        stability_score = nmi_sum / len(pairwise_combinations)\n",
       "        return stability_score\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def __init__(self, df: pd.DataFrame, row_func: Optional[Callable[[pd.Series], str]] = None, name= \"pandas_index\", columns: Optional[List[str]] = None):\n",
       "    \"\"\"\n",
       "        Initialize a PandasIndex object.\n",
       "        \n",
       "        Args:\n",
       "            df: A pandas DataFrame to index.\n",
       "            row_func: An optional function to process rows before adding them to the index.\n",
       "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
       "        \"\"\"\n",
       "    if row_func is None:\n",
       "        row_func = lambda row: str(row)\n",
       "    self.row_func = row_func\n",
       "\n",
       "    self.df = df\n",
       "    MemoryIndex.__init__(self,name=name) # Initialize the parent MemoryIndex class\n",
       "    \n",
       "    # Initialize the row-wise index\n",
       "    for _, row in df.iterrows():\n",
       "        self.add_to_index(row_func(row))\n",
       "    \n",
       "    self.columns: Dict[str, MemoryIndex] = {} \n",
       "\n",
       "    # Set up columns during initialization\n",
       "    if columns is None:\n",
       "        self.setup_columns()\n",
       "    else:\n",
       "        self.setup_columns(columns)\n",
       "    self.save()\n",
       "    for col in self.columns:\n",
       "        self.columns[col].save()\n",
       "    self.executed_tasks = []\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def setup_columns(self, columns: Optional[List[str]] = None):\n",
       "    \"\"\"\n",
       "        Set up columns for indexing.\n",
       "        \n",
       "        Args:\n",
       "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
       "        \"\"\"\n",
       "    if columns is None:\n",
       "        # Use string columns or columns with lists containing a single string by default\n",
       "        columns = [col for col in self.df.columns if self.df[col].apply(lambda x: isinstance(x, str) or (isinstance(x, list) and len(x) == 1 and isinstance(x[0], str))).all()]\n",
       "\n",
       "    for col in columns:\n",
       "        self.columns[col] = MemoryIndex.from_pandas(self.df, columns=col, name=f\"{self.name}_{col}\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def query_columns(self, query: str, columns: List[str]) -> List[Tuple[str, float]]:\n",
       "    \"\"\"\n",
       "        Query the indexed columns of the DataFrame.\n",
       "        \n",
       "        Args:\n",
       "            query: The search query as a string.\n",
       "            columns: A list of column names to query.\n",
       "        \n",
       "        Returns:\n",
       "            A list of tuples containing the matched value and its similarity score.\n",
       "        \"\"\"\n",
       "    results = []\n",
       "    for col in columns:\n",
       "        if col in self.columns:\n",
       "            results.extend(self.columns[col].faiss_query(query))\n",
       "        else:\n",
       "            raise KeyError(f\"Column '{col}' not found in PandaDb columns dictionary.\")\n",
       "    return results\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def add_row(self, row: pd.Series) -> None:\n",
       "    \"\"\"\n",
       "        Add a row to the DataFrame and update the row and column indexes.\n",
       "\n",
       "        Args:\n",
       "            row: A pandas Series representing the row to add.\n",
       "        \"\"\"\n",
       "    self.df = pd.concat([self.df, row.to_frame().T], ignore_index=True)\n",
       "    self.add_to_index(self.row_func(row))\n",
       "\n",
       "    for col in self.columns:\n",
       "        if col in row:\n",
       "            self.columns[col].add_to_index(row[col])\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "def remove_row(self, index: int) -> None:\n",
       "    \"\"\"\n",
       "        Remove a row from the DataFrame and update the row and column indexes.\n",
       "\n",
       "        Args:\n",
       "            index: The index of the row to remove.\n",
       "        \"\"\"\n",
       "    if 0 <= index < len(self.df):\n",
       "        self.remove_from_index(self.values[index])\n",
       "\n",
       "        for col in self.columns:\n",
       "            self.columns[col].remove_from_index(self.columns[col].values[index])\n",
       "\n",
       "        self.df.drop(index, inplace=True)\n",
       "        self.df.reset_index(drop=True, inplace=True)\n",
       "    else:\n",
       "        raise IndexError(f\"Index {index} is out of bounds for DataFrame with length {len(self.df)}\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value     \n",
       "def rows_from_value(self, value: Union[str, int, float], column: Optional[str] = None) -> pd.DataFrame:\n",
       "    \"\"\"\n",
       "        Return all rows of the DataFrame that have a particular value in the row index or a column index.\n",
       "\n",
       "        Args:\n",
       "            value: The value to search for in the DataFrame.\n",
       "            column: The name of the column to search in. If None, search in the row index.\n",
       "\n",
       "        Returns:\n",
       "            A pandas DataFrame containing the rows with the specified value.\n",
       "        \"\"\"\n",
       "    if column is None:\n",
       "        return self.df.loc[self.df.index == value]\n",
       "    else:\n",
       "        if column in self.df.columns:\n",
       "            return self.df.loc[self.df[column] == value]\n",
       "        else:\n",
       "            raise KeyError(f\"Column '{column}' not found in the DataFrame.\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def apply_llmtask(self, path: List[List[int]], chatbot: Chat, write_func= None, columns: Optional[List[str]] = None) -> pd.DataFrame:\n",
       "    \"\"\"\n",
       "        Apply a writing task to the specified columns or the main index, and create new modified indexes and a corresponding DataFrame with new values.\n",
       "\n",
       "        Args:\n",
       "            write_task: An instance of a writing task (subclass of BaseTask).\n",
       "            columns: A list of column names to apply the writing task to, or None (default) to apply the task to the main index.\n",
       "\n",
       "        Returns:\n",
       "            A pandas DataFrame containing the modified values in the specified columns or a new column with the modified values of the main index.\n",
       "        \"\"\"\n",
       "    modified_df = self.df.copy()\n",
       "    \n",
       "\n",
       "    if columns is None:\n",
       "        # Apply the writing task to the main index\n",
       "        write_index = self\n",
       "        write_task = LLMWriter(write_index, path, chatbot, write_func=write_func, context= self.df)\n",
       "        \n",
       "        new_index = write_task.write()\n",
       "\n",
       "        # Create a mapping of old values to new values\n",
       "        old_to_new_values = dict(zip(self.values, new_index.values))\n",
       "\n",
       "        # Update the row values in the modified DataFrame\n",
       "        modified_df['new_column'] = modified_df.apply(lambda row: old_to_new_values.get(self.row_func(row), self.row_func(row)), axis=1)\n",
       "    else:\n",
       "        # Iterate over the specified columns\n",
       "        for col in columns:\n",
       "            if col in self.columns:\n",
       "                # Apply the writing task to the column\n",
       "                write_index = self.columns[col]\n",
       "                write_task = LLMWriter(write_index, path, chatbot)\n",
       "                new_index = write_task.write()\n",
       "\n",
       "                # Create a mapping of old values to new values\n",
       "                old_to_new_values = dict(zip(self.columns[col].values, new_index.values))\n",
       "\n",
       "                # Update the column values in the modified DataFrame\n",
       "                modified_df[col] = modified_df[col].apply(lambda x: old_to_new_values.get(x, x))\n",
       "\n",
       "                # Update the column's MemoryIndex\n",
       "                self.columns[col] = new_index\n",
       "                self.columns[col].save()\n",
       "            else:\n",
       "                raise KeyError(f\"Column '{col}' not found in PandasIndex columns dictionary.\")\n",
       "        #remove context from the write_task to avoid memory leak\n",
       "    write_task.context = None\n",
       "    self.executed_tasks.append({\"task\": write_task, \"output\": modified_df})\n",
       "    \n",
       "    return modified_df\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "from typing import Callable, List, Optional, Tuple, Dict, Union\n",
       "\n",
       "import pandas as pd\n",
       "\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "from babydragon.chat.chat import Chat\n",
       "\n",
       "from babydragon.tasks.llm_task import LLMWriter\n",
       "\n",
       "class PandasIndex(MemoryIndex):\n",
       "    \"\"\"\n",
       "    A class to create an index of a pandas DataFrame, allowing querying on specified columns.\n",
       "    Inherits from MemoryIndex class.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, df: pd.DataFrame, row_func: Optional[Callable[[pd.Series], str]] = None, name= \"pandas_index\", columns: Optional[List[str]] = None):\n",
       "        \"\"\"\n",
       "        Initialize a PandasIndex object.\n",
       "        \n",
       "        Args:\n",
       "            df: A pandas DataFrame to index.\n",
       "            row_func: An optional function to process rows before adding them to the index.\n",
       "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
       "        \"\"\"\n",
       "        if row_func is None:\n",
       "            row_func = lambda row: str(row)\n",
       "        self.row_func = row_func\n",
       "\n",
       "        self.df = df\n",
       "        MemoryIndex.__init__(self,name=name) # Initialize the parent MemoryIndex class\n",
       "        \n",
       "        # Initialize the row-wise index\n",
       "        for _, row in df.iterrows():\n",
       "            self.add_to_index(row_func(row))\n",
       "        \n",
       "        self.columns: Dict[str, MemoryIndex] = {} \n",
       "\n",
       "        # Set up columns during initialization\n",
       "        if columns is None:\n",
       "            self.setup_columns()\n",
       "        else:\n",
       "            self.setup_columns(columns)\n",
       "        self.save()\n",
       "        for col in self.columns:\n",
       "            self.columns[col].save()\n",
       "        self.executed_tasks = []\n",
       "\n",
       "    def setup_columns(self, columns: Optional[List[str]] = None):\n",
       "        \"\"\"\n",
       "        Set up columns for indexing.\n",
       "        \n",
       "        Args:\n",
       "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
       "        \"\"\"\n",
       "        if columns is None:\n",
       "            # Use string columns or columns with lists containing a single string by default\n",
       "            columns = [col for col in self.df.columns if self.df[col].apply(lambda x: isinstance(x, str) or (isinstance(x, list) and len(x) == 1 and isinstance(x[0], str))).all()]\n",
       "\n",
       "        for col in columns:\n",
       "            self.columns[col] = MemoryIndex.from_pandas(self.df, columns=col, name=f\"{self.name}_{col}\")\n",
       "\n",
       "    def query_columns(self, query: str, columns: List[str]) -> List[Tuple[str, float]]:\n",
       "        \"\"\"\n",
       "        Query the indexed columns of the DataFrame.\n",
       "        \n",
       "        Args:\n",
       "            query: The search query as a string.\n",
       "            columns: A list of column names to query.\n",
       "        \n",
       "        Returns:\n",
       "            A list of tuples containing the matched value and its similarity score.\n",
       "        \"\"\"\n",
       "        results = []\n",
       "        for col in columns:\n",
       "            if col in self.columns:\n",
       "                results.extend(self.columns[col].faiss_query(query))\n",
       "            else:\n",
       "                raise KeyError(f\"Column '{col}' not found in PandaDb columns dictionary.\")\n",
       "        return results\n",
       "    \n",
       "    def add_row(self, row: pd.Series) -> None:\n",
       "        \"\"\"\n",
       "        Add a row to the DataFrame and update the row and column indexes.\n",
       "\n",
       "        Args:\n",
       "            row: A pandas Series representing the row to add.\n",
       "        \"\"\"\n",
       "        self.df = pd.concat([self.df, row.to_frame().T], ignore_index=True)\n",
       "        self.add_to_index(self.row_func(row))\n",
       "\n",
       "        for col in self.columns:\n",
       "            if col in row:\n",
       "                self.columns[col].add_to_index(row[col])\n",
       "\n",
       "\n",
       "    def remove_row(self, index: int) -> None:\n",
       "        \"\"\"\n",
       "        Remove a row from the DataFrame and update the row and column indexes.\n",
       "\n",
       "        Args:\n",
       "            index: The index of the row to remove.\n",
       "        \"\"\"\n",
       "        if 0 <= index < len(self.df):\n",
       "            self.remove_from_index(self.values[index])\n",
       "\n",
       "            for col in self.columns:\n",
       "                self.columns[col].remove_from_index(self.columns[col].values[index])\n",
       "\n",
       "            self.df.drop(index, inplace=True)\n",
       "            self.df.reset_index(drop=True, inplace=True)\n",
       "        else:\n",
       "            raise IndexError(f\"Index {index} is out of bounds for DataFrame with length {len(self.df)}\")\n",
       "    \n",
       "    def rows_from_value(self, value: Union[str, int, float], column: Optional[str] = None) -> pd.DataFrame:\n",
       "        \"\"\"\n",
       "        Return all rows of the DataFrame that have a particular value in the row index or a column index.\n",
       "\n",
       "        Args:\n",
       "            value: The value to search for in the DataFrame.\n",
       "            column: The name of the column to search in. If None, search in the row index.\n",
       "\n",
       "        Returns:\n",
       "            A pandas DataFrame containing the rows with the specified value.\n",
       "        \"\"\"\n",
       "        if column is None:\n",
       "            return self.df.loc[self.df.index == value]\n",
       "        else:\n",
       "            if column in self.df.columns:\n",
       "                return self.df.loc[self.df[column] == value]\n",
       "            else:\n",
       "                raise KeyError(f\"Column '{column}' not found in the DataFrame.\")\n",
       "\n",
       "    def apply_llmtask(self, path: List[List[int]], chatbot: Chat, write_func= None, columns: Optional[List[str]] = None) -> pd.DataFrame:\n",
       "        \"\"\"\n",
       "        Apply a writing task to the specified columns or the main index, and create new modified indexes and a corresponding DataFrame with new values.\n",
       "\n",
       "        Args:\n",
       "            write_task: An instance of a writing task (subclass of BaseTask).\n",
       "            columns: A list of column names to apply the writing task to, or None (default) to apply the task to the main index.\n",
       "\n",
       "        Returns:\n",
       "            A pandas DataFrame containing the modified values in the specified columns or a new column with the modified values of the main index.\n",
       "        \"\"\"\n",
       "        modified_df = self.df.copy()\n",
       "        \n",
       "\n",
       "        if columns is None:\n",
       "            # Apply the writing task to the main index\n",
       "            write_index = self\n",
       "            write_task = LLMWriter(write_index, path, chatbot, write_func=write_func, context= self.df)\n",
       "            \n",
       "            new_index = write_task.write()\n",
       "\n",
       "            # Create a mapping of old values to new values\n",
       "            old_to_new_values = dict(zip(self.values, new_index.values))\n",
       "\n",
       "            # Update the row values in the modified DataFrame\n",
       "            modified_df['new_column'] = modified_df.apply(lambda row: old_to_new_values.get(self.row_func(row), self.row_func(row)), axis=1)\n",
       "        else:\n",
       "            # Iterate over the specified columns\n",
       "            for col in columns:\n",
       "                if col in self.columns:\n",
       "                    # Apply the writing task to the column\n",
       "                    write_index = self.columns[col]\n",
       "                    write_task = LLMWriter(write_index, path, chatbot)\n",
       "                    new_index = write_task.write()\n",
       "\n",
       "                    # Create a mapping of old values to new values\n",
       "                    old_to_new_values = dict(zip(self.columns[col].values, new_index.values))\n",
       "\n",
       "                    # Update the column values in the modified DataFrame\n",
       "                    modified_df[col] = modified_df[col].apply(lambda x: old_to_new_values.get(x, x))\n",
       "\n",
       "                    # Update the column's MemoryIndex\n",
       "                    self.columns[col] = new_index\n",
       "                    self.columns[col].save()\n",
       "                else:\n",
       "                    raise KeyError(f\"Column '{col}' not found in PandasIndex columns dictionary.\")\n",
       "        #remove context from the write_task to avoid memory leak\n",
       "        write_task.context = None\n",
       "        self.executed_tasks.append({\"task\": write_task, \"output\": modified_df})\n",
       "        \n",
       "        return modified_df\n",
       "\n",
       "\n",
       "\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(\n",
       "    self,\n",
       "    directory_path: str,\n",
       "    name: str = \"python_index\",\n",
       "    save_path: Optional[str] = None,\n",
       "    load: bool = False,\n",
       "    minify_code: bool = False,\n",
       "    remove_docstrings: bool = False,\n",
       "    tokenizer: Optional[tiktoken.Encoding] = None,\n",
       "):\n",
       "    # Initialize the MemoryIndex\n",
       "    MemoryIndex.__init__(\n",
       "        self,\n",
       "        name=name,\n",
       "        save_path=save_path,\n",
       "        load=load,\n",
       "        tokenizer=tokenizer,\n",
       "    )\n",
       "    # Initialize the PythonParser\n",
       "    PythonParser.__init__(\n",
       "        self,\n",
       "        directory_path=directory_path,\n",
       "        minify_code=minify_code,\n",
       "        remove_docstrings=remove_docstrings,\n",
       "    )\n",
       "\n",
       "    if not load:\n",
       "        # Extract functions and classes source code\n",
       "        function_source_codes, class_source_codes, _, _ = self.process_directory()\n",
       "        print(\n",
       "            \"Indexing {} functions and {} classes\".format(\n",
       "                len(function_source_codes), len(class_source_codes)\n",
       "            )\n",
       "        )\n",
       "        # Concatenate function and class source code and index them\n",
       "        codes = function_source_codes + class_source_codes\n",
       "        for code in codes:\n",
       "            self.add_to_index(code)\n",
       "\n",
       "        self.save()\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value from typing import Optional\n",
       "\n",
       "import tiktoken\n",
       "\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "from babydragon.processors.parsers.python_parser import PythonParser\n",
       "\n",
       "\n",
       "class PythonIndex(MemoryIndex, PythonParser):\n",
       "    def __init__(\n",
       "        self,\n",
       "        directory_path: str,\n",
       "        name: str = \"python_index\",\n",
       "        save_path: Optional[str] = None,\n",
       "        load: bool = False,\n",
       "        minify_code: bool = False,\n",
       "        remove_docstrings: bool = False,\n",
       "        tokenizer: Optional[tiktoken.Encoding] = None,\n",
       "    ):\n",
       "        # Initialize the MemoryIndex\n",
       "        MemoryIndex.__init__(\n",
       "            self,\n",
       "            name=name,\n",
       "            save_path=save_path,\n",
       "            load=load,\n",
       "            tokenizer=tokenizer,\n",
       "        )\n",
       "        # Initialize the PythonParser\n",
       "        PythonParser.__init__(\n",
       "            self,\n",
       "            directory_path=directory_path,\n",
       "            minify_code=minify_code,\n",
       "            remove_docstrings=remove_docstrings,\n",
       "        )\n",
       "\n",
       "        if not load:\n",
       "            # Extract functions and classes source code\n",
       "            function_source_codes, class_source_codes, _, _ = self.process_directory()\n",
       "            print(\n",
       "                \"Indexing {} functions and {} classes\".format(\n",
       "                    len(function_source_codes), len(class_source_codes)\n",
       "                )\n",
       "            )\n",
       "            # Concatenate function and class source code and index them\n",
       "            codes = function_source_codes + class_source_codes\n",
       "            for code in codes:\n",
       "                self.add_to_index(code)\n",
       "\n",
       "            self.save()\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value # This is the __init__.py file for the package.\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def __init__(\n",
       "    self,\n",
       "    name: str = \"memory\",\n",
       "    max_memory: Optional[int] = None,\n",
       "    tokenizer: Optional[Any] = None,\n",
       ") -> None:\n",
       "    \"\"\"\n",
       "        Initialize the BaseThread instance.\n",
       "\n",
       "        :param name: The name of the memory thread. Defaults to 'memory'.\n",
       "        :param max_memory: The maximum number of tokens allowed in the memory thread.\n",
       "                           Defaults to None, which means no limit.\n",
       "        :param tokenizer: The tokenizer to be used for tokenizing messages.\n",
       "                          Defaults to None, which means using the tiktoken encoding for the 'gpt-3.5-turbo' model.\n",
       "        \"\"\"\n",
       "    self.name = name\n",
       "    self.max_memory = max_memory\n",
       "    self.memory_thread = []\n",
       "    self.time_stamps = []\n",
       "    self.message_tokens = []\n",
       "    self.total_tokens = 0\n",
       "    if tokenizer is None:\n",
       "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def __getitem__(self, idx):\n",
       "    return self.memory_thread[idx]\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def __len__(self):\n",
       "    return len(self.memory_thread)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def reset_memory(self) -> None:\n",
       "    \"\"\"\n",
       "        Reset the memory thread.\n",
       "        \"\"\"\n",
       "    self.memory_thread = []\n",
       "    self.time_stamps = []\n",
       "    self.message_tokens = []\n",
       "    self.total_tokens = 0\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_message_tokens(self, message_dict: dict) -> int:\n",
       "    \"\"\"\n",
       "        Calculate the number of tokens in a message, including the role token.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        :return: The total number of tokens in the message.\n",
       "        \"\"\"\n",
       "    message_dict = check_dict(message_dict)\n",
       "    message = message_dict[\"content\"]\n",
       "    return len(self.tokenizer.encode(message)) + 6  # +6 for the role token\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_message_role(self, message_dict: dict) -> str:\n",
       "    \"\"\"\n",
       "        Get the role of the message from a message dictionary.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        :return: The role of the message.\n",
       "        \"\"\"\n",
       "    message_dict = check_dict(message_dict)\n",
       "    return message_dict[\"role\"]\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def add_message(self, message_dict: dict) -> None:\n",
       "    \"\"\"\n",
       "        Add a message to the memory thread.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        \"\"\"\n",
       "    message_tokens = self.get_message_tokens(message_dict)\n",
       "\n",
       "    if (\n",
       "        self.max_memory is None\n",
       "        or self.total_tokens + message_tokens <= self.max_memory\n",
       "    ):\n",
       "        # add the message_dict to the memory_thread\n",
       "        # update the total number of tokens\n",
       "        self.memory_thread.append(message_dict)\n",
       "        self.total_tokens += message_tokens\n",
       "        self.message_tokens.append(message_tokens)\n",
       "        time_stamp = time.time()\n",
       "        self.time_stamps.append(time_stamp)\n",
       "    else:\n",
       "        display(\n",
       "            Markdown(\n",
       "                \"The memory BaseThread is full, the last message was not added\"\n",
       "            )\n",
       "        )\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def remove_message(\n",
       "    self, message_dict: Union[dict, None] = None, idx: Union[int, None] = None\n",
       ") -> None:\n",
       "    \"\"\"\n",
       "        Remove a message from the memory thread.\n",
       "        \"\"\"\n",
       "    if message_dict is None and idx is None:\n",
       "        raise Exception(\"You need to provide either a message_dict or an idx\")\n",
       "    elif message_dict is not None and idx is not None:\n",
       "        raise Exception(\"You need to provide either a message_dict or an idx\")\n",
       "\n",
       "    if idx is None:\n",
       "        message_dict = check_dict(message_dict)\n",
       "        search_results = self.find_message(message_dict)\n",
       "        if search_results is not None:\n",
       "            idx = search_results[-1][\"idx\"]\n",
       "            message = search_results[-1][\"message_dict\"]\n",
       "            self.memory_thread.pop(idx)\n",
       "            self.message_tokens.pop(idx)\n",
       "            self.time_stamps.pop(idx)\n",
       "            self.total_tokens -= self.get_message_tokens(message)\n",
       "        else:\n",
       "            raise Exception(\"The message was not found in the memory BaseThread\")\n",
       "    else:\n",
       "        if idx < len(self.memory_thread):\n",
       "            message = self.memory_thread.pop(idx)\n",
       "            self.total_tokens -= self.get_message_tokens(message)\n",
       "        else:\n",
       "            raise Exception(\"The index was out bound\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def find_message(\n",
       "    self, message: Union[dict, str], role: Union[str, None] = None\n",
       ") -> Union[None, list]:\n",
       "    \"\"\"\n",
       "        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\n",
       "        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\n",
       "    # check if the message is a dictioanry or a string\n",
       "    message = message if isinstance(message, str) else check_dict(message)\n",
       "    search_results = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        target = (\n",
       "            message_dict if isinstance(message, dict) else message_dict[\"content\"]\n",
       "        )\n",
       "        if target == message and (role is None or message_dict[\"role\"] == role):\n",
       "            search_results.append({\"idx\": idx, \"message_dict\": message_dict})\n",
       "    return search_results if len(search_results) > 0 else None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def find_role(self, role: str) -> Union[None, list]:\n",
       "    \"\"\"\n",
       "        Find all messages with a specific role in the memory thread.\n",
       "        \"\"\"\n",
       "    search_results = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if message_dict[\"role\"] == role:\n",
       "            search_results.append({\"idx\": idx, \"message_dict\": message_dict})\n",
       "    return search_results if len(search_results) > 0 else None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def last_message(self, role: Union[str, None] = None) -> Union[None, dict]:\n",
       "    \"\"\"\n",
       "        Get the last message in the memory thread with a specific role.\"\"\"\n",
       "    if role is None:\n",
       "        return self.memory_thread[-1]\n",
       "    else:\n",
       "        for message_dict in reversed(self.memory_thread):\n",
       "            if message_dict[\"role\"] == role:\n",
       "                return message_dict\n",
       "        return None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def first_message(self, role: Union[str, None] = None) -> Union[None, dict]:\n",
       "    \"\"\"\n",
       "        Get the first message in the memory thread with a specific role.\"\"\"\n",
       "    if role is None:\n",
       "        return self.memory_thread[0]\n",
       "    else:\n",
       "        for message_dict in self.memory_thread:\n",
       "            if message_dict[\"role\"] == role:\n",
       "                return message_dict\n",
       "        return None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def messages_before(\n",
       "    self, message: dict, role: Union[str, None] = None\n",
       ") -> Union[None, list]:\n",
       "    \"\"\"\n",
       "        Get all messages before a specific message in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if message_dict == message and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            messages = self.memory_thread[:idx]\n",
       "            break\n",
       "    return messages if len(messages) > 0 else None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def messages_before(\n",
       "    self, message: dict, role: Union[str, None] = None\n",
       ") -> Union[None, list]:\n",
       "    \"\"\"\n",
       "        Get all messages after a specific message in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if message_dict == message and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            messages = self.memory_thread[idx + 1 :]\n",
       "            break\n",
       "    return messages if len(messages) > 0 else None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def messages_between(\n",
       "    self, start_message: dict, end_message: dict, role: Union[str, None] = None\n",
       ") -> Union[None, list]:\n",
       "    \"\"\"\n",
       "        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if message_dict == start_message and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            start_idx = idx\n",
       "            break\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if message_dict == end_message and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            end_idx = idx\n",
       "            break\n",
       "    messages = self.memory_thread[start_idx + 1 : end_idx]\n",
       "    return messages if len(messages) > 0 else None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def messages_more_tokens(self, tokens: int, role: Union[str, None] = None):\n",
       "    \"\"\"\n",
       "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if self.message_tokens[idx] > tokens and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "    return messages if len(messages) > 0 else None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def messages_less_tokens(self, tokens: int, role: Union[str, None] = None):\n",
       "    \"\"\"\n",
       "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if self.message_tokens[idx] < tokens and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "    return messages if len(messages) > 0 else None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def messages_between_tokens(\n",
       "    self, start_tokens: int, end_tokens: int, role: Union[str, None] = None\n",
       "):\n",
       "    \"\"\"\n",
       "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if (\n",
       "            self.message_tokens[idx] > start_tokens\n",
       "            and self.message_tokens[idx] < end_tokens\n",
       "            and (role is None or message_dict[\"role\"] == role)\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "    return messages if len(messages) > 0 else None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def messages_before_time(self, time_stamp, role: Union[str, None] = None):\n",
       "    \"\"\"\n",
       "        Get all messages before a specific time in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if self.time_stamps[idx] < time_stamp and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "    return messages if len(messages) > 0 else None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def messages_after_time(self, time_stamp, role: Union[str, None] = None):\n",
       "    \"\"\"\n",
       "        Get all messages after a specific time in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if self.time_stamps[idx] > time_stamp and (\n",
       "            role is None or message_dict[\"role\"] == role\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "    return messages if len(messages) > 0 else None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def messages_between_time(\n",
       "    self, start_time, end_time, role: Union[str, None] = None\n",
       "):\n",
       "    \"\"\"\n",
       "        Get all messages between two specific times in the memory thread with a specific role.\"\"\"\n",
       "    messages = []\n",
       "    for idx, message_dict in enumerate(self.memory_thread):\n",
       "        if (\n",
       "            self.time_stamps[idx] > start_time\n",
       "            and self.time_stamps[idx] < end_time\n",
       "            and (role is None or message_dict[\"role\"] == role)\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "    return messages if len(messages) > 0 else None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def token_bound_history(\n",
       "    self, max_tokens: int, max_history=None, role: Union[str, None] = None\n",
       "):\n",
       "    messages = []\n",
       "    indices = []\n",
       "    tokens = 0\n",
       "    if max_history is None:\n",
       "        max_history = len(self.memory_thread)\n",
       "\n",
       "    for idx, message_dict in enumerate(reversed(self.memory_thread)):\n",
       "        if (\n",
       "            tokens + self.message_tokens[idx] < max_tokens\n",
       "            and (role is None or message_dict[\"role\"] == role)\n",
       "            and idx < max_history\n",
       "        ):\n",
       "            messages.append(message_dict)\n",
       "            indices.append(len(self.memory_thread) - 1 - idx)\n",
       "            tokens += self.message_tokens[idx]\n",
       "        else:\n",
       "            break\n",
       "    return messages, indices if len(messages) > 0 else (None, None)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import time\n",
       "from typing import Any, Dict, Optional, Union\n",
       "\n",
       "import tiktoken\n",
       "from IPython.display import Markdown, display\n",
       "\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "from babydragon.utils.oai import check_dict, mark_question\n",
       "\n",
       "\n",
       "class BaseThread:\n",
       "    \"\"\"\n",
       "    This class is used to keep track of the memory thread of a conversation and the total number of tokens.\n",
       "    All conversation memories should subclass this class. If max_memory is None, it has\n",
       "    no limit to the number of tokens that can be stored in the memory thread.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        name: str = \"memory\",\n",
       "        max_memory: Optional[int] = None,\n",
       "        tokenizer: Optional[Any] = None,\n",
       "    ) -> None:\n",
       "        \"\"\"\n",
       "        Initialize the BaseThread instance.\n",
       "\n",
       "        :param name: The name of the memory thread. Defaults to 'memory'.\n",
       "        :param max_memory: The maximum number of tokens allowed in the memory thread.\n",
       "                           Defaults to None, which means no limit.\n",
       "        :param tokenizer: The tokenizer to be used for tokenizing messages.\n",
       "                          Defaults to None, which means using the tiktoken encoding for the 'gpt-3.5-turbo' model.\n",
       "        \"\"\"\n",
       "        self.name = name\n",
       "        self.max_memory = max_memory\n",
       "        self.memory_thread = []\n",
       "        self.time_stamps = []\n",
       "        self.message_tokens = []\n",
       "        self.total_tokens = 0\n",
       "        if tokenizer is None:\n",
       "            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
       "\n",
       "    def __getitem__(self, idx):\n",
       "        return self.memory_thread[idx]\n",
       "\n",
       "    def __len__(self):\n",
       "        return len(self.memory_thread)\n",
       "\n",
       "    def reset_memory(self) -> None:\n",
       "        \"\"\"\n",
       "        Reset the memory thread.\n",
       "        \"\"\"\n",
       "        self.memory_thread = []\n",
       "        self.time_stamps = []\n",
       "        self.message_tokens = []\n",
       "        self.total_tokens = 0\n",
       "\n",
       "    def get_message_tokens(self, message_dict: dict) -> int:\n",
       "        \"\"\"\n",
       "        Calculate the number of tokens in a message, including the role token.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        :return: The total number of tokens in the message.\n",
       "        \"\"\"\n",
       "        message_dict = check_dict(message_dict)\n",
       "        message = message_dict[\"content\"]\n",
       "        return len(self.tokenizer.encode(message)) + 6  # +6 for the role token\n",
       "\n",
       "    def get_message_role(self, message_dict: dict) -> str:\n",
       "        \"\"\"\n",
       "        Get the role of the message from a message dictionary.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        :return: The role of the message.\n",
       "        \"\"\"\n",
       "        message_dict = check_dict(message_dict)\n",
       "        return message_dict[\"role\"]\n",
       "\n",
       "    def add_message(self, message_dict: dict) -> None:\n",
       "        \"\"\"\n",
       "        Add a message to the memory thread.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        \"\"\"\n",
       "        message_tokens = self.get_message_tokens(message_dict)\n",
       "\n",
       "        if (\n",
       "            self.max_memory is None\n",
       "            or self.total_tokens + message_tokens <= self.max_memory\n",
       "        ):\n",
       "            # add the message_dict to the memory_thread\n",
       "            # update the total number of tokens\n",
       "            self.memory_thread.append(message_dict)\n",
       "            self.total_tokens += message_tokens\n",
       "            self.message_tokens.append(message_tokens)\n",
       "            time_stamp = time.time()\n",
       "            self.time_stamps.append(time_stamp)\n",
       "        else:\n",
       "            display(\n",
       "                Markdown(\n",
       "                    \"The memory BaseThread is full, the last message was not added\"\n",
       "                )\n",
       "            )\n",
       "\n",
       "    def remove_message(\n",
       "        self, message_dict: Union[dict, None] = None, idx: Union[int, None] = None\n",
       "    ) -> None:\n",
       "        \"\"\"\n",
       "        Remove a message from the memory thread.\n",
       "        \"\"\"\n",
       "        if message_dict is None and idx is None:\n",
       "            raise Exception(\"You need to provide either a message_dict or an idx\")\n",
       "        elif message_dict is not None and idx is not None:\n",
       "            raise Exception(\"You need to provide either a message_dict or an idx\")\n",
       "\n",
       "        if idx is None:\n",
       "            message_dict = check_dict(message_dict)\n",
       "            search_results = self.find_message(message_dict)\n",
       "            if search_results is not None:\n",
       "                idx = search_results[-1][\"idx\"]\n",
       "                message = search_results[-1][\"message_dict\"]\n",
       "                self.memory_thread.pop(idx)\n",
       "                self.message_tokens.pop(idx)\n",
       "                self.time_stamps.pop(idx)\n",
       "                self.total_tokens -= self.get_message_tokens(message)\n",
       "            else:\n",
       "                raise Exception(\"The message was not found in the memory BaseThread\")\n",
       "        else:\n",
       "            if idx < len(self.memory_thread):\n",
       "                message = self.memory_thread.pop(idx)\n",
       "                self.total_tokens -= self.get_message_tokens(message)\n",
       "            else:\n",
       "                raise Exception(\"The index was out bound\")\n",
       "\n",
       "    def find_message(\n",
       "        self, message: Union[dict, str], role: Union[str, None] = None\n",
       "    ) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\n",
       "        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\n",
       "        # check if the message is a dictioanry or a string\n",
       "        message = message if isinstance(message, str) else check_dict(message)\n",
       "        search_results = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            target = (\n",
       "                message_dict if isinstance(message, dict) else message_dict[\"content\"]\n",
       "            )\n",
       "            if target == message and (role is None or message_dict[\"role\"] == role):\n",
       "                search_results.append({\"idx\": idx, \"message_dict\": message_dict})\n",
       "        return search_results if len(search_results) > 0 else None\n",
       "\n",
       "    def find_role(self, role: str) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Find all messages with a specific role in the memory thread.\n",
       "        \"\"\"\n",
       "        search_results = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict[\"role\"] == role:\n",
       "                search_results.append({\"idx\": idx, \"message_dict\": message_dict})\n",
       "        return search_results if len(search_results) > 0 else None\n",
       "\n",
       "    def last_message(self, role: Union[str, None] = None) -> Union[None, dict]:\n",
       "        \"\"\"\n",
       "        Get the last message in the memory thread with a specific role.\"\"\"\n",
       "        if role is None:\n",
       "            return self.memory_thread[-1]\n",
       "        else:\n",
       "            for message_dict in reversed(self.memory_thread):\n",
       "                if message_dict[\"role\"] == role:\n",
       "                    return message_dict\n",
       "            return None\n",
       "\n",
       "    def first_message(self, role: Union[str, None] = None) -> Union[None, dict]:\n",
       "        \"\"\"\n",
       "        Get the first message in the memory thread with a specific role.\"\"\"\n",
       "        if role is None:\n",
       "            return self.memory_thread[0]\n",
       "        else:\n",
       "            for message_dict in self.memory_thread:\n",
       "                if message_dict[\"role\"] == role:\n",
       "                    return message_dict\n",
       "            return None\n",
       "\n",
       "    def messages_before(\n",
       "        self, message: dict, role: Union[str, None] = None\n",
       "    ) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Get all messages before a specific message in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict == message and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages = self.memory_thread[:idx]\n",
       "                break\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_before(\n",
       "        self, message: dict, role: Union[str, None] = None\n",
       "    ) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Get all messages after a specific message in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict == message and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages = self.memory_thread[idx + 1 :]\n",
       "                break\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_between(\n",
       "        self, start_message: dict, end_message: dict, role: Union[str, None] = None\n",
       "    ) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict == start_message and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                start_idx = idx\n",
       "                break\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict == end_message and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                end_idx = idx\n",
       "                break\n",
       "        messages = self.memory_thread[start_idx + 1 : end_idx]\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_more_tokens(self, tokens: int, role: Union[str, None] = None):\n",
       "        \"\"\"\n",
       "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if self.message_tokens[idx] > tokens and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_less_tokens(self, tokens: int, role: Union[str, None] = None):\n",
       "        \"\"\"\n",
       "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if self.message_tokens[idx] < tokens and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_between_tokens(\n",
       "        self, start_tokens: int, end_tokens: int, role: Union[str, None] = None\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if (\n",
       "                self.message_tokens[idx] > start_tokens\n",
       "                and self.message_tokens[idx] < end_tokens\n",
       "                and (role is None or message_dict[\"role\"] == role)\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_before_time(self, time_stamp, role: Union[str, None] = None):\n",
       "        \"\"\"\n",
       "        Get all messages before a specific time in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if self.time_stamps[idx] < time_stamp and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_after_time(self, time_stamp, role: Union[str, None] = None):\n",
       "        \"\"\"\n",
       "        Get all messages after a specific time in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if self.time_stamps[idx] > time_stamp and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_between_time(\n",
       "        self, start_time, end_time, role: Union[str, None] = None\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Get all messages between two specific times in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if (\n",
       "                self.time_stamps[idx] > start_time\n",
       "                and self.time_stamps[idx] < end_time\n",
       "                and (role is None or message_dict[\"role\"] == role)\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def token_bound_history(\n",
       "        self, max_tokens: int, max_history=None, role: Union[str, None] = None\n",
       "    ):\n",
       "        messages = []\n",
       "        indices = []\n",
       "        tokens = 0\n",
       "        if max_history is None:\n",
       "            max_history = len(self.memory_thread)\n",
       "\n",
       "        for idx, message_dict in enumerate(reversed(self.memory_thread)):\n",
       "            if (\n",
       "                tokens + self.message_tokens[idx] < max_tokens\n",
       "                and (role is None or message_dict[\"role\"] == role)\n",
       "                and idx < max_history\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "                indices.append(len(self.memory_thread) - 1 - idx)\n",
       "                tokens += self.message_tokens[idx]\n",
       "            else:\n",
       "                break\n",
       "        return messages, indices if len(messages) > 0 else (None, None)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def __init__(\n",
       "    self, name=\"fifo_memory\", max_memory=None, longterm_thread=None, redundant=True\n",
       "):\n",
       "\n",
       "    BaseThread.__init__(self, name=name, max_memory=None)\n",
       "    if redundant is True:\n",
       "        self.redundant_thread = BaseThread(name=\"lucid_memory\", max_memory=None)\n",
       "    else:\n",
       "        self.redundant_thread = None\n",
       "    if longterm_thread is None:\n",
       "        self.longterm_thread = BaseThread(name=\"longterm_memory\", max_memory=None)\n",
       "    else:\n",
       "        self.longterm_thread = longterm_thread\n",
       "    # create an alias for the memory_thread to make the code more readable\n",
       "    self.fifo_thread = self.memory_thread\n",
       "    self.max_memory = max_memory\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def to_longterm(self, idx: int):\n",
       "    \"\"\"move the message at the index idx to the longterm_memory\"\"\"\n",
       "    # move the message at the index idx to the longterm_memory\n",
       "    display(\n",
       "        Markdown(\n",
       "            \"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(\n",
       "                idx\n",
       "            )\n",
       "        )\n",
       "    )\n",
       "    message = copy.deepcopy(self.memory_thread[idx])\n",
       "    # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\n",
       "    self.longterm_thread.add_message(message)\n",
       "    self.remove_message(idx=idx)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def add_message(self, message_dict: dict):\n",
       "    \"\"\"add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\n",
       "    # message_dict = {\"role\": role, \"content\": content}\n",
       "    # chek that the message_dict is a dictionary or a list of dictionaries\n",
       "    message_dict = check_dict(message_dict)\n",
       "    if self.redundant_thread is not None:\n",
       "        self.redundant_thread.add_message(message_dict)\n",
       "    message_tokens = self.get_message_tokens(message_dict)\n",
       "\n",
       "    if self.total_tokens + message_tokens > self.max_memory:\n",
       "        while self.total_tokens + message_tokens > self.max_memory:\n",
       "            if len(self.memory_thread) > 0:\n",
       "                self.to_longterm(idx=0)\n",
       "        super().add_message(message_dict)\n",
       "\n",
       "    else:\n",
       "        # add the message_dict to the memory_thread\n",
       "        # update the total number of tokens\n",
       "        super().add_message(message_dict)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import copy\n",
       "\n",
       "from IPython.display import Markdown, display\n",
       "\n",
       "from babydragon.memory.threads.base_thread import BaseThread\n",
       "from babydragon.utils.oai import check_dict\n",
       "\n",
       "\n",
       "class FifoThread(BaseThread):\n",
       "    \"\"\"FIFO Memory BaseThread, the oldest messages are removed first when reaching the max_memory limit, the memory is defined in terms of tokens,\n",
       "    outs are passe to the longterm_memory, lucid_memory is a redundant memory that stores all the messages\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self, name=\"fifo_memory\", max_memory=None, longterm_thread=None, redundant=True\n",
       "    ):\n",
       "\n",
       "        BaseThread.__init__(self, name=name, max_memory=None)\n",
       "        if redundant is True:\n",
       "            self.redundant_thread = BaseThread(name=\"lucid_memory\", max_memory=None)\n",
       "        else:\n",
       "            self.redundant_thread = None\n",
       "        if longterm_thread is None:\n",
       "            self.longterm_thread = BaseThread(name=\"longterm_memory\", max_memory=None)\n",
       "        else:\n",
       "            self.longterm_thread = longterm_thread\n",
       "        # create an alias for the memory_thread to make the code more readable\n",
       "        self.fifo_thread = self.memory_thread\n",
       "        self.max_memory = max_memory\n",
       "\n",
       "    def to_longterm(self, idx: int):\n",
       "        \"\"\"move the message at the index idx to the longterm_memory\"\"\"\n",
       "        # move the message at the index idx to the longterm_memory\n",
       "        display(\n",
       "            Markdown(\n",
       "                \"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(\n",
       "                    idx\n",
       "                )\n",
       "            )\n",
       "        )\n",
       "        message = copy.deepcopy(self.memory_thread[idx])\n",
       "        # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\n",
       "        self.longterm_thread.add_message(message)\n",
       "        self.remove_message(idx=idx)\n",
       "\n",
       "    def add_message(self, message_dict: dict):\n",
       "        \"\"\"add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\n",
       "        # message_dict = {\"role\": role, \"content\": content}\n",
       "        # chek that the message_dict is a dictionary or a list of dictionaries\n",
       "        message_dict = check_dict(message_dict)\n",
       "        if self.redundant_thread is not None:\n",
       "            self.redundant_thread.add_message(message_dict)\n",
       "        message_tokens = self.get_message_tokens(message_dict)\n",
       "\n",
       "        if self.total_tokens + message_tokens > self.max_memory:\n",
       "            while self.total_tokens + message_tokens > self.max_memory:\n",
       "                if len(self.memory_thread) > 0:\n",
       "                    self.to_longterm(idx=0)\n",
       "            super().add_message(message_dict)\n",
       "\n",
       "        else:\n",
       "            # add the message_dict to the memory_thread\n",
       "            # update the total number of tokens\n",
       "            super().add_message(message_dict)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\n",
       "    BaseThread.__init__(self, name=name, max_memory=None)\n",
       "    MemoryIndex.__init__(self, index=None, name=name)\n",
       "    self.max_context = max_context\n",
       "    self.use_mark = use_mark\n",
       "    self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def index_message(self, message: str, verbose: bool = False):\n",
       "    \"\"\"index a message in the faiss index, the message is embedded and added to the index\n",
       "        self.values and self.embeddings and self.index are updated\n",
       "        \"\"\"\n",
       "\n",
       "    self.add_to_index(value=message, verbose=verbose)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def add_message(self, message_dict: dict, verbose: bool = False):\n",
       "    \"\"\"add a message to the memory thread, the message is embedded and added to the index\n",
       "        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\n",
       "        \"\"\"\n",
       "    # print(\"checking the dict\")\n",
       "    message_dict = check_dict(message_dict)\n",
       "    # print(\"trying to add the message\")\n",
       "    BaseThread.add_message(self, message_dict)\n",
       "    # print(message_dict)\n",
       "    message = message_dict[\"content\"]\n",
       "    self.index_message(message, verbose=verbose)\n",
       "    return True\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\n",
       "    \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\n",
       "    if self.use_mark:\n",
       "        query = mark_question(query)\n",
       "    return MemoryIndex.token_bound_query(self, query, k, max_tokens)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def sorted_query(\n",
       "    self,\n",
       "    query,\n",
       "    k: int = 10,\n",
       "    max_tokens: int = 4000,\n",
       "    reverse: bool = False,\n",
       "    return_from_thread=True,\n",
       "):\n",
       "    \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\n",
       "        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\n",
       "        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\n",
       "        \"\"\"\n",
       "    unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(\n",
       "        query, k, max_tokens=max_tokens\n",
       "    )\n",
       "    # sort the messages\n",
       "\n",
       "    sorted_messages = [\n",
       "        unsorted_messages[i]\n",
       "        for i in sorted(\n",
       "            range(len(unsorted_messages)), key=lambda k: unsorted_indices[k]\n",
       "        )\n",
       "    ]\n",
       "    sorted_scores = [\n",
       "        unsorted_scores[i]\n",
       "        for i in sorted(\n",
       "            range(len(unsorted_scores)), key=lambda k: unsorted_indices[k]\n",
       "        )\n",
       "    ]\n",
       "    sorted_indices = [\n",
       "        unsorted_indices[i]\n",
       "        for i in sorted(\n",
       "            range(len(unsorted_indices)), key=lambda k: unsorted_indices[k]\n",
       "        )\n",
       "    ]\n",
       "    if reverse:\n",
       "        sorted_messages.reverse()\n",
       "        sorted_scores.reverse()\n",
       "        sorted_indices.reverse()\n",
       "    if return_from_thread:\n",
       "        sorted_messages = [self.memory_thread[i] for i in sorted_indices]\n",
       "    return sorted_messages, sorted_scores, sorted_indices\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def weighted_query(\n",
       "    self,\n",
       "    query,\n",
       "    k: int = 10,\n",
       "    max_tokens: int = 4000,\n",
       "    decay_factor: float = 0.1,\n",
       "    temporal_weight: float = 0.5,\n",
       "    order_by: str = \"chronological\",\n",
       "    reverse: bool = False,\n",
       ") -> list:\n",
       "    \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\n",
       "        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\n",
       "        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\n",
       "        The order_by parameter controls the order of the results. If it is set to 'similarity', the results are sorted in similarity order. If it is set to 'chronological', the results are sorted in chronological order with the most recent message first.\n",
       "        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\n",
       "        \"\"\"\n",
       "    # Validate order_by parameter\n",
       "    if order_by not in (\"similarity\", \"chronological\"):\n",
       "        raise ValueError(\n",
       "            \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
       "        )\n",
       "\n",
       "    # Get similarity-based results\n",
       "    sim_messages, sim_scores, sim_indices = self.sorted_query(\n",
       "        query, k, max_tokens=max_tokens\n",
       "    )\n",
       "\n",
       "    # Get token-bound history\n",
       "    hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\n",
       "\n",
       "    # Combine messages and indices\n",
       "    combined_messages = sim_messages + hist_messages\n",
       "    combined_indices = sim_indices + hist_indices\n",
       "\n",
       "    # Create the local_index and populate it\n",
       "    self.local_index = MemoryIndex(name=\"local_index\")\n",
       "    for message in combined_messages:\n",
       "        self.local_index.add_to_index(value=message, verbose=False)\n",
       "\n",
       "    # Perform a new query on the combined index\n",
       "    (\n",
       "        new_query_results,\n",
       "        new_query_scores,\n",
       "        new_query_indices,\n",
       "    ) = self.local_index.token_bound_query(\n",
       "        query, k=len(combined_messages), max_tokens=max_tokens\n",
       "    )\n",
       "\n",
       "    # Compute temporal weights\n",
       "    temporal_weights = [\n",
       "        np.exp(-decay_factor * i) for i in range(len(combined_messages))\n",
       "    ]\n",
       "    temporal_weights = [\n",
       "        w / sum(temporal_weights) for w in temporal_weights\n",
       "    ]  # Normalize the temporal weights\n",
       "\n",
       "    # Combine similarity scores and temporal weights\n",
       "    weighted_scores = []\n",
       "    for i in range(len(new_query_scores)):\n",
       "        sim_score = new_query_scores[i]\n",
       "        temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\n",
       "        weighted_score = (\n",
       "            1 - temporal_weight\n",
       "        ) * sim_score + temporal_weight * temp_weight\n",
       "        weighted_scores.append(weighted_score)\n",
       "\n",
       "    # Sort the results based on the order_by parameter\n",
       "    if order_by == \"similarity\":\n",
       "        sorting_key = lambda k: weighted_scores[k]\n",
       "    elif order_by == \"chronological\":  # order_by == 'chronological'\n",
       "        sorting_key = lambda k: new_query_indices[k]\n",
       "    else:\n",
       "        raise ValueError(\n",
       "            \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
       "        )\n",
       "\n",
       "    sorted_indices = [\n",
       "        new_query_indices[i]\n",
       "        for i in sorted(\n",
       "            range(len(new_query_indices)), key=sorting_key, reverse=not reverse\n",
       "        )\n",
       "    ]\n",
       "    sorted_results = [\n",
       "        new_query_results[i]\n",
       "        for i in sorted(\n",
       "            range(len(new_query_results)), key=sorting_key, reverse=not reverse\n",
       "        )\n",
       "    ]\n",
       "    sorted_scores = [\n",
       "        weighted_scores[i]\n",
       "        for i in sorted(\n",
       "            range(len(weighted_scores)), key=sorting_key, reverse=not reverse\n",
       "        )\n",
       "    ]\n",
       "\n",
       "    # Return only the top k results without exceeding max_tokens\n",
       "    final_results, final_scores, final_indices = [], [], []\n",
       "    current_tokens = 0\n",
       "    for i in range(min(k, len(sorted_results))):\n",
       "        message_tokens = self.get_message_tokens(sorted_results[i])\n",
       "        if current_tokens + message_tokens <= max_tokens:\n",
       "            final_results.append(sorted_results[i])\n",
       "            final_scores.append(sorted_scores[i])\n",
       "            final_indices.append(sorted_indices[i])\n",
       "            current_tokens += message_tokens\n",
       "        else:\n",
       "            break\n",
       "\n",
       "    return final_results, final_scores, final_indices\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value from typing import Optional\n",
       "\n",
       "import faiss\n",
       "import numpy as np\n",
       "\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "from babydragon.memory.threads.base_thread import BaseThread\n",
       "from babydragon.utils.oai import check_dict, mark_question\n",
       "\n",
       "\n",
       "class VectorThread(BaseThread, MemoryIndex):\n",
       "    \"\"\"vector BaseThread, creates a faiss index with the messages and allows to search for similar messages, memory BaseThread can return messages in either similarity or chronological order\n",
       "    add a parameter to choose the order of the messages\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\n",
       "        BaseThread.__init__(self, name=name, max_memory=None)\n",
       "        MemoryIndex.__init__(self, index=None, name=name)\n",
       "        self.max_context = max_context\n",
       "        self.use_mark = use_mark\n",
       "        self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
       "\n",
       "    def index_message(self, message: str, verbose: bool = False):\n",
       "        \"\"\"index a message in the faiss index, the message is embedded and added to the index\n",
       "        self.values and self.embeddings and self.index are updated\n",
       "        \"\"\"\n",
       "\n",
       "        self.add_to_index(value=message, verbose=verbose)\n",
       "\n",
       "    def add_message(self, message_dict: dict, verbose: bool = False):\n",
       "        \"\"\"add a message to the memory thread, the message is embedded and added to the index\n",
       "        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\n",
       "        \"\"\"\n",
       "        # print(\"checking the dict\")\n",
       "        message_dict = check_dict(message_dict)\n",
       "        # print(\"trying to add the message\")\n",
       "        BaseThread.add_message(self, message_dict)\n",
       "        # print(message_dict)\n",
       "        message = message_dict[\"content\"]\n",
       "        self.index_message(message, verbose=verbose)\n",
       "        return True\n",
       "\n",
       "    def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\n",
       "        \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\n",
       "        if self.use_mark:\n",
       "            query = mark_question(query)\n",
       "        return MemoryIndex.token_bound_query(self, query, k, max_tokens)\n",
       "\n",
       "    def sorted_query(\n",
       "        self,\n",
       "        query,\n",
       "        k: int = 10,\n",
       "        max_tokens: int = 4000,\n",
       "        reverse: bool = False,\n",
       "        return_from_thread=True,\n",
       "    ):\n",
       "        \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\n",
       "        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\n",
       "        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\n",
       "        \"\"\"\n",
       "        unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(\n",
       "            query, k, max_tokens=max_tokens\n",
       "        )\n",
       "        # sort the messages\n",
       "\n",
       "        sorted_messages = [\n",
       "            unsorted_messages[i]\n",
       "            for i in sorted(\n",
       "                range(len(unsorted_messages)), key=lambda k: unsorted_indices[k]\n",
       "            )\n",
       "        ]\n",
       "        sorted_scores = [\n",
       "            unsorted_scores[i]\n",
       "            for i in sorted(\n",
       "                range(len(unsorted_scores)), key=lambda k: unsorted_indices[k]\n",
       "            )\n",
       "        ]\n",
       "        sorted_indices = [\n",
       "            unsorted_indices[i]\n",
       "            for i in sorted(\n",
       "                range(len(unsorted_indices)), key=lambda k: unsorted_indices[k]\n",
       "            )\n",
       "        ]\n",
       "        if reverse:\n",
       "            sorted_messages.reverse()\n",
       "            sorted_scores.reverse()\n",
       "            sorted_indices.reverse()\n",
       "        if return_from_thread:\n",
       "            sorted_messages = [self.memory_thread[i] for i in sorted_indices]\n",
       "        return sorted_messages, sorted_scores, sorted_indices\n",
       "\n",
       "    def weighted_query(\n",
       "        self,\n",
       "        query,\n",
       "        k: int = 10,\n",
       "        max_tokens: int = 4000,\n",
       "        decay_factor: float = 0.1,\n",
       "        temporal_weight: float = 0.5,\n",
       "        order_by: str = \"chronological\",\n",
       "        reverse: bool = False,\n",
       "    ) -> list:\n",
       "        \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\n",
       "        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\n",
       "        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\n",
       "        The order_by parameter controls the order of the results. If it is set to 'similarity', the results are sorted in similarity order. If it is set to 'chronological', the results are sorted in chronological order with the most recent message first.\n",
       "        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\n",
       "        \"\"\"\n",
       "        # Validate order_by parameter\n",
       "        if order_by not in (\"similarity\", \"chronological\"):\n",
       "            raise ValueError(\n",
       "                \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
       "            )\n",
       "\n",
       "        # Get similarity-based results\n",
       "        sim_messages, sim_scores, sim_indices = self.sorted_query(\n",
       "            query, k, max_tokens=max_tokens\n",
       "        )\n",
       "\n",
       "        # Get token-bound history\n",
       "        hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\n",
       "\n",
       "        # Combine messages and indices\n",
       "        combined_messages = sim_messages + hist_messages\n",
       "        combined_indices = sim_indices + hist_indices\n",
       "\n",
       "        # Create the local_index and populate it\n",
       "        self.local_index = MemoryIndex(name=\"local_index\")\n",
       "        for message in combined_messages:\n",
       "            self.local_index.add_to_index(value=message, verbose=False)\n",
       "\n",
       "        # Perform a new query on the combined index\n",
       "        (\n",
       "            new_query_results,\n",
       "            new_query_scores,\n",
       "            new_query_indices,\n",
       "        ) = self.local_index.token_bound_query(\n",
       "            query, k=len(combined_messages), max_tokens=max_tokens\n",
       "        )\n",
       "\n",
       "        # Compute temporal weights\n",
       "        temporal_weights = [\n",
       "            np.exp(-decay_factor * i) for i in range(len(combined_messages))\n",
       "        ]\n",
       "        temporal_weights = [\n",
       "            w / sum(temporal_weights) for w in temporal_weights\n",
       "        ]  # Normalize the temporal weights\n",
       "\n",
       "        # Combine similarity scores and temporal weights\n",
       "        weighted_scores = []\n",
       "        for i in range(len(new_query_scores)):\n",
       "            sim_score = new_query_scores[i]\n",
       "            temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\n",
       "            weighted_score = (\n",
       "                1 - temporal_weight\n",
       "            ) * sim_score + temporal_weight * temp_weight\n",
       "            weighted_scores.append(weighted_score)\n",
       "\n",
       "        # Sort the results based on the order_by parameter\n",
       "        if order_by == \"similarity\":\n",
       "            sorting_key = lambda k: weighted_scores[k]\n",
       "        elif order_by == \"chronological\":  # order_by == 'chronological'\n",
       "            sorting_key = lambda k: new_query_indices[k]\n",
       "        else:\n",
       "            raise ValueError(\n",
       "                \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
       "            )\n",
       "\n",
       "        sorted_indices = [\n",
       "            new_query_indices[i]\n",
       "            for i in sorted(\n",
       "                range(len(new_query_indices)), key=sorting_key, reverse=not reverse\n",
       "            )\n",
       "        ]\n",
       "        sorted_results = [\n",
       "            new_query_results[i]\n",
       "            for i in sorted(\n",
       "                range(len(new_query_results)), key=sorting_key, reverse=not reverse\n",
       "            )\n",
       "        ]\n",
       "        sorted_scores = [\n",
       "            weighted_scores[i]\n",
       "            for i in sorted(\n",
       "                range(len(weighted_scores)), key=sorting_key, reverse=not reverse\n",
       "            )\n",
       "        ]\n",
       "\n",
       "        # Return only the top k results without exceeding max_tokens\n",
       "        final_results, final_scores, final_indices = [], [], []\n",
       "        current_tokens = 0\n",
       "        for i in range(min(k, len(sorted_results))):\n",
       "            message_tokens = self.get_message_tokens(sorted_results[i])\n",
       "            if current_tokens + message_tokens <= max_tokens:\n",
       "                final_results.append(sorted_results[i])\n",
       "                final_scores.append(sorted_scores[i])\n",
       "                final_indices.append(sorted_indices[i])\n",
       "                current_tokens += message_tokens\n",
       "            else:\n",
       "                break\n",
       "\n",
       "        return final_results, final_scores, final_indices\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value # This is the __init__.py file for the package.\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def get_embedding_size(self):\n",
       "    return ADA_EMBEDDING_SIZE\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def embed(self, data, embed_mark=False, verbose=False):\n",
       "\n",
       "    if embed_mark is False and type(data) is dict and \"content\" in data:\n",
       "        if verbose is True:\n",
       "            print(\"Embedding without mark\", data[\"content\"])\n",
       "        out = openai.Embedding.create(\n",
       "            input=data[\"content\"], engine=\"text-embedding-ada-002\"\n",
       "        )\n",
       "    else:\n",
       "        if len(str(data)) > MAX_CONTEXT_LENGTH:\n",
       "            data = str(data)[:MAX_CONTEXT_LENGTH]\n",
       "        if verbose is True:\n",
       "            print(\"Embedding without preprocessing the input\", data)\n",
       "        out = openai.Embedding.create(\n",
       "            input=str(data), engine=\"text-embedding-ada-002\"\n",
       "        )\n",
       "    return out.data[0].embedding\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "def parse_and_embed_functions(input_str: str) -> List[np.ndarray]:\n",
       "    # Parse the input string with libcst\n",
       "    module = cst.parse_module(input_str)\n",
       "\n",
       "    # Find all the functions in the module and embed them separately\n",
       "    embeddings = []\n",
       "    for node in module.body:\n",
       "\n",
       "        if isinstance(node, cst.FunctionDef) or isinstance(node, cst.ClassDef):\n",
       "            func_str = cst.Module(body=[node]).code\n",
       "            print(\"Function string\", func_str)\n",
       "            embedding = openai.Embedding.create(\n",
       "                input=str(func_str)[:MAX_CONTEXT_LENGTH],\n",
       "                engine=\"text-embedding-ada-002\",\n",
       "            )\n",
       "            if embedding is not None:\n",
       "                embeddings.append(embedding.data[0].embedding)\n",
       "\n",
       "    avg_embedding = avg_embeddings(embeddings)\n",
       "    print(avg_embedding.shape)\n",
       "    return avg_embedding\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "def avg_embeddings(embeddings: List[np.ndarray]) -> np.ndarray:\n",
       "    print(\"Embeddings len\", len(embeddings))\n",
       "    # convert embeddings to numpy array\n",
       "    embeddings = np.array(embeddings)\n",
       "    print(\"Embedding Matrix Shape\", embeddings.shape)\n",
       "    return np.array([np.sum(embeddings.T, axis=1)]).astype(np.float32)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value from typing import List\n",
       "\n",
       "import libcst as cst\n",
       "import numpy as np\n",
       "import openai\n",
       "\n",
       "ADA_EMBEDDING_SIZE = 1536\n",
       "MAX_CONTEXT_LENGTH = 8100\n",
       "\n",
       "\n",
       "class OpenAiEmbedder:\n",
       "    def get_embedding_size(self):\n",
       "        return ADA_EMBEDDING_SIZE\n",
       "\n",
       "    def embed(self, data, embed_mark=False, verbose=False):\n",
       "\n",
       "        if embed_mark is False and type(data) is dict and \"content\" in data:\n",
       "            if verbose is True:\n",
       "                print(\"Embedding without mark\", data[\"content\"])\n",
       "            out = openai.Embedding.create(\n",
       "                input=data[\"content\"], engine=\"text-embedding-ada-002\"\n",
       "            )\n",
       "        else:\n",
       "            if len(str(data)) > MAX_CONTEXT_LENGTH:\n",
       "                data = str(data)[:MAX_CONTEXT_LENGTH]\n",
       "            if verbose is True:\n",
       "                print(\"Embedding without preprocessing the input\", data)\n",
       "            out = openai.Embedding.create(\n",
       "                input=str(data), engine=\"text-embedding-ada-002\"\n",
       "            )\n",
       "        return out.data[0].embedding\n",
       "\n",
       "\n",
       "def parse_and_embed_functions(input_str: str) -> List[np.ndarray]:\n",
       "    # Parse the input string with libcst\n",
       "    module = cst.parse_module(input_str)\n",
       "\n",
       "    # Find all the functions in the module and embed them separately\n",
       "    embeddings = []\n",
       "    for node in module.body:\n",
       "\n",
       "        if isinstance(node, cst.FunctionDef) or isinstance(node, cst.ClassDef):\n",
       "            func_str = cst.Module(body=[node]).code\n",
       "            print(\"Function string\", func_str)\n",
       "            embedding = openai.Embedding.create(\n",
       "                input=str(func_str)[:MAX_CONTEXT_LENGTH],\n",
       "                engine=\"text-embedding-ada-002\",\n",
       "            )\n",
       "            if embedding is not None:\n",
       "                embeddings.append(embedding.data[0].embedding)\n",
       "\n",
       "    avg_embedding = avg_embeddings(embeddings)\n",
       "    print(avg_embedding.shape)\n",
       "    return avg_embedding\n",
       "\n",
       "\n",
       "def avg_embeddings(embeddings: List[np.ndarray]) -> np.ndarray:\n",
       "    print(\"Embeddings len\", len(embeddings))\n",
       "    # convert embeddings to numpy array\n",
       "    embeddings = np.array(embeddings)\n",
       "    print(\"Embedding Matrix Shape\", embeddings.shape)\n",
       "    return np.array([np.sum(embeddings.T, axis=1)]).astype(np.float32)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def get_embedding_size(self):\n",
       "    return COHERE_EMBEDDING_SIZE\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def embed(self, data, embed_mark=False, verbose=False):\n",
       "    try:\n",
       "        if embed_mark is False and type(data) is dict and \"content\" in data:\n",
       "            if verbose is True:\n",
       "                print(\"Embedding without mark\", data[\"content\"])\n",
       "            out = co.embed(input=data[\"content\"]).embeddings\n",
       "        else:\n",
       "            if verbose is True:\n",
       "                print(\"Embedding without preprocessing the input\", data)\n",
       "            out = co.embed(input=str(data)).embeddings\n",
       "\n",
       "    except:\n",
       "        raise ValueError(\"The data  is not valid\", data)\n",
       "    return out\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import cohere as co\n",
       "\n",
       "COHERE_EMBEDDING_SIZE = 512\n",
       "\n",
       "\n",
       "class CohereEmbedder:\n",
       "    def get_embedding_size(self):\n",
       "        return COHERE_EMBEDDING_SIZE\n",
       "\n",
       "    def embed(self, data, embed_mark=False, verbose=False):\n",
       "        try:\n",
       "            if embed_mark is False and type(data) is dict and \"content\" in data:\n",
       "                if verbose is True:\n",
       "                    print(\"Embedding without mark\", data[\"content\"])\n",
       "                out = co.embed(input=data[\"content\"]).embeddings\n",
       "            else:\n",
       "                if verbose is True:\n",
       "                    print(\"Embedding without preprocessing the input\", data)\n",
       "                out = co.embed(input=str(data)).embeddings\n",
       "\n",
       "        except:\n",
       "            raise ValueError(\"The data  is not valid\", data)\n",
       "        return out\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def get_embedding_size(self):\n",
       "    return 356\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def embed(\n",
       "    data,\n",
       "    key=\"content\",\n",
       "    model_name=\"all-MiniLM-L6-v2\",\n",
       "    cores=1,\n",
       "    gpu=False,\n",
       "    batch_size=128,\n",
       "):\n",
       "    \"\"\"\n",
       "        Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n",
       "        \"\"\"\n",
       "    print(\"Embedding data\")\n",
       "    model = SentenceTransformer(model_name)\n",
       "    print(\"Model loaded\")\n",
       "\n",
       "    sentences = data[key].tolist()\n",
       "    unique_sentences = data[key].unique()\n",
       "    print(\"Unique sentences\", len(unique_sentences))\n",
       "\n",
       "    if cores == 1:\n",
       "        embeddings = model.encode(\n",
       "            unique_sentences, show_progress_bar=True, batch_size=batch_size\n",
       "        )\n",
       "    else:\n",
       "        devices = [\"cpu\"] * cores\n",
       "        if gpu:\n",
       "            devices = None  # use all CUDA devices\n",
       "\n",
       "        # Start the multi-process pool on multiple devices\n",
       "        print(\"Multi-process pool starting\")\n",
       "        pool = model.start_multi_process_pool(devices)\n",
       "        print(\"Multi-process pool started\")\n",
       "\n",
       "        chunk_size = math.ceil(len(unique_sentences) / cores)\n",
       "\n",
       "        # Compute the embeddings using the multi-process pool\n",
       "        embeddings = model.encode_multi_process(\n",
       "            unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size\n",
       "        )\n",
       "        model.stop_multi_process_pool(pool)\n",
       "\n",
       "    print(\"Embeddings computed\")\n",
       "\n",
       "    mapping = {\n",
       "        sentence: embedding\n",
       "        for sentence, embedding in zip(unique_sentences, embeddings)\n",
       "    }\n",
       "    embeddings = np.array([mapping[sentence] for sentence in sentences])\n",
       "\n",
       "    return embeddings\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import math\n",
       "\n",
       "import numpy as np\n",
       "from sentence_transformers import SentenceTransformer\n",
       "\n",
       "\n",
       "class SBERTEmbedder:\n",
       "    def get_embedding_size(self):\n",
       "        return 356\n",
       "\n",
       "    def embed(\n",
       "        data,\n",
       "        key=\"content\",\n",
       "        model_name=\"all-MiniLM-L6-v2\",\n",
       "        cores=1,\n",
       "        gpu=False,\n",
       "        batch_size=128,\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n",
       "        \"\"\"\n",
       "        print(\"Embedding data\")\n",
       "        model = SentenceTransformer(model_name)\n",
       "        print(\"Model loaded\")\n",
       "\n",
       "        sentences = data[key].tolist()\n",
       "        unique_sentences = data[key].unique()\n",
       "        print(\"Unique sentences\", len(unique_sentences))\n",
       "\n",
       "        if cores == 1:\n",
       "            embeddings = model.encode(\n",
       "                unique_sentences, show_progress_bar=True, batch_size=batch_size\n",
       "            )\n",
       "        else:\n",
       "            devices = [\"cpu\"] * cores\n",
       "            if gpu:\n",
       "                devices = None  # use all CUDA devices\n",
       "\n",
       "            # Start the multi-process pool on multiple devices\n",
       "            print(\"Multi-process pool starting\")\n",
       "            pool = model.start_multi_process_pool(devices)\n",
       "            print(\"Multi-process pool started\")\n",
       "\n",
       "            chunk_size = math.ceil(len(unique_sentences) / cores)\n",
       "\n",
       "            # Compute the embeddings using the multi-process pool\n",
       "            embeddings = model.encode_multi_process(\n",
       "                unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size\n",
       "            )\n",
       "            model.stop_multi_process_pool(pool)\n",
       "\n",
       "        print(\"Embeddings computed\")\n",
       "\n",
       "        mapping = {\n",
       "            sentence: embedding\n",
       "            for sentence, embedding in zip(unique_sentences, embeddings)\n",
       "        }\n",
       "        embeddings = np.array([mapping[sentence] for sentence in sentences])\n",
       "\n",
       "        return embeddings\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self):\n",
       "    self.base_url = \"https://www.arxiv-vanity.com/\"\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def _get_vanity_url(self, arxiv_id):\n",
       "    return urljoin(self.base_url, \"papers/\" + arxiv_id)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def _fetch_html(self, url):\n",
       "    response = requests.get(url)\n",
       "    if response.status_code == 200:\n",
       "        return response.text\n",
       "    else:\n",
       "        return None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def _extract_main_content(self, html):\n",
       "    soup = BeautifulSoup(html, \"html.parser\")\n",
       "    paragraphs = soup.find_all(\"div\", {\"class\": \"ltx_para\"})\n",
       "    content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\n",
       "    return content\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def parse_paper(self, arxiv_id):\n",
       "    vanity_url = self._get_vanity_url(arxiv_id)\n",
       "    html = self._fetch_html(vanity_url)\n",
       "    if html is not None:\n",
       "        return self._extract_main_content(html)\n",
       "    else:\n",
       "        return None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self):\n",
       "    self.base_url = \"http://export.arxiv.org/api/query?\"\n",
       "    self.pdf_download_url = \"https://arxiv.org/pdf/\"\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def search(self, query, max_results=10):\n",
       "    url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\n",
       "    response = requests.get(url)\n",
       "    if response.status_code == 200:\n",
       "        return response.text\n",
       "    else:\n",
       "        return None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def download_pdf(self, paper_key, save_directory=\"./\"):\n",
       "    pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\n",
       "    response = requests.get(pdf_url)\n",
       "    if response.status_code == 200:\n",
       "        with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\n",
       "            f.write(response.content)\n",
       "        print(f\"PDF for {paper_key} downloaded successfully.\")\n",
       "    else:\n",
       "        print(f\"Error downloading PDF for {paper_key}.\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self):\n",
       "    self.api = ArxivAPI()\n",
       "    self.vanity_parser = ArxivVanityParser()\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def _parse_arxiv_id(self, url):\n",
       "    return url.split(\"/\")[-1]\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def parse_papers(self, query, max_results=10):\n",
       "    search_results = self.api.search(query, max_results)\n",
       "    if search_results is not None:\n",
       "        soup = BeautifulSoup(search_results, \"html.parser\")\n",
       "        entries = soup.find_all(\"entry\")\n",
       "        paper_list = []\n",
       "        for entry in entries:\n",
       "            paper_dict = {}\n",
       "            arxiv_id = self._parse_arxiv_id(entry.id.string)\n",
       "            paper_dict[\"arxiv_id\"] = arxiv_id\n",
       "            paper_dict[\"title\"] = entry.title.string\n",
       "            paper_dict[\"summary\"] = entry.summary.string\n",
       "            paper_dict[\"content\"] = self.vanity_parser.parse_paper(str(arxiv_id))\n",
       "            if paper_dict[\"content\"] == None:\n",
       "                continue\n",
       "            paper_list.append(paper_dict)\n",
       "        return paper_list\n",
       "    else:\n",
       "        return None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import os\n",
       "from urllib.parse import urljoin, urlparse\n",
       "\n",
       "import requests\n",
       "from bs4 import BeautifulSoup\n",
       "\n",
       "\n",
       "class ArxivVanityParser:\n",
       "    def __init__(self):\n",
       "        self.base_url = \"https://www.arxiv-vanity.com/\"\n",
       "\n",
       "    def _get_vanity_url(self, arxiv_id):\n",
       "        return urljoin(self.base_url, \"papers/\" + arxiv_id)\n",
       "\n",
       "    def _fetch_html(self, url):\n",
       "        response = requests.get(url)\n",
       "        if response.status_code == 200:\n",
       "            return response.text\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "    def _extract_main_content(self, html):\n",
       "        soup = BeautifulSoup(html, \"html.parser\")\n",
       "        paragraphs = soup.find_all(\"div\", {\"class\": \"ltx_para\"})\n",
       "        content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\n",
       "        return content\n",
       "\n",
       "    def parse_paper(self, arxiv_id):\n",
       "        vanity_url = self._get_vanity_url(arxiv_id)\n",
       "        html = self._fetch_html(vanity_url)\n",
       "        if html is not None:\n",
       "            return self._extract_main_content(html)\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "\n",
       "class ArxivAPI:\n",
       "    def __init__(self):\n",
       "        self.base_url = \"http://export.arxiv.org/api/query?\"\n",
       "        self.pdf_download_url = \"https://arxiv.org/pdf/\"\n",
       "\n",
       "    def search(self, query, max_results=10):\n",
       "        url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\n",
       "        response = requests.get(url)\n",
       "        if response.status_code == 200:\n",
       "            return response.text\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "    def download_pdf(self, paper_key, save_directory=\"./\"):\n",
       "        pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\n",
       "        response = requests.get(pdf_url)\n",
       "        if response.status_code == 200:\n",
       "            with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\n",
       "                f.write(response.content)\n",
       "            print(f\"PDF for {paper_key} downloaded successfully.\")\n",
       "        else:\n",
       "            print(f\"Error downloading PDF for {paper_key}.\")\n",
       "\n",
       "\n",
       "class ArxivParser:\n",
       "    def __init__(self):\n",
       "        self.api = ArxivAPI()\n",
       "        self.vanity_parser = ArxivVanityParser()\n",
       "\n",
       "    def _parse_arxiv_id(self, url):\n",
       "        return url.split(\"/\")[-1]\n",
       "\n",
       "    def parse_papers(self, query, max_results=10):\n",
       "        search_results = self.api.search(query, max_results)\n",
       "        if search_results is not None:\n",
       "            soup = BeautifulSoup(search_results, \"html.parser\")\n",
       "            entries = soup.find_all(\"entry\")\n",
       "            paper_list = []\n",
       "            for entry in entries:\n",
       "                paper_dict = {}\n",
       "                arxiv_id = self._parse_arxiv_id(entry.id.string)\n",
       "                paper_dict[\"arxiv_id\"] = arxiv_id\n",
       "                paper_dict[\"title\"] = entry.title.string\n",
       "                paper_dict[\"summary\"] = entry.summary.string\n",
       "                paper_dict[\"content\"] = self.vanity_parser.parse_paper(str(arxiv_id))\n",
       "                if paper_dict[\"content\"] == None:\n",
       "                    continue\n",
       "                paper_list.append(paper_dict)\n",
       "            return paper_list\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    # Usage example\n",
       "    parser = ArxivParser()\n",
       "    papers = parser.parse_papers(\"SVD\", max_results=5)\n",
       "    for paper in papers:\n",
       "        print(paper[\"title\"])\n",
       "        print(paper[\"arxiv_id\"])\n",
       "        print(paper[\"summary\"])\n",
       "        print(paper[\"content\"])\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(\n",
       "    self,\n",
       "    base_directory: str,\n",
       "    username=None,\n",
       "    repo_name=None,\n",
       "    code_parsers=None,\n",
       "    minify_code: bool = False,\n",
       "    remove_docstrings: bool = False,\n",
       "):\n",
       "    self.username = username\n",
       "    self.repo_name = repo_name\n",
       "    self.base_directory = base_directory\n",
       "    self.github = Github()\n",
       "    self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\n",
       "    repo_path = self.clone_repo(self.repo.clone_url)\n",
       "\n",
       "    OsProcessor.__init__(self, repo_path)\n",
       "    self.code_parsers = code_parsers or [\n",
       "        PythonParser(\n",
       "            repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\n",
       "        )\n",
       "    ]\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_public_repos(self):\n",
       "    \"\"\"Returns a list of all public repos for the user.\"\"\"\n",
       "    user = self.github.get_user(self.username)\n",
       "    return user.get_repos()\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def clone_repo(self, repo_url: str):\n",
       "    \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\n",
       "    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
       "    target_directory = os.path.join(self.base_directory, repo_name)\n",
       "\n",
       "    if os.path.exists(target_directory):\n",
       "        shutil.rmtree(target_directory)\n",
       "\n",
       "    subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
       "\n",
       "    return target_directory\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def process_repo(self, repo_path=None):\n",
       "    \"\"\"Processes the repo at the specified path.\n",
       "        If no path is specified, the repo at self.directory_path is processed.\n",
       "        Returns the list of parsed functions and classes.\"\"\"\n",
       "    if repo_path is None:\n",
       "        repo_path = self.directory_path\n",
       "\n",
       "    for code_parser in self.code_parsers:\n",
       "        code_parser.directory_path = repo_path\n",
       "        code_parser.process_directory(repo_path)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def process_repos(self):\n",
       "    \"\"\"Processes all public repos for the user.\"\"\"\n",
       "    for repo in self.get_public_repos():\n",
       "        if not repo.private:\n",
       "            print(f\"Processing repo: {repo.name}\")\n",
       "            repo_path = self.clone_repo(repo.clone_url)\n",
       "            self.process_repo(repo_path)\n",
       "            shutil.rmtree(repo_path)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_repo(self, repo_name):\n",
       "    \"\"\"Returns the repo with the specified name.\"\"\"\n",
       "    user = self.github.get_user(self.username)\n",
       "    return user.get_repo(repo_name)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def process_single_repo(self):\n",
       "\n",
       "    repo = self.get_repo(self.repo_name)\n",
       "    print(f\"Processing repo: {self.repo_name}\")\n",
       "    repo_path = self.clone_repo(repo.clone_url)\n",
       "    self.process_repo(repo_path)\n",
       "    shutil.rmtree(repo_path)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_issues(self, state=\"open\"):\n",
       "    \"\"\"\n",
       "        Returns a list of all issues in the repo with the specified state.\n",
       "        \"\"\"\n",
       "    issues = []\n",
       "    for issue in self.repo.get_issues(state=state):\n",
       "        issues.append(issue)\n",
       "    return issues\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def parse_issues(self, state=\"open\"):\n",
       "    \"\"\"\n",
       "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
       "        Each dict contains the issue number, title, body, and labels.\n",
       "        \"\"\"\n",
       "    parsed_issues = []\n",
       "    issues = self.get_issues(state=state)\n",
       "    for issue in issues:\n",
       "        parsed_issue = {\n",
       "            \"number\": issue.number,\n",
       "            \"title\": issue.title,\n",
       "            \"body\": issue.body,\n",
       "            \"labels\": [label.name for label in issue.labels],\n",
       "        }\n",
       "        parsed_issues.append(parsed_issue)\n",
       "    return parsed_issues\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_commits(self):\n",
       "    \"\"\"\n",
       "        Returns a list of all commits in the main branch of the repository.\n",
       "        \"\"\"\n",
       "    commits = []\n",
       "    branch = self.repo.get_branch(\"main\")\n",
       "    for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
       "        commits.append(commit)\n",
       "    return commits\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def parse_commits(self):\n",
       "    \"\"\"\n",
       "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
       "        Each dict contains the commit sha, commit message, and author information.\n",
       "        \"\"\"\n",
       "    parsed_commits = []\n",
       "    commits = self.get_commits()\n",
       "    for commit in commits:\n",
       "        parsed_commit = {\n",
       "            \"sha\": commit.sha,\n",
       "            \"message\": commit.commit.message,\n",
       "            \"author\": {\n",
       "                \"name\": commit.commit.author.name,\n",
       "                \"email\": commit.commit.author.email,\n",
       "                \"date\": commit.commit.author.date,\n",
       "            },\n",
       "        }\n",
       "        parsed_commits.append(parsed_commit)\n",
       "    return parsed_commits\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import os\n",
       "import shutil\n",
       "import subprocess\n",
       "from typing import List\n",
       "\n",
       "from github import Github\n",
       "\n",
       "from babydragon.processors.os_processor import OsProcessor\n",
       "from babydragon.processors.parsers.python_parser import PythonParser\n",
       "\n",
       "\n",
       "class GithubProcessor(OsProcessor):\n",
       "    def __init__(\n",
       "        self,\n",
       "        base_directory: str,\n",
       "        username=None,\n",
       "        repo_name=None,\n",
       "        code_parsers=None,\n",
       "        minify_code: bool = False,\n",
       "        remove_docstrings: bool = False,\n",
       "    ):\n",
       "        self.username = username\n",
       "        self.repo_name = repo_name\n",
       "        self.base_directory = base_directory\n",
       "        self.github = Github()\n",
       "        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\n",
       "        repo_path = self.clone_repo(self.repo.clone_url)\n",
       "\n",
       "        OsProcessor.__init__(self, repo_path)\n",
       "        self.code_parsers = code_parsers or [\n",
       "            PythonParser(\n",
       "                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\n",
       "            )\n",
       "        ]\n",
       "\n",
       "    def get_public_repos(self):\n",
       "        \"\"\"Returns a list of all public repos for the user.\"\"\"\n",
       "        user = self.github.get_user(self.username)\n",
       "        return user.get_repos()\n",
       "\n",
       "    def clone_repo(self, repo_url: str):\n",
       "        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\n",
       "        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
       "        target_directory = os.path.join(self.base_directory, repo_name)\n",
       "\n",
       "        if os.path.exists(target_directory):\n",
       "            shutil.rmtree(target_directory)\n",
       "\n",
       "        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
       "\n",
       "        return target_directory\n",
       "\n",
       "    def process_repo(self, repo_path=None):\n",
       "        \"\"\"Processes the repo at the specified path.\n",
       "        If no path is specified, the repo at self.directory_path is processed.\n",
       "        Returns the list of parsed functions and classes.\"\"\"\n",
       "        if repo_path is None:\n",
       "            repo_path = self.directory_path\n",
       "\n",
       "        for code_parser in self.code_parsers:\n",
       "            code_parser.directory_path = repo_path\n",
       "            code_parser.process_directory(repo_path)\n",
       "\n",
       "    def process_repos(self):\n",
       "        \"\"\"Processes all public repos for the user.\"\"\"\n",
       "        for repo in self.get_public_repos():\n",
       "            if not repo.private:\n",
       "                print(f\"Processing repo: {repo.name}\")\n",
       "                repo_path = self.clone_repo(repo.clone_url)\n",
       "                self.process_repo(repo_path)\n",
       "                shutil.rmtree(repo_path)\n",
       "\n",
       "    def get_repo(self, repo_name):\n",
       "        \"\"\"Returns the repo with the specified name.\"\"\"\n",
       "        user = self.github.get_user(self.username)\n",
       "        return user.get_repo(repo_name)\n",
       "\n",
       "    def process_single_repo(self):\n",
       "\n",
       "        repo = self.get_repo(self.repo_name)\n",
       "        print(f\"Processing repo: {self.repo_name}\")\n",
       "        repo_path = self.clone_repo(repo.clone_url)\n",
       "        self.process_repo(repo_path)\n",
       "        shutil.rmtree(repo_path)\n",
       "\n",
       "    def get_issues(self, state=\"open\"):\n",
       "        \"\"\"\n",
       "        Returns a list of all issues in the repo with the specified state.\n",
       "        \"\"\"\n",
       "        issues = []\n",
       "        for issue in self.repo.get_issues(state=state):\n",
       "            issues.append(issue)\n",
       "        return issues\n",
       "\n",
       "    def parse_issues(self, state=\"open\"):\n",
       "        \"\"\"\n",
       "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
       "        Each dict contains the issue number, title, body, and labels.\n",
       "        \"\"\"\n",
       "        parsed_issues = []\n",
       "        issues = self.get_issues(state=state)\n",
       "        for issue in issues:\n",
       "            parsed_issue = {\n",
       "                \"number\": issue.number,\n",
       "                \"title\": issue.title,\n",
       "                \"body\": issue.body,\n",
       "                \"labels\": [label.name for label in issue.labels],\n",
       "            }\n",
       "            parsed_issues.append(parsed_issue)\n",
       "        return parsed_issues\n",
       "\n",
       "    def get_commits(self):\n",
       "        \"\"\"\n",
       "        Returns a list of all commits in the main branch of the repository.\n",
       "        \"\"\"\n",
       "        commits = []\n",
       "        branch = self.repo.get_branch(\"main\")\n",
       "        for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
       "            commits.append(commit)\n",
       "        return commits\n",
       "\n",
       "    def parse_commits(self):\n",
       "        \"\"\"\n",
       "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
       "        Each dict contains the commit sha, commit message, and author information.\n",
       "        \"\"\"\n",
       "        parsed_commits = []\n",
       "        commits = self.get_commits()\n",
       "        for commit in commits:\n",
       "            parsed_commit = {\n",
       "                \"sha\": commit.sha,\n",
       "                \"message\": commit.commit.message,\n",
       "                \"author\": {\n",
       "                    \"name\": commit.commit.author.name,\n",
       "                    \"email\": commit.commit.author.email,\n",
       "                    \"date\": commit.commit.author.date,\n",
       "                },\n",
       "            }\n",
       "            parsed_commits.append(parsed_commit)\n",
       "        return parsed_commits\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self, directory_path: str):\n",
       "    self.directory_path = directory_path\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\n",
       "    \"\"\"Returns a list of all files in a directory\"\"\"\n",
       "    if directory_path is None:\n",
       "        directory_path = self.directory_path\n",
       "\n",
       "    all_files = []\n",
       "    for root, _, files in os.walk(directory_path):\n",
       "        for file in files:\n",
       "            all_files.append(os.path.join(root, file))\n",
       "\n",
       "    return all_files\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_files_with_extension(\n",
       "    self, extension: str, directory_path: Optional[str] = None\n",
       ") -> List[str]:\n",
       "    \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\n",
       "    if directory_path is None:\n",
       "        directory_path = self.directory_path\n",
       "\n",
       "    all_files = self.get_all_files(directory_path)\n",
       "    files_with_extension = [file for file in all_files if file.endswith(extension)]\n",
       "\n",
       "    return files_with_extension\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_file_extension(self, file_path: str) -> str:\n",
       "    \"\"\"Returns the extension of a file\"\"\"\n",
       "    return Path(file_path).suffix\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\n",
       "    \"\"\"Returns a list of all subdirectories in a directory\"\"\"\n",
       "    if directory_path is None:\n",
       "        directory_path = self.directory_path\n",
       "\n",
       "    subdirectories = [\n",
       "        os.path.join(directory_path, d)\n",
       "        for d in os.listdir(directory_path)\n",
       "        if os.path.isdir(os.path.join(directory_path, d))\n",
       "    ]\n",
       "\n",
       "    return subdirectories\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def create_directory(self, directory_path: str) -> None:\n",
       "    \"\"\"Creates a directory if it does not exist\"\"\"\n",
       "    if not os.path.exists(directory_path):\n",
       "        os.makedirs(directory_path)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def delete_directory(self, directory_path: str) -> None:\n",
       "    \"\"\"Deletes a directory if it exists\"\"\"\n",
       "    if os.path.exists(directory_path):\n",
       "        shutil.rmtree(directory_path)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def copy_file(self, source_path: str, destination_path: str) -> None:\n",
       "    \"\"\"Copies a file from one location to another\"\"\"\n",
       "    shutil.copy2(source_path, destination_path)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def move_file(self, source_path: str, destination_path: str) -> None:\n",
       "    \"\"\"Moves a file from one location to another\"\"\"\n",
       "    shutil.move(source_path, destination_path)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import os\n",
       "import shutil\n",
       "from pathlib import Path\n",
       "from typing import Dict, List, Optional\n",
       "\n",
       "\n",
       "class OsProcessor:\n",
       "    def __init__(self, directory_path: str):\n",
       "        self.directory_path = directory_path\n",
       "\n",
       "    def get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\n",
       "        \"\"\"Returns a list of all files in a directory\"\"\"\n",
       "        if directory_path is None:\n",
       "            directory_path = self.directory_path\n",
       "\n",
       "        all_files = []\n",
       "        for root, _, files in os.walk(directory_path):\n",
       "            for file in files:\n",
       "                all_files.append(os.path.join(root, file))\n",
       "\n",
       "        return all_files\n",
       "\n",
       "    def get_files_with_extension(\n",
       "        self, extension: str, directory_path: Optional[str] = None\n",
       "    ) -> List[str]:\n",
       "        \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\n",
       "        if directory_path is None:\n",
       "            directory_path = self.directory_path\n",
       "\n",
       "        all_files = self.get_all_files(directory_path)\n",
       "        files_with_extension = [file for file in all_files if file.endswith(extension)]\n",
       "\n",
       "        return files_with_extension\n",
       "\n",
       "    def get_file_extension(self, file_path: str) -> str:\n",
       "        \"\"\"Returns the extension of a file\"\"\"\n",
       "        return Path(file_path).suffix\n",
       "\n",
       "    def get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\n",
       "        \"\"\"Returns a list of all subdirectories in a directory\"\"\"\n",
       "        if directory_path is None:\n",
       "            directory_path = self.directory_path\n",
       "\n",
       "        subdirectories = [\n",
       "            os.path.join(directory_path, d)\n",
       "            for d in os.listdir(directory_path)\n",
       "            if os.path.isdir(os.path.join(directory_path, d))\n",
       "        ]\n",
       "\n",
       "        return subdirectories\n",
       "\n",
       "    def create_directory(self, directory_path: str) -> None:\n",
       "        \"\"\"Creates a directory if it does not exist\"\"\"\n",
       "        if not os.path.exists(directory_path):\n",
       "            os.makedirs(directory_path)\n",
       "\n",
       "    def delete_directory(self, directory_path: str) -> None:\n",
       "        \"\"\"Deletes a directory if it exists\"\"\"\n",
       "        if os.path.exists(directory_path):\n",
       "            shutil.rmtree(directory_path)\n",
       "\n",
       "    def copy_file(self, source_path: str, destination_path: str) -> None:\n",
       "        \"\"\"Copies a file from one location to another\"\"\"\n",
       "        shutil.copy2(source_path, destination_path)\n",
       "\n",
       "    def move_file(self, source_path: str, destination_path: str) -> None:\n",
       "        \"\"\"Moves a file from one location to another\"\"\"\n",
       "        shutil.move(source_path, destination_path)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self):\n",
       "    self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def search(self, query, max_results=10):\n",
       "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
       "    record = Entrez.read(handle)\n",
       "    handle.close()\n",
       "    return record[\"IdList\"]\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def fetch_abstract(self, pubmed_id):\n",
       "    handle = Entrez.efetch(\n",
       "        db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\"\n",
       "    )\n",
       "    abstract = handle.read()\n",
       "    handle.close()\n",
       "    return abstract\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def fetch_pmc_full_text(self, pubmed_id):\n",
       "    # Get the PMC ID for the PubMed ID\n",
       "    handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\n",
       "    record = Entrez.read(handle)\n",
       "    handle.close()\n",
       "    pmc_id = None\n",
       "    for link in record[0][\"LinkSetDb\"]:\n",
       "        if link[\"DbTo\"] == \"pmc\":\n",
       "            pmc_id = link[\"Link\"][0][\"Id\"]\n",
       "            break\n",
       "\n",
       "    if not pmc_id:\n",
       "        return None\n",
       "\n",
       "    # Fetch the PMC article XML\n",
       "    handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\n",
       "    xml_content = handle.read()\n",
       "    handle.close()\n",
       "\n",
       "    # Parse the XML and extract the full text\n",
       "    soup = BeautifulSoup(xml_content, \"xml\")\n",
       "    full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
       "\n",
       "    return full_text\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self):\n",
       "    self.api = PubmedAPI()\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def parse_papers(self, query, max_results=10):\n",
       "    pubmed_ids = self.api.search(query, max_results)\n",
       "    paper_list = []\n",
       "    for pubmed_id in pubmed_ids:\n",
       "        paper_dict = {}\n",
       "        paper_dict[\"pubmed_id\"] = pubmed_id\n",
       "        paper_dict[\"abstract\"] = self.api.fetch_abstract(pubmed_id)\n",
       "        paper_dict[\"content\"] = self.api.fetch_pmc_full_text(pubmed_id)\n",
       "        paper_list.append(paper_dict)\n",
       "    return paper_list\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value from Bio import Entrez\n",
       "from bs4 import BeautifulSoup\n",
       "\n",
       "\n",
       "class PubmedAPI:\n",
       "    def __init__(self):\n",
       "        self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
       "\n",
       "    def search(self, query, max_results=10):\n",
       "        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
       "        record = Entrez.read(handle)\n",
       "        handle.close()\n",
       "        return record[\"IdList\"]\n",
       "\n",
       "    def fetch_abstract(self, pubmed_id):\n",
       "        handle = Entrez.efetch(\n",
       "            db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\"\n",
       "        )\n",
       "        abstract = handle.read()\n",
       "        handle.close()\n",
       "        return abstract\n",
       "\n",
       "    def fetch_pmc_full_text(self, pubmed_id):\n",
       "        # Get the PMC ID for the PubMed ID\n",
       "        handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\n",
       "        record = Entrez.read(handle)\n",
       "        handle.close()\n",
       "        pmc_id = None\n",
       "        for link in record[0][\"LinkSetDb\"]:\n",
       "            if link[\"DbTo\"] == \"pmc\":\n",
       "                pmc_id = link[\"Link\"][0][\"Id\"]\n",
       "                break\n",
       "\n",
       "        if not pmc_id:\n",
       "            return None\n",
       "\n",
       "        # Fetch the PMC article XML\n",
       "        handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\n",
       "        xml_content = handle.read()\n",
       "        handle.close()\n",
       "\n",
       "        # Parse the XML and extract the full text\n",
       "        soup = BeautifulSoup(xml_content, \"xml\")\n",
       "        full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
       "\n",
       "        return full_text\n",
       "\n",
       "\n",
       "class PubmedParser:\n",
       "    def __init__(self):\n",
       "        self.api = PubmedAPI()\n",
       "\n",
       "    def parse_papers(self, query, max_results=10):\n",
       "        pubmed_ids = self.api.search(query, max_results)\n",
       "        paper_list = []\n",
       "        for pubmed_id in pubmed_ids:\n",
       "            paper_dict = {}\n",
       "            paper_dict[\"pubmed_id\"] = pubmed_id\n",
       "            paper_dict[\"abstract\"] = self.api.fetch_abstract(pubmed_id)\n",
       "            paper_dict[\"content\"] = self.api.fetch_pmc_full_text(pubmed_id)\n",
       "            paper_list.append(paper_dict)\n",
       "        return paper_list\n",
       "\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    # Usage example\n",
       "    parser = PubmedParser()\n",
       "    papers = parser.parse_papers(\"cancer immunotherapy\", max_results=7)\n",
       "    for paper in papers:\n",
       "        print(f\"PubMed ID: {paper['pubmed_id']}\")\n",
       "        print(f\"Abstract:\\n{paper['abstract']}\\n\")\n",
       "        if paper[\"content\"]:\n",
       "            print(f\"Content:\\n{paper['content']}\\n\")\n",
       "        else:\n",
       "            print(\"none\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value # This is the __init__.py file for the package.\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self, repo_name):\n",
       "    self.g = Github()\n",
       "    self.repo = self.g.get_repo(repo_name)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_issues(self, state=\"open\"):\n",
       "    \"\"\"\n",
       "        Returns a list of all issues in the repo with the specified state.\n",
       "        \"\"\"\n",
       "    issues = []\n",
       "    for issue in self.repo.get_issues(state=state):\n",
       "        issues.append(issue)\n",
       "    return issues\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def parse_issues(self, state=\"open\"):\n",
       "    \"\"\"\n",
       "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
       "        Each dict contains the issue number, title, body, and labels.\n",
       "        \"\"\"\n",
       "    parsed_issues = []\n",
       "    issues = self.get_issues(state=state)\n",
       "    for issue in issues:\n",
       "        parsed_issue = {\n",
       "            \"number\": issue.number,\n",
       "            \"title\": issue.title,\n",
       "            \"body\": issue.body,\n",
       "            \"labels\": [label.name for label in issue.labels],\n",
       "        }\n",
       "        parsed_issues.append(parsed_issue)\n",
       "    return parsed_issues\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self, repo_name):\n",
       "    self.g = Github()\n",
       "    self.repo = self.g.get_repo(repo_name)\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_commits(self):\n",
       "    \"\"\"\n",
       "        Returns a list of all commits in the main branch of the repository.\n",
       "        \"\"\"\n",
       "    commits = []\n",
       "    branch = self.repo.get_branch(\"main\")\n",
       "    for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
       "        commits.append(commit)\n",
       "    return commits\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def parse_commits(self):\n",
       "    \"\"\"\n",
       "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
       "        Each dict contains the commit sha, commit message, and author information.\n",
       "        \"\"\"\n",
       "    parsed_commits = []\n",
       "    commits = self.get_commits()\n",
       "    for commit in commits:\n",
       "        parsed_commit = {\n",
       "            \"sha\": commit.sha,\n",
       "            \"message\": commit.commit.message,\n",
       "            \"author\": {\n",
       "                \"name\": commit.commit.author.name,\n",
       "                \"email\": commit.commit.author.email,\n",
       "                \"date\": commit.commit.author.date,\n",
       "            },\n",
       "        }\n",
       "        parsed_commits.append(parsed_commit)\n",
       "    return parsed_commits\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value from github import Github\n",
       "\n",
       "\n",
       "class IssueParser:\n",
       "    def __init__(self, repo_name):\n",
       "        self.g = Github()\n",
       "        self.repo = self.g.get_repo(repo_name)\n",
       "\n",
       "    def get_issues(self, state=\"open\"):\n",
       "        \"\"\"\n",
       "        Returns a list of all issues in the repo with the specified state.\n",
       "        \"\"\"\n",
       "        issues = []\n",
       "        for issue in self.repo.get_issues(state=state):\n",
       "            issues.append(issue)\n",
       "        return issues\n",
       "\n",
       "    def parse_issues(self, state=\"open\"):\n",
       "        \"\"\"\n",
       "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
       "        Each dict contains the issue number, title, body, and labels.\n",
       "        \"\"\"\n",
       "        parsed_issues = []\n",
       "        issues = self.get_issues(state=state)\n",
       "        for issue in issues:\n",
       "            parsed_issue = {\n",
       "                \"number\": issue.number,\n",
       "                \"title\": issue.title,\n",
       "                \"body\": issue.body,\n",
       "                \"labels\": [label.name for label in issue.labels],\n",
       "            }\n",
       "            parsed_issues.append(parsed_issue)\n",
       "        return parsed_issues\n",
       "\n",
       "\n",
       "class CommitParser:\n",
       "    def __init__(self, repo_name):\n",
       "        self.g = Github()\n",
       "        self.repo = self.g.get_repo(repo_name)\n",
       "\n",
       "    def get_commits(self):\n",
       "        \"\"\"\n",
       "        Returns a list of all commits in the main branch of the repository.\n",
       "        \"\"\"\n",
       "        commits = []\n",
       "        branch = self.repo.get_branch(\"main\")\n",
       "        for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
       "            commits.append(commit)\n",
       "        return commits\n",
       "\n",
       "    def parse_commits(self):\n",
       "        \"\"\"\n",
       "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
       "        Each dict contains the commit sha, commit message, and author information.\n",
       "        \"\"\"\n",
       "        parsed_commits = []\n",
       "        commits = self.get_commits()\n",
       "        for commit in commits:\n",
       "            parsed_commit = {\n",
       "                \"sha\": commit.sha,\n",
       "                \"message\": commit.commit.message,\n",
       "                \"author\": {\n",
       "                    \"name\": commit.commit.author.name,\n",
       "                    \"email\": commit.commit.author.email,\n",
       "                    \"date\": commit.commit.author.date,\n",
       "                },\n",
       "            }\n",
       "            parsed_commits.append(parsed_commit)\n",
       "        return parsed_commits\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\n",
       "    self.directory_path = directory_path\n",
       "    self.visitor = visitor\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def _process_file(self, file_path: str):\n",
       "    with open(file_path, \"r\") as file:\n",
       "        source_code = file.read()\n",
       "\n",
       "    try:\n",
       "        tree = cst.parse_module(source_code)\n",
       "    except cst.ParserSyntaxError:\n",
       "        print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
       "        return\n",
       "\n",
       "    tree.visit(self.visitor)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def process_file(self, file_path: str):\n",
       "    # Run flake8 on the file\n",
       "    result = subprocess.run(\n",
       "        [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
       "    )\n",
       "\n",
       "    if result.returncode != 0:\n",
       "        print(f\"Skipping file with syntax error: {file_path}\")\n",
       "        print(result.stderr.decode(\"utf-8\"))\n",
       "        return\n",
       "\n",
       "    with open(file_path, \"r\") as f:\n",
       "        source_code = f.read()\n",
       "\n",
       "    try:\n",
       "        tree = cst.parse_module(source_code)\n",
       "        tree.visit(self.visitor)\n",
       "    except cst.ParserSyntaxError as e:\n",
       "        print(f\"Syntax error: {e}\")\n",
       "        print(f\"Skipping file with syntax error: {file_path}\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def process_directory(self) -> List[str]:\n",
       "    function_source_codes = []\n",
       "    class_source_codes = []\n",
       "\n",
       "    for root, _, files in os.walk(self.directory_path):\n",
       "        for file in files:\n",
       "            if file.endswith(\".py\"):\n",
       "                file_path = os.path.join(root, file)\n",
       "                self._process_file(file_path)\n",
       "\n",
       "    function_source_codes = self.visitor.function_source_codes\n",
       "    function_nodes = self.visitor.function_nodes\n",
       "    class_source_codes = self.visitor.class_source_codes\n",
       "    class_nodes = self.visitor.class_nodes\n",
       "\n",
       "    return function_source_codes, class_source_codes, function_nodes, class_nodes\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def clone_repo(self, repo_url):\n",
       "    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
       "    target_directory = os.path.join(self.directory_path, repo_name)\n",
       "\n",
       "    if os.path.exists(target_directory):\n",
       "        shutil.rmtree(target_directory)\n",
       "\n",
       "    subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
       "\n",
       "    return target_directory\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(\n",
       "    self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
       "):\n",
       "    self.username = username\n",
       "    self.repo_name = repo_name\n",
       "    self.github = Github()\n",
       "    self.directory_processor = None\n",
       "    self.function_source_codes = []\n",
       "    self.class_source_codes = []\n",
       "    self.visitor = visitor\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_public_repos(self):\n",
       "    user = self.github.get_user(self.username)\n",
       "    return user.get_repos()\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def process_repos(self, base_directory):\n",
       "    self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
       "    for repo in self.get_public_repos():\n",
       "        if not repo.private:\n",
       "            print(f\"Processing repo: {repo.name}\")\n",
       "            repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
       "            (\n",
       "                function_source_codes,\n",
       "                class_source_codes,\n",
       "            ) = self.directory_processor.process_directory()\n",
       "            self.function_source_codes.extend(function_source_codes)\n",
       "            self.class_source_codes.extend(class_source_codes)\n",
       "            shutil.rmtree(repo_path)\n",
       "\n",
       "    return self.directory_processor\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(\n",
       "    self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
       "):\n",
       "    self.username = username\n",
       "    self.repo_name = repo_name\n",
       "    self.github = Github()\n",
       "    self.directory_processor = None\n",
       "    self.function_source_codes = []\n",
       "    self.function_nodes = []\n",
       "    self.class_source_codes = []\n",
       "    self.class_nodes = []\n",
       "    self.visitor = visitor\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_repo(self, repo_name):\n",
       "    user = self.github.get_user(self.username)\n",
       "    return user.get_repo(repo_name)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def process_repo(self, base_directory):\n",
       "    self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
       "    repo = self.get_repo(self.repo_name)\n",
       "    print(f\"Processing repo: {self.repo_name}\")\n",
       "    repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
       "    (\n",
       "        function_source_codes,\n",
       "        class_source_codes,\n",
       "        function_nodes,\n",
       "        class_nodes,\n",
       "    ) = self.directory_processor.process_directory()\n",
       "    self.function_source_codes.extend(function_source_codes)\n",
       "    self.function_nodes.extend(function_nodes)\n",
       "    self.class_source_codes.extend(class_source_codes)\n",
       "    self.class_nodes.extend(class_nodes)\n",
       "    shutil.rmtree(repo_path)\n",
       "    return self.directory_processor\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_values(self):\n",
       "    # concatenate the function and class source codes\n",
       "    self.function_source_codes.extend(self.class_source_codes)\n",
       "    self.function_nodes.extend(self.class_nodes)\n",
       "    return self.function_source_codes, self.function_nodes\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import os\n",
       "import shutil\n",
       "import subprocess\n",
       "from typing import List\n",
       "\n",
       "import libcst as cst\n",
       "from github import Github\n",
       "\n",
       "from babydragon.working_memory.parsers.visitors import FunctionAndClassVisitor\n",
       "\n",
       "\n",
       "class DirectoryProcessor:\n",
       "    def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\n",
       "        self.directory_path = directory_path\n",
       "        self.visitor = visitor\n",
       "\n",
       "    def _process_file(self, file_path: str):\n",
       "        with open(file_path, \"r\") as file:\n",
       "            source_code = file.read()\n",
       "\n",
       "        try:\n",
       "            tree = cst.parse_module(source_code)\n",
       "        except cst.ParserSyntaxError:\n",
       "            print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
       "            return\n",
       "\n",
       "        tree.visit(self.visitor)\n",
       "\n",
       "    def process_file(self, file_path: str):\n",
       "        # Run flake8 on the file\n",
       "        result = subprocess.run(\n",
       "            [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
       "        )\n",
       "\n",
       "        if result.returncode != 0:\n",
       "            print(f\"Skipping file with syntax error: {file_path}\")\n",
       "            print(result.stderr.decode(\"utf-8\"))\n",
       "            return\n",
       "\n",
       "        with open(file_path, \"r\") as f:\n",
       "            source_code = f.read()\n",
       "\n",
       "        try:\n",
       "            tree = cst.parse_module(source_code)\n",
       "            tree.visit(self.visitor)\n",
       "        except cst.ParserSyntaxError as e:\n",
       "            print(f\"Syntax error: {e}\")\n",
       "            print(f\"Skipping file with syntax error: {file_path}\")\n",
       "\n",
       "    def process_directory(self) -> List[str]:\n",
       "        function_source_codes = []\n",
       "        class_source_codes = []\n",
       "\n",
       "        for root, _, files in os.walk(self.directory_path):\n",
       "            for file in files:\n",
       "                if file.endswith(\".py\"):\n",
       "                    file_path = os.path.join(root, file)\n",
       "                    self._process_file(file_path)\n",
       "\n",
       "        function_source_codes = self.visitor.function_source_codes\n",
       "        function_nodes = self.visitor.function_nodes\n",
       "        class_source_codes = self.visitor.class_source_codes\n",
       "        class_nodes = self.visitor.class_nodes\n",
       "\n",
       "        return function_source_codes, class_source_codes, function_nodes, class_nodes\n",
       "\n",
       "    def clone_repo(self, repo_url):\n",
       "        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
       "        target_directory = os.path.join(self.directory_path, repo_name)\n",
       "\n",
       "        if os.path.exists(target_directory):\n",
       "            shutil.rmtree(target_directory)\n",
       "\n",
       "        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
       "\n",
       "        return target_directory\n",
       "\n",
       "\n",
       "class GitHubUserProcessor:\n",
       "    def __init__(\n",
       "        self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
       "    ):\n",
       "        self.username = username\n",
       "        self.repo_name = repo_name\n",
       "        self.github = Github()\n",
       "        self.directory_processor = None\n",
       "        self.function_source_codes = []\n",
       "        self.class_source_codes = []\n",
       "        self.visitor = visitor\n",
       "\n",
       "    def get_public_repos(self):\n",
       "        user = self.github.get_user(self.username)\n",
       "        return user.get_repos()\n",
       "\n",
       "    def process_repos(self, base_directory):\n",
       "        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
       "        for repo in self.get_public_repos():\n",
       "            if not repo.private:\n",
       "                print(f\"Processing repo: {repo.name}\")\n",
       "                repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
       "                (\n",
       "                    function_source_codes,\n",
       "                    class_source_codes,\n",
       "                ) = self.directory_processor.process_directory()\n",
       "                self.function_source_codes.extend(function_source_codes)\n",
       "                self.class_source_codes.extend(class_source_codes)\n",
       "                shutil.rmtree(repo_path)\n",
       "\n",
       "        return self.directory_processor\n",
       "\n",
       "\n",
       "class GitHubRepoProcessor:\n",
       "    def __init__(\n",
       "        self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
       "    ):\n",
       "        self.username = username\n",
       "        self.repo_name = repo_name\n",
       "        self.github = Github()\n",
       "        self.directory_processor = None\n",
       "        self.function_source_codes = []\n",
       "        self.function_nodes = []\n",
       "        self.class_source_codes = []\n",
       "        self.class_nodes = []\n",
       "        self.visitor = visitor\n",
       "\n",
       "    def get_repo(self, repo_name):\n",
       "        user = self.github.get_user(self.username)\n",
       "        return user.get_repo(repo_name)\n",
       "\n",
       "    def process_repo(self, base_directory):\n",
       "        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
       "        repo = self.get_repo(self.repo_name)\n",
       "        print(f\"Processing repo: {self.repo_name}\")\n",
       "        repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
       "        (\n",
       "            function_source_codes,\n",
       "            class_source_codes,\n",
       "            function_nodes,\n",
       "            class_nodes,\n",
       "        ) = self.directory_processor.process_directory()\n",
       "        self.function_source_codes.extend(function_source_codes)\n",
       "        self.function_nodes.extend(function_nodes)\n",
       "        self.class_source_codes.extend(class_source_codes)\n",
       "        self.class_nodes.extend(class_nodes)\n",
       "        shutil.rmtree(repo_path)\n",
       "        return self.directory_processor\n",
       "\n",
       "    def get_values(self):\n",
       "        # concatenate the function and class source codes\n",
       "        self.function_source_codes.extend(self.class_source_codes)\n",
       "        self.function_nodes.extend(self.class_nodes)\n",
       "        return self.function_source_codes, self.function_nodes\n",
       "\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    username = \"Danielpatrickhug\"\n",
       "    repo_name = \"GitModel\"\n",
       "    base_directory = \"work_folder\"\n",
       "\n",
       "    # Make sure the work folder exists\n",
       "    if not os.path.exists(base_directory):\n",
       "        os.mkdir(base_directory)\n",
       "\n",
       "    repo_processor = GitHubRepoProcessor(username=username, repo_name=repo_name)\n",
       "    count = repo_processor.process_repo(base_directory)\n",
       "\n",
       "    # Print the list of function source codes\n",
       "    for i, function_source_code in enumerate(\n",
       "        repo_processor.function_source_codes, start=1\n",
       "    ):\n",
       "        print(f\"Function {i} source code:\\n{function_source_code}\\n\")\n",
       "\n",
       "    # Print the list of class source codes\n",
       "    for i, class_source_code in enumerate(repo_processor.class_source_codes, start=1):\n",
       "        print(f\"Class {i} source code:\\n{class_source_code}\\n\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self, code: str = None):\n",
       "\n",
       "    self.code = code\n",
       "    self.output_code = None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def minify(self):\n",
       "    if self.code:\n",
       "        self.output_code = self.minify_code(self.code)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_minified_code(self):\n",
       "    if not self.output_code:\n",
       "        self.minify()\n",
       "    return self.output_code\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "@staticmethod\n",
       "def minify_code(code: str) -> str:\n",
       "    return minify(code)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value @staticmethod\n",
       "def extract_docstring(function_def: cst.FunctionDef) -> str:\n",
       "    docstring = None\n",
       "\n",
       "    for stmt in function_def.body.body:\n",
       "        if isinstance(stmt, cst.SimpleStatementLine):\n",
       "            for expr in stmt.body:\n",
       "                if isinstance(expr, cst.Expr) and isinstance(\n",
       "                    expr.value, cst.SimpleString\n",
       "                ):\n",
       "                    docstring = expr.value.value.strip('\"').strip(\"'\")\n",
       "                    break\n",
       "        if docstring is not None:\n",
       "            break\n",
       "\n",
       "    if docstring is not None:\n",
       "        return docstring.strip()\n",
       "    else:\n",
       "        function_name = function_def.name.value\n",
       "        return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self):\n",
       "    self.function_source_codes = []\n",
       "    self.function_nodes = []\n",
       "    self.class_source_codes = []\n",
       "    self.class_nodes = []\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "    \"\"\"This method is called for every FunctionDef node in the tree.\n",
       "        and it does the following:\n",
       "        1. Gets the source code for the node\n",
       "        2. Adds the node to the list of function nodes\n",
       "        3. Adds the source code to the list of function source codes\n",
       "        \"\"\"\n",
       "    function_source_code = cst.Module([]).code_for_node(node)\n",
       "    self.function_nodes.append(node)\n",
       "    self.function_source_codes.append(function_source_code)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
       "    \"\"\"This method is called for every ClassDef node in the tree.\n",
       "        and it does the following:\n",
       "        1. Gets the source code for the node\n",
       "        2. Adds the node to the list of class nodes\n",
       "        3. Adds the source code to the list of class source codes\n",
       "        \"\"\"\n",
       "    class_source_code = cst.Module([]).code_for_node(node)\n",
       "    self.class_nodes.append(node)\n",
       "    self.class_source_codes.append(class_source_code)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(\n",
       "    self,\n",
       "    directory_path: str,\n",
       "    visitor: Optional[FunctionAndClassVisitor] = None,\n",
       "    minify_code: bool = False,\n",
       "    remove_docstrings: bool = False,\n",
       "):\n",
       "    super().__init__(directory_path)\n",
       "    self.visitor = visitor if visitor else FunctionAndClassVisitor()\n",
       "    self.minify_code = minify_code\n",
       "    self.remove_docstrings = remove_docstrings\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def remove_docstring(self, tree: cst.Module) -> str:\n",
       "    \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\n",
       "\n",
       "    # Remove docstrings using a transformer\n",
       "    class DocstringRemover(cst.CSTTransformer):\n",
       "        def leave_FunctionDef(\n",
       "            self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
       "        ) -> cst.FunctionDef:\n",
       "            docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
       "            if docstring.startswith(\"No docstring\"):\n",
       "                return updated_node\n",
       "\n",
       "            return updated_node.with_changes(\n",
       "                body=updated_node.body.with_changes(\n",
       "                    body=[\n",
       "                        stmt\n",
       "                        for stmt in updated_node.body.body\n",
       "                        if not (\n",
       "                            isinstance(stmt, cst.SimpleStatementLine)\n",
       "                            and any(\n",
       "                                isinstance(expr, cst.Expr)\n",
       "                                and isinstance(expr.value, cst.SimpleString)\n",
       "                                for expr in stmt.body\n",
       "                            )\n",
       "                        )\n",
       "                    ]\n",
       "                )\n",
       "            )\n",
       "\n",
       "    tree = tree.visit(DocstringRemover())\n",
       "    return tree.code\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def leave_FunctionDef(\n",
       "    self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
       ") -> cst.FunctionDef:\n",
       "    docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
       "    if docstring.startswith(\"No docstring\"):\n",
       "        return updated_node\n",
       "\n",
       "    return updated_node.with_changes(\n",
       "        body=updated_node.body.with_changes(\n",
       "            body=[\n",
       "                stmt\n",
       "                for stmt in updated_node.body.body\n",
       "                if not (\n",
       "                    isinstance(stmt, cst.SimpleStatementLine)\n",
       "                    and any(\n",
       "                        isinstance(expr, cst.Expr)\n",
       "                        and isinstance(expr.value, cst.SimpleString)\n",
       "                        for expr in stmt.body\n",
       "                    )\n",
       "                )\n",
       "            ]\n",
       "        )\n",
       "    )\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def _process_file(self, file_path: str):\n",
       "    \"\"\"This method is called for every file in the directory.\n",
       "        It does the following:\n",
       "        1. Reads the file\n",
       "        2. Parses the file\n",
       "        3. Visits the file with the visitor\n",
       "        \"\"\"\n",
       "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
       "        source_code = file.read()\n",
       "\n",
       "    try:\n",
       "        tree = cst.parse_module(source_code)\n",
       "    except cst.ParserSyntaxError:\n",
       "        print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
       "        return\n",
       "\n",
       "    tree.visit(self.visitor)\n",
       "\n",
       "    # Remove docstrings if specified\n",
       "    if self.remove_docstrings:\n",
       "        source_code = self.remove_docstring(source_code, tree)\n",
       "\n",
       "    # Minify the code if specified\n",
       "    if self.minify_code:\n",
       "        minifier = PythonMinifier(source_code)\n",
       "        source_code = minifier.get_minified_code()\n",
       "\n",
       "    # Add the processed code to the corresponding list in the visitor\n",
       "    self.visitor.function_source_codes.append(source_code)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def process_file(self, file_path: str):\n",
       "    \"\"\"This method is called for every file in the directory.\n",
       "        It does the following:\n",
       "        1. Runs flake8 on the file\n",
       "        if flake8 returns a non-zero exit code, it means the file has a syntax error\n",
       "        2. Reads the file\n",
       "        3. Parses the file\n",
       "        4. Visits the file with the visitor\n",
       "\n",
       "        \"\"\"\n",
       "    result = subprocess.run(\n",
       "        [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
       "    )\n",
       "\n",
       "    if result.returncode != 0:\n",
       "        print(f\"Skipping file with syntax error: {file_path}\")\n",
       "        print(result.stderr.decode(\"utf-8\"))\n",
       "        return\n",
       "\n",
       "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
       "        source_code = f.read()\n",
       "\n",
       "    try:\n",
       "        tree = cst.parse_module(source_code)\n",
       "        tree.visit(self.visitor)\n",
       "    except cst.ParserSyntaxError as e:\n",
       "        print(f\"Syntax error: {e}\")\n",
       "        print(f\"Skipping file with syntax error: {file_path}\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def process_directory(\n",
       "    self,\n",
       ") -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\n",
       "    \"\"\"This method is called for every directory.\n",
       "        It does the following:\n",
       "        1. Gets all the python files in the directory\n",
       "        2. Processes each file\n",
       "        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\n",
       "        \"\"\"\n",
       "    function_source_codes = []\n",
       "    class_source_codes = []\n",
       "\n",
       "    python_files = self.get_files_with_extension(\".py\")\n",
       "\n",
       "    for file_path in python_files:\n",
       "        self._process_file(file_path)\n",
       "\n",
       "    function_source_codes = self.visitor.function_source_codes\n",
       "    function_nodes = self.visitor.function_nodes\n",
       "    class_source_codes = self.visitor.class_source_codes\n",
       "    class_nodes = self.visitor.class_nodes\n",
       "\n",
       "    return function_source_codes, class_source_codes, function_nodes, class_nodes\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import os\n",
       "import subprocess\n",
       "from typing import List, Optional, Tuple, Union\n",
       "\n",
       "import libcst as cst\n",
       "from python_minifier import minify\n",
       "\n",
       "from babydragon.processors.os_processor import OsProcessor\n",
       "\n",
       "\n",
       "class PythonMinifier:\n",
       "    def __init__(self, code: str = None):\n",
       "\n",
       "        self.code = code\n",
       "        self.output_code = None\n",
       "\n",
       "    def minify(self):\n",
       "        if self.code:\n",
       "            self.output_code = self.minify_code(self.code)\n",
       "\n",
       "    def get_minified_code(self):\n",
       "        if not self.output_code:\n",
       "            self.minify()\n",
       "        return self.output_code\n",
       "\n",
       "    @staticmethod\n",
       "    def minify_code(code: str) -> str:\n",
       "        return minify(code)\n",
       "\n",
       "\n",
       "class PythonDocstringExtractor:\n",
       "    @staticmethod\n",
       "    def extract_docstring(function_def: cst.FunctionDef) -> str:\n",
       "        docstring = None\n",
       "\n",
       "        for stmt in function_def.body.body:\n",
       "            if isinstance(stmt, cst.SimpleStatementLine):\n",
       "                for expr in stmt.body:\n",
       "                    if isinstance(expr, cst.Expr) and isinstance(\n",
       "                        expr.value, cst.SimpleString\n",
       "                    ):\n",
       "                        docstring = expr.value.value.strip('\"').strip(\"'\")\n",
       "                        break\n",
       "            if docstring is not None:\n",
       "                break\n",
       "\n",
       "        if docstring is not None:\n",
       "            return docstring.strip()\n",
       "        else:\n",
       "            function_name = function_def.name.value\n",
       "            return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n",
       "\n",
       "\n",
       "class FunctionAndClassVisitor(cst.CSTVisitor):\n",
       "    def __init__(self):\n",
       "        self.function_source_codes = []\n",
       "        self.function_nodes = []\n",
       "        self.class_source_codes = []\n",
       "        self.class_nodes = []\n",
       "\n",
       "    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "        \"\"\"This method is called for every FunctionDef node in the tree.\n",
       "        and it does the following:\n",
       "        1. Gets the source code for the node\n",
       "        2. Adds the node to the list of function nodes\n",
       "        3. Adds the source code to the list of function source codes\n",
       "        \"\"\"\n",
       "        function_source_code = cst.Module([]).code_for_node(node)\n",
       "        self.function_nodes.append(node)\n",
       "        self.function_source_codes.append(function_source_code)\n",
       "\n",
       "    def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
       "        \"\"\"This method is called for every ClassDef node in the tree.\n",
       "        and it does the following:\n",
       "        1. Gets the source code for the node\n",
       "        2. Adds the node to the list of class nodes\n",
       "        3. Adds the source code to the list of class source codes\n",
       "        \"\"\"\n",
       "        class_source_code = cst.Module([]).code_for_node(node)\n",
       "        self.class_nodes.append(node)\n",
       "        self.class_source_codes.append(class_source_code)\n",
       "\n",
       "\n",
       "class PythonParser(OsProcessor):\n",
       "    def __init__(\n",
       "        self,\n",
       "        directory_path: str,\n",
       "        visitor: Optional[FunctionAndClassVisitor] = None,\n",
       "        minify_code: bool = False,\n",
       "        remove_docstrings: bool = False,\n",
       "    ):\n",
       "        super().__init__(directory_path)\n",
       "        self.visitor = visitor if visitor else FunctionAndClassVisitor()\n",
       "        self.minify_code = minify_code\n",
       "        self.remove_docstrings = remove_docstrings\n",
       "\n",
       "    def remove_docstring(self, tree: cst.Module) -> str:\n",
       "        \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\n",
       "\n",
       "        # Remove docstrings using a transformer\n",
       "        class DocstringRemover(cst.CSTTransformer):\n",
       "            def leave_FunctionDef(\n",
       "                self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
       "            ) -> cst.FunctionDef:\n",
       "                docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
       "                if docstring.startswith(\"No docstring\"):\n",
       "                    return updated_node\n",
       "\n",
       "                return updated_node.with_changes(\n",
       "                    body=updated_node.body.with_changes(\n",
       "                        body=[\n",
       "                            stmt\n",
       "                            for stmt in updated_node.body.body\n",
       "                            if not (\n",
       "                                isinstance(stmt, cst.SimpleStatementLine)\n",
       "                                and any(\n",
       "                                    isinstance(expr, cst.Expr)\n",
       "                                    and isinstance(expr.value, cst.SimpleString)\n",
       "                                    for expr in stmt.body\n",
       "                                )\n",
       "                            )\n",
       "                        ]\n",
       "                    )\n",
       "                )\n",
       "\n",
       "        tree = tree.visit(DocstringRemover())\n",
       "        return tree.code\n",
       "\n",
       "    def _process_file(self, file_path: str):\n",
       "        \"\"\"This method is called for every file in the directory.\n",
       "        It does the following:\n",
       "        1. Reads the file\n",
       "        2. Parses the file\n",
       "        3. Visits the file with the visitor\n",
       "        \"\"\"\n",
       "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
       "            source_code = file.read()\n",
       "\n",
       "        try:\n",
       "            tree = cst.parse_module(source_code)\n",
       "        except cst.ParserSyntaxError:\n",
       "            print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
       "            return\n",
       "\n",
       "        tree.visit(self.visitor)\n",
       "\n",
       "        # Remove docstrings if specified\n",
       "        if self.remove_docstrings:\n",
       "            source_code = self.remove_docstring(source_code, tree)\n",
       "\n",
       "        # Minify the code if specified\n",
       "        if self.minify_code:\n",
       "            minifier = PythonMinifier(source_code)\n",
       "            source_code = minifier.get_minified_code()\n",
       "\n",
       "        # Add the processed code to the corresponding list in the visitor\n",
       "        self.visitor.function_source_codes.append(source_code)\n",
       "\n",
       "    def process_file(self, file_path: str):\n",
       "        \"\"\"This method is called for every file in the directory.\n",
       "        It does the following:\n",
       "        1. Runs flake8 on the file\n",
       "        if flake8 returns a non-zero exit code, it means the file has a syntax error\n",
       "        2. Reads the file\n",
       "        3. Parses the file\n",
       "        4. Visits the file with the visitor\n",
       "\n",
       "        \"\"\"\n",
       "        result = subprocess.run(\n",
       "            [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
       "        )\n",
       "\n",
       "        if result.returncode != 0:\n",
       "            print(f\"Skipping file with syntax error: {file_path}\")\n",
       "            print(result.stderr.decode(\"utf-8\"))\n",
       "            return\n",
       "\n",
       "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
       "            source_code = f.read()\n",
       "\n",
       "        try:\n",
       "            tree = cst.parse_module(source_code)\n",
       "            tree.visit(self.visitor)\n",
       "        except cst.ParserSyntaxError as e:\n",
       "            print(f\"Syntax error: {e}\")\n",
       "            print(f\"Skipping file with syntax error: {file_path}\")\n",
       "\n",
       "    def process_directory(\n",
       "        self,\n",
       "    ) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\n",
       "        \"\"\"This method is called for every directory.\n",
       "        It does the following:\n",
       "        1. Gets all the python files in the directory\n",
       "        2. Processes each file\n",
       "        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\n",
       "        \"\"\"\n",
       "        function_source_codes = []\n",
       "        class_source_codes = []\n",
       "\n",
       "        python_files = self.get_files_with_extension(\".py\")\n",
       "\n",
       "        for file_path in python_files:\n",
       "            self._process_file(file_path)\n",
       "\n",
       "        function_source_codes = self.visitor.function_source_codes\n",
       "        function_nodes = self.visitor.function_nodes\n",
       "        class_source_codes = self.visitor.class_source_codes\n",
       "        class_nodes = self.visitor.class_nodes\n",
       "\n",
       "        return function_source_codes, class_source_codes, function_nodes, class_nodes\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def visit_Call(self, node: cst.Call) -> None:\n",
       "    function_name = None\n",
       "    if isinstance(node.func, cst.Name):\n",
       "        function_name = node.func.value\n",
       "\n",
       "    if function_name:\n",
       "        pos = self.get_metadata(PositionProvider, node).start\n",
       "        print(\n",
       "            f\"Function '{function_name}' called at line {pos.line}, column {pos.column} with arguments:\"\n",
       "        )\n",
       "\n",
       "        for arg in node.args:\n",
       "            arg_start_pos = self.get_metadata(PositionProvider, arg).start\n",
       "            arg_value = arg.value\n",
       "            if isinstance(arg_value, cst.SimpleString):\n",
       "                arg_value = arg_value.evaluated_value\n",
       "            print(\n",
       "                f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\n",
       "            )\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self):\n",
       "    self.count = 0\n",
       "    self.functions_with_operation_dict = {}\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "    self.current_function = node\n",
       "    self.functions_with_operation_dict[node.name] = []\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "    self.current_function = None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def visit_BinaryOperation(self, node: cst.BinaryOperation) -> None:\n",
       "    if isinstance(node.operator, cst.Multiply) or isinstance(\n",
       "        node.operator, cst.BitAnd\n",
       "    ):\n",
       "        self.count += 1\n",
       "        if self.current_function:\n",
       "            self.functions_with_operation_dict[self.current_function.name].append(\n",
       "                cst.Module([]).code_for_node(node)\n",
       "            )\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def visit_Call(self, node: cst.Call) -> None:\n",
       "    if m.matches(node, m.Call(func=m.Attribute(attr=m.Name(\"dot\")))) or m.matches(\n",
       "        node, m.Call(func=m.Name(\"dot\"), args=[m.Arg(), m.Arg()])\n",
       "    ):\n",
       "        self.count += 1\n",
       "        if self.current_function:\n",
       "            self.functions_with_operation_dict[self.current_function.name].append(\n",
       "                cst.Module([]).code_for_node(node)\n",
       "            )\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self):\n",
       "    self.function_source_codes = []\n",
       "    self.function_nodes = []\n",
       "    self.class_source_codes = []\n",
       "    self.class_nodes = []\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "    function_source_code = cst.Module([]).code_for_node(node)\n",
       "    # add in place summary and code mod\n",
       "    self.function_nodes.append(node)\n",
       "    self.function_source_codes.append(function_source_code)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
       "    class_source_code = cst.Module([]).code_for_node(node)\n",
       "    # add in place summary and code mod\n",
       "    self.class_nodes.append(node)\n",
       "    self.class_source_codes.append(class_source_code)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self):\n",
       "    # stack for storing the canonical name of the current function\n",
       "    self.stack: List[Tuple[str, ...]] = []\n",
       "    # store the annotations\n",
       "    self.annotations: Dict[\n",
       "        Tuple[str, ...],  # key: tuple of canonical class/function name\n",
       "        Tuple[cst.Parameters, Optional[cst.Annotation]],  # value: (params, returns)\n",
       "    ] = {}\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def visit_ClassDef(self, node: cst.ClassDef) -> Optional[bool]:\n",
       "    self.stack.append(node.name.value)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def leave_ClassDef(self, node: cst.ClassDef) -> None:\n",
       "    self.stack.pop()\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def visit_FunctionDef(self, node: cst.FunctionDef) -> Optional[bool]:\n",
       "    self.stack.append(node.name.value)\n",
       "    self.annotations[tuple(self.stack)] = (node.params, node.returns)\n",
       "    return False  # pyi files don't support inner functions, return False to stop the traversal.\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "    self.stack.pop()\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value from typing import Dict, List, Optional, Tuple\n",
       "\n",
       "import libcst as cst\n",
       "import libcst.matchers as m\n",
       "from libcst.metadata import PositionProvider\n",
       "\n",
       "\n",
       "# A custom visitor to find function calls and their arguments\n",
       "class FunctionCallFinder(cst.CSTVisitor):\n",
       "    METADATA_DEPENDENCIES = (PositionProvider,)\n",
       "\n",
       "    def visit_Call(self, node: cst.Call) -> None:\n",
       "        function_name = None\n",
       "        if isinstance(node.func, cst.Name):\n",
       "            function_name = node.func.value\n",
       "\n",
       "        if function_name:\n",
       "            pos = self.get_metadata(PositionProvider, node).start\n",
       "            print(\n",
       "                f\"Function '{function_name}' called at line {pos.line}, column {pos.column} with arguments:\"\n",
       "            )\n",
       "\n",
       "            for arg in node.args:\n",
       "                arg_start_pos = self.get_metadata(PositionProvider, arg).start\n",
       "                arg_value = arg.value\n",
       "                if isinstance(arg_value, cst.SimpleString):\n",
       "                    arg_value = arg_value.evaluated_value\n",
       "                print(\n",
       "                    f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\n",
       "                )\n",
       "\n",
       "\n",
       "class MultiplicationCounterVisitor(cst.CSTVisitor):\n",
       "    def __init__(self):\n",
       "        self.count = 0\n",
       "        self.functions_with_operation_dict = {}\n",
       "\n",
       "    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "        self.current_function = node\n",
       "        self.functions_with_operation_dict[node.name] = []\n",
       "\n",
       "    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "        self.current_function = None\n",
       "\n",
       "    def visit_BinaryOperation(self, node: cst.BinaryOperation) -> None:\n",
       "        if isinstance(node.operator, cst.Multiply) or isinstance(\n",
       "            node.operator, cst.BitAnd\n",
       "        ):\n",
       "            self.count += 1\n",
       "            if self.current_function:\n",
       "                self.functions_with_operation_dict[self.current_function.name].append(\n",
       "                    cst.Module([]).code_for_node(node)\n",
       "                )\n",
       "\n",
       "    def visit_Call(self, node: cst.Call) -> None:\n",
       "        if m.matches(node, m.Call(func=m.Attribute(attr=m.Name(\"dot\")))) or m.matches(\n",
       "            node, m.Call(func=m.Name(\"dot\"), args=[m.Arg(), m.Arg()])\n",
       "        ):\n",
       "            self.count += 1\n",
       "            if self.current_function:\n",
       "                self.functions_with_operation_dict[self.current_function.name].append(\n",
       "                    cst.Module([]).code_for_node(node)\n",
       "                )\n",
       "\n",
       "\n",
       "class FunctionAndClassVisitor(cst.CSTVisitor):\n",
       "    def __init__(self):\n",
       "        self.function_source_codes = []\n",
       "        self.function_nodes = []\n",
       "        self.class_source_codes = []\n",
       "        self.class_nodes = []\n",
       "\n",
       "    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "        function_source_code = cst.Module([]).code_for_node(node)\n",
       "        # add in place summary and code mod\n",
       "        self.function_nodes.append(node)\n",
       "        self.function_source_codes.append(function_source_code)\n",
       "\n",
       "    def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
       "        class_source_code = cst.Module([]).code_for_node(node)\n",
       "        # add in place summary and code mod\n",
       "        self.class_nodes.append(node)\n",
       "        self.class_source_codes.append(class_source_code)\n",
       "\n",
       "\n",
       "class TypingCollector(cst.CSTVisitor):\n",
       "    def __init__(self):\n",
       "        # stack for storing the canonical name of the current function\n",
       "        self.stack: List[Tuple[str, ...]] = []\n",
       "        # store the annotations\n",
       "        self.annotations: Dict[\n",
       "            Tuple[str, ...],  # key: tuple of canonical class/function name\n",
       "            Tuple[cst.Parameters, Optional[cst.Annotation]],  # value: (params, returns)\n",
       "        ] = {}\n",
       "\n",
       "    def visit_ClassDef(self, node: cst.ClassDef) -> Optional[bool]:\n",
       "        self.stack.append(node.name.value)\n",
       "\n",
       "    def leave_ClassDef(self, node: cst.ClassDef) -> None:\n",
       "        self.stack.pop()\n",
       "\n",
       "    def visit_FunctionDef(self, node: cst.FunctionDef) -> Optional[bool]:\n",
       "        self.stack.append(node.name.value)\n",
       "        self.annotations[tuple(self.stack)] = (node.params, node.returns)\n",
       "        return False  # pyi files don't support inner functions, return False to stop the traversal.\n",
       "\n",
       "    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "        self.stack.pop()\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value # This is the __init__.py file for the package.\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self, index: MemoryIndex, path: List[List[int]], max_workers: int = 1):\n",
       "    \"\"\"\n",
       "        Initialize a BaseTask instance.\n",
       "\n",
       "        :param index: List of strings representing the queries.\n",
       "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
       "        :param max_workers: Maximum number of worker threads (default is 4).\n",
       "        \"\"\"\n",
       "    self.index = index\n",
       "    self.path = path\n",
       "    self.results = []\n",
       "    self.max_workers = max_workers\n",
       "    self.parallel = True if max_workers > 1 else False\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
       "    \"\"\"\n",
       "        to be implemented by subclasses:\n",
       "\n",
       "        :param sub_path: List of indices representing the sub-task's sequence.\n",
       "        :return: List of strings representing the responses for each query in the sub-task.\n",
       "        \"\"\"\n",
       "\n",
       "    sub_results = []\n",
       "    for i in sub_path:\n",
       "        response = self.index[i]\n",
       "        sub_results.append(response)\n",
       "    return sub_results\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def execute_task(self) -> None:\n",
       "    \"\"\"\n",
       "        Execute the task by concurrently processing sub-tasks using worker threads.\n",
       "        \"\"\"\n",
       "    \n",
       "    for sub_path in self.path:\n",
       "        self.results.append(self._execute_sub_task(sub_path))\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import copy\n",
       "from concurrent.futures import ThreadPoolExecutor\n",
       "from typing import Any, List\n",
       "\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "\n",
       "\n",
       "class BaseTask:\n",
       "    def __init__(self, index: MemoryIndex, path: List[List[int]], max_workers: int = 1):\n",
       "        \"\"\"\n",
       "        Initialize a BaseTask instance.\n",
       "\n",
       "        :param index: List of strings representing the queries.\n",
       "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
       "        :param max_workers: Maximum number of worker threads (default is 4).\n",
       "        \"\"\"\n",
       "        self.index = index\n",
       "        self.path = path\n",
       "        self.results = []\n",
       "        self.max_workers = max_workers\n",
       "        self.parallel = True if max_workers > 1 else False\n",
       "\n",
       "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
       "        \"\"\"\n",
       "        to be implemented by subclasses:\n",
       "\n",
       "        :param sub_path: List of indices representing the sub-task's sequence.\n",
       "        :return: List of strings representing the responses for each query in the sub-task.\n",
       "        \"\"\"\n",
       "\n",
       "        sub_results = []\n",
       "        for i in sub_path:\n",
       "            response = self.index[i]\n",
       "            sub_results.append(response)\n",
       "        return sub_results\n",
       "\n",
       "    def execute_task(self) -> None:\n",
       "        \"\"\"\n",
       "        Execute the task by concurrently processing sub-tasks using worker threads.\n",
       "        \"\"\"\n",
       "        \n",
       "        for sub_path in self.path:\n",
       "            self.results.append(self._execute_sub_task(sub_path))\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(\n",
       "    self,\n",
       "    index: MemoryIndex,\n",
       "    path: List[List[int]],\n",
       "    chatbot: Chat,\n",
       "    read_func = None,\n",
       "    max_workers: int = 1,\n",
       "):\n",
       "    \"\"\"\n",
       "        Initialize a LLMReadTask instance.\n",
       "\n",
       "        :param index: List of strings representing the queries.\n",
       "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
       "        :param chatbot: Chatbot instance used for executing queries.\n",
       "        :param max_workers: Maximum number of worker threads (default is 4).\n",
       "        \"\"\"\n",
       "    BaseTask.__init__(self, index, path, max_workers)\n",
       "    self.chatbot = chatbot\n",
       "    self.read_func = read_func if read_func else self.llm_response\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def llm_response(chatbot: Chat, message: str, string_out=False):\n",
       "    if string_out:\n",
       "        return chatbot.reply(message)\n",
       "    return chatbot.query(message)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
       "    \"\"\"\n",
       "        Execute a sub-task using a separate copy of the chatbot instance. each sub-stasks uses a\n",
       "        a clean memory instance.\n",
       "\n",
       "        :param sub_path: List of indices representing the sub-task's sequence.\n",
       "        :return: List of strings representing the responses for each query in the sub-task.\n",
       "        \"\"\"\n",
       "    if self.parallel:\n",
       "        # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
       "        chatbot_instance = copy.deepcopy(self.chatbot)\n",
       "    else:\n",
       "        chatbot_instance = self.chatbot\n",
       "    if isinstance(self.chatbot, BaseThread):\n",
       "        chatbot_instance.reset_memory()\n",
       "\n",
       "    sub_results = []\n",
       "    for i in sub_path:\n",
       "        response = self.read_func(chatbot_instance, self.index.values[i])\n",
       "        sub_results.append(response)\n",
       "    return sub_results\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def read(self):\n",
       "    self.execute_task()\n",
       "    return self.results\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(\n",
       "    self,\n",
       "    index: MemoryIndex,\n",
       "    path: List[List[int]],\n",
       "    chatbot: Chat,\n",
       "    write_func = None,\n",
       "    context= None,\n",
       "    task_name=\"summary\",\n",
       "    max_workers: int = 1,\n",
       "):\n",
       "    \"\"\"\n",
       "        Initialize a LLMWriteTask instance.\n",
       "\n",
       "        :param index: List of strings representing the queries.\n",
       "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
       "        :param chatbot: Chatbot instance used for executing queries.\n",
       "        :param max_workers: Maximum number of worker threads (default is 4).\n",
       "        \"\"\"\n",
       "    BaseTask.__init__(self, index, path, max_workers)\n",
       "    self.chatbot = chatbot\n",
       "    self.write_func = write_func if write_func else self.llm_response\n",
       "    self.new_index_name = self.index.name + f\"_{task_name}\"\n",
       "    self.context = context\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "@staticmethod\n",
       "def llm_response(chatbot: Chat, message: str, context = None, id = None):\n",
       "    return chatbot.reply(message)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
       "    \"\"\"\n",
       "        Execute a sub-task using a separate copy of the chatbot instance.\n",
       "\n",
       "        :param sub_path: List of indices representing the sub-task's sequence.\n",
       "        :return: List of strings representing the responses for each query in the sub-task.\n",
       "        \"\"\"\n",
       "    if self.parallel:\n",
       "        # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
       "        chatbot_instance = copy.deepcopy(self.chatbot)\n",
       "    else:\n",
       "        chatbot_instance = self.chatbot\n",
       "    if isinstance(self.chatbot, BaseThread):\n",
       "        chatbot_instance.reset_memory()\n",
       "\n",
       "    sub_results = {}\n",
       "    for i in sub_path:\n",
       "        current_val = self.index.values[i]\n",
       "        response = self.write_func(chatbot_instance, current_val, self.context, id = i)\n",
       "        sub_results[i] = response\n",
       "    return sub_results\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def write(self):\n",
       "    self.execute_task()\n",
       "    content_to_write = []\n",
       "    for sub_result in self.results:\n",
       "        for index_id, response in sub_result.items():\n",
       "            content_to_write.append((index_id, response))\n",
       "        # sort the content to write by index_id\n",
       "    content_to_write.sort(key=lambda x: x[0])\n",
       "    self.new_index = MemoryIndex(name=self.new_index_name)\n",
       "    self.new_index.init_index(values=[x[1] for x in content_to_write])\n",
       "    self.new_index.save()\n",
       "    return self.new_index\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import copy\n",
       "from typing import Any, List\n",
       "from babydragon.chat.chat import Chat\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "from babydragon.memory.threads.base_thread import BaseThread\n",
       "from babydragon.tasks.base_task import BaseTask\n",
       "\n",
       "\n",
       "class LLMReader(BaseTask):\n",
       "    def __init__(\n",
       "        self,\n",
       "        index: MemoryIndex,\n",
       "        path: List[List[int]],\n",
       "        chatbot: Chat,\n",
       "        read_func = None,\n",
       "        max_workers: int = 1,\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Initialize a LLMReadTask instance.\n",
       "\n",
       "        :param index: List of strings representing the queries.\n",
       "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
       "        :param chatbot: Chatbot instance used for executing queries.\n",
       "        :param max_workers: Maximum number of worker threads (default is 4).\n",
       "        \"\"\"\n",
       "        BaseTask.__init__(self, index, path, max_workers)\n",
       "        self.chatbot = chatbot\n",
       "        self.read_func = read_func if read_func else self.llm_response\n",
       "\n",
       "    def llm_response(chatbot: Chat, message: str, string_out=False):\n",
       "        if string_out:\n",
       "            return chatbot.reply(message)\n",
       "        return chatbot.query(message)\n",
       "\n",
       "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
       "        \"\"\"\n",
       "        Execute a sub-task using a separate copy of the chatbot instance. each sub-stasks uses a\n",
       "        a clean memory instance.\n",
       "\n",
       "        :param sub_path: List of indices representing the sub-task's sequence.\n",
       "        :return: List of strings representing the responses for each query in the sub-task.\n",
       "        \"\"\"\n",
       "        if self.parallel:\n",
       "            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
       "            chatbot_instance = copy.deepcopy(self.chatbot)\n",
       "        else:\n",
       "            chatbot_instance = self.chatbot\n",
       "        if isinstance(self.chatbot, BaseThread):\n",
       "            chatbot_instance.reset_memory()\n",
       "\n",
       "        sub_results = []\n",
       "        for i in sub_path:\n",
       "            response = self.read_func(chatbot_instance, self.index.values[i])\n",
       "            sub_results.append(response)\n",
       "        return sub_results\n",
       "\n",
       "    def read(self):\n",
       "        self.execute_task()\n",
       "        return self.results\n",
       "\n",
       "\n",
       "class LLMWriter(BaseTask):\n",
       "    def __init__(\n",
       "        self,\n",
       "        index: MemoryIndex,\n",
       "        path: List[List[int]],\n",
       "        chatbot: Chat,\n",
       "        write_func = None,\n",
       "        context= None,\n",
       "        task_name=\"summary\",\n",
       "        max_workers: int = 1,\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Initialize a LLMWriteTask instance.\n",
       "\n",
       "        :param index: List of strings representing the queries.\n",
       "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
       "        :param chatbot: Chatbot instance used for executing queries.\n",
       "        :param max_workers: Maximum number of worker threads (default is 4).\n",
       "        \"\"\"\n",
       "        BaseTask.__init__(self, index, path, max_workers)\n",
       "        self.chatbot = chatbot\n",
       "        self.write_func = write_func if write_func else self.llm_response\n",
       "        self.new_index_name = self.index.name + f\"_{task_name}\"\n",
       "        self.context = context\n",
       "\n",
       "    @staticmethod\n",
       "    def llm_response(chatbot: Chat, message: str, context = None, id = None):\n",
       "        return chatbot.reply(message)\n",
       "\n",
       "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
       "        \"\"\"\n",
       "        Execute a sub-task using a separate copy of the chatbot instance.\n",
       "\n",
       "        :param sub_path: List of indices representing the sub-task's sequence.\n",
       "        :return: List of strings representing the responses for each query in the sub-task.\n",
       "        \"\"\"\n",
       "        if self.parallel:\n",
       "            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
       "            chatbot_instance = copy.deepcopy(self.chatbot)\n",
       "        else:\n",
       "            chatbot_instance = self.chatbot\n",
       "        if isinstance(self.chatbot, BaseThread):\n",
       "            chatbot_instance.reset_memory()\n",
       "\n",
       "        sub_results = {}\n",
       "        for i in sub_path:\n",
       "            current_val = self.index.values[i]\n",
       "            response = self.write_func(chatbot_instance, current_val, self.context, id = i)\n",
       "            sub_results[i] = response\n",
       "        return sub_results\n",
       "\n",
       "    def write(self):\n",
       "        self.execute_task()\n",
       "        content_to_write = []\n",
       "        for sub_result in self.results:\n",
       "            for index_id, response in sub_result.items():\n",
       "                content_to_write.append((index_id, response))\n",
       "        # sort the content to write by index_id\n",
       "        content_to_write.sort(key=lambda x: x[0])\n",
       "        self.new_index = MemoryIndex(name=self.new_index_name)\n",
       "        self.new_index.init_index(values=[x[1] for x in content_to_write])\n",
       "        self.new_index.save()\n",
       "        return self.new_index\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value # This is the __init__.py file for the package.\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "def mark_system(system_prompt):\n",
       "    return {\"role\": \"system\", \"content\": system_prompt}\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "def mark_answer(answer):\n",
       "    return {\"role\": \"assistant\", \"content\": answer}\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "def mark_question(question):\n",
       "    return {\"role\": \"user\", \"content\": question}\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "def check_dict(message_dict):\n",
       "    if (\n",
       "        type(message_dict) is list\n",
       "        and len(message_dict) == 1\n",
       "        and type(message_dict[0]) is dict\n",
       "    ):\n",
       "        message_dict = message_dict[0]\n",
       "    elif type(message_dict) is not dict:\n",
       "        raise Exception(\n",
       "            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\n",
       "            message_dict,\n",
       "            type(message_dict),\n",
       "        )\n",
       "    return message_dict\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "def get_mark_from_response(response):\n",
       "    # return the answer from the response\n",
       "    role = response[\"choices\"][0][\"message\"][\"role\"]\n",
       "    message = response[\"choices\"][0][\"message\"][\"content\"]\n",
       "    return {\"role\": role, \"content\": message}\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "def get_str_from_response(response):\n",
       "    # return the answer from the response\n",
       "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import openai\n",
       "\n",
       "\n",
       "def mark_system(system_prompt):\n",
       "    return {\"role\": \"system\", \"content\": system_prompt}\n",
       "\n",
       "\n",
       "def mark_answer(answer):\n",
       "    return {\"role\": \"assistant\", \"content\": answer}\n",
       "\n",
       "\n",
       "def mark_question(question):\n",
       "    return {\"role\": \"user\", \"content\": question}\n",
       "\n",
       "\n",
       "def check_dict(message_dict):\n",
       "    if (\n",
       "        type(message_dict) is list\n",
       "        and len(message_dict) == 1\n",
       "        and type(message_dict[0]) is dict\n",
       "    ):\n",
       "        message_dict = message_dict[0]\n",
       "    elif type(message_dict) is not dict:\n",
       "        raise Exception(\n",
       "            \"The message_dict should be a dictionary or a [dictionary] instead it is \",\n",
       "            message_dict,\n",
       "            type(message_dict),\n",
       "        )\n",
       "    return message_dict\n",
       "\n",
       "\n",
       "def get_mark_from_response(response):\n",
       "    # return the answer from the response\n",
       "    role = response[\"choices\"][0][\"message\"][\"role\"]\n",
       "    message = response[\"choices\"][0][\"message\"][\"content\"]\n",
       "    return {\"role\": role, \"content\": message}\n",
       "\n",
       "\n",
       "def get_str_from_response(response):\n",
       "    # return the answer from the response\n",
       "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self):\n",
       "    self.git_memory = None\n",
       "    self.commit_index = None\n",
       "    self.contextual_memory = None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def generate_meta_code(self, user_input):\n",
       "    # Transform user input into meta-code representation\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def load_git_memory(self, git_memory):\n",
       "    # Retrieve GitMemory context\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def load_commit_context(self, commit_index):\n",
       "    # Retrieve commit context\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def load_contextual_resources(self, contextual_memory):\n",
       "    # Load contextual resources required for code generation\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_current_draft(self, commit_context):\n",
       "    # Get the current draft code from commit context\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def generate_modifications(self, draft_code, meta_code, context_resources):\n",
       "    # Generate modifications to the draft code based on meta-code and context resources\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def extract_source_code(self, modifications, context_resources):\n",
       "    # Extract source code from modifications and context resources\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def apply_modifications(self, draft_code, modifications):\n",
       "    # Apply modifications to the draft code\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def validate_draft(self, updated_draft, source_code):\n",
       "    # Validate the updated draft code and check for consistency\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def compare_draft_with_objective(self, updated_draft, meta_code):\n",
       "    # Compare the updated draft code with user's goal based on meta-code\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def store_draft_to_commit_index(self, commit_index, updated_draft):\n",
       "    # Store the updated draft code in the commit index\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def rollback(self, commit_index):\n",
       "    # Roll back the draft code to the previous state in the commit index\n",
       "    pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def codeGenerationIteration(\n",
       "    self, userInput, gitMemory, commitIndex, contextualMemory\n",
       "):\n",
       "    metaCode = self.generateMetaCode(userInput)\n",
       "\n",
       "    # Concurrent processing - Step 1\n",
       "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
       "        gitMemoryContext = executor.submit(self.loadGitMemory, gitMemory)\n",
       "        commitContext = executor.submit(self.loadCommitContext, commitIndex)\n",
       "        contextResources = executor.submit(\n",
       "            self.loadContextualResources, contextualMemory\n",
       "        )\n",
       "\n",
       "        gitMemoryContext = gitMemoryContext.result()\n",
       "        commitContext = commitContext.result()\n",
       "        contextResources = contextResources.result()\n",
       "\n",
       "    draftCode = self.getCurrentDraft(commitContext)\n",
       "\n",
       "    # Concurrent processing - Step 2\n",
       "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
       "        modifications = executor.submit(\n",
       "            self.generateModifications, draftCode, metaCode, contextResources\n",
       "        )\n",
       "        sourceCode = executor.submit(\n",
       "            self.extractSourceCode, modifications, contextResources\n",
       "        )\n",
       "\n",
       "        modifications = modifications.result()\n",
       "        sourceCode = sourceCode.result()\n",
       "\n",
       "    updatedDraft = self.applyModifications(draftCode, modifications)\n",
       "\n",
       "    # Concurrent processing - Step 3\n",
       "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
       "        validationResult = executor.submit(\n",
       "            self.validateDraft, updatedDraft, sourceCode\n",
       "        )\n",
       "        comparisonResult = executor.submit(\n",
       "            self.compareDraftWithObjective, updatedDraft, metaCode\n",
       "        )\n",
       "\n",
       "        validationResult = validationResult.result()\n",
       "        comparisonResult = comparisonResult.result()\n",
       "\n",
       "    if validationResult and comparisonResult:\n",
       "        self.storeDraftToCommitIndex(commitIndex, updatedDraft)\n",
       "    else:\n",
       "        updatedDraft = self.rollback(commitIndex)\n",
       "\n",
       "    return updatedDraft\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value import concurrent.futures\n",
       "\n",
       "\n",
       "class CodeGenerator:\n",
       "    def __init__(self):\n",
       "        self.git_memory = None\n",
       "        self.commit_index = None\n",
       "        self.contextual_memory = None\n",
       "\n",
       "    def generate_meta_code(self, user_input):\n",
       "        # Transform user input into meta-code representation\n",
       "        pass\n",
       "\n",
       "    def load_git_memory(self, git_memory):\n",
       "        # Retrieve GitMemory context\n",
       "        pass\n",
       "\n",
       "    def load_commit_context(self, commit_index):\n",
       "        # Retrieve commit context\n",
       "        pass\n",
       "\n",
       "    def load_contextual_resources(self, contextual_memory):\n",
       "        # Load contextual resources required for code generation\n",
       "        pass\n",
       "\n",
       "    def get_current_draft(self, commit_context):\n",
       "        # Get the current draft code from commit context\n",
       "        pass\n",
       "\n",
       "    def generate_modifications(self, draft_code, meta_code, context_resources):\n",
       "        # Generate modifications to the draft code based on meta-code and context resources\n",
       "        pass\n",
       "\n",
       "    def extract_source_code(self, modifications, context_resources):\n",
       "        # Extract source code from modifications and context resources\n",
       "        pass\n",
       "\n",
       "    def apply_modifications(self, draft_code, modifications):\n",
       "        # Apply modifications to the draft code\n",
       "        pass\n",
       "\n",
       "    def validate_draft(self, updated_draft, source_code):\n",
       "        # Validate the updated draft code and check for consistency\n",
       "        pass\n",
       "\n",
       "    def compare_draft_with_objective(self, updated_draft, meta_code):\n",
       "        # Compare the updated draft code with user's goal based on meta-code\n",
       "        pass\n",
       "\n",
       "    def store_draft_to_commit_index(self, commit_index, updated_draft):\n",
       "        # Store the updated draft code in the commit index\n",
       "        pass\n",
       "\n",
       "    def rollback(self, commit_index):\n",
       "        # Roll back the draft code to the previous state in the commit index\n",
       "        pass\n",
       "\n",
       "    def codeGenerationIteration(\n",
       "        self, userInput, gitMemory, commitIndex, contextualMemory\n",
       "    ):\n",
       "        metaCode = self.generateMetaCode(userInput)\n",
       "\n",
       "        # Concurrent processing - Step 1\n",
       "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
       "            gitMemoryContext = executor.submit(self.loadGitMemory, gitMemory)\n",
       "            commitContext = executor.submit(self.loadCommitContext, commitIndex)\n",
       "            contextResources = executor.submit(\n",
       "                self.loadContextualResources, contextualMemory\n",
       "            )\n",
       "\n",
       "            gitMemoryContext = gitMemoryContext.result()\n",
       "            commitContext = commitContext.result()\n",
       "            contextResources = contextResources.result()\n",
       "\n",
       "        draftCode = self.getCurrentDraft(commitContext)\n",
       "\n",
       "        # Concurrent processing - Step 2\n",
       "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
       "            modifications = executor.submit(\n",
       "                self.generateModifications, draftCode, metaCode, contextResources\n",
       "            )\n",
       "            sourceCode = executor.submit(\n",
       "                self.extractSourceCode, modifications, contextResources\n",
       "            )\n",
       "\n",
       "            modifications = modifications.result()\n",
       "            sourceCode = sourceCode.result()\n",
       "\n",
       "        updatedDraft = self.applyModifications(draftCode, modifications)\n",
       "\n",
       "        # Concurrent processing - Step 3\n",
       "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
       "            validationResult = executor.submit(\n",
       "                self.validateDraft, updatedDraft, sourceCode\n",
       "            )\n",
       "            comparisonResult = executor.submit(\n",
       "                self.compareDraftWithObjective, updatedDraft, metaCode\n",
       "            )\n",
       "\n",
       "            validationResult = validationResult.result()\n",
       "            comparisonResult = comparisonResult.result()\n",
       "\n",
       "        if validationResult and comparisonResult:\n",
       "            self.storeDraftToCommitIndex(commitIndex, updatedDraft)\n",
       "        else:\n",
       "            updatedDraft = self.rollback(commitIndex)\n",
       "\n",
       "        return updatedDraft\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value # This is the __init__.py file for the package.\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value # This is the __init__.py file for the package.\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value  was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value # This is the __init__.py file for the package.\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self, code: str = None):\n",
       "\n",
       "    self.code = code\n",
       "    self.output_code = None\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def minify(self):\n",
       "    if self.code:\n",
       "        self.output_code = self.minify_code(self.code)\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def get_minified_code(self):\n",
       "    if not self.output_code:\n",
       "        self.minify()\n",
       "    return self.output_code\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "@staticmethod\n",
       "def minify_code(code: str) -> str:\n",
       "    return minify(code)\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value @staticmethod\n",
       "def extract_docstring(function_def: cst.FunctionDef) -> str:\n",
       "    docstring = None\n",
       "\n",
       "    for stmt in function_def.body.body:\n",
       "        if isinstance(stmt, cst.SimpleStatementLine):\n",
       "            for expr in stmt.body:\n",
       "                if isinstance(expr, cst.Expr) and isinstance(\n",
       "                    expr.value, cst.SimpleString\n",
       "                ):\n",
       "                    docstring = expr.value.value.strip('\"').strip(\"'\")\n",
       "                    break\n",
       "        if docstring is not None:\n",
       "            break\n",
       "\n",
       "    if docstring is not None:\n",
       "        return docstring.strip()\n",
       "    else:\n",
       "        function_name = function_def.name.value\n",
       "        return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value def __init__(self, username, repo_name):\n",
       "    super().__init__()\n",
       "    self.username = username\n",
       "    self.repo_name = repo_name\n",
       "    self.parser = GitHubRepoProcessor(username, repo_name)\n",
       "    self.minifier = PythonMinifier()\n",
       "    self.docstring_extractor = PythonDocstringExtractor()\n",
       "    self.directory_parser = None\n",
       "    self.min_code_index = None\n",
       "    self.doc_string_index = None\n",
       "    self.libcst_node_index = None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def create_code_index(self, base_directory):\n",
       "    self.directory_parser = self.parser.process_repo(base_directory)\n",
       "    code_values, code_nodes = self.parser.get_values()\n",
       "    self.code_index = self.init_index(values=code_values)\n",
       "    self.code_index.save()\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "def create_indexes(self, base_directory):\n",
       "    self.directory_parser = self.parser.process_repo(base_directory)\n",
       "    code_values, code_nodes = self.parser.get_values()\n",
       "    self.code_index = self.init_index(values=code_values)\n",
       "\n",
       "    min_code_values = []\n",
       "    doc_string_values = []\n",
       "    for code_value, code_node in zip(code_values, code_nodes):\n",
       "        minifier = PythonMinifier(code=code_value)\n",
       "        min_code = minifier.get_minified_code()\n",
       "        doc_string = self.docstring_extractor.extract_docstring(code_node)\n",
       "        min_code_values.append(min_code)\n",
       "        doc_string_values.append(doc_string)\n",
       "    self.doc_string_index = self.init_index(values=doc_string_values)\n",
       "    self.min_code_index = self.init_index(values=min_code_values)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value from typing import List, Optional, Union\n",
       "\n",
       "import libcst as cst\n",
       "from python_minifier import minify\n",
       "\n",
       "from babydragon.memory.indexes.memory_index import MemoryIndex\n",
       "from babydragon.working_memory.parsers.git_processor import GitHubRepoProcessor\n",
       "\n",
       "\n",
       "class PythonMinifier:\n",
       "    def __init__(self, code: str = None):\n",
       "\n",
       "        self.code = code\n",
       "        self.output_code = None\n",
       "\n",
       "    def minify(self):\n",
       "        if self.code:\n",
       "            self.output_code = self.minify_code(self.code)\n",
       "\n",
       "    def get_minified_code(self):\n",
       "        if not self.output_code:\n",
       "            self.minify()\n",
       "        return self.output_code\n",
       "\n",
       "    @staticmethod\n",
       "    def minify_code(code: str) -> str:\n",
       "        return minify(code)\n",
       "\n",
       "\n",
       "class PythonDocstringExtractor:\n",
       "    @staticmethod\n",
       "    def extract_docstring(function_def: cst.FunctionDef) -> str:\n",
       "        docstring = None\n",
       "\n",
       "        for stmt in function_def.body.body:\n",
       "            if isinstance(stmt, cst.SimpleStatementLine):\n",
       "                for expr in stmt.body:\n",
       "                    if isinstance(expr, cst.Expr) and isinstance(\n",
       "                        expr.value, cst.SimpleString\n",
       "                    ):\n",
       "                        docstring = expr.value.value.strip('\"').strip(\"'\")\n",
       "                        break\n",
       "            if docstring is not None:\n",
       "                break\n",
       "\n",
       "        if docstring is not None:\n",
       "            return docstring.strip()\n",
       "        else:\n",
       "            function_name = function_def.name.value\n",
       "            return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n",
       "\n",
       "\n",
       "class GitMemory(MemoryIndex):\n",
       "    def __init__(self, username, repo_name):\n",
       "        super().__init__()\n",
       "        self.username = username\n",
       "        self.repo_name = repo_name\n",
       "        self.parser = GitHubRepoProcessor(username, repo_name)\n",
       "        self.minifier = PythonMinifier()\n",
       "        self.docstring_extractor = PythonDocstringExtractor()\n",
       "        self.directory_parser = None\n",
       "        self.min_code_index = None\n",
       "        self.doc_string_index = None\n",
       "        self.libcst_node_index = None\n",
       "\n",
       "    def create_code_index(self, base_directory):\n",
       "        self.directory_parser = self.parser.process_repo(base_directory)\n",
       "        code_values, code_nodes = self.parser.get_values()\n",
       "        self.code_index = self.init_index(values=code_values)\n",
       "        self.code_index.save()\n",
       "\n",
       "    def create_indexes(self, base_directory):\n",
       "        self.directory_parser = self.parser.process_repo(base_directory)\n",
       "        code_values, code_nodes = self.parser.get_values()\n",
       "        self.code_index = self.init_index(values=code_values)\n",
       "\n",
       "        min_code_values = []\n",
       "        doc_string_values = []\n",
       "        for code_value, code_node in zip(code_values, code_nodes):\n",
       "            minifier = PythonMinifier(code=code_value)\n",
       "            min_code = minifier.get_minified_code()\n",
       "            doc_string = self.docstring_extractor.extract_docstring(code_node)\n",
       "            min_code_values.append(min_code)\n",
       "            doc_string_values.append(doc_string)\n",
       "        self.doc_string_index = self.init_index(values=doc_string_values)\n",
       "        self.min_code_index = self.init_index(values=min_code_values)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value # This is the __init__.py file for the package.\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class Prompter:\n",
       "    \"\"\"\n",
       "    This class handles the system and user prompts and the prompt_func. By subclassing and overriding the\n",
       "    prompt_func, you can change the way the prompts are composed.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, system_prompt: str = None, user_prompt: str = None):\n",
       "        \"\"\"\n",
       "        Initialize the Prompter with system and user prompts.\n",
       "\n",
       "        :param system_prompt: A string representing the system prompt.\n",
       "        :param user_prompt: A string representing the user prompt.\n",
       "        \"\"\"\n",
       "        if system_prompt is None:\n",
       "            self.system_prompt = DEFAULT_SYSTEM_PROMPT\n",
       "            self.user_defined_system_prompt = False\n",
       "        else:\n",
       "            self.system_prompt = system_prompt\n",
       "            self.user_defined_system_prompt = True\n",
       "        if user_prompt is None:\n",
       "            self.user_prompt = self.default_user_prompt\n",
       "            self.user_defined_user_prompt = False\n",
       "        else:\n",
       "            self.user_prompt = user_prompt\n",
       "            self.user_defined_user_prompt = True\n",
       "\n",
       "        self.prompt_func: Callable[[str], Tuple[List[str], str]] = self.one_shot_prompt\n",
       "\n",
       "    def default_user_prompt(self, message: str) -> str:\n",
       "        return DEFAULT_USER_PROMPT.format(question=message)\n",
       "\n",
       "    def one_shot_prompt(self, message: str) -> Tuple[List[str], str]:\n",
       "        \"\"\"\n",
       "        Compose the prompt for the chat-gpt API.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing a list of strings representing the prompt and a string representing the marked question.\n",
       "        \"\"\"\n",
       "        marked_question = mark_question(self.user_prompt(message))\n",
       "        prompt = [mark_system(self.system_prompt)] + [marked_question]\n",
       "        return prompt, marked_question\n",
       "\n",
       "    def update_system_prompt(self, new_prompt: str) -> None:\n",
       "        \"\"\"\n",
       "        Update the system prompt.\n",
       "\n",
       "        :param new_prompt: A string representing the new system prompt.\n",
       "        \"\"\"\n",
       "        self.system_prompt = new_prompt\n",
       "\n",
       "    def update_user_prompt(self, new_prompt: str) -> None:\n",
       "        \"\"\"\n",
       "        Update the user prompt.\n",
       "\n",
       "        :param new_prompt: A string representing the new user prompt.\n",
       "        \"\"\"\n",
       "        self.user_prompt = new_prompt\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class BaseChat:\n",
       "    \"\"\"\n",
       "    This is the base class for chatbots, defining the basic functions that a chatbot should have, mainly the calls to\n",
       "    chat-gpt API, and a basic Gradio interface. It has a prompt_func that acts as a placeholder for a call to chat-gpt\n",
       "    API without any additional messages. It can be overridden by subclasses to add additional messages to the prompt.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, model: str = None, max_output_tokens: int = 200):\n",
       "        \"\"\"\n",
       "        Initialize the BaseChat with a model and max_output_tokens.\n",
       "\n",
       "        :param model: A string representing the chat model to be used.\n",
       "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
       "        \"\"\"\n",
       "        if model is None:\n",
       "            self.model = \"gpt-3.5-turbo\"\n",
       "        else:\n",
       "            self.model = model\n",
       "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
       "        self.max_output_tokens = max_output_tokens\n",
       "        self.failed_responses = []\n",
       "        self.outputs = []\n",
       "        self.inputs = []\n",
       "        self.prompts = []\n",
       "        self.prompt_func = self.identity_prompter\n",
       "\n",
       "    def identity_prompter(self, message: str) -> Tuple[List[Dict], str]:\n",
       "        \"\"\"\n",
       "        A simple identity prompter that takes a message and returns the message marked as a question.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing the marked question and the original message.\n",
       "        \"\"\"\n",
       "        return [mark_question(message)], mark_question(message)\n",
       "\n",
       "    def chat_response(\n",
       "        self, prompt: List[dict], max_tokens: int = None\n",
       "    ) -> Tuple[Dict, bool]:\n",
       "        \"\"\"\n",
       "        Call the OpenAI API with the given prompt and maximum number of output tokens.\n",
       "\n",
       "        :param prompt: A list of strings representing the prompt to send to the API.\n",
       "        :param max_output_tokens: An integer representing the maximum number of output tokens.\n",
       "        :return: A tuple containing the API response as a dictionary and a boolean indicating success.\n",
       "        \"\"\"\n",
       "        if max_tokens is None:\n",
       "            max_tokens = self.max_output_tokens\n",
       "        try:\n",
       "            print(\"Trying to call OpenAI API...\")\n",
       "            response = openai.ChatCompletion.create(\n",
       "                model=self.model,\n",
       "                messages=prompt,\n",
       "                max_tokens=max_tokens,\n",
       "            )\n",
       "            return response, True\n",
       "\n",
       "        except openai.error.APIError as e:\n",
       "            print(e)\n",
       "            fail_response = {\n",
       "                \"choices\": [\n",
       "                    {\n",
       "                        \"message\": {\n",
       "                            \"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"\n",
       "                        }\n",
       "                    }\n",
       "                ]\n",
       "            }\n",
       "            self.failed_responses.append(fail_response)\n",
       "            return fail_response, False\n",
       "\n",
       "    def reply(self, message: str, verbose: bool = True) -> str:\n",
       "        \"\"\"\n",
       "        Reply to a given message using the chatbot.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "        return self.query(message, verbose)[\"content\"]\n",
       "\n",
       "    def query(self, message: str, verbose: bool = True) -> str:\n",
       "        \"\"\"\n",
       "        Query the chatbot with a given message, optionally showing the input and output messages as Markdown.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "\n",
       "        prompt, _ = self.prompt_func(message)\n",
       "        response, success = self.chat_response(prompt)\n",
       "        if verbose:\n",
       "            display(Markdown(\"#### Question: \\n {question}\".format(question=message)))\n",
       "        if success:\n",
       "            answer = get_mark_from_response(response)\n",
       "            self.outputs.append(answer)\n",
       "            self.inputs.append(message)\n",
       "            self.prompts.append(prompt)\n",
       "            if verbose:\n",
       "                display(\n",
       "                    Markdown(\n",
       "                        \" #### Anwser: \\n {answer}\".format(\n",
       "                            answer=get_str_from_response(response)\n",
       "                        )\n",
       "                    )\n",
       "                )\n",
       "            return answer\n",
       "        else:\n",
       "            raise Exception(\"OpenAI API Error inside query function\")\n",
       "\n",
       "    def reset_logs(self):\n",
       "        \"\"\"\n",
       "        Reset the chatbot's memory.\n",
       "        \"\"\"\n",
       "        self.outputs = []\n",
       "        self.inputs = []\n",
       "        self.prompts = []\n",
       "\n",
       "    def get_logs(self):\n",
       "        \"\"\"\n",
       "        Get the chatbot's memory.\n",
       "\n",
       "        :return: A tuple containing the chatbot's memory as three lists of strings.\n",
       "        \"\"\"\n",
       "        return self.inputs, self.outputs, self.prompts\n",
       "\n",
       "    def run_text(\n",
       "        self, text: str, state: List[Tuple[str, str]]\n",
       "    ) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\n",
       "        \"\"\"\n",
       "        Process the user's text input and update the chat state.\n",
       "\n",
       "        :param text: A string representing the user input.\n",
       "        :param state: A list of tuples representing the current chat state.\n",
       "        :return: A tuple containing the updated chat state as two lists of tuples.\n",
       "        \"\"\"\n",
       "        print(\"===============Running run_text =============\")\n",
       "        print(\"Inputs:\", text)\n",
       "        try:\n",
       "            print(\"======>Current memory:\\n %s\" % self.memory_thread)\n",
       "        except:\n",
       "            print(\"======>No memory\")\n",
       "        response = self.reply(text)\n",
       "        state = state + [(text, response)]\n",
       "        print(\"Outputs:\", state)\n",
       "        return state, state\n",
       "\n",
       "    def gradio(self):\n",
       "        \"\"\"\n",
       "        Create and launch a Gradio interface for the chatbot.\n",
       "        \"\"\"\n",
       "        with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n",
       "            chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\n",
       "            state = gr.State([])\n",
       "            with gr.Row():\n",
       "                with gr.Column(scale=1):\n",
       "                    txt = gr.Textbox(\n",
       "                        show_label=False,\n",
       "                        placeholder=\"Enter text and press enter, or upload an image\",\n",
       "                    ).style(container=False)\n",
       "                with gr.Column(scale=0.15, min_width=0):\n",
       "                    clear = gr.Button(\"Clear️\")\n",
       "\n",
       "            txt.submit(self.run_text, [txt, state], [chatbot, state])\n",
       "            txt.submit(lambda: \"\", None, txt)\n",
       "            demo.launch(server_name=\"localhost\", server_port=7860)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class Chat(BaseChat, Prompter):\n",
       "    \"\"\"\n",
       "    This class combines the BaseChat and Prompter classes to create a oneshot chatbot with a system and user prompt,\n",
       "    and the ability to handle multiple index_dict.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        model: str = None,\n",
       "        max_output_tokens: int = 1000,\n",
       "        system_prompt: str = None,\n",
       "        user_prompt: str = None,\n",
       "        index_dict: Optional[Dict[str,MemoryIndex]] = None,\n",
       "        max_index_memory: int = 1000,\n",
       "    ) -> None:\n",
       "        BaseChat.__init__(self, model=model, max_output_tokens=max_output_tokens)\n",
       "        Prompter.__init__(self, system_prompt=system_prompt, user_prompt=user_prompt)\n",
       "        self.index_dict = index_dict\n",
       "        self.setup_indices(max_index_memory)\n",
       "\n",
       "    def setup_indices(self, max_index_memory):\n",
       "        \"\"\"setup the index_dict for the chatbot. Change the system and user prompts to the index prompts if they are not user defined if there is an index.\"\"\"\n",
       "        if self.index_dict is not None:\n",
       "            self.current_index = list(self.index_dict.keys())[0]\n",
       "            self.system_prompt = (\n",
       "                INDEX_SYSTEM_PROMPT\n",
       "                if self.user_defined_system_prompt is False\n",
       "                else self.system_prompt\n",
       "            )\n",
       "            self.user_prompt = (\n",
       "                self.get_index_hints\n",
       "                if self.user_defined_user_prompt is False\n",
       "                else self.user_prompt\n",
       "            )\n",
       "        self.max_index_memory = max_index_memory\n",
       "        #set the last index to be the current index\n",
       "        self.current_index  = list(self.index_dict.keys())[-1]\n",
       "\n",
       "    def get_index_hints(\n",
       "        self, question: str, k: int = 10, max_tokens: int = None\n",
       "    ) -> str:\n",
       "        \"\"\"\n",
       "        Get hints from the current index for the given question.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include from the index.\n",
       "        :param max_tokens: The maximum number of tokens to be retrieved from the index.\n",
       "        :return: A string representing the hint prompt with the question.\n",
       "        \"\"\"\n",
       "        if max_tokens is None:\n",
       "            max_tokens = self.max_index_memory\n",
       "        hints = []\n",
       "        if self.current_index is not None:\n",
       "            index_instance = self.index_dict[self.current_index]\n",
       "            if isinstance(index_instance, MemoryIndex):\n",
       "                hints, _, _ = index_instance.token_bound_query(\n",
       "                    question, k=k, max_tokens=max_tokens\n",
       "                )\n",
       "            else:\n",
       "                raise ValueError(\"The current index is not a valid index instance.\")\n",
       "            hints_string = \"\\n\".join(hints)\n",
       "            hint_prompt = INDEX_HINT_PROMPT\n",
       "            question_intro = QUESTION_INTRO\n",
       "            return hint_prompt.format(\n",
       "                hints_string=hints_string\n",
       "            ) + question_intro.format(question=question)\n",
       "        else:\n",
       "            return question\n",
       "\n",
       "    def set_current_index(self, index_name: Optional[str]) -> None:\n",
       "        \"\"\"\n",
       "        Set the current index to be used for hints.\n",
       "\n",
       "        :param index_name: A string representing the index name or None to clear the current index.\n",
       "        :raise ValueError: If the provided index name is not available.\n",
       "        \"\"\"\n",
       "        if self.index_dict is None:\n",
       "            raise ValueError(\"No index_dict are available.\")\n",
       "        elif index_name in self.index_dict:\n",
       "            self.current_index = index_name\n",
       "        elif index_name is None:\n",
       "            self.current_index = None\n",
       "        else:\n",
       "            raise ValueError(\"The provided index name is not available.\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class FifoChat(FifoThread, Chat):\n",
       "    \"\"\"\n",
       "    A chatbot class that combines FIFO Memory Thread, BaseChat, and Prompter. The oldest messages are removed first\n",
       "    when reaching the max_memory limit. The memory is defined in terms of tokens, and outs are passed to the\n",
       "    longterm_memory. The lucid_memory is a redundant memory that stores all the messages.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        model: Optional[str] = None,\n",
       "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
       "        system_prompt: Optional[str] = None,\n",
       "        user_prompt: Optional[str] = None,\n",
       "        name: str = \"fifo_memory\",\n",
       "        max_index_memory: int = 400,\n",
       "        max_fifo_memory: int = 2048,\n",
       "        max_output_tokens: int = 1000,\n",
       "        longterm_thread: Optional[BaseThread] = None,\n",
       "    ):\n",
       "\n",
       "        FifoThread.__init__(\n",
       "            self, name=name, max_memory=max_fifo_memory, longterm_thread=longterm_thread\n",
       "        )\n",
       "        Chat.__init__(\n",
       "            self,\n",
       "            model=model,\n",
       "            index_dict=index_dict,\n",
       "            max_output_tokens=max_output_tokens,\n",
       "            max_index_memory=max_index_memory,\n",
       "            system_prompt=system_prompt,\n",
       "            user_prompt=user_prompt,\n",
       "        )\n",
       "\n",
       "        self.prompt_func = self.fifo_memory_prompt\n",
       "\n",
       "    def fifo_memory_prompt(self, message: str) -> Tuple[List[dict], dict]:\n",
       "        \"\"\"\n",
       "        Compose the prompt for the chat-gpt API, including the system prompt and memory thread.\n",
       "\n",
       "        :param message: A string representing the user message.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "        marked_question = mark_question(self.user_prompt(message))\n",
       "        prompt = (\n",
       "            [mark_system(self.system_prompt)] + self.memory_thread + [marked_question]\n",
       "        )\n",
       "        return prompt, marked_question\n",
       "\n",
       "    def query(self, question: str, verbose: bool = True) -> str:\n",
       "        \"\"\"\n",
       "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
       "        and added to the memory.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "        # First call the base class's query method\n",
       "        answer = BaseChat.query(self, message=question, verbose=verbose)\n",
       "        marked_question = mark_question(question)\n",
       "        # Add the marked question and answer to the memory\n",
       "        self.add_message(marked_question)\n",
       "        self.add_message(answer)\n",
       "\n",
       "        return answer\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class VectorChat(VectorThread, Chat):\n",
       "    \"\"\"\n",
       "    A chatbot class that combines Vector Memory Thread, BaseChat, and Prompter. Memory prompt is constructed by\n",
       "    filling the memory with the k most similar messages to the question until the max prompt memory tokens are reached.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        model: Optional[str] = None,\n",
       "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
       "        name: str = \"vector_memory\",\n",
       "        max_index_memory: int = 400,\n",
       "        max_vector_memory: int = 2048,\n",
       "        max_output_tokens: int = 1000,\n",
       "        system_prompt: str = None,\n",
       "        user_prompt: str = None,\n",
       "    ):\n",
       "        VectorThread.__init__(self, name=name, max_context=max_vector_memory)\n",
       "        Chat.__init__(\n",
       "            self,\n",
       "            model=model,\n",
       "            index_dict=index_dict,\n",
       "            max_output_tokens=max_output_tokens,\n",
       "            max_index_memory=max_index_memory,\n",
       "            system_prompt=system_prompt,\n",
       "            user_prompt=user_prompt,\n",
       "        )\n",
       "        self.max_vector_memory = self.max_context\n",
       "        self.prompt_func = self.vector_memory_prompt\n",
       "\n",
       "    def vector_memory_prompt(\n",
       "        self, message: str, k: int = 10\n",
       "    ) -> Tuple[List[dict], dict]:\n",
       "        \"\"\"\n",
       "        Combine system prompt, k most similar messages to the question, and the user prompt.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include in the prompt.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "        sorted_messages, sorted_scores, sorted_indices = self.sorted_query(\n",
       "            message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
       "        )\n",
       "        marked_question = mark_question(self.user_prompt(message))\n",
       "        prompt = [mark_system(self.system_prompt)] + sorted_messages + [marked_question]\n",
       "        return prompt, marked_question\n",
       "\n",
       "    def weighted_memory_prompt(\n",
       "        self,\n",
       "        message: str,\n",
       "        k: int = 10,\n",
       "        decay_factor: float = 0.1,\n",
       "        temporal_weight: float = 0.5,\n",
       "    ) -> Tuple[List[dict], dict]:\n",
       "        \"\"\"\n",
       "        Combine system prompt, weighted k most similar messages to the question, and the user prompt.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include in the prompt.\n",
       "        :param decay_factor: A float representing the decay factor for weighting.\n",
       "        :param temporal_weight: A float representing the weight of the temporal aspect.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "        weighted_messages, weighted_scores, weighted_indices = self.weighted_query(\n",
       "            message,\n",
       "            k=k,\n",
       "            max_tokens=self.max_vector_memory,\n",
       "            decay_factor=decay_factor,\n",
       "            temporal_weight=temporal_weight,\n",
       "            order_by=\"chronological\",\n",
       "            reverse=True,\n",
       "        )\n",
       "        marked_question = mark_question(self.user_prompt(message))\n",
       "        prompt = (\n",
       "            [mark_system(self.system_prompt)] + weighted_messages + [marked_question]\n",
       "        )\n",
       "        return prompt, marked_question\n",
       "\n",
       "    def query(self, question: str, verbose: bool = False) -> str:\n",
       "        \"\"\"\n",
       "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
       "        and added to the memory.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "        # First call the base class's query method\n",
       "        answer = BaseChat.query(self, message=question, verbose=verbose)\n",
       "        marked_question = mark_question(question)\n",
       "        # Add the marked question and answer to the memory\n",
       "        self.add_message(marked_question)\n",
       "        self.add_message(answer)\n",
       "        return answer\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class FifoVectorChat(FifoThread, Chat):\n",
       "    \"\"\"\n",
       "    A chatbot class that combines FIFO Memory Thread, Vector Memory Thread, BaseChat, and Prompter.\n",
       "    The memory prompt is constructed by including both FIFO memory and Vector memory.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        model: str = None,\n",
       "        index_dict: Optional[Dict[str, Union[PandasIndex, MemoryIndex]]] = None,\n",
       "        system_prompt: str = None,\n",
       "        user_prompt: str = None,\n",
       "        name: str = \"fifo_vector_memory\",\n",
       "        max_memory: int = 2048,\n",
       "        max_index_memory: int = 400,\n",
       "        max_output_tokens: int = 1000,\n",
       "        longterm_thread: Optional[VectorThread] = None,\n",
       "        longterm_frac: float = 0.5,\n",
       "    ):\n",
       "        self.total_max_memory = max_memory\n",
       "\n",
       "        self.setup_longterm_memory(longterm_thread, max_memory, longterm_frac)\n",
       "        FifoThread.__init__(\n",
       "            self,\n",
       "            name=name,\n",
       "            max_memory=self.max_fifo_memory,\n",
       "            longterm_thread=self.longterm_thread,\n",
       "        )\n",
       "        Chat.__init__(\n",
       "            self,\n",
       "            model=model,\n",
       "            index_dict=index_dict,\n",
       "            max_output_tokens=max_output_tokens,\n",
       "            max_index_memory=max_index_memory,\n",
       "            system_prompt=system_prompt,\n",
       "            user_prompt=user_prompt,\n",
       "        )\n",
       "        self.prompt_func = self.fifovector_memory_prompt\n",
       "        self.prompt_list = []\n",
       "\n",
       "    def setup_longterm_memory(\n",
       "        self,\n",
       "        longterm_thread: Optional[VectorThread],\n",
       "        max_memory: int,\n",
       "        longterm_frac: float,\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Set up long-term memory by allocating memory for the FIFO and Vector memory components.\n",
       "\n",
       "        :param longterm_thread: An optional VectorThread for long-term memory.\n",
       "        :param max_memory: The maximum amount of memory for the chatbot.\n",
       "        :param longterm_frac: The fraction of memory dedicated to long-term memory.\n",
       "        \"\"\"\n",
       "        if longterm_thread is None:\n",
       "            self.longterm_frac = longterm_frac\n",
       "            self.max_fifo_memory = int(max_memory * (1 - self.longterm_frac))\n",
       "            self.max_vector_memory = max_memory - self.max_fifo_memory\n",
       "            self.longterm_thread = VectorThread(\n",
       "                name=\"longterm_memory\", max_context=self.max_vector_memory\n",
       "            )\n",
       "        else:\n",
       "            self.longterm_thread = longterm_thread\n",
       "            self.max_vector_memory = self.longterm_thread.max_context\n",
       "            self.max_fifo_memory = self.total_max_memory - self.max_vector_memory\n",
       "            self.longterm_frac = self.max_vector_memory / self.total_max_memory\n",
       "\n",
       "    def fifovector_memory_prompt(\n",
       "        self, message: str, k: int = 10\n",
       "    ) -> Tuple[List[dict], dict]:\n",
       "        \"\"\"\n",
       "        Combine the system prompt, long-term memory (vector memory), short-term memory (FIFO memory), and the user prompt.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param k: The number of most similar messages to include from the long-term memory.\n",
       "        :return: A tuple containing a list of strings as the prompt and the marked question.\n",
       "        \"\"\"\n",
       "        prompt = [mark_system(self.system_prompt)]\n",
       "        if (\n",
       "            len(self.longterm_thread.memory_thread) > 0\n",
       "            and self.longterm_thread.total_tokens <= self.max_vector_memory\n",
       "        ):\n",
       "            prompt += self.longterm_thread.memory_thread\n",
       "        elif (\n",
       "            len(self.longterm_thread.memory_thread) > 0\n",
       "            and self.longterm_thread.total_tokens > self.max_vector_memory\n",
       "        ):\n",
       "            (\n",
       "                sorted_messages,\n",
       "                sorted_scores,\n",
       "                sorted_indices,\n",
       "            ) = self.longterm_thread.sorted_query(\n",
       "                message, k=k, max_tokens=self.max_vector_memory, reverse=True\n",
       "            )\n",
       "            prompt += sorted_messages\n",
       "\n",
       "        prompt += self.memory_thread\n",
       "        marked_question = mark_question(self.user_prompt(message))\n",
       "        prompt += [marked_question]\n",
       "        return prompt, marked_question\n",
       "\n",
       "    def query(self, question: str, verbose: bool = False) -> str:\n",
       "        \"\"\"\n",
       "        Query the chatbot with a given question. The question is added to the memory, and the answer is returned\n",
       "        and added to the memory.\n",
       "\n",
       "        :param question: A string representing the user question.\n",
       "        :param verbose: A boolean indicating whether to display input and output messages as Markdown.\n",
       "        :return: A string representing the chatbot's response.\n",
       "        \"\"\"\n",
       "        answer = BaseChat.query(self, message=question, verbose=verbose)\n",
       "        marked_question = mark_question(question)\n",
       "        self.add_message(marked_question)\n",
       "        self.add_message(answer)\n",
       "        return answer\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "class MemoryIndex:\n",
       "    \"\"\"\n",
       "    this class is a wrapper for a faiss index, it contains information about the format of the index the faiss index itself\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        index: Optional[faiss.Index] = None,\n",
       "        values: Optional[List[str]] = None,\n",
       "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
       "        name: str = \"memory_index\",\n",
       "        save_path: Optional[str] = None,\n",
       "        load: bool = False,\n",
       "        tokenizer: Optional[tiktoken.Encoding] = None,\n",
       "    ):\n",
       "\n",
       "        self.name = name\n",
       "        self.embedder = OpenAiEmbedder()\n",
       "        if save_path is None:\n",
       "            save_path = \"storage\"\n",
       "\n",
       "        self.save_path = save_path\n",
       "\n",
       "        # Create the 'storage' folder if it does not exist\n",
       "        os.makedirs(self.save_path, exist_ok=True)\n",
       "        self.values = []\n",
       "        if load is True:\n",
       "            self.load()\n",
       "        else:\n",
       "            self.init_index(index, values, embeddings)\n",
       "        if tokenizer is None:\n",
       "            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
       "        else:\n",
       "            self.tokenizer = tokenizer\n",
       "        self.query_history = []\n",
       "        self.save()\n",
       "\n",
       "    def init_index(\n",
       "        self,\n",
       "        index: Optional[faiss.Index] = None,\n",
       "        values: Optional[List[str]] = None,\n",
       "        embeddings: Optional[List[Union[List[float], np.ndarray]]] = None,\n",
       "    ) -> None:\n",
       "\n",
       "        \"\"\"\n",
       "        initializes the index, there are 4 cases:\n",
       "        1. we create a new index from scratch\n",
       "        2. we create a new index from a list of embeddings and values\n",
       "        3. we create a new index from a faiss index and values list\n",
       "        4. we load an index from a file\n",
       "        \"\"\"\n",
       "        # fist case is when we create a new index from scratch\n",
       "        if index is None and values is None and embeddings is None:\n",
       "            print(\"Creating a new index\")\n",
       "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
       "            self.values = []\n",
       "        # second case is where we create the index from a list of embeddings\n",
       "        elif (\n",
       "            index is None\n",
       "            and values is not None\n",
       "            and embeddings is not None\n",
       "            and len(values) == len(embeddings)\n",
       "        ):\n",
       "            print(\"Creating a new index from a list of embeddings and values\")\n",
       "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
       "            for embedding, value in zip(embeddings, values):\n",
       "                self.add_to_index(value, embedding)\n",
       "        # third case is where we create the index from a faiss index and values list\n",
       "        elif (\n",
       "            isinstance(index, faiss.Index)\n",
       "            and index.d == self.embedder.get_embedding_size()\n",
       "            and type(values) == list\n",
       "            and len(values) == index.ntotal\n",
       "        ):\n",
       "            print(\"Creating a new index from a faiss index and values list\")\n",
       "            self.index = index\n",
       "            self.values = values\n",
       "        # fourth case is where we create an index from a list of values, the values are embedded and the index is created\n",
       "        elif index is None and values is not None and embeddings is None:\n",
       "            print(\"Creating a new index from a list of values\")\n",
       "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
       "            i = 0\n",
       "            for value in values:\n",
       "                # print the value id to see the progress\n",
       "                print(\"Embedding value \", i, \" of \", len(values))\n",
       "                # start tracking the time using time\n",
       "                start = time.time()\n",
       "                self.add_to_index(value)\n",
       "                # print the time it took to embed the value\n",
       "                print(\"Embedding value \", i, \" took \", time.time() - start, \" seconds\")\n",
       "                i += 1\n",
       "        else:\n",
       "            raise ValueError(\n",
       "                \"The index is not a valid faiss index or the embedding dimension is not correct\"\n",
       "            )\n",
       "        \n",
       "    @classmethod\n",
       "    def from_pandas(\n",
       "        cls,\n",
       "        data_frame: Union[pd.DataFrame, str],\n",
       "        columns: Optional[Union[str, List[str]]] = None,\n",
       "        name: str = 'memory_index',\n",
       "        save_path: Optional[str] = None,\n",
       "        in_place: bool = True,\n",
       "        embeddings_col: Optional[str] = None,\n",
       "    ) -> 'MemoryIndex':\n",
       "        \"\"\"\n",
       "        Initialize a MemoryIndex object from a pandas DataFrame.\n",
       "\n",
       "        Args:\n",
       "            data_frame: The DataFrame or path to a CSV file.\n",
       "            columns: The columns of the DataFrame to use as values.\n",
       "            name: The name of the index.\n",
       "            save_path: The path to save the index.\n",
       "            in_place: Whether to work on the DataFrame in place or create a copy.\n",
       "            embeddings_col: The column name containing the embeddings.\n",
       "\n",
       "        Returns:\n",
       "            A MemoryIndex object initialized with values and embeddings from the DataFrame.\n",
       "        \"\"\"\n",
       "\n",
       "        if isinstance(data_frame, str) and data_frame.endswith(\".csv\") and os.path.isfile(data_frame):\n",
       "            print(\"Loading the CSV file\")\n",
       "            try:\n",
       "                data_frame = pd.read_csv(data_frame)\n",
       "            except:\n",
       "                raise ValueError(\"The CSV file is not valid\")\n",
       "            name = data_frame.split(\"/\")[-1].split(\".\")[0]\n",
       "        elif isinstance(data_frame, pd.core.frame.DataFrame) and columns is not None:\n",
       "            print(\"Loading the DataFrame\")\n",
       "            if not in_place:\n",
       "                data_frame = copy.deepcopy(data_frame)\n",
       "        else:\n",
       "            raise ValueError(\"The data_frame is not a valid pandas dataframe or the columns are not valid or the path is not valid\")\n",
       "\n",
       "        values, embeddings = cls.extract_values_and_embeddings(data_frame, columns, embeddings_col)\n",
       "        return cls(values=values, embeddings=embeddings, name=name, save_path=save_path)\n",
       "\n",
       "    @staticmethod\n",
       "    def extract_values_and_embeddings(\n",
       "        data_frame: pd.DataFrame,\n",
       "        columns: Union[str, List[str]],\n",
       "        embeddings_col: Optional[str],\n",
       "    ) -> Tuple[List[str], Optional[List[np.ndarray]]]:\n",
       "        \"\"\"\n",
       "        Extract values and embeddings from a pandas DataFrame.\n",
       "\n",
       "        Args:\n",
       "            data_frame: The DataFrame to extract values and embeddings from.\n",
       "            columns: The columns of the DataFrame to use as values.\n",
       "            embeddings_col: The column name containing the embeddings.\n",
       "\n",
       "        Returns:\n",
       "            A tuple containing two lists: one with the extracted values and one with the extracted embeddings (if any).\n",
       "        \"\"\"\n",
       "\n",
       "        if isinstance(columns, list) and len(columns) > 1:\n",
       "            data_frame[\"values_combined\"] = data_frame[columns].apply(lambda x: ' '.join(x), axis=1)\n",
       "            columns = \"values_combined\"\n",
       "        elif isinstance(columns, list) and len(columns) == 1:\n",
       "            columns = columns[0]\n",
       "        elif not isinstance(columns, str):\n",
       "            raise ValueError(\"The columns are not valid\")\n",
       "\n",
       "        values = []\n",
       "        embeddings = []\n",
       "\n",
       "        for _, row in data_frame.iterrows():\n",
       "            value = row[columns]\n",
       "            values.append(value)\n",
       "\n",
       "            if embeddings_col is not None:\n",
       "                embedding = row[embeddings_col]\n",
       "                embeddings.append(embedding)\n",
       "\n",
       "        return values, embeddings if embeddings_col is not None else None\n",
       "\n",
       "    def add_to_index(\n",
       "        self,\n",
       "        value: str,\n",
       "        embedding: Optional[Union[List[float], np.ndarray, str]] = None,\n",
       "        verbose: bool = True,\n",
       "    ) -> None:\n",
       "        \"\"\"\n",
       "        index a message in the faiss index, the message is embedded (if embedding is not provided) and the id is saved in the values list\n",
       "        \"\"\"\n",
       "        if value not in self.values:\n",
       "            if embedding is None:\n",
       "                embedding = self.embedder.embed(value)\n",
       "                if verbose:\n",
       "                    display(\n",
       "                        Markdown(\"The value {value} was embedded\".format(value=value))\n",
       "                    )\n",
       "            if embedding is not None:\n",
       "                if type(embedding) is list:\n",
       "                    embedding = np.array([embedding])\n",
       "                elif type(embedding) is str:\n",
       "                    embedding = eval(embedding)\n",
       "                    embedding = np.array([embedding]).astype(np.float32)\n",
       "                elif type(embedding) is not np.ndarray:\n",
       "                    raise ValueError(\"The embedding is not a valid type\")\n",
       "\n",
       "                # Ensure that the embedding is a 2D numpy array\n",
       "                if embedding.ndim == 1:\n",
       "                    embedding = embedding.reshape(1, -1)\n",
       "                # print(\"embedding is \", embedding)\n",
       "                # print(\"embedding type is \", type(embedding))\n",
       "                # print(\"embedding shape is \", embedding.shape)\n",
       "                self.index.add(embedding)\n",
       "                self.values.append(value)\n",
       "                self.save()\n",
       "        else:\n",
       "            if verbose:\n",
       "                display(\n",
       "                    Markdown(\n",
       "                        \"The value {value} was already in the index\".format(value=value)\n",
       "                    )\n",
       "                )\n",
       "                \n",
       "    def remove_from_index(self, value: str) -> None:\n",
       "            \"\"\"\n",
       "            Remove a value from the index and the values list.\n",
       "            Args:\n",
       "                value: The value to remove from the index.\n",
       "            \"\"\"\n",
       "            index = self.get_index_by_value(value)\n",
       "            if index is not None:\n",
       "                # Remove the value from the values list\n",
       "                self.values.pop(index)\n",
       "\n",
       "                # Remove the corresponding embedding from the index\n",
       "                id_selector = faiss.IDSelectorArray(np.array([index], dtype=np.int64))\n",
       "                self.index.remove_ids(id_selector)\n",
       "\n",
       "                # Save the changes\n",
       "                self.save()\n",
       "            else:\n",
       "                print(f\"The value '{value}' was not found in the index.\")\n",
       "\n",
       "    def get_embedding_by_index(self, index: int) -> np.ndarray:\n",
       "        \"\"\"\n",
       "        Get the embedding corresponding to a certain index value.\n",
       "        \"\"\"\n",
       "        if index < 0 or index >= len(self.values):\n",
       "            raise ValueError(\"The index is out of range\")\n",
       "\n",
       "        # Fetch the embedding from the Faiss index\n",
       "        embedding = self.index.reconstruct(index)\n",
       "\n",
       "        return embedding\n",
       "\n",
       "    def get_index_by_value(self, value: str) -> Optional[int]:\n",
       "        \"\"\"\n",
       "        Get the index corresponding to a value in self.values.\n",
       "        \"\"\"\n",
       "        if value in self.values:\n",
       "            index = self.values.index(value)\n",
       "            return index\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "    def get_embedding_by_value(self, value: str) -> Optional[np.ndarray]:\n",
       "        \"\"\"\n",
       "        Get the embedding corresponding to a certain value in self.values.\n",
       "        \"\"\"\n",
       "        index = self.get_index_by_value(value)\n",
       "        if index is not None:\n",
       "            embedding = self.get_embedding_by_index(index)\n",
       "            return embedding\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "    def get_all_embeddings(self) -> np.ndarray:\n",
       "        \"\"\"\n",
       "        Get all the embeddings in the index.\n",
       "        \"\"\"\n",
       "        embeddings = []\n",
       "        for i in range(len(self.values)):\n",
       "            embeddings.append(self.get_embedding_by_index(i))\n",
       "        self.embeddings = np.array(embeddings)\n",
       "        return self.embeddings\n",
       "\n",
       "    def faiss_query(self, query: str, k: int = 10) -> Tuple[List[str], List[float]]:\n",
       "        \"\"\"Query the faiss index for the top-k most similar values to the query\"\"\"\n",
       "\n",
       "        # Embed the data\n",
       "        embedding = self.embedder.embed(query)\n",
       "        if k > len(self.values):\n",
       "            k = len(self.values)\n",
       "        # Query the Faiss index for the top-K most similar values\n",
       "        D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\n",
       "        # Get the values corresponding to the indices\n",
       "        values = [self.values[i] for i in I[0]]\n",
       "        scores = [d for d in D[0]]\n",
       "        return values, scores, I\n",
       "\n",
       "    def token_bound_query(self, query, k=10, max_tokens=4000):\n",
       "        \"\"\"Query the faiss index for the top-k most similar values to the query, but bound the number of tokens retrieved by the max_tokens parameter\"\"\"\n",
       "        returned_tokens = 0\n",
       "        top_k_hint = []\n",
       "        scores = []\n",
       "        tokens = []\n",
       "        indices = []\n",
       "\n",
       "        if len(self.values) > 0:\n",
       "            top_k, scores, indices = self.faiss_query(query, k=min(k, len(self.values)))\n",
       "\n",
       "            for hint in top_k:\n",
       "                # mark the message and gets the length in tokens\n",
       "                message_tokens = len(self.tokenizer.encode(hint))\n",
       "                tokens.append(message_tokens)\n",
       "                if returned_tokens + message_tokens <= max_tokens:\n",
       "                    top_k_hint += [hint]\n",
       "                    returned_tokens += message_tokens\n",
       "\n",
       "            self.query_history.append(\n",
       "                {\n",
       "                    \"query\": query,\n",
       "                    \"hints\": top_k_hint,\n",
       "                    \"scores\": scores,\n",
       "                    \"indices\": indices,\n",
       "                    \"hints_tokens\": tokens,\n",
       "                    \"returned_tokens\": returned_tokens,\n",
       "                    \"max_tokens\": max_tokens,\n",
       "                    \"k\": k,\n",
       "                }\n",
       "            )\n",
       "\n",
       "        return top_k_hint, scores, indices\n",
       "\n",
       "    def save(self):\n",
       "        \"\"\"Save the index to disk using faiss and json and numpy\"\"\"\n",
       "        # Create the directory to save the index, values, and embeddings\n",
       "        save_directory = os.path.join(self.save_path, self.name)\n",
       "        os.makedirs(save_directory, exist_ok=True)\n",
       "\n",
       "        # Save the FAISS index\n",
       "        index_filename = os.path.join(save_directory, f\"{self.name}_index.faiss\")\n",
       "        faiss.write_index(self.index, index_filename)\n",
       "\n",
       "        # Save the index values\n",
       "        values_filename = os.path.join(save_directory, f\"{self.name}_values.json\")\n",
       "        with open(values_filename, \"w\") as f:\n",
       "            json.dump(self.values, f)\n",
       "\n",
       "        # Save the numpy array of the embeddings\n",
       "        embeddings_filename = os.path.join(\n",
       "            save_directory, f\"{self.name}_embeddings.npz\"\n",
       "        )\n",
       "        # print(f\"embs: {self.get_all_embeddings().shape}\")\n",
       "        np.savez_compressed(embeddings_filename, self.get_all_embeddings())\n",
       "\n",
       "    def load(self):\n",
       "        \"\"\"Load the index, values, and embeddings from disk\"\"\"\n",
       "        # Set the directory to load the index, values, and embeddings from\n",
       "        load_directory = os.path.join(self.save_path, self.name)\n",
       "\n",
       "        # Load the FAISS index\n",
       "        index_filename = os.path.join(load_directory, f\"{self.name}_index.faiss\")\n",
       "        self.index = faiss.read_index(index_filename)\n",
       "\n",
       "        # Load the index values\n",
       "        values_filename = os.path.join(load_directory, f\"{self.name}_values.json\")\n",
       "        with open(values_filename, \"r\") as f:\n",
       "            self.values = json.load(f)\n",
       "\n",
       "        # Load the numpy array of the embeddings\n",
       "        embeddings_filename = os.path.join(\n",
       "            load_directory, f\"{self.name}_embeddings.npz\"\n",
       "        )\n",
       "        embeddings_data = np.load(embeddings_filename)\n",
       "        self.embeddings = embeddings_data[\"arr_0\"]\n",
       "\n",
       "    def prune_index(\n",
       "        self,\n",
       "        constraint: Optional[str] = None,\n",
       "        regex_pattern: Optional[str] = None,\n",
       "        length_constraint: Optional[int] = None,\n",
       "    ) -> \"MemoryIndex\":\n",
       "        \"\"\"Prune the index based on the constraint provided. Currently, only regex and length constraints are supported.\"\"\"\n",
       "\n",
       "        if constraint is not None:\n",
       "            if constraint == \"regex\":\n",
       "                if regex_pattern is None:\n",
       "                    raise ValueError(\n",
       "                        \"regex_pattern must be provided for regex constraint.\"\n",
       "                    )\n",
       "                pruned_values, pruned_embeddings = self._prune_by_regex(regex_pattern)\n",
       "            elif constraint == \"length\":\n",
       "                if length_constraint is None:\n",
       "                    raise ValueError(\n",
       "                        \"length_constraint must be provided for length constraint.\"\n",
       "                    )\n",
       "                pruned_values, pruned_embeddings = self._prune_by_length(\n",
       "                    length_constraint\n",
       "                )\n",
       "            else:\n",
       "                raise ValueError(\"Invalid constraint type provided.\")\n",
       "        else:\n",
       "            raise ValueError(\"constraint must be provided for pruning the index.\")\n",
       "\n",
       "        # Create a new index with pruned values and embeddings\n",
       "        pruned_memory_index = MemoryIndex(\n",
       "            values=pruned_values,\n",
       "            embeddings=pruned_embeddings,\n",
       "            name=self.name + \"_pruned\",\n",
       "        )\n",
       "\n",
       "        return pruned_memory_index\n",
       "\n",
       "    def _prune_by_regex(self, regex_pattern: str) -> Tuple[List[str], List[np.ndarray]]:\n",
       "        \"\"\"Prune the index by the regex pattern provided.\"\"\"\n",
       "        pruned_values = []\n",
       "        pruned_embeddings = []\n",
       "\n",
       "        for value in self.values:\n",
       "            if re.search(regex_pattern, value):\n",
       "                pruned_values.append(value)\n",
       "                pruned_embeddings.append(self.get_embedding_by_value(value))\n",
       "\n",
       "        return pruned_values, pruned_embeddings\n",
       "\n",
       "    def _prune_by_length(\n",
       "        self, length_constraint: int\n",
       "    ) -> Tuple[List[str], List[np.ndarray]]:\n",
       "        \"\"\"Prune the index by the length constraint provided.\"\"\"\n",
       "        pruned_values = []\n",
       "        pruned_embeddings = []\n",
       "\n",
       "        for value in self.values:\n",
       "            if len(value) >= length_constraint:\n",
       "                pruned_values.append(value)\n",
       "                pruned_embeddings.append(self.get_embedding_by_value(value))\n",
       "\n",
       "        return pruned_values, pruned_embeddings\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class MemoryKernel(MemoryIndex):\n",
       "    def __init__(self, mem_index, name=\"memory_kernel\", k=2, save_path=None):\n",
       "        \"\"\"\n",
       "        Initialize the MemoryKernel with a MemoryIndex instance, a name, k value, and save path.\n",
       "\n",
       "        Args:\n",
       "            mem_index (MemoryIndex): A MemoryIndex instance.\n",
       "            name (str, optional): The name of the MemoryKernel. Defaults to \"memory_kernel\".\n",
       "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
       "            save_path (str, optional): The path to save the MemoryKernel. Defaults to None.\n",
       "        \"\"\"\n",
       "        super().__init__(\n",
       "            index=mem_index.index,\n",
       "            values=mem_index.values,\n",
       "            embeddings=mem_index.embeddings,\n",
       "            name=name,\n",
       "            save_path=save_path,\n",
       "        )\n",
       "        self.k = k\n",
       "        self.create_k_hop_index(k=k)\n",
       "\n",
       "    def cos_sim(self, a, b):\n",
       "        \"\"\"\n",
       "        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
       "        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
       "        \"\"\"\n",
       "        if not isinstance(a, np.ndarray):\n",
       "            a = np.array(a)\n",
       "\n",
       "        if not isinstance(b, np.ndarray):\n",
       "            b = np.array(b)\n",
       "\n",
       "        if len(a.shape) == 1:\n",
       "            a = a[np.newaxis, :]\n",
       "\n",
       "        if len(b.shape) == 1:\n",
       "            b = b[np.newaxis, :]\n",
       "\n",
       "        a_norm = a / np.linalg.norm(a, ord=2, axis=1, keepdims=True)\n",
       "        b_norm = b / np.linalg.norm(b, ord=2, axis=1, keepdims=True)\n",
       "        return np.dot(a_norm, b_norm.T)\n",
       "\n",
       "    def compute_kernel(\n",
       "        self, embedding_set, threshold=0.65, use_softmax=False, cos_sim_batch=True\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Compute the adjacency matrix of the graph.\n",
       "\n",
       "        Parameters:\n",
       "        embedding_set (numpy array): The embedding matrix of the nodes.\n",
       "        threshold (float): The threshold for the adjacency matrix.\n",
       "        use_softmax (bool): Whether to use softmax to compute the adjacency matrix.\n",
       "        cos_sim_batch (bool): Whether to use batch processing to compute the cosine similarity.\n",
       "\n",
       "        Returns:\n",
       "        adj_matrix (numpy array): The adjacency matrix of the graph.\n",
       "        \"\"\"\n",
       "\n",
       "        A = self.cos_sim(embedding_set, embedding_set)\n",
       "        if use_softmax:\n",
       "            # softmax\n",
       "            A = np.exp(A)\n",
       "            A = A / np.sum(A, axis=1)[:, np.newaxis]\n",
       "        adj_matrix = np.zeros_like(A)\n",
       "        adj_matrix[A > threshold] = 1\n",
       "        adj_matrix[A <= threshold] = 0\n",
       "        adj_matrix = adj_matrix.astype(np.float32)\n",
       "        return adj_matrix\n",
       "\n",
       "    def k_hop_message_passing(self, A, node_features, k):\n",
       "        \"\"\"\n",
       "        Compute the k-hop adjacency matrix and aggregated features using message passing.\n",
       "\n",
       "        Parameters:\n",
       "        A (numpy array): The adjacency matrix of the graph.\n",
       "        node_features (numpy array): The feature matrix of the nodes.\n",
       "        k (int): The number of hops for message passing.\n",
       "\n",
       "        Returns:\n",
       "        A_k (numpy array): The k-hop adjacency matrix.\n",
       "        agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n",
       "        \"\"\"\n",
       "\n",
       "        print(\"Compute the k-hop adjacency matrix\")\n",
       "        A_k = np.linalg.matrix_power(A, k)\n",
       "\n",
       "        print(\"Aggregate the messages from the k-hop neighborhood:\")\n",
       "        agg_features = node_features.copy()\n",
       "\n",
       "        for i in tqdm(range(k)):\n",
       "            agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n",
       "\n",
       "        return A_k, agg_features\n",
       "\n",
       "    def graph_sylvester_embedding(self, G, m: int, ts: np.ndarray) -> np.ndarray:\n",
       "        \"\"\"\n",
       "        Compute the spectral kernel descriptor or the Spectral Graph Wavelet descriptor.\n",
       "\n",
       "        Args:\n",
       "            G (Tuple): A tuple containing the graph's vertices (V) and weights (W).\n",
       "            m (int): The number of singular values to consider.\n",
       "            ts (np.ndarray): The spectral scales.\n",
       "\n",
       "        Returns:\n",
       "            np.ndarray: The node_embeddings matrix.\n",
       "        \"\"\"\n",
       "        V, W = G\n",
       "        n = len(V)\n",
       "        D_BE = np.diag(W.sum(axis=1))\n",
       "        L_BE = np.identity(n) - np.dot(\n",
       "            np.diag(1 / np.sqrt(D_BE.diagonal())),\n",
       "            np.dot(W, np.diag(1 / np.sqrt(D_BE.diagonal()))),\n",
       "        )\n",
       "\n",
       "        A = W\n",
       "        B = L_BE\n",
       "        C = np.identity(n)\n",
       "        X = solve_sylvester(A, B, C)\n",
       "\n",
       "        U, S, Vh = svd(X, full_matrices=False)\n",
       "        U_m = U[:, :m]\n",
       "        S_m = S[:m]\n",
       "\n",
       "        node_embeddings = np.zeros((n, m))\n",
       "\n",
       "        for i in range(n):\n",
       "            for s in range(m):\n",
       "                # Spectral kernel descriptor\n",
       "                node_embeddings[i, s] = np.exp(-ts[s] * S_m[s]) * U_m[i, s]\n",
       "\n",
       "        return node_embeddings\n",
       "\n",
       "    def gen_gse_embeddings(self, A, embeddings, m: int = 7):\n",
       "        \"\"\"\n",
       "        Generate Graph Sylvester Embeddings.\n",
       "\n",
       "        Args:\n",
       "            A (np.ndarray): The adjacency matrix of the graph.\n",
       "            embeddings (np.ndarray): The original node embeddings.\n",
       "            m (int, optional): The number of spectral scales. Defaults to 7.\n",
       "\n",
       "        Returns:\n",
       "            np.ndarray: The generated Graph Sylvester Embeddings.\n",
       "        \"\"\"\n",
       "        V = list(range(len(embeddings)))\n",
       "        W = A\n",
       "\n",
       "        G = (V, W)\n",
       "        ts = np.linspace(0, 1, m)  # equally spaced scales\n",
       "\n",
       "        gse_embeddings = self.graph_sylvester_embedding(G, m, ts)\n",
       "        return gse_embeddings\n",
       "\n",
       "    def create_k_hop_index(self, k=2):\n",
       "        \"\"\"\n",
       "        Create a k-hop index by computing the adjacency matrix, k-hop adjacency matrix,\n",
       "        aggregated features, and updating the memory index.\n",
       "\n",
       "        Args:\n",
       "            k (int, optional): The number of hops for message passing. Defaults to 2.\n",
       "        \"\"\"\n",
       "        self.k = k\n",
       "        print(\"Computing the adjacency matrix\")\n",
       "        print(\"Embeddings shape: \", self.embeddings.shape)\n",
       "        self.A = self.compute_kernel(self.embeddings, threshold=0.65, use_softmax=False)\n",
       "        print(\"Computing the k-hop adjacency matrix and aggregated features\")\n",
       "        self.A_k, self.node_embeddings = self.k_hop_message_passing(\n",
       "            self.A, self.embeddings, k\n",
       "        )\n",
       "        print(\"Updating the memory index\")\n",
       "        self.k_hop_index = MemoryIndex(name=self.name)\n",
       "        self.k_hop_index.init_index(values=self.values, embeddings=self.node_embeddings)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class MemoryKernelGroup(MemoryKernel):\n",
       "    def __init__(self, memory_kernel_dict: Dict[str, MemoryKernel], name=\"memory_kernel_group\"):\n",
       "        \"\"\"\n",
       "        Initialize the MemoryKernelGroup with a dictionary of MemoryKernel instances.\n",
       "\n",
       "        Args:\n",
       "            memory_kernel_dict (Dict[str, MemoryKernel]): A dictionary of MemoryKernel instances.\n",
       "            name (str, optional): The name of the MemoryKernelGroup. Defaults to \"memory_kernel_group\".\n",
       "        \"\"\"\n",
       "        self.memory_kernel_dict = memory_kernel_dict\n",
       "        self.name = name\n",
       "\n",
       "    def create_paths_hdbscan(\n",
       "        self, embeddings: np.ndarray, num_clusters: int\n",
       "    ) -> List[List[int]]:\n",
       "        \"\"\"\n",
       "        Create paths using the HDBSCAN clustering algorithm.\n",
       "\n",
       "        Args:\n",
       "            embeddings (np.ndarray): The embeddings to be clustered.\n",
       "            num_clusters (int): The minimum number of clusters.\n",
       "\n",
       "        Returns:\n",
       "            List[List[int]]: A list of lists containing the clustered paths.\n",
       "        \"\"\"\n",
       "        clusterer = hdbscan.HDBSCAN(min_cluster_size=num_clusters)\n",
       "        cluster_assignments = clusterer.fit_predict(embeddings)\n",
       "\n",
       "        paths = [[] for _ in range(num_clusters)]\n",
       "        for i, cluster in enumerate(cluster_assignments):\n",
       "            paths[cluster].append(i)\n",
       "        paths = [path for path in paths if path]\n",
       "        return paths\n",
       "\n",
       "    def create_paths_spectral_clustering(\n",
       "        self, embeddings: np.ndarray, num_clusters: int\n",
       "    ) -> List[List[int]]:\n",
       "        \"\"\"\n",
       "        Create paths using the spectral clustering algorithm.\n",
       "\n",
       "        Args:\n",
       "            embeddings (np.ndarray): The embeddings to be clustered.\n",
       "            num_clusters (int): The number of clusters.\n",
       "\n",
       "        Returns:\n",
       "            List[List[int]]: A list of lists containing the clustered paths.\n",
       "        \"\"\"\n",
       "        spectral_clustering = SpectralClustering(\n",
       "            n_clusters=num_clusters, affinity=\"nearest_neighbors\", random_state=42\n",
       "        )\n",
       "        cluster_assignments = spectral_clustering.fit_predict(embeddings)\n",
       "\n",
       "        paths = [[] for _ in range(num_clusters)]\n",
       "        for i, cluster in enumerate(cluster_assignments):\n",
       "            paths[cluster].append(i)\n",
       "        paths = [path for path in paths if path]\n",
       "        return paths\n",
       "\n",
       "    def calc_shgo_mode(self, scores: List[float]) -> float:\n",
       "        \"\"\"\n",
       "        Calculate the mode of the given scores using the SHGO optimization algorithm.\n",
       "\n",
       "        Args:\n",
       "            scores (List[float]): The scores for which the mode is to be calculated.\n",
       "\n",
       "        Returns:\n",
       "            float: The mode of the given scores.\n",
       "        \"\"\"\n",
       "        def objective(x):\n",
       "            return -self.estimate_pdf(scores)(x)\n",
       "\n",
       "        bounds = [(min(scores), max(scores))]\n",
       "        result = scipy.optimize.shgo(objective, bounds)\n",
       "        return result.x\n",
       "\n",
       "    def estimate_pdf(self, scores: List[float]) -> callable:\n",
       "        \"\"\"\n",
       "        Estimate the probability density function of the given scores.\n",
       "\n",
       "        Args:\n",
       "            scores (List[float]): The scores for which the PDF is to be estimated.\n",
       "\n",
       "        Returns:\n",
       "            callable: A callable object representing the PDF.\n",
       "        \"\"\"\n",
       "        pdf = scipy.stats.gaussian_kde(scores)\n",
       "        return pdf\n",
       "\n",
       "    def print_path(self, kernel_label: str, path: List[int]) -> None:\n",
       "        \"\"\"\n",
       "        Print the path for the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "            kernel_label (str): The label of the kernel.\n",
       "            path (List[int]): The path to be printed.\n",
       "        \"\"\"\n",
       "        for i in path:\n",
       "            print(self.memory_kernel_dict[kernel_label].values[i])\n",
       "\n",
       "    def sort_paths_by_mode_distance(\n",
       "        self, kernel_label: str, distance_metric: str = \"cosine\"\n",
       "    )-> None:\n",
       "        \"\"\"\n",
       "        Sort paths by the mode distance of the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "            kernel_label (str): The label of the kernel.\n",
       "            distance_metric (str, optional): The distance metric to be used. Defaults to \"cosine\".\n",
       "        \"\"\"\n",
       "        paths = self.path_group[kernel_label]\n",
       "        memory_kernel = self.memory_kernel_dict[kernel_label]\n",
       "        sorted_paths = []\n",
       "        for i, path in enumerate(paths):\n",
       "            cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
       "            cluster_embeddings = np.array(cluster_embeddings)\n",
       "            cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
       "            if distance_metric == \"cosine\" or distance_metric == \"guassian\":\n",
       "                scores = [\n",
       "                    (i, cosine(cluster_mean, emb))\n",
       "                    for i, emb in zip(path, cluster_embeddings)\n",
       "                ]\n",
       "            elif distance_metric == \"euclidean\":\n",
       "                scores = [\n",
       "                    (i, np.linalg.norm(cluster_mean - emb))\n",
       "                    for i, emb in zip(path, cluster_embeddings)\n",
       "                ]\n",
       "            score_values = [score for _, score in scores]  # Extract score values\n",
       "            mu = self.calc_shgo_mode(score_values)\n",
       "            sigma = np.std(score_values)\n",
       "            if distance_metric == \"guassian\":\n",
       "                scores = [\n",
       "                    (i, np.exp(-((x - mu) ** 2) / (2 * sigma**2))) for i, x in scores\n",
       "                ]\n",
       "            # Sort path by score\n",
       "            sorted_path_and_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
       "            sorted_path = [x[0] for x in sorted_path_and_scores]\n",
       "            sorted_paths.append(sorted_path)\n",
       "        self.path_group[kernel_label] = sorted_paths\n",
       "\n",
       "    def sort_paths_by_kernel_density(\n",
       "        self, kernel_label: str, distance_metric: str = \"cosine\"\n",
       "    ) -> None:\n",
       "        \"\"\"\n",
       "        Sort paths by the mode distance of the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "            kernel_label (str): The label of the kernel.\n",
       "            distance_metric (str, optional): The distance metric to be used. Defaults to \"cosine\".\n",
       "        \"\"\"\n",
       "        paths = self.path_group[kernel_label]\n",
       "        memory_kernel = self.memory_kernel_dict[kernel_label]\n",
       "        sorted_paths = []\n",
       "        for i, path in enumerate(paths):\n",
       "            cluster_embeddings = [memory_kernel.node_embeddings[i] for i in path]\n",
       "            cluster_embeddings = np.array(cluster_embeddings)\n",
       "            cluster_mean = np.mean(cluster_embeddings, axis=0)\n",
       "            if distance_metric == \"cosine\":\n",
       "                scores = [\n",
       "                    (i, cosine(cluster_mean, emb))\n",
       "                    for i, emb in zip(path, cluster_embeddings)\n",
       "                ]\n",
       "            elif distance_metric == \"euclidean\":\n",
       "                scores = [\n",
       "                    (i, np.linalg.norm(cluster_mean - emb))\n",
       "                    for i, emb in zip(path, cluster_embeddings)\n",
       "                ]\n",
       "            score_values = [score for _, score in scores]  # Extract score values\n",
       "\n",
       "            # Estimate PDF using Kernel Density Estimation\n",
       "            kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(\n",
       "                np.array(score_values).reshape(-1, 1)\n",
       "            )\n",
       "            kde_scores = [kde.score_samples([[x]])[0] for _, x in scores]\n",
       "\n",
       "            # Sort path by score\n",
       "            sorted_path_and_scores = sorted(\n",
       "                zip(path, kde_scores), key=lambda x: x[1], reverse=True\n",
       "            )\n",
       "            sorted_path = [x[0] for x in sorted_path_and_scores]\n",
       "            sorted_paths.append(sorted_path)\n",
       "        self.path_group[kernel_label] = sorted_paths\n",
       "\n",
       "    def gen_index_aligned_kernel(\n",
       "        self, chatbot: Chat, parent_kernel_label: str, child_kernel_label: str\n",
       "    ) -> None:\n",
       "        \"\"\"\n",
       "        Generate an index-aligned kernel using LLMWriter for the given parent and child kernel labels.\n",
       "\n",
       "        Args:\n",
       "            chatbot (Chat): The Chat instance.\n",
       "            parent_kernel_label (str): The label of the parent kernel.\n",
       "            child_kernel_label (str): The label of the child kernel.\n",
       "        \"\"\"\n",
       "        llm_writer = LLMWriter(\n",
       "            index=self.memory_kernel_dict[parent_kernel_label],\n",
       "            path=self.path_group[parent_kernel_label],\n",
       "            chatbot=chatbot,\n",
       "            write_func=None,\n",
       "            max_workers=1,\n",
       "        )\n",
       "        new_index = llm_writer.write()\n",
       "        new_memory_kernel = MemoryKernel(mem_index=new_index, name=child_kernel_label)\n",
       "        new_memory_kernel.create_k_hop_index(k=2)\n",
       "        self.memory_kernel_dict[child_kernel_label] = new_memory_kernel\n",
       "\n",
       "    def generate_path_groups(self, method: str = \"hdbscan\") -> None:\n",
       "        \"\"\"\n",
       "        Generate path groups for all memory kernels in the memory_kernel_dict using the specified clustering method.\n",
       "\n",
       "        Args:\n",
       "            method (str, optional): The clustering method to be used. Defaults to \"hdbscan\".\n",
       "        \"\"\"\n",
       "        path_group = {}\n",
       "        for k, v in self.memory_kernel_dict.items():\n",
       "            embeddings = v.node_embeddings\n",
       "            num_clusters = int(np.sqrt(len(embeddings)))\n",
       "            if method == \"hdbscan\":\n",
       "                paths = self.create_paths_hdbscan(embeddings, num_clusters)\n",
       "            elif method == \"spectral_clustering\":\n",
       "                paths = self.create_paths_spectral_clustering(embeddings, num_clusters)\n",
       "            path_group[k] = paths\n",
       "\n",
       "        self.path_group = path_group\n",
       "\n",
       "    def batch_sort_kernel_group(self, kernel_label: str):\n",
       "        \"\"\"\n",
       "        Batch sort the kernel group by the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "        \"\"\"\n",
       "        if all(\n",
       "            [\n",
       "                v.node_embeddings.shape\n",
       "                == self.memory_kernel_dict[kernel_label].node_embeddings.shape\n",
       "                for k, v in self.memory_kernel_dict.items()\n",
       "            ]\n",
       "        ):\n",
       "            self.memory_kernel_sort(self.path_group[kernel_label])\n",
       "        else:\n",
       "            return ValueError(\"Not all kernels are of the same dimensions.\")\n",
       "\n",
       "    def memory_kernel_sort(self, paths: List[List[int]]):\n",
       "        pass\n",
       "\n",
       "    def is_kernel_group_isomorphic(self):\n",
       "        pass\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class MemoryKernelGroupStabilityAnalysis:\n",
       "    def __init__(self, memory_kernel_group: MemoryKernelGroup):\n",
       "        \"\"\"\n",
       "        Initialize the MemoryKernelGroupStabilityAnalysis with a MemoryKernelGroup instance.\n",
       "\n",
       "        Args:\n",
       "            memory_kernel_group (MemoryKernelGroup): A MemoryKernelGroup instance.\n",
       "        \"\"\"\n",
       "        self.memory_kernel_group = memory_kernel_group\n",
       "\n",
       "    def get_cluster_labels(self, kernel_label: str) -> Tuple[np.ndarray, int]:\n",
       "        \"\"\"\n",
       "        Get the cluster labels for the specified kernel label.\n",
       "\n",
       "        Args:\n",
       "            kernel_label (str): The label of the kernel.\n",
       "\n",
       "        Returns:\n",
       "            Tuple[np.ndarray, int]: A tuple containing an array of cluster labels and the number of clusters.\n",
       "        \"\"\"\n",
       "        paths = self.memory_kernel_group.path_group[kernel_label]\n",
       "        num_clusters = len(paths)\n",
       "        cluster_labels = np.empty(\n",
       "            len(\n",
       "                self.memory_kernel_group.memory_kernel_dict[\n",
       "                    kernel_label\n",
       "                ].node_embeddings\n",
       "            ),\n",
       "            dtype=int,\n",
       "        )\n",
       "\n",
       "        for cluster_index, path in enumerate(paths):\n",
       "            cluster_labels[path] = cluster_index\n",
       "\n",
       "        return cluster_labels, num_clusters\n",
       "\n",
       "    def compute_nmi(self, kernel_label1: str, kernel_label2: str) -> float:\n",
       "        \"\"\"\n",
       "        Compute the normalized mutual information (NMI) between two kernel by labels.\n",
       "\n",
       "        Args:\n",
       "            kernel_label1 (str): The first kernel label.\n",
       "            kernel_label2 (str): The second kernel label.\n",
       "\n",
       "        Returns:\n",
       "            float: The NMI value between the two kernel labels.\n",
       "        \"\"\"\n",
       "        cluster_labels1, _ = self.get_cluster_labels(kernel_label1)\n",
       "        cluster_labels2, _ = self.get_cluster_labels(kernel_label2)\n",
       "        nmi = normalized_mutual_info_score(cluster_labels1, cluster_labels2)\n",
       "        return nmi\n",
       "\n",
       "    def evaluate_stability(self) -> float:\n",
       "        \"\"\"\n",
       "        Evaluate the stability of the MemoryKernelGroup by calculating the average NMI between all pairs of kernels.\n",
       "\n",
       "        Returns:\n",
       "            float: The stability score of the MemoryKernelGroup.\n",
       "        \"\"\"\n",
       "        kernel_labels = list(self.memory_kernel_group.memory_kernel_dict.keys())\n",
       "        pairwise_combinations = list(itertools.combinations(kernel_labels, 2))\n",
       "        nmi_sum = 0\n",
       "\n",
       "        for kernel_label1, kernel_label2 in pairwise_combinations:\n",
       "            nmi = self.compute_nmi(kernel_label1, kernel_label2)\n",
       "            nmi_sum += nmi\n",
       "\n",
       "        stability_score = nmi_sum / len(pairwise_combinations)\n",
       "        return stability_score\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "class PandasIndex(MemoryIndex):\n",
       "    \"\"\"\n",
       "    A class to create an index of a pandas DataFrame, allowing querying on specified columns.\n",
       "    Inherits from MemoryIndex class.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, df: pd.DataFrame, row_func: Optional[Callable[[pd.Series], str]] = None, name= \"pandas_index\", columns: Optional[List[str]] = None):\n",
       "        \"\"\"\n",
       "        Initialize a PandasIndex object.\n",
       "        \n",
       "        Args:\n",
       "            df: A pandas DataFrame to index.\n",
       "            row_func: An optional function to process rows before adding them to the index.\n",
       "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
       "        \"\"\"\n",
       "        if row_func is None:\n",
       "            row_func = lambda row: str(row)\n",
       "        self.row_func = row_func\n",
       "\n",
       "        self.df = df\n",
       "        MemoryIndex.__init__(self,name=name) # Initialize the parent MemoryIndex class\n",
       "        \n",
       "        # Initialize the row-wise index\n",
       "        for _, row in df.iterrows():\n",
       "            self.add_to_index(row_func(row))\n",
       "        \n",
       "        self.columns: Dict[str, MemoryIndex] = {} \n",
       "\n",
       "        # Set up columns during initialization\n",
       "        if columns is None:\n",
       "            self.setup_columns()\n",
       "        else:\n",
       "            self.setup_columns(columns)\n",
       "        self.save()\n",
       "        for col in self.columns:\n",
       "            self.columns[col].save()\n",
       "        self.executed_tasks = []\n",
       "\n",
       "    def setup_columns(self, columns: Optional[List[str]] = None):\n",
       "        \"\"\"\n",
       "        Set up columns for indexing.\n",
       "        \n",
       "        Args:\n",
       "            columns: An optional list of column names to index. By default, it will index all string columns and columns containing lists with a single string.\n",
       "        \"\"\"\n",
       "        if columns is None:\n",
       "            # Use string columns or columns with lists containing a single string by default\n",
       "            columns = [col for col in self.df.columns if self.df[col].apply(lambda x: isinstance(x, str) or (isinstance(x, list) and len(x) == 1 and isinstance(x[0], str))).all()]\n",
       "\n",
       "        for col in columns:\n",
       "            self.columns[col] = MemoryIndex.from_pandas(self.df, columns=col, name=f\"{self.name}_{col}\")\n",
       "\n",
       "    def query_columns(self, query: str, columns: List[str]) -> List[Tuple[str, float]]:\n",
       "        \"\"\"\n",
       "        Query the indexed columns of the DataFrame.\n",
       "        \n",
       "        Args:\n",
       "            query: The search query as a string.\n",
       "            columns: A list of column names to query.\n",
       "        \n",
       "        Returns:\n",
       "            A list of tuples containing the matched value and its similarity score.\n",
       "        \"\"\"\n",
       "        results = []\n",
       "        for col in columns:\n",
       "            if col in self.columns:\n",
       "                results.extend(self.columns[col].faiss_query(query))\n",
       "            else:\n",
       "                raise KeyError(f\"Column '{col}' not found in PandaDb columns dictionary.\")\n",
       "        return results\n",
       "    \n",
       "    def add_row(self, row: pd.Series) -> None:\n",
       "        \"\"\"\n",
       "        Add a row to the DataFrame and update the row and column indexes.\n",
       "\n",
       "        Args:\n",
       "            row: A pandas Series representing the row to add.\n",
       "        \"\"\"\n",
       "        self.df = pd.concat([self.df, row.to_frame().T], ignore_index=True)\n",
       "        self.add_to_index(self.row_func(row))\n",
       "\n",
       "        for col in self.columns:\n",
       "            if col in row:\n",
       "                self.columns[col].add_to_index(row[col])\n",
       "\n",
       "\n",
       "    def remove_row(self, index: int) -> None:\n",
       "        \"\"\"\n",
       "        Remove a row from the DataFrame and update the row and column indexes.\n",
       "\n",
       "        Args:\n",
       "            index: The index of the row to remove.\n",
       "        \"\"\"\n",
       "        if 0 <= index < len(self.df):\n",
       "            self.remove_from_index(self.values[index])\n",
       "\n",
       "            for col in self.columns:\n",
       "                self.columns[col].remove_from_index(self.columns[col].values[index])\n",
       "\n",
       "            self.df.drop(index, inplace=True)\n",
       "            self.df.reset_index(drop=True, inplace=True)\n",
       "        else:\n",
       "            raise IndexError(f\"Index {index} is out of bounds for DataFrame with length {len(self.df)}\")\n",
       "    \n",
       "    def rows_from_value(self, value: Union[str, int, float], column: Optional[str] = None) -> pd.DataFrame:\n",
       "        \"\"\"\n",
       "        Return all rows of the DataFrame that have a particular value in the row index or a column index.\n",
       "\n",
       "        Args:\n",
       "            value: The value to search for in the DataFrame.\n",
       "            column: The name of the column to search in. If None, search in the row index.\n",
       "\n",
       "        Returns:\n",
       "            A pandas DataFrame containing the rows with the specified value.\n",
       "        \"\"\"\n",
       "        if column is None:\n",
       "            return self.df.loc[self.df.index == value]\n",
       "        else:\n",
       "            if column in self.df.columns:\n",
       "                return self.df.loc[self.df[column] == value]\n",
       "            else:\n",
       "                raise KeyError(f\"Column '{column}' not found in the DataFrame.\")\n",
       "\n",
       "    def apply_llmtask(self, path: List[List[int]], chatbot: Chat, write_func= None, columns: Optional[List[str]] = None) -> pd.DataFrame:\n",
       "        \"\"\"\n",
       "        Apply a writing task to the specified columns or the main index, and create new modified indexes and a corresponding DataFrame with new values.\n",
       "\n",
       "        Args:\n",
       "            write_task: An instance of a writing task (subclass of BaseTask).\n",
       "            columns: A list of column names to apply the writing task to, or None (default) to apply the task to the main index.\n",
       "\n",
       "        Returns:\n",
       "            A pandas DataFrame containing the modified values in the specified columns or a new column with the modified values of the main index.\n",
       "        \"\"\"\n",
       "        modified_df = self.df.copy()\n",
       "        \n",
       "\n",
       "        if columns is None:\n",
       "            # Apply the writing task to the main index\n",
       "            write_index = self\n",
       "            write_task = LLMWriter(write_index, path, chatbot, write_func=write_func, context= self.df)\n",
       "            \n",
       "            new_index = write_task.write()\n",
       "\n",
       "            # Create a mapping of old values to new values\n",
       "            old_to_new_values = dict(zip(self.values, new_index.values))\n",
       "\n",
       "            # Update the row values in the modified DataFrame\n",
       "            modified_df['new_column'] = modified_df.apply(lambda row: old_to_new_values.get(self.row_func(row), self.row_func(row)), axis=1)\n",
       "        else:\n",
       "            # Iterate over the specified columns\n",
       "            for col in columns:\n",
       "                if col in self.columns:\n",
       "                    # Apply the writing task to the column\n",
       "                    write_index = self.columns[col]\n",
       "                    write_task = LLMWriter(write_index, path, chatbot)\n",
       "                    new_index = write_task.write()\n",
       "\n",
       "                    # Create a mapping of old values to new values\n",
       "                    old_to_new_values = dict(zip(self.columns[col].values, new_index.values))\n",
       "\n",
       "                    # Update the column values in the modified DataFrame\n",
       "                    modified_df[col] = modified_df[col].apply(lambda x: old_to_new_values.get(x, x))\n",
       "\n",
       "                    # Update the column's MemoryIndex\n",
       "                    self.columns[col] = new_index\n",
       "                    self.columns[col].save()\n",
       "                else:\n",
       "                    raise KeyError(f\"Column '{col}' not found in PandasIndex columns dictionary.\")\n",
       "        #remove context from the write_task to avoid memory leak\n",
       "        write_task.context = None\n",
       "        self.executed_tasks.append({\"task\": write_task, \"output\": modified_df})\n",
       "        \n",
       "        return modified_df\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class PythonIndex(MemoryIndex, PythonParser):\n",
       "    def __init__(\n",
       "        self,\n",
       "        directory_path: str,\n",
       "        name: str = \"python_index\",\n",
       "        save_path: Optional[str] = None,\n",
       "        load: bool = False,\n",
       "        minify_code: bool = False,\n",
       "        remove_docstrings: bool = False,\n",
       "        tokenizer: Optional[tiktoken.Encoding] = None,\n",
       "    ):\n",
       "        # Initialize the MemoryIndex\n",
       "        MemoryIndex.__init__(\n",
       "            self,\n",
       "            name=name,\n",
       "            save_path=save_path,\n",
       "            load=load,\n",
       "            tokenizer=tokenizer,\n",
       "        )\n",
       "        # Initialize the PythonParser\n",
       "        PythonParser.__init__(\n",
       "            self,\n",
       "            directory_path=directory_path,\n",
       "            minify_code=minify_code,\n",
       "            remove_docstrings=remove_docstrings,\n",
       "        )\n",
       "\n",
       "        if not load:\n",
       "            # Extract functions and classes source code\n",
       "            function_source_codes, class_source_codes, _, _ = self.process_directory()\n",
       "            print(\n",
       "                \"Indexing {} functions and {} classes\".format(\n",
       "                    len(function_source_codes), len(class_source_codes)\n",
       "                )\n",
       "            )\n",
       "            # Concatenate function and class source code and index them\n",
       "            codes = function_source_codes + class_source_codes\n",
       "            for code in codes:\n",
       "                self.add_to_index(code)\n",
       "\n",
       "            self.save()\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class BaseThread:\n",
       "    \"\"\"\n",
       "    This class is used to keep track of the memory thread of a conversation and the total number of tokens.\n",
       "    All conversation memories should subclass this class. If max_memory is None, it has\n",
       "    no limit to the number of tokens that can be stored in the memory thread.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        name: str = \"memory\",\n",
       "        max_memory: Optional[int] = None,\n",
       "        tokenizer: Optional[Any] = None,\n",
       "    ) -> None:\n",
       "        \"\"\"\n",
       "        Initialize the BaseThread instance.\n",
       "\n",
       "        :param name: The name of the memory thread. Defaults to 'memory'.\n",
       "        :param max_memory: The maximum number of tokens allowed in the memory thread.\n",
       "                           Defaults to None, which means no limit.\n",
       "        :param tokenizer: The tokenizer to be used for tokenizing messages.\n",
       "                          Defaults to None, which means using the tiktoken encoding for the 'gpt-3.5-turbo' model.\n",
       "        \"\"\"\n",
       "        self.name = name\n",
       "        self.max_memory = max_memory\n",
       "        self.memory_thread = []\n",
       "        self.time_stamps = []\n",
       "        self.message_tokens = []\n",
       "        self.total_tokens = 0\n",
       "        if tokenizer is None:\n",
       "            self.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
       "\n",
       "    def __getitem__(self, idx):\n",
       "        return self.memory_thread[idx]\n",
       "\n",
       "    def __len__(self):\n",
       "        return len(self.memory_thread)\n",
       "\n",
       "    def reset_memory(self) -> None:\n",
       "        \"\"\"\n",
       "        Reset the memory thread.\n",
       "        \"\"\"\n",
       "        self.memory_thread = []\n",
       "        self.time_stamps = []\n",
       "        self.message_tokens = []\n",
       "        self.total_tokens = 0\n",
       "\n",
       "    def get_message_tokens(self, message_dict: dict) -> int:\n",
       "        \"\"\"\n",
       "        Calculate the number of tokens in a message, including the role token.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        :return: The total number of tokens in the message.\n",
       "        \"\"\"\n",
       "        message_dict = check_dict(message_dict)\n",
       "        message = message_dict[\"content\"]\n",
       "        return len(self.tokenizer.encode(message)) + 6  # +6 for the role token\n",
       "\n",
       "    def get_message_role(self, message_dict: dict) -> str:\n",
       "        \"\"\"\n",
       "        Get the role of the message from a message dictionary.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        :return: The role of the message.\n",
       "        \"\"\"\n",
       "        message_dict = check_dict(message_dict)\n",
       "        return message_dict[\"role\"]\n",
       "\n",
       "    def add_message(self, message_dict: dict) -> None:\n",
       "        \"\"\"\n",
       "        Add a message to the memory thread.\n",
       "\n",
       "        :param message_dict: A dictionary containing the role and content of the message.\n",
       "        \"\"\"\n",
       "        message_tokens = self.get_message_tokens(message_dict)\n",
       "\n",
       "        if (\n",
       "            self.max_memory is None\n",
       "            or self.total_tokens + message_tokens <= self.max_memory\n",
       "        ):\n",
       "            # add the message_dict to the memory_thread\n",
       "            # update the total number of tokens\n",
       "            self.memory_thread.append(message_dict)\n",
       "            self.total_tokens += message_tokens\n",
       "            self.message_tokens.append(message_tokens)\n",
       "            time_stamp = time.time()\n",
       "            self.time_stamps.append(time_stamp)\n",
       "        else:\n",
       "            display(\n",
       "                Markdown(\n",
       "                    \"The memory BaseThread is full, the last message was not added\"\n",
       "                )\n",
       "            )\n",
       "\n",
       "    def remove_message(\n",
       "        self, message_dict: Union[dict, None] = None, idx: Union[int, None] = None\n",
       "    ) -> None:\n",
       "        \"\"\"\n",
       "        Remove a message from the memory thread.\n",
       "        \"\"\"\n",
       "        if message_dict is None and idx is None:\n",
       "            raise Exception(\"You need to provide either a message_dict or an idx\")\n",
       "        elif message_dict is not None and idx is not None:\n",
       "            raise Exception(\"You need to provide either a message_dict or an idx\")\n",
       "\n",
       "        if idx is None:\n",
       "            message_dict = check_dict(message_dict)\n",
       "            search_results = self.find_message(message_dict)\n",
       "            if search_results is not None:\n",
       "                idx = search_results[-1][\"idx\"]\n",
       "                message = search_results[-1][\"message_dict\"]\n",
       "                self.memory_thread.pop(idx)\n",
       "                self.message_tokens.pop(idx)\n",
       "                self.time_stamps.pop(idx)\n",
       "                self.total_tokens -= self.get_message_tokens(message)\n",
       "            else:\n",
       "                raise Exception(\"The message was not found in the memory BaseThread\")\n",
       "        else:\n",
       "            if idx < len(self.memory_thread):\n",
       "                message = self.memory_thread.pop(idx)\n",
       "                self.total_tokens -= self.get_message_tokens(message)\n",
       "            else:\n",
       "                raise Exception(\"The index was out bound\")\n",
       "\n",
       "    def find_message(\n",
       "        self, message: Union[dict, str], role: Union[str, None] = None\n",
       "    ) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Find a message in the memory thread. If the message is a dictionary, it will search for the exact match.\n",
       "        If the message is a string, it will search for the string in the content of the message dictionary.\"\"\"\n",
       "        # check if the message is a dictioanry or a string\n",
       "        message = message if isinstance(message, str) else check_dict(message)\n",
       "        search_results = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            target = (\n",
       "                message_dict if isinstance(message, dict) else message_dict[\"content\"]\n",
       "            )\n",
       "            if target == message and (role is None or message_dict[\"role\"] == role):\n",
       "                search_results.append({\"idx\": idx, \"message_dict\": message_dict})\n",
       "        return search_results if len(search_results) > 0 else None\n",
       "\n",
       "    def find_role(self, role: str) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Find all messages with a specific role in the memory thread.\n",
       "        \"\"\"\n",
       "        search_results = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict[\"role\"] == role:\n",
       "                search_results.append({\"idx\": idx, \"message_dict\": message_dict})\n",
       "        return search_results if len(search_results) > 0 else None\n",
       "\n",
       "    def last_message(self, role: Union[str, None] = None) -> Union[None, dict]:\n",
       "        \"\"\"\n",
       "        Get the last message in the memory thread with a specific role.\"\"\"\n",
       "        if role is None:\n",
       "            return self.memory_thread[-1]\n",
       "        else:\n",
       "            for message_dict in reversed(self.memory_thread):\n",
       "                if message_dict[\"role\"] == role:\n",
       "                    return message_dict\n",
       "            return None\n",
       "\n",
       "    def first_message(self, role: Union[str, None] = None) -> Union[None, dict]:\n",
       "        \"\"\"\n",
       "        Get the first message in the memory thread with a specific role.\"\"\"\n",
       "        if role is None:\n",
       "            return self.memory_thread[0]\n",
       "        else:\n",
       "            for message_dict in self.memory_thread:\n",
       "                if message_dict[\"role\"] == role:\n",
       "                    return message_dict\n",
       "            return None\n",
       "\n",
       "    def messages_before(\n",
       "        self, message: dict, role: Union[str, None] = None\n",
       "    ) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Get all messages before a specific message in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict == message and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages = self.memory_thread[:idx]\n",
       "                break\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_before(\n",
       "        self, message: dict, role: Union[str, None] = None\n",
       "    ) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Get all messages after a specific message in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict == message and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages = self.memory_thread[idx + 1 :]\n",
       "                break\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_between(\n",
       "        self, start_message: dict, end_message: dict, role: Union[str, None] = None\n",
       "    ) -> Union[None, list]:\n",
       "        \"\"\"\n",
       "        Get all messages between two specific messages in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict == start_message and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                start_idx = idx\n",
       "                break\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if message_dict == end_message and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                end_idx = idx\n",
       "                break\n",
       "        messages = self.memory_thread[start_idx + 1 : end_idx]\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_more_tokens(self, tokens: int, role: Union[str, None] = None):\n",
       "        \"\"\"\n",
       "        Get all messages with more tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if self.message_tokens[idx] > tokens and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_less_tokens(self, tokens: int, role: Union[str, None] = None):\n",
       "        \"\"\"\n",
       "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if self.message_tokens[idx] < tokens and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_between_tokens(\n",
       "        self, start_tokens: int, end_tokens: int, role: Union[str, None] = None\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Get all messages with less tokens than a specific number in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if (\n",
       "                self.message_tokens[idx] > start_tokens\n",
       "                and self.message_tokens[idx] < end_tokens\n",
       "                and (role is None or message_dict[\"role\"] == role)\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_before_time(self, time_stamp, role: Union[str, None] = None):\n",
       "        \"\"\"\n",
       "        Get all messages before a specific time in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if self.time_stamps[idx] < time_stamp and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_after_time(self, time_stamp, role: Union[str, None] = None):\n",
       "        \"\"\"\n",
       "        Get all messages after a specific time in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if self.time_stamps[idx] > time_stamp and (\n",
       "                role is None or message_dict[\"role\"] == role\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def messages_between_time(\n",
       "        self, start_time, end_time, role: Union[str, None] = None\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Get all messages between two specific times in the memory thread with a specific role.\"\"\"\n",
       "        messages = []\n",
       "        for idx, message_dict in enumerate(self.memory_thread):\n",
       "            if (\n",
       "                self.time_stamps[idx] > start_time\n",
       "                and self.time_stamps[idx] < end_time\n",
       "                and (role is None or message_dict[\"role\"] == role)\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "        return messages if len(messages) > 0 else None\n",
       "\n",
       "    def token_bound_history(\n",
       "        self, max_tokens: int, max_history=None, role: Union[str, None] = None\n",
       "    ):\n",
       "        messages = []\n",
       "        indices = []\n",
       "        tokens = 0\n",
       "        if max_history is None:\n",
       "            max_history = len(self.memory_thread)\n",
       "\n",
       "        for idx, message_dict in enumerate(reversed(self.memory_thread)):\n",
       "            if (\n",
       "                tokens + self.message_tokens[idx] < max_tokens\n",
       "                and (role is None or message_dict[\"role\"] == role)\n",
       "                and idx < max_history\n",
       "            ):\n",
       "                messages.append(message_dict)\n",
       "                indices.append(len(self.memory_thread) - 1 - idx)\n",
       "                tokens += self.message_tokens[idx]\n",
       "            else:\n",
       "                break\n",
       "        return messages, indices if len(messages) > 0 else (None, None)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class FifoThread(BaseThread):\n",
       "    \"\"\"FIFO Memory BaseThread, the oldest messages are removed first when reaching the max_memory limit, the memory is defined in terms of tokens,\n",
       "    outs are passe to the longterm_memory, lucid_memory is a redundant memory that stores all the messages\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self, name=\"fifo_memory\", max_memory=None, longterm_thread=None, redundant=True\n",
       "    ):\n",
       "\n",
       "        BaseThread.__init__(self, name=name, max_memory=None)\n",
       "        if redundant is True:\n",
       "            self.redundant_thread = BaseThread(name=\"lucid_memory\", max_memory=None)\n",
       "        else:\n",
       "            self.redundant_thread = None\n",
       "        if longterm_thread is None:\n",
       "            self.longterm_thread = BaseThread(name=\"longterm_memory\", max_memory=None)\n",
       "        else:\n",
       "            self.longterm_thread = longterm_thread\n",
       "        # create an alias for the memory_thread to make the code more readable\n",
       "        self.fifo_thread = self.memory_thread\n",
       "        self.max_memory = max_memory\n",
       "\n",
       "    def to_longterm(self, idx: int):\n",
       "        \"\"\"move the message at the index idx to the longterm_memory\"\"\"\n",
       "        # move the message at the index idx to the longterm_memory\n",
       "        display(\n",
       "            Markdown(\n",
       "                \"The memory BaseThread is full, the message with index {} was moved to the longterm memory\".format(\n",
       "                    idx\n",
       "                )\n",
       "            )\n",
       "        )\n",
       "        message = copy.deepcopy(self.memory_thread[idx])\n",
       "        # print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\n",
       "        self.longterm_thread.add_message(message)\n",
       "        self.remove_message(idx=idx)\n",
       "\n",
       "    def add_message(self, message_dict: dict):\n",
       "        \"\"\"add a message to the memory_thread, if the memory_thread is full remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages until enough space is available\"\"\"\n",
       "        # message_dict = {\"role\": role, \"content\": content}\n",
       "        # chek that the message_dict is a dictionary or a list of dictionaries\n",
       "        message_dict = check_dict(message_dict)\n",
       "        if self.redundant_thread is not None:\n",
       "            self.redundant_thread.add_message(message_dict)\n",
       "        message_tokens = self.get_message_tokens(message_dict)\n",
       "\n",
       "        if self.total_tokens + message_tokens > self.max_memory:\n",
       "            while self.total_tokens + message_tokens > self.max_memory:\n",
       "                if len(self.memory_thread) > 0:\n",
       "                    self.to_longterm(idx=0)\n",
       "            super().add_message(message_dict)\n",
       "\n",
       "        else:\n",
       "            # add the message_dict to the memory_thread\n",
       "            # update the total number of tokens\n",
       "            super().add_message(message_dict)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class VectorThread(BaseThread, MemoryIndex):\n",
       "    \"\"\"vector BaseThread, creates a faiss index with the messages and allows to search for similar messages, memory BaseThread can return messages in either similarity or chronological order\n",
       "    add a parameter to choose the order of the messages\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, name=\"vector_memory\", max_context=2048, use_mark=False):\n",
       "        BaseThread.__init__(self, name=name, max_memory=None)\n",
       "        MemoryIndex.__init__(self, index=None, name=name)\n",
       "        self.max_context = max_context\n",
       "        self.use_mark = use_mark\n",
       "        self.local_index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
       "\n",
       "    def index_message(self, message: str, verbose: bool = False):\n",
       "        \"\"\"index a message in the faiss index, the message is embedded and added to the index\n",
       "        self.values and self.embeddings and self.index are updated\n",
       "        \"\"\"\n",
       "\n",
       "        self.add_to_index(value=message, verbose=verbose)\n",
       "\n",
       "    def add_message(self, message_dict: dict, verbose: bool = False):\n",
       "        \"\"\"add a message to the memory thread, the message is embedded and added to the index\n",
       "        self.values and self.embeddings and self.index are updated. If use_mark is False only the content of the messages is embedded\n",
       "        \"\"\"\n",
       "        # print(\"checking the dict\")\n",
       "        message_dict = check_dict(message_dict)\n",
       "        # print(\"trying to add the message\")\n",
       "        BaseThread.add_message(self, message_dict)\n",
       "        # print(message_dict)\n",
       "        message = message_dict[\"content\"]\n",
       "        self.index_message(message, verbose=verbose)\n",
       "        return True\n",
       "\n",
       "    def token_bound_query(self, query, k: int = 10, max_tokens: int = 4000):\n",
       "        \"\"\"returns the k most similar messages to the query, sorted in similarity order\"\"\"\n",
       "        if self.use_mark:\n",
       "            query = mark_question(query)\n",
       "        return MemoryIndex.token_bound_query(self, query, k, max_tokens)\n",
       "\n",
       "    def sorted_query(\n",
       "        self,\n",
       "        query,\n",
       "        k: int = 10,\n",
       "        max_tokens: int = 4000,\n",
       "        reverse: bool = False,\n",
       "        return_from_thread=True,\n",
       "    ):\n",
       "        \"\"\"returns the k most similar messages to the query, sorted in chronological order with the most recent message first\n",
       "        if return_from_thread is True the messages are returned from the memory thread, otherwise they are returned from the index\n",
       "        if reverse is True the messages are returned in reverse chronological order, with the oldest message first\n",
       "        \"\"\"\n",
       "        unsorted_messages, unsorted_scores, unsorted_indices = self.token_bound_query(\n",
       "            query, k, max_tokens=max_tokens\n",
       "        )\n",
       "        # sort the messages\n",
       "\n",
       "        sorted_messages = [\n",
       "            unsorted_messages[i]\n",
       "            for i in sorted(\n",
       "                range(len(unsorted_messages)), key=lambda k: unsorted_indices[k]\n",
       "            )\n",
       "        ]\n",
       "        sorted_scores = [\n",
       "            unsorted_scores[i]\n",
       "            for i in sorted(\n",
       "                range(len(unsorted_scores)), key=lambda k: unsorted_indices[k]\n",
       "            )\n",
       "        ]\n",
       "        sorted_indices = [\n",
       "            unsorted_indices[i]\n",
       "            for i in sorted(\n",
       "                range(len(unsorted_indices)), key=lambda k: unsorted_indices[k]\n",
       "            )\n",
       "        ]\n",
       "        if reverse:\n",
       "            sorted_messages.reverse()\n",
       "            sorted_scores.reverse()\n",
       "            sorted_indices.reverse()\n",
       "        if return_from_thread:\n",
       "            sorted_messages = [self.memory_thread[i] for i in sorted_indices]\n",
       "        return sorted_messages, sorted_scores, sorted_indices\n",
       "\n",
       "    def weighted_query(\n",
       "        self,\n",
       "        query,\n",
       "        k: int = 10,\n",
       "        max_tokens: int = 4000,\n",
       "        decay_factor: float = 0.1,\n",
       "        temporal_weight: float = 0.5,\n",
       "        order_by: str = \"chronological\",\n",
       "        reverse: bool = False,\n",
       "    ) -> list:\n",
       "        \"\"\"Returns the k most similar messages to the query, sorted in either similarity or chronological order. The results are weighted by a combination of similarity scores and temporal weights.\n",
       "        The temporal weights are computed using an exponential decay function with the decay factor as the decay rate. The temporal weight of the most recent message is 1 and the temporal weight of the oldest message is 0.\n",
       "        The temporal weight of a message is multiplied by the temporal_weight parameter to control the relative importance of the temporal weights. The default value of 0.5 means that the temporal weights are equally important as the similarity scores.\n",
       "        The order_by parameter controls the order of the results. If it is set to 'similarity', the results are sorted in similarity order. If it is set to 'chronological', the results are sorted in chronological order with the most recent message first.\n",
       "        If reverse is True, the results are sorted in reverse chronological order with the oldest message first.\n",
       "        \"\"\"\n",
       "        # Validate order_by parameter\n",
       "        if order_by not in (\"similarity\", \"chronological\"):\n",
       "            raise ValueError(\n",
       "                \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
       "            )\n",
       "\n",
       "        # Get similarity-based results\n",
       "        sim_messages, sim_scores, sim_indices = self.sorted_query(\n",
       "            query, k, max_tokens=max_tokens\n",
       "        )\n",
       "\n",
       "        # Get token-bound history\n",
       "        hist_messages, hist_indices = self.token_bound_history(max_tokens=max_tokens)\n",
       "\n",
       "        # Combine messages and indices\n",
       "        combined_messages = sim_messages + hist_messages\n",
       "        combined_indices = sim_indices + hist_indices\n",
       "\n",
       "        # Create the local_index and populate it\n",
       "        self.local_index = MemoryIndex(name=\"local_index\")\n",
       "        for message in combined_messages:\n",
       "            self.local_index.add_to_index(value=message, verbose=False)\n",
       "\n",
       "        # Perform a new query on the combined index\n",
       "        (\n",
       "            new_query_results,\n",
       "            new_query_scores,\n",
       "            new_query_indices,\n",
       "        ) = self.local_index.token_bound_query(\n",
       "            query, k=len(combined_messages), max_tokens=max_tokens\n",
       "        )\n",
       "\n",
       "        # Compute temporal weights\n",
       "        temporal_weights = [\n",
       "            np.exp(-decay_factor * i) for i in range(len(combined_messages))\n",
       "        ]\n",
       "        temporal_weights = [\n",
       "            w / sum(temporal_weights) for w in temporal_weights\n",
       "        ]  # Normalize the temporal weights\n",
       "\n",
       "        # Combine similarity scores and temporal weights\n",
       "        weighted_scores = []\n",
       "        for i in range(len(new_query_scores)):\n",
       "            sim_score = new_query_scores[i]\n",
       "            temp_weight = temporal_weights[combined_indices.index(new_query_indices[i])]\n",
       "            weighted_score = (\n",
       "                1 - temporal_weight\n",
       "            ) * sim_score + temporal_weight * temp_weight\n",
       "            weighted_scores.append(weighted_score)\n",
       "\n",
       "        # Sort the results based on the order_by parameter\n",
       "        if order_by == \"similarity\":\n",
       "            sorting_key = lambda k: weighted_scores[k]\n",
       "        elif order_by == \"chronological\":  # order_by == 'chronological'\n",
       "            sorting_key = lambda k: new_query_indices[k]\n",
       "        else:\n",
       "            raise ValueError(\n",
       "                \"Invalid value for order_by parameter. It should be either 'similarity' or 'chronological'.\"\n",
       "            )\n",
       "\n",
       "        sorted_indices = [\n",
       "            new_query_indices[i]\n",
       "            for i in sorted(\n",
       "                range(len(new_query_indices)), key=sorting_key, reverse=not reverse\n",
       "            )\n",
       "        ]\n",
       "        sorted_results = [\n",
       "            new_query_results[i]\n",
       "            for i in sorted(\n",
       "                range(len(new_query_results)), key=sorting_key, reverse=not reverse\n",
       "            )\n",
       "        ]\n",
       "        sorted_scores = [\n",
       "            weighted_scores[i]\n",
       "            for i in sorted(\n",
       "                range(len(weighted_scores)), key=sorting_key, reverse=not reverse\n",
       "            )\n",
       "        ]\n",
       "\n",
       "        # Return only the top k results without exceeding max_tokens\n",
       "        final_results, final_scores, final_indices = [], [], []\n",
       "        current_tokens = 0\n",
       "        for i in range(min(k, len(sorted_results))):\n",
       "            message_tokens = self.get_message_tokens(sorted_results[i])\n",
       "            if current_tokens + message_tokens <= max_tokens:\n",
       "                final_results.append(sorted_results[i])\n",
       "                final_scores.append(sorted_scores[i])\n",
       "                final_indices.append(sorted_indices[i])\n",
       "                current_tokens += message_tokens\n",
       "            else:\n",
       "                break\n",
       "\n",
       "        return final_results, final_scores, final_indices\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class OpenAiEmbedder:\n",
       "    def get_embedding_size(self):\n",
       "        return ADA_EMBEDDING_SIZE\n",
       "\n",
       "    def embed(self, data, embed_mark=False, verbose=False):\n",
       "\n",
       "        if embed_mark is False and type(data) is dict and \"content\" in data:\n",
       "            if verbose is True:\n",
       "                print(\"Embedding without mark\", data[\"content\"])\n",
       "            out = openai.Embedding.create(\n",
       "                input=data[\"content\"], engine=\"text-embedding-ada-002\"\n",
       "            )\n",
       "        else:\n",
       "            if len(str(data)) > MAX_CONTEXT_LENGTH:\n",
       "                data = str(data)[:MAX_CONTEXT_LENGTH]\n",
       "            if verbose is True:\n",
       "                print(\"Embedding without preprocessing the input\", data)\n",
       "            out = openai.Embedding.create(\n",
       "                input=str(data), engine=\"text-embedding-ada-002\"\n",
       "            )\n",
       "        return out.data[0].embedding\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class CohereEmbedder:\n",
       "    def get_embedding_size(self):\n",
       "        return COHERE_EMBEDDING_SIZE\n",
       "\n",
       "    def embed(self, data, embed_mark=False, verbose=False):\n",
       "        try:\n",
       "            if embed_mark is False and type(data) is dict and \"content\" in data:\n",
       "                if verbose is True:\n",
       "                    print(\"Embedding without mark\", data[\"content\"])\n",
       "                out = co.embed(input=data[\"content\"]).embeddings\n",
       "            else:\n",
       "                if verbose is True:\n",
       "                    print(\"Embedding without preprocessing the input\", data)\n",
       "                out = co.embed(input=str(data)).embeddings\n",
       "\n",
       "        except:\n",
       "            raise ValueError(\"The data  is not valid\", data)\n",
       "        return out\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class SBERTEmbedder:\n",
       "    def get_embedding_size(self):\n",
       "        return 356\n",
       "\n",
       "    def embed(\n",
       "        data,\n",
       "        key=\"content\",\n",
       "        model_name=\"all-MiniLM-L6-v2\",\n",
       "        cores=1,\n",
       "        gpu=False,\n",
       "        batch_size=128,\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n",
       "        \"\"\"\n",
       "        print(\"Embedding data\")\n",
       "        model = SentenceTransformer(model_name)\n",
       "        print(\"Model loaded\")\n",
       "\n",
       "        sentences = data[key].tolist()\n",
       "        unique_sentences = data[key].unique()\n",
       "        print(\"Unique sentences\", len(unique_sentences))\n",
       "\n",
       "        if cores == 1:\n",
       "            embeddings = model.encode(\n",
       "                unique_sentences, show_progress_bar=True, batch_size=batch_size\n",
       "            )\n",
       "        else:\n",
       "            devices = [\"cpu\"] * cores\n",
       "            if gpu:\n",
       "                devices = None  # use all CUDA devices\n",
       "\n",
       "            # Start the multi-process pool on multiple devices\n",
       "            print(\"Multi-process pool starting\")\n",
       "            pool = model.start_multi_process_pool(devices)\n",
       "            print(\"Multi-process pool started\")\n",
       "\n",
       "            chunk_size = math.ceil(len(unique_sentences) / cores)\n",
       "\n",
       "            # Compute the embeddings using the multi-process pool\n",
       "            embeddings = model.encode_multi_process(\n",
       "                unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size\n",
       "            )\n",
       "            model.stop_multi_process_pool(pool)\n",
       "\n",
       "        print(\"Embeddings computed\")\n",
       "\n",
       "        mapping = {\n",
       "            sentence: embedding\n",
       "            for sentence, embedding in zip(unique_sentences, embeddings)\n",
       "        }\n",
       "        embeddings = np.array([mapping[sentence] for sentence in sentences])\n",
       "\n",
       "        return embeddings\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class ArxivVanityParser:\n",
       "    def __init__(self):\n",
       "        self.base_url = \"https://www.arxiv-vanity.com/\"\n",
       "\n",
       "    def _get_vanity_url(self, arxiv_id):\n",
       "        return urljoin(self.base_url, \"papers/\" + arxiv_id)\n",
       "\n",
       "    def _fetch_html(self, url):\n",
       "        response = requests.get(url)\n",
       "        if response.status_code == 200:\n",
       "            return response.text\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "    def _extract_main_content(self, html):\n",
       "        soup = BeautifulSoup(html, \"html.parser\")\n",
       "        paragraphs = soup.find_all(\"div\", {\"class\": \"ltx_para\"})\n",
       "        content = {idx: p.get_text() for idx, p in enumerate(paragraphs)}\n",
       "        return content\n",
       "\n",
       "    def parse_paper(self, arxiv_id):\n",
       "        vanity_url = self._get_vanity_url(arxiv_id)\n",
       "        html = self._fetch_html(vanity_url)\n",
       "        if html is not None:\n",
       "            return self._extract_main_content(html)\n",
       "        else:\n",
       "            return None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class ArxivAPI:\n",
       "    def __init__(self):\n",
       "        self.base_url = \"http://export.arxiv.org/api/query?\"\n",
       "        self.pdf_download_url = \"https://arxiv.org/pdf/\"\n",
       "\n",
       "    def search(self, query, max_results=10):\n",
       "        url = f\"{self.base_url}search_query={query}&max_results={max_results}\"\n",
       "        response = requests.get(url)\n",
       "        if response.status_code == 200:\n",
       "            return response.text\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "    def download_pdf(self, paper_key, save_directory=\"./\"):\n",
       "        pdf_url = f\"{self.pdf_download_url}{paper_key}.pdf\"\n",
       "        response = requests.get(pdf_url)\n",
       "        if response.status_code == 200:\n",
       "            with open(os.path.join(save_directory, f\"{paper_key}.pdf\"), \"wb\") as f:\n",
       "                f.write(response.content)\n",
       "            print(f\"PDF for {paper_key} downloaded successfully.\")\n",
       "        else:\n",
       "            print(f\"Error downloading PDF for {paper_key}.\")\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class ArxivParser:\n",
       "    def __init__(self):\n",
       "        self.api = ArxivAPI()\n",
       "        self.vanity_parser = ArxivVanityParser()\n",
       "\n",
       "    def _parse_arxiv_id(self, url):\n",
       "        return url.split(\"/\")[-1]\n",
       "\n",
       "    def parse_papers(self, query, max_results=10):\n",
       "        search_results = self.api.search(query, max_results)\n",
       "        if search_results is not None:\n",
       "            soup = BeautifulSoup(search_results, \"html.parser\")\n",
       "            entries = soup.find_all(\"entry\")\n",
       "            paper_list = []\n",
       "            for entry in entries:\n",
       "                paper_dict = {}\n",
       "                arxiv_id = self._parse_arxiv_id(entry.id.string)\n",
       "                paper_dict[\"arxiv_id\"] = arxiv_id\n",
       "                paper_dict[\"title\"] = entry.title.string\n",
       "                paper_dict[\"summary\"] = entry.summary.string\n",
       "                paper_dict[\"content\"] = self.vanity_parser.parse_paper(str(arxiv_id))\n",
       "                if paper_dict[\"content\"] == None:\n",
       "                    continue\n",
       "                paper_list.append(paper_dict)\n",
       "            return paper_list\n",
       "        else:\n",
       "            return None\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class GithubProcessor(OsProcessor):\n",
       "    def __init__(\n",
       "        self,\n",
       "        base_directory: str,\n",
       "        username=None,\n",
       "        repo_name=None,\n",
       "        code_parsers=None,\n",
       "        minify_code: bool = False,\n",
       "        remove_docstrings: bool = False,\n",
       "    ):\n",
       "        self.username = username\n",
       "        self.repo_name = repo_name\n",
       "        self.base_directory = base_directory\n",
       "        self.github = Github()\n",
       "        self.repo = self.github.get_repo(f\"{username}/{repo_name}\")\n",
       "        repo_path = self.clone_repo(self.repo.clone_url)\n",
       "\n",
       "        OsProcessor.__init__(self, repo_path)\n",
       "        self.code_parsers = code_parsers or [\n",
       "            PythonParser(\n",
       "                repo_path, minify_code=minify_code, remove_docstrings=remove_docstrings\n",
       "            )\n",
       "        ]\n",
       "\n",
       "    def get_public_repos(self):\n",
       "        \"\"\"Returns a list of all public repos for the user.\"\"\"\n",
       "        user = self.github.get_user(self.username)\n",
       "        return user.get_repos()\n",
       "\n",
       "    def clone_repo(self, repo_url: str):\n",
       "        \"\"\"Clones the repo at the specified url and returns the path to the repo.\"\"\"\n",
       "        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
       "        target_directory = os.path.join(self.base_directory, repo_name)\n",
       "\n",
       "        if os.path.exists(target_directory):\n",
       "            shutil.rmtree(target_directory)\n",
       "\n",
       "        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
       "\n",
       "        return target_directory\n",
       "\n",
       "    def process_repo(self, repo_path=None):\n",
       "        \"\"\"Processes the repo at the specified path.\n",
       "        If no path is specified, the repo at self.directory_path is processed.\n",
       "        Returns the list of parsed functions and classes.\"\"\"\n",
       "        if repo_path is None:\n",
       "            repo_path = self.directory_path\n",
       "\n",
       "        for code_parser in self.code_parsers:\n",
       "            code_parser.directory_path = repo_path\n",
       "            code_parser.process_directory(repo_path)\n",
       "\n",
       "    def process_repos(self):\n",
       "        \"\"\"Processes all public repos for the user.\"\"\"\n",
       "        for repo in self.get_public_repos():\n",
       "            if not repo.private:\n",
       "                print(f\"Processing repo: {repo.name}\")\n",
       "                repo_path = self.clone_repo(repo.clone_url)\n",
       "                self.process_repo(repo_path)\n",
       "                shutil.rmtree(repo_path)\n",
       "\n",
       "    def get_repo(self, repo_name):\n",
       "        \"\"\"Returns the repo with the specified name.\"\"\"\n",
       "        user = self.github.get_user(self.username)\n",
       "        return user.get_repo(repo_name)\n",
       "\n",
       "    def process_single_repo(self):\n",
       "\n",
       "        repo = self.get_repo(self.repo_name)\n",
       "        print(f\"Processing repo: {self.repo_name}\")\n",
       "        repo_path = self.clone_repo(repo.clone_url)\n",
       "        self.process_repo(repo_path)\n",
       "        shutil.rmtree(repo_path)\n",
       "\n",
       "    def get_issues(self, state=\"open\"):\n",
       "        \"\"\"\n",
       "        Returns a list of all issues in the repo with the specified state.\n",
       "        \"\"\"\n",
       "        issues = []\n",
       "        for issue in self.repo.get_issues(state=state):\n",
       "            issues.append(issue)\n",
       "        return issues\n",
       "\n",
       "    def parse_issues(self, state=\"open\"):\n",
       "        \"\"\"\n",
       "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
       "        Each dict contains the issue number, title, body, and labels.\n",
       "        \"\"\"\n",
       "        parsed_issues = []\n",
       "        issues = self.get_issues(state=state)\n",
       "        for issue in issues:\n",
       "            parsed_issue = {\n",
       "                \"number\": issue.number,\n",
       "                \"title\": issue.title,\n",
       "                \"body\": issue.body,\n",
       "                \"labels\": [label.name for label in issue.labels],\n",
       "            }\n",
       "            parsed_issues.append(parsed_issue)\n",
       "        return parsed_issues\n",
       "\n",
       "    def get_commits(self):\n",
       "        \"\"\"\n",
       "        Returns a list of all commits in the main branch of the repository.\n",
       "        \"\"\"\n",
       "        commits = []\n",
       "        branch = self.repo.get_branch(\"main\")\n",
       "        for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
       "            commits.append(commit)\n",
       "        return commits\n",
       "\n",
       "    def parse_commits(self):\n",
       "        \"\"\"\n",
       "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
       "        Each dict contains the commit sha, commit message, and author information.\n",
       "        \"\"\"\n",
       "        parsed_commits = []\n",
       "        commits = self.get_commits()\n",
       "        for commit in commits:\n",
       "            parsed_commit = {\n",
       "                \"sha\": commit.sha,\n",
       "                \"message\": commit.commit.message,\n",
       "                \"author\": {\n",
       "                    \"name\": commit.commit.author.name,\n",
       "                    \"email\": commit.commit.author.email,\n",
       "                    \"date\": commit.commit.author.date,\n",
       "                },\n",
       "            }\n",
       "            parsed_commits.append(parsed_commit)\n",
       "        return parsed_commits\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class OsProcessor:\n",
       "    def __init__(self, directory_path: str):\n",
       "        self.directory_path = directory_path\n",
       "\n",
       "    def get_all_files(self, directory_path: Optional[str] = None) -> List[str]:\n",
       "        \"\"\"Returns a list of all files in a directory\"\"\"\n",
       "        if directory_path is None:\n",
       "            directory_path = self.directory_path\n",
       "\n",
       "        all_files = []\n",
       "        for root, _, files in os.walk(directory_path):\n",
       "            for file in files:\n",
       "                all_files.append(os.path.join(root, file))\n",
       "\n",
       "        return all_files\n",
       "\n",
       "    def get_files_with_extension(\n",
       "        self, extension: str, directory_path: Optional[str] = None\n",
       "    ) -> List[str]:\n",
       "        \"\"\"Returns a list of all files in a directory with a given extension\"\"\"\n",
       "        if directory_path is None:\n",
       "            directory_path = self.directory_path\n",
       "\n",
       "        all_files = self.get_all_files(directory_path)\n",
       "        files_with_extension = [file for file in all_files if file.endswith(extension)]\n",
       "\n",
       "        return files_with_extension\n",
       "\n",
       "    def get_file_extension(self, file_path: str) -> str:\n",
       "        \"\"\"Returns the extension of a file\"\"\"\n",
       "        return Path(file_path).suffix\n",
       "\n",
       "    def get_subdirectories(self, directory_path: Optional[str] = None) -> List[str]:\n",
       "        \"\"\"Returns a list of all subdirectories in a directory\"\"\"\n",
       "        if directory_path is None:\n",
       "            directory_path = self.directory_path\n",
       "\n",
       "        subdirectories = [\n",
       "            os.path.join(directory_path, d)\n",
       "            for d in os.listdir(directory_path)\n",
       "            if os.path.isdir(os.path.join(directory_path, d))\n",
       "        ]\n",
       "\n",
       "        return subdirectories\n",
       "\n",
       "    def create_directory(self, directory_path: str) -> None:\n",
       "        \"\"\"Creates a directory if it does not exist\"\"\"\n",
       "        if not os.path.exists(directory_path):\n",
       "            os.makedirs(directory_path)\n",
       "\n",
       "    def delete_directory(self, directory_path: str) -> None:\n",
       "        \"\"\"Deletes a directory if it exists\"\"\"\n",
       "        if os.path.exists(directory_path):\n",
       "            shutil.rmtree(directory_path)\n",
       "\n",
       "    def copy_file(self, source_path: str, destination_path: str) -> None:\n",
       "        \"\"\"Copies a file from one location to another\"\"\"\n",
       "        shutil.copy2(source_path, destination_path)\n",
       "\n",
       "    def move_file(self, source_path: str, destination_path: str) -> None:\n",
       "        \"\"\"Moves a file from one location to another\"\"\"\n",
       "        shutil.move(source_path, destination_path)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class PubmedAPI:\n",
       "    def __init__(self):\n",
       "        self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
       "\n",
       "    def search(self, query, max_results=10):\n",
       "        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
       "        record = Entrez.read(handle)\n",
       "        handle.close()\n",
       "        return record[\"IdList\"]\n",
       "\n",
       "    def fetch_abstract(self, pubmed_id):\n",
       "        handle = Entrez.efetch(\n",
       "            db=\"pubmed\", id=pubmed_id, retmode=\"text\", rettype=\"abstract\"\n",
       "        )\n",
       "        abstract = handle.read()\n",
       "        handle.close()\n",
       "        return abstract\n",
       "\n",
       "    def fetch_pmc_full_text(self, pubmed_id):\n",
       "        # Get the PMC ID for the PubMed ID\n",
       "        handle = Entrez.elink(dbfrom=\"pubmed\", id=pubmed_id, cmd=\"prlinks\")\n",
       "        record = Entrez.read(handle)\n",
       "        handle.close()\n",
       "        pmc_id = None\n",
       "        for link in record[0][\"LinkSetDb\"]:\n",
       "            if link[\"DbTo\"] == \"pmc\":\n",
       "                pmc_id = link[\"Link\"][0][\"Id\"]\n",
       "                break\n",
       "\n",
       "        if not pmc_id:\n",
       "            return None\n",
       "\n",
       "        # Fetch the PMC article XML\n",
       "        handle = Entrez.efetch(db=\"pmc\", id=pmc_id, retmode=\"xml\")\n",
       "        xml_content = handle.read()\n",
       "        handle.close()\n",
       "\n",
       "        # Parse the XML and extract the full text\n",
       "        soup = BeautifulSoup(xml_content, \"xml\")\n",
       "        full_text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
       "\n",
       "        return full_text\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class PubmedParser:\n",
       "    def __init__(self):\n",
       "        self.api = PubmedAPI()\n",
       "\n",
       "    def parse_papers(self, query, max_results=10):\n",
       "        pubmed_ids = self.api.search(query, max_results)\n",
       "        paper_list = []\n",
       "        for pubmed_id in pubmed_ids:\n",
       "            paper_dict = {}\n",
       "            paper_dict[\"pubmed_id\"] = pubmed_id\n",
       "            paper_dict[\"abstract\"] = self.api.fetch_abstract(pubmed_id)\n",
       "            paper_dict[\"content\"] = self.api.fetch_pmc_full_text(pubmed_id)\n",
       "            paper_list.append(paper_dict)\n",
       "        return paper_list\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class IssueParser:\n",
       "    def __init__(self, repo_name):\n",
       "        self.g = Github()\n",
       "        self.repo = self.g.get_repo(repo_name)\n",
       "\n",
       "    def get_issues(self, state=\"open\"):\n",
       "        \"\"\"\n",
       "        Returns a list of all issues in the repo with the specified state.\n",
       "        \"\"\"\n",
       "        issues = []\n",
       "        for issue in self.repo.get_issues(state=state):\n",
       "            issues.append(issue)\n",
       "        return issues\n",
       "\n",
       "    def parse_issues(self, state=\"open\"):\n",
       "        \"\"\"\n",
       "        Parses all issues in the repo with the specified state and returns a list of dicts.\n",
       "        Each dict contains the issue number, title, body, and labels.\n",
       "        \"\"\"\n",
       "        parsed_issues = []\n",
       "        issues = self.get_issues(state=state)\n",
       "        for issue in issues:\n",
       "            parsed_issue = {\n",
       "                \"number\": issue.number,\n",
       "                \"title\": issue.title,\n",
       "                \"body\": issue.body,\n",
       "                \"labels\": [label.name for label in issue.labels],\n",
       "            }\n",
       "            parsed_issues.append(parsed_issue)\n",
       "        return parsed_issues\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class CommitParser:\n",
       "    def __init__(self, repo_name):\n",
       "        self.g = Github()\n",
       "        self.repo = self.g.get_repo(repo_name)\n",
       "\n",
       "    def get_commits(self):\n",
       "        \"\"\"\n",
       "        Returns a list of all commits in the main branch of the repository.\n",
       "        \"\"\"\n",
       "        commits = []\n",
       "        branch = self.repo.get_branch(\"main\")\n",
       "        for commit in self.repo.get_commits(sha=branch.commit.sha):\n",
       "            commits.append(commit)\n",
       "        return commits\n",
       "\n",
       "    def parse_commits(self):\n",
       "        \"\"\"\n",
       "        Parses all commits in the main branch of the repository and returns a list of dicts.\n",
       "        Each dict contains the commit sha, commit message, and author information.\n",
       "        \"\"\"\n",
       "        parsed_commits = []\n",
       "        commits = self.get_commits()\n",
       "        for commit in commits:\n",
       "            parsed_commit = {\n",
       "                \"sha\": commit.sha,\n",
       "                \"message\": commit.commit.message,\n",
       "                \"author\": {\n",
       "                    \"name\": commit.commit.author.name,\n",
       "                    \"email\": commit.commit.author.email,\n",
       "                    \"date\": commit.commit.author.date,\n",
       "                },\n",
       "            }\n",
       "            parsed_commits.append(parsed_commit)\n",
       "        return parsed_commits\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class DirectoryProcessor:\n",
       "    def __init__(self, directory_path: str, visitor=FunctionAndClassVisitor()):\n",
       "        self.directory_path = directory_path\n",
       "        self.visitor = visitor\n",
       "\n",
       "    def _process_file(self, file_path: str):\n",
       "        with open(file_path, \"r\") as file:\n",
       "            source_code = file.read()\n",
       "\n",
       "        try:\n",
       "            tree = cst.parse_module(source_code)\n",
       "        except cst.ParserSyntaxError:\n",
       "            print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
       "            return\n",
       "\n",
       "        tree.visit(self.visitor)\n",
       "\n",
       "    def process_file(self, file_path: str):\n",
       "        # Run flake8 on the file\n",
       "        result = subprocess.run(\n",
       "            [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
       "        )\n",
       "\n",
       "        if result.returncode != 0:\n",
       "            print(f\"Skipping file with syntax error: {file_path}\")\n",
       "            print(result.stderr.decode(\"utf-8\"))\n",
       "            return\n",
       "\n",
       "        with open(file_path, \"r\") as f:\n",
       "            source_code = f.read()\n",
       "\n",
       "        try:\n",
       "            tree = cst.parse_module(source_code)\n",
       "            tree.visit(self.visitor)\n",
       "        except cst.ParserSyntaxError as e:\n",
       "            print(f\"Syntax error: {e}\")\n",
       "            print(f\"Skipping file with syntax error: {file_path}\")\n",
       "\n",
       "    def process_directory(self) -> List[str]:\n",
       "        function_source_codes = []\n",
       "        class_source_codes = []\n",
       "\n",
       "        for root, _, files in os.walk(self.directory_path):\n",
       "            for file in files:\n",
       "                if file.endswith(\".py\"):\n",
       "                    file_path = os.path.join(root, file)\n",
       "                    self._process_file(file_path)\n",
       "\n",
       "        function_source_codes = self.visitor.function_source_codes\n",
       "        function_nodes = self.visitor.function_nodes\n",
       "        class_source_codes = self.visitor.class_source_codes\n",
       "        class_nodes = self.visitor.class_nodes\n",
       "\n",
       "        return function_source_codes, class_source_codes, function_nodes, class_nodes\n",
       "\n",
       "    def clone_repo(self, repo_url):\n",
       "        repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
       "        target_directory = os.path.join(self.directory_path, repo_name)\n",
       "\n",
       "        if os.path.exists(target_directory):\n",
       "            shutil.rmtree(target_directory)\n",
       "\n",
       "        subprocess.run([\"git\", \"clone\", repo_url, target_directory])\n",
       "\n",
       "        return target_directory\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class GitHubUserProcessor:\n",
       "    def __init__(\n",
       "        self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
       "    ):\n",
       "        self.username = username\n",
       "        self.repo_name = repo_name\n",
       "        self.github = Github()\n",
       "        self.directory_processor = None\n",
       "        self.function_source_codes = []\n",
       "        self.class_source_codes = []\n",
       "        self.visitor = visitor\n",
       "\n",
       "    def get_public_repos(self):\n",
       "        user = self.github.get_user(self.username)\n",
       "        return user.get_repos()\n",
       "\n",
       "    def process_repos(self, base_directory):\n",
       "        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
       "        for repo in self.get_public_repos():\n",
       "            if not repo.private:\n",
       "                print(f\"Processing repo: {repo.name}\")\n",
       "                repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
       "                (\n",
       "                    function_source_codes,\n",
       "                    class_source_codes,\n",
       "                ) = self.directory_processor.process_directory()\n",
       "                self.function_source_codes.extend(function_source_codes)\n",
       "                self.class_source_codes.extend(class_source_codes)\n",
       "                shutil.rmtree(repo_path)\n",
       "\n",
       "        return self.directory_processor\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class GitHubRepoProcessor:\n",
       "    def __init__(\n",
       "        self, username=None, repo_name=None, visitor=FunctionAndClassVisitor()\n",
       "    ):\n",
       "        self.username = username\n",
       "        self.repo_name = repo_name\n",
       "        self.github = Github()\n",
       "        self.directory_processor = None\n",
       "        self.function_source_codes = []\n",
       "        self.function_nodes = []\n",
       "        self.class_source_codes = []\n",
       "        self.class_nodes = []\n",
       "        self.visitor = visitor\n",
       "\n",
       "    def get_repo(self, repo_name):\n",
       "        user = self.github.get_user(self.username)\n",
       "        return user.get_repo(repo_name)\n",
       "\n",
       "    def process_repo(self, base_directory):\n",
       "        self.directory_processor = DirectoryProcessor(base_directory, self.visitor)\n",
       "        repo = self.get_repo(self.repo_name)\n",
       "        print(f\"Processing repo: {self.repo_name}\")\n",
       "        repo_path = self.directory_processor.clone_repo(repo.clone_url)\n",
       "        (\n",
       "            function_source_codes,\n",
       "            class_source_codes,\n",
       "            function_nodes,\n",
       "            class_nodes,\n",
       "        ) = self.directory_processor.process_directory()\n",
       "        self.function_source_codes.extend(function_source_codes)\n",
       "        self.function_nodes.extend(function_nodes)\n",
       "        self.class_source_codes.extend(class_source_codes)\n",
       "        self.class_nodes.extend(class_nodes)\n",
       "        shutil.rmtree(repo_path)\n",
       "        return self.directory_processor\n",
       "\n",
       "    def get_values(self):\n",
       "        # concatenate the function and class source codes\n",
       "        self.function_source_codes.extend(self.class_source_codes)\n",
       "        self.function_nodes.extend(self.class_nodes)\n",
       "        return self.function_source_codes, self.function_nodes\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class PythonMinifier:\n",
       "    def __init__(self, code: str = None):\n",
       "\n",
       "        self.code = code\n",
       "        self.output_code = None\n",
       "\n",
       "    def minify(self):\n",
       "        if self.code:\n",
       "            self.output_code = self.minify_code(self.code)\n",
       "\n",
       "    def get_minified_code(self):\n",
       "        if not self.output_code:\n",
       "            self.minify()\n",
       "        return self.output_code\n",
       "\n",
       "    @staticmethod\n",
       "    def minify_code(code: str) -> str:\n",
       "        return minify(code)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class PythonDocstringExtractor:\n",
       "    @staticmethod\n",
       "    def extract_docstring(function_def: cst.FunctionDef) -> str:\n",
       "        docstring = None\n",
       "\n",
       "        for stmt in function_def.body.body:\n",
       "            if isinstance(stmt, cst.SimpleStatementLine):\n",
       "                for expr in stmt.body:\n",
       "                    if isinstance(expr, cst.Expr) and isinstance(\n",
       "                        expr.value, cst.SimpleString\n",
       "                    ):\n",
       "                        docstring = expr.value.value.strip('\"').strip(\"'\")\n",
       "                        break\n",
       "            if docstring is not None:\n",
       "                break\n",
       "\n",
       "        if docstring is not None:\n",
       "            return docstring.strip()\n",
       "        else:\n",
       "            function_name = function_def.name.value\n",
       "            return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class FunctionAndClassVisitor(cst.CSTVisitor):\n",
       "    def __init__(self):\n",
       "        self.function_source_codes = []\n",
       "        self.function_nodes = []\n",
       "        self.class_source_codes = []\n",
       "        self.class_nodes = []\n",
       "\n",
       "    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "        \"\"\"This method is called for every FunctionDef node in the tree.\n",
       "        and it does the following:\n",
       "        1. Gets the source code for the node\n",
       "        2. Adds the node to the list of function nodes\n",
       "        3. Adds the source code to the list of function source codes\n",
       "        \"\"\"\n",
       "        function_source_code = cst.Module([]).code_for_node(node)\n",
       "        self.function_nodes.append(node)\n",
       "        self.function_source_codes.append(function_source_code)\n",
       "\n",
       "    def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
       "        \"\"\"This method is called for every ClassDef node in the tree.\n",
       "        and it does the following:\n",
       "        1. Gets the source code for the node\n",
       "        2. Adds the node to the list of class nodes\n",
       "        3. Adds the source code to the list of class source codes\n",
       "        \"\"\"\n",
       "        class_source_code = cst.Module([]).code_for_node(node)\n",
       "        self.class_nodes.append(node)\n",
       "        self.class_source_codes.append(class_source_code)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class PythonParser(OsProcessor):\n",
       "    def __init__(\n",
       "        self,\n",
       "        directory_path: str,\n",
       "        visitor: Optional[FunctionAndClassVisitor] = None,\n",
       "        minify_code: bool = False,\n",
       "        remove_docstrings: bool = False,\n",
       "    ):\n",
       "        super().__init__(directory_path)\n",
       "        self.visitor = visitor if visitor else FunctionAndClassVisitor()\n",
       "        self.minify_code = minify_code\n",
       "        self.remove_docstrings = remove_docstrings\n",
       "\n",
       "    def remove_docstring(self, tree: cst.Module) -> str:\n",
       "        \"\"\"Removes docstrings from the given code and returns the code without docstrings.\"\"\"\n",
       "\n",
       "        # Remove docstrings using a transformer\n",
       "        class DocstringRemover(cst.CSTTransformer):\n",
       "            def leave_FunctionDef(\n",
       "                self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
       "            ) -> cst.FunctionDef:\n",
       "                docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
       "                if docstring.startswith(\"No docstring\"):\n",
       "                    return updated_node\n",
       "\n",
       "                return updated_node.with_changes(\n",
       "                    body=updated_node.body.with_changes(\n",
       "                        body=[\n",
       "                            stmt\n",
       "                            for stmt in updated_node.body.body\n",
       "                            if not (\n",
       "                                isinstance(stmt, cst.SimpleStatementLine)\n",
       "                                and any(\n",
       "                                    isinstance(expr, cst.Expr)\n",
       "                                    and isinstance(expr.value, cst.SimpleString)\n",
       "                                    for expr in stmt.body\n",
       "                                )\n",
       "                            )\n",
       "                        ]\n",
       "                    )\n",
       "                )\n",
       "\n",
       "        tree = tree.visit(DocstringRemover())\n",
       "        return tree.code\n",
       "\n",
       "    def _process_file(self, file_path: str):\n",
       "        \"\"\"This method is called for every file in the directory.\n",
       "        It does the following:\n",
       "        1. Reads the file\n",
       "        2. Parses the file\n",
       "        3. Visits the file with the visitor\n",
       "        \"\"\"\n",
       "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
       "            source_code = file.read()\n",
       "\n",
       "        try:\n",
       "            tree = cst.parse_module(source_code)\n",
       "        except cst.ParserSyntaxError:\n",
       "            print(f\"Skipping file {file_path}: Failed to parse syntax\")\n",
       "            return\n",
       "\n",
       "        tree.visit(self.visitor)\n",
       "\n",
       "        # Remove docstrings if specified\n",
       "        if self.remove_docstrings:\n",
       "            source_code = self.remove_docstring(source_code, tree)\n",
       "\n",
       "        # Minify the code if specified\n",
       "        if self.minify_code:\n",
       "            minifier = PythonMinifier(source_code)\n",
       "            source_code = minifier.get_minified_code()\n",
       "\n",
       "        # Add the processed code to the corresponding list in the visitor\n",
       "        self.visitor.function_source_codes.append(source_code)\n",
       "\n",
       "    def process_file(self, file_path: str):\n",
       "        \"\"\"This method is called for every file in the directory.\n",
       "        It does the following:\n",
       "        1. Runs flake8 on the file\n",
       "        if flake8 returns a non-zero exit code, it means the file has a syntax error\n",
       "        2. Reads the file\n",
       "        3. Parses the file\n",
       "        4. Visits the file with the visitor\n",
       "\n",
       "        \"\"\"\n",
       "        result = subprocess.run(\n",
       "            [\"flake8\", \"--select=E999\", file_path], capture_output=True\n",
       "        )\n",
       "\n",
       "        if result.returncode != 0:\n",
       "            print(f\"Skipping file with syntax error: {file_path}\")\n",
       "            print(result.stderr.decode(\"utf-8\"))\n",
       "            return\n",
       "\n",
       "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
       "            source_code = f.read()\n",
       "\n",
       "        try:\n",
       "            tree = cst.parse_module(source_code)\n",
       "            tree.visit(self.visitor)\n",
       "        except cst.ParserSyntaxError as e:\n",
       "            print(f\"Syntax error: {e}\")\n",
       "            print(f\"Skipping file with syntax error: {file_path}\")\n",
       "\n",
       "    def process_directory(\n",
       "        self,\n",
       "    ) -> Tuple[List[str], List[str], List[cst.FunctionDef], List[cst.ClassDef]]:\n",
       "        \"\"\"This method is called for every directory.\n",
       "        It does the following:\n",
       "        1. Gets all the python files in the directory\n",
       "        2. Processes each file\n",
       "        3. Returns the list of function source codes, class source codes, function nodes, and class nodes\n",
       "        \"\"\"\n",
       "        function_source_codes = []\n",
       "        class_source_codes = []\n",
       "\n",
       "        python_files = self.get_files_with_extension(\".py\")\n",
       "\n",
       "        for file_path in python_files:\n",
       "            self._process_file(file_path)\n",
       "\n",
       "        function_source_codes = self.visitor.function_source_codes\n",
       "        function_nodes = self.visitor.function_nodes\n",
       "        class_source_codes = self.visitor.class_source_codes\n",
       "        class_nodes = self.visitor.class_nodes\n",
       "\n",
       "        return function_source_codes, class_source_codes, function_nodes, class_nodes\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "# Remove docstrings using a transformer\n",
       "class DocstringRemover(cst.CSTTransformer):\n",
       "    def leave_FunctionDef(\n",
       "        self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
       "    ) -> cst.FunctionDef:\n",
       "        docstring = PythonDocstringExtractor.extract_docstring(original_node)\n",
       "        if docstring.startswith(\"No docstring\"):\n",
       "            return updated_node\n",
       "\n",
       "        return updated_node.with_changes(\n",
       "            body=updated_node.body.with_changes(\n",
       "                body=[\n",
       "                    stmt\n",
       "                    for stmt in updated_node.body.body\n",
       "                    if not (\n",
       "                        isinstance(stmt, cst.SimpleStatementLine)\n",
       "                        and any(\n",
       "                            isinstance(expr, cst.Expr)\n",
       "                            and isinstance(expr.value, cst.SimpleString)\n",
       "                            for expr in stmt.body\n",
       "                        )\n",
       "                    )\n",
       "                ]\n",
       "            )\n",
       "        )\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "# A custom visitor to find function calls and their arguments\n",
       "class FunctionCallFinder(cst.CSTVisitor):\n",
       "    METADATA_DEPENDENCIES = (PositionProvider,)\n",
       "\n",
       "    def visit_Call(self, node: cst.Call) -> None:\n",
       "        function_name = None\n",
       "        if isinstance(node.func, cst.Name):\n",
       "            function_name = node.func.value\n",
       "\n",
       "        if function_name:\n",
       "            pos = self.get_metadata(PositionProvider, node).start\n",
       "            print(\n",
       "                f\"Function '{function_name}' called at line {pos.line}, column {pos.column} with arguments:\"\n",
       "            )\n",
       "\n",
       "            for arg in node.args:\n",
       "                arg_start_pos = self.get_metadata(PositionProvider, arg).start\n",
       "                arg_value = arg.value\n",
       "                if isinstance(arg_value, cst.SimpleString):\n",
       "                    arg_value = arg_value.evaluated_value\n",
       "                print(\n",
       "                    f\"- Argument at line {arg_start_pos.line}, column {arg_start_pos.column}: {arg_value}\"\n",
       "                )\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class MultiplicationCounterVisitor(cst.CSTVisitor):\n",
       "    def __init__(self):\n",
       "        self.count = 0\n",
       "        self.functions_with_operation_dict = {}\n",
       "\n",
       "    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "        self.current_function = node\n",
       "        self.functions_with_operation_dict[node.name] = []\n",
       "\n",
       "    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "        self.current_function = None\n",
       "\n",
       "    def visit_BinaryOperation(self, node: cst.BinaryOperation) -> None:\n",
       "        if isinstance(node.operator, cst.Multiply) or isinstance(\n",
       "            node.operator, cst.BitAnd\n",
       "        ):\n",
       "            self.count += 1\n",
       "            if self.current_function:\n",
       "                self.functions_with_operation_dict[self.current_function.name].append(\n",
       "                    cst.Module([]).code_for_node(node)\n",
       "                )\n",
       "\n",
       "    def visit_Call(self, node: cst.Call) -> None:\n",
       "        if m.matches(node, m.Call(func=m.Attribute(attr=m.Name(\"dot\")))) or m.matches(\n",
       "            node, m.Call(func=m.Name(\"dot\"), args=[m.Arg(), m.Arg()])\n",
       "        ):\n",
       "            self.count += 1\n",
       "            if self.current_function:\n",
       "                self.functions_with_operation_dict[self.current_function.name].append(\n",
       "                    cst.Module([]).code_for_node(node)\n",
       "                )\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class FunctionAndClassVisitor(cst.CSTVisitor):\n",
       "    def __init__(self):\n",
       "        self.function_source_codes = []\n",
       "        self.function_nodes = []\n",
       "        self.class_source_codes = []\n",
       "        self.class_nodes = []\n",
       "\n",
       "    def visit_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "        function_source_code = cst.Module([]).code_for_node(node)\n",
       "        # add in place summary and code mod\n",
       "        self.function_nodes.append(node)\n",
       "        self.function_source_codes.append(function_source_code)\n",
       "\n",
       "    def visit_ClassDef(self, node: cst.ClassDef) -> None:\n",
       "        class_source_code = cst.Module([]).code_for_node(node)\n",
       "        # add in place summary and code mod\n",
       "        self.class_nodes.append(node)\n",
       "        self.class_source_codes.append(class_source_code)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class TypingCollector(cst.CSTVisitor):\n",
       "    def __init__(self):\n",
       "        # stack for storing the canonical name of the current function\n",
       "        self.stack: List[Tuple[str, ...]] = []\n",
       "        # store the annotations\n",
       "        self.annotations: Dict[\n",
       "            Tuple[str, ...],  # key: tuple of canonical class/function name\n",
       "            Tuple[cst.Parameters, Optional[cst.Annotation]],  # value: (params, returns)\n",
       "        ] = {}\n",
       "\n",
       "    def visit_ClassDef(self, node: cst.ClassDef) -> Optional[bool]:\n",
       "        self.stack.append(node.name.value)\n",
       "\n",
       "    def leave_ClassDef(self, node: cst.ClassDef) -> None:\n",
       "        self.stack.pop()\n",
       "\n",
       "    def visit_FunctionDef(self, node: cst.FunctionDef) -> Optional[bool]:\n",
       "        self.stack.append(node.name.value)\n",
       "        self.annotations[tuple(self.stack)] = (node.params, node.returns)\n",
       "        return False  # pyi files don't support inner functions, return False to stop the traversal.\n",
       "\n",
       "    def leave_FunctionDef(self, node: cst.FunctionDef) -> None:\n",
       "        self.stack.pop()\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class BaseTask:\n",
       "    def __init__(self, index: MemoryIndex, path: List[List[int]], max_workers: int = 1):\n",
       "        \"\"\"\n",
       "        Initialize a BaseTask instance.\n",
       "\n",
       "        :param index: List of strings representing the queries.\n",
       "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
       "        :param max_workers: Maximum number of worker threads (default is 4).\n",
       "        \"\"\"\n",
       "        self.index = index\n",
       "        self.path = path\n",
       "        self.results = []\n",
       "        self.max_workers = max_workers\n",
       "        self.parallel = True if max_workers > 1 else False\n",
       "\n",
       "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
       "        \"\"\"\n",
       "        to be implemented by subclasses:\n",
       "\n",
       "        :param sub_path: List of indices representing the sub-task's sequence.\n",
       "        :return: List of strings representing the responses for each query in the sub-task.\n",
       "        \"\"\"\n",
       "\n",
       "        sub_results = []\n",
       "        for i in sub_path:\n",
       "            response = self.index[i]\n",
       "            sub_results.append(response)\n",
       "        return sub_results\n",
       "\n",
       "    def execute_task(self) -> None:\n",
       "        \"\"\"\n",
       "        Execute the task by concurrently processing sub-tasks using worker threads.\n",
       "        \"\"\"\n",
       "        \n",
       "        for sub_path in self.path:\n",
       "            self.results.append(self._execute_sub_task(sub_path))\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class LLMReader(BaseTask):\n",
       "    def __init__(\n",
       "        self,\n",
       "        index: MemoryIndex,\n",
       "        path: List[List[int]],\n",
       "        chatbot: Chat,\n",
       "        read_func = None,\n",
       "        max_workers: int = 1,\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Initialize a LLMReadTask instance.\n",
       "\n",
       "        :param index: List of strings representing the queries.\n",
       "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
       "        :param chatbot: Chatbot instance used for executing queries.\n",
       "        :param max_workers: Maximum number of worker threads (default is 4).\n",
       "        \"\"\"\n",
       "        BaseTask.__init__(self, index, path, max_workers)\n",
       "        self.chatbot = chatbot\n",
       "        self.read_func = read_func if read_func else self.llm_response\n",
       "\n",
       "    def llm_response(chatbot: Chat, message: str, string_out=False):\n",
       "        if string_out:\n",
       "            return chatbot.reply(message)\n",
       "        return chatbot.query(message)\n",
       "\n",
       "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
       "        \"\"\"\n",
       "        Execute a sub-task using a separate copy of the chatbot instance. each sub-stasks uses a\n",
       "        a clean memory instance.\n",
       "\n",
       "        :param sub_path: List of indices representing the sub-task's sequence.\n",
       "        :return: List of strings representing the responses for each query in the sub-task.\n",
       "        \"\"\"\n",
       "        if self.parallel:\n",
       "            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
       "            chatbot_instance = copy.deepcopy(self.chatbot)\n",
       "        else:\n",
       "            chatbot_instance = self.chatbot\n",
       "        if isinstance(self.chatbot, BaseThread):\n",
       "            chatbot_instance.reset_memory()\n",
       "\n",
       "        sub_results = []\n",
       "        for i in sub_path:\n",
       "            response = self.read_func(chatbot_instance, self.index.values[i])\n",
       "            sub_results.append(response)\n",
       "        return sub_results\n",
       "\n",
       "    def read(self):\n",
       "        self.execute_task()\n",
       "        return self.results\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class LLMWriter(BaseTask):\n",
       "    def __init__(\n",
       "        self,\n",
       "        index: MemoryIndex,\n",
       "        path: List[List[int]],\n",
       "        chatbot: Chat,\n",
       "        write_func = None,\n",
       "        context= None,\n",
       "        task_name=\"summary\",\n",
       "        max_workers: int = 1,\n",
       "    ):\n",
       "        \"\"\"\n",
       "        Initialize a LLMWriteTask instance.\n",
       "\n",
       "        :param index: List of strings representing the queries.\n",
       "        :param path: List of lists, each sub-list defines a sequence over which the task is executed.\n",
       "        :param chatbot: Chatbot instance used for executing queries.\n",
       "        :param max_workers: Maximum number of worker threads (default is 4).\n",
       "        \"\"\"\n",
       "        BaseTask.__init__(self, index, path, max_workers)\n",
       "        self.chatbot = chatbot\n",
       "        self.write_func = write_func if write_func else self.llm_response\n",
       "        self.new_index_name = self.index.name + f\"_{task_name}\"\n",
       "        self.context = context\n",
       "\n",
       "    @staticmethod\n",
       "    def llm_response(chatbot: Chat, message: str, context = None, id = None):\n",
       "        return chatbot.reply(message)\n",
       "\n",
       "    def _execute_sub_task(self, sub_path: List[int]) -> List[str]:\n",
       "        \"\"\"\n",
       "        Execute a sub-task using a separate copy of the chatbot instance.\n",
       "\n",
       "        :param sub_path: List of indices representing the sub-task's sequence.\n",
       "        :return: List of strings representing the responses for each query in the sub-task.\n",
       "        \"\"\"\n",
       "        if self.parallel:\n",
       "            # copy the chatbot instance and resets the memory before making the queries in case of multi-threading\n",
       "            chatbot_instance = copy.deepcopy(self.chatbot)\n",
       "        else:\n",
       "            chatbot_instance = self.chatbot\n",
       "        if isinstance(self.chatbot, BaseThread):\n",
       "            chatbot_instance.reset_memory()\n",
       "\n",
       "        sub_results = {}\n",
       "        for i in sub_path:\n",
       "            current_val = self.index.values[i]\n",
       "            response = self.write_func(chatbot_instance, current_val, self.context, id = i)\n",
       "            sub_results[i] = response\n",
       "        return sub_results\n",
       "\n",
       "    def write(self):\n",
       "        self.execute_task()\n",
       "        content_to_write = []\n",
       "        for sub_result in self.results:\n",
       "            for index_id, response in sub_result.items():\n",
       "                content_to_write.append((index_id, response))\n",
       "        # sort the content to write by index_id\n",
       "        content_to_write.sort(key=lambda x: x[0])\n",
       "        self.new_index = MemoryIndex(name=self.new_index_name)\n",
       "        self.new_index.init_index(values=[x[1] for x in content_to_write])\n",
       "        self.new_index.save()\n",
       "        return self.new_index\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class CodeGenerator:\n",
       "    def __init__(self):\n",
       "        self.git_memory = None\n",
       "        self.commit_index = None\n",
       "        self.contextual_memory = None\n",
       "\n",
       "    def generate_meta_code(self, user_input):\n",
       "        # Transform user input into meta-code representation\n",
       "        pass\n",
       "\n",
       "    def load_git_memory(self, git_memory):\n",
       "        # Retrieve GitMemory context\n",
       "        pass\n",
       "\n",
       "    def load_commit_context(self, commit_index):\n",
       "        # Retrieve commit context\n",
       "        pass\n",
       "\n",
       "    def load_contextual_resources(self, contextual_memory):\n",
       "        # Load contextual resources required for code generation\n",
       "        pass\n",
       "\n",
       "    def get_current_draft(self, commit_context):\n",
       "        # Get the current draft code from commit context\n",
       "        pass\n",
       "\n",
       "    def generate_modifications(self, draft_code, meta_code, context_resources):\n",
       "        # Generate modifications to the draft code based on meta-code and context resources\n",
       "        pass\n",
       "\n",
       "    def extract_source_code(self, modifications, context_resources):\n",
       "        # Extract source code from modifications and context resources\n",
       "        pass\n",
       "\n",
       "    def apply_modifications(self, draft_code, modifications):\n",
       "        # Apply modifications to the draft code\n",
       "        pass\n",
       "\n",
       "    def validate_draft(self, updated_draft, source_code):\n",
       "        # Validate the updated draft code and check for consistency\n",
       "        pass\n",
       "\n",
       "    def compare_draft_with_objective(self, updated_draft, meta_code):\n",
       "        # Compare the updated draft code with user's goal based on meta-code\n",
       "        pass\n",
       "\n",
       "    def store_draft_to_commit_index(self, commit_index, updated_draft):\n",
       "        # Store the updated draft code in the commit index\n",
       "        pass\n",
       "\n",
       "    def rollback(self, commit_index):\n",
       "        # Roll back the draft code to the previous state in the commit index\n",
       "        pass\n",
       "\n",
       "    def codeGenerationIteration(\n",
       "        self, userInput, gitMemory, commitIndex, contextualMemory\n",
       "    ):\n",
       "        metaCode = self.generateMetaCode(userInput)\n",
       "\n",
       "        # Concurrent processing - Step 1\n",
       "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
       "            gitMemoryContext = executor.submit(self.loadGitMemory, gitMemory)\n",
       "            commitContext = executor.submit(self.loadCommitContext, commitIndex)\n",
       "            contextResources = executor.submit(\n",
       "                self.loadContextualResources, contextualMemory\n",
       "            )\n",
       "\n",
       "            gitMemoryContext = gitMemoryContext.result()\n",
       "            commitContext = commitContext.result()\n",
       "            contextResources = contextResources.result()\n",
       "\n",
       "        draftCode = self.getCurrentDraft(commitContext)\n",
       "\n",
       "        # Concurrent processing - Step 2\n",
       "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
       "            modifications = executor.submit(\n",
       "                self.generateModifications, draftCode, metaCode, contextResources\n",
       "            )\n",
       "            sourceCode = executor.submit(\n",
       "                self.extractSourceCode, modifications, contextResources\n",
       "            )\n",
       "\n",
       "            modifications = modifications.result()\n",
       "            sourceCode = sourceCode.result()\n",
       "\n",
       "        updatedDraft = self.applyModifications(draftCode, modifications)\n",
       "\n",
       "        # Concurrent processing - Step 3\n",
       "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
       "            validationResult = executor.submit(\n",
       "                self.validateDraft, updatedDraft, sourceCode\n",
       "            )\n",
       "            comparisonResult = executor.submit(\n",
       "                self.compareDraftWithObjective, updatedDraft, metaCode\n",
       "            )\n",
       "\n",
       "            validationResult = validationResult.result()\n",
       "            comparisonResult = comparisonResult.result()\n",
       "\n",
       "        if validationResult and comparisonResult:\n",
       "            self.storeDraftToCommitIndex(commitIndex, updatedDraft)\n",
       "        else:\n",
       "            updatedDraft = self.rollback(commitIndex)\n",
       "\n",
       "        return updatedDraft\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class PythonMinifier:\n",
       "    def __init__(self, code: str = None):\n",
       "\n",
       "        self.code = code\n",
       "        self.output_code = None\n",
       "\n",
       "    def minify(self):\n",
       "        if self.code:\n",
       "            self.output_code = self.minify_code(self.code)\n",
       "\n",
       "    def get_minified_code(self):\n",
       "        if not self.output_code:\n",
       "            self.minify()\n",
       "        return self.output_code\n",
       "\n",
       "    @staticmethod\n",
       "    def minify_code(code: str) -> str:\n",
       "        return minify(code)\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class PythonDocstringExtractor:\n",
       "    @staticmethod\n",
       "    def extract_docstring(function_def: cst.FunctionDef) -> str:\n",
       "        docstring = None\n",
       "\n",
       "        for stmt in function_def.body.body:\n",
       "            if isinstance(stmt, cst.SimpleStatementLine):\n",
       "                for expr in stmt.body:\n",
       "                    if isinstance(expr, cst.Expr) and isinstance(\n",
       "                        expr.value, cst.SimpleString\n",
       "                    ):\n",
       "                        docstring = expr.value.value.strip('\"').strip(\"'\")\n",
       "                        break\n",
       "            if docstring is not None:\n",
       "                break\n",
       "\n",
       "        if docstring is not None:\n",
       "            return docstring.strip()\n",
       "        else:\n",
       "            function_name = function_def.name.value\n",
       "            return f\"No docstring provided for function '{function_name}'. Please add a docstring to describe this function.\"\n",
       " was already in the index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The value \n",
       "\n",
       "class GitMemory(MemoryIndex):\n",
       "    def __init__(self, username, repo_name):\n",
       "        super().__init__()\n",
       "        self.username = username\n",
       "        self.repo_name = repo_name\n",
       "        self.parser = GitHubRepoProcessor(username, repo_name)\n",
       "        self.minifier = PythonMinifier()\n",
       "        self.docstring_extractor = PythonDocstringExtractor()\n",
       "        self.directory_parser = None\n",
       "        self.min_code_index = None\n",
       "        self.doc_string_index = None\n",
       "        self.libcst_node_index = None\n",
       "\n",
       "    def create_code_index(self, base_directory):\n",
       "        self.directory_parser = self.parser.process_repo(base_directory)\n",
       "        code_values, code_nodes = self.parser.get_values()\n",
       "        self.code_index = self.init_index(values=code_values)\n",
       "        self.code_index.save()\n",
       "\n",
       "    def create_indexes(self, base_directory):\n",
       "        self.directory_parser = self.parser.process_repo(base_directory)\n",
       "        code_values, code_nodes = self.parser.get_values()\n",
       "        self.code_index = self.init_index(values=code_values)\n",
       "\n",
       "        min_code_values = []\n",
       "        doc_string_values = []\n",
       "        for code_value, code_node in zip(code_values, code_nodes):\n",
       "            minifier = PythonMinifier(code=code_value)\n",
       "            min_code = minifier.get_minified_code()\n",
       "            doc_string = self.docstring_extractor.extract_docstring(code_node)\n",
       "            min_code_values.append(min_code)\n",
       "            doc_string_values.append(doc_string)\n",
       "        self.doc_string_index = self.init_index(values=doc_string_values)\n",
       "        self.min_code_index = self.init_index(values=min_code_values)\n",
       " was embedded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "babyindex = PythonIndex(\n",
    "    babydragon_path, name=\"babyd_index\", minify_code=False, load=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = FifoChat(\n",
    "    index_dict={\"babyindex\": babyindex}, name=\"babyd_chatbot\", max_fifo_memory=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babyindex.faiss_query(\"PythonIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to call OpenAI API...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Question: \n",
       " Can you explain me memorythread?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " #### Anwser: \n",
       " Certainly! `memory_thread` appears to be a list object that is used to store memory in various classes. It is initialized and reset in different ways depending on the class in which it is used. For example, in the `FIFOMemoryThread` class, `memory_thread` is an instance attribute that is defined in the `__init__` method and reset with the `reset_memory` method. In the `MemoryIndex` class, `memory_thread` is inherited from a superclass and is accessed using the `__getitem__` method. Additionally, the length of `memory_thread` can be obtained using the `__len__` method defined in various classes that include it. Overall, `memory_thread` seems to be a fundamental aspect of memory storage within the classes utilizing it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Certainly! `memory_thread` appears to be a list object that is used to store memory in various classes. It is initialized and reset in different ways depending on the class in which it is used. For example, in the `FIFOMemoryThread` class, `memory_thread` is an instance attribute that is defined in the `__init__` method and reset with the `reset_memory` method. In the `MemoryIndex` class, `memory_thread` is inherited from a superclass and is accessed using the `__getitem__` method. Additionally, the length of `memory_thread` can be obtained using the `__len__` method defined in various classes that include it. Overall, `memory_thread` seems to be a fundamental aspect of memory storage within the classes utilizing it.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.reply(\"Can you explain me memorythread?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
